{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"On this page: Triply Documentation TriplyDB TriplyETL Triply Documentation \u00b6 What can we help you with? TriplyDB \u00b6 TriplyDB is a state-of-the-art linked database / triple store that is used by organizations of any size: from start-ups to orgs with 10K+ employees. Learn more about how to use TriplyDB TriplyETL \u00b6 Use TriplyETL to quickly connect your data sources to your linked database / triple store. TriplyETL can be extract, transform, enrich, validate, and load linked data. Learn more about how to use TriplyETL Didn't find what you were looking for? Contact us via our form or by e-mailing to info@triply.cc .","title":"Home"},{"location":"#triply-documentation","text":"What can we help you with?","title":"Triply Documentation"},{"location":"#triplydb","text":"TriplyDB is a state-of-the-art linked database / triple store that is used by organizations of any size: from start-ups to orgs with 10K+ employees. Learn more about how to use TriplyDB","title":"TriplyDB"},{"location":"#triplyetl","text":"Use TriplyETL to quickly connect your data sources to your linked database / triple store. TriplyETL can be extract, transform, enrich, validate, and load linked data. Learn more about how to use TriplyETL Didn't find what you were looking for? Contact us via our form or by e-mailing to info@triply.cc .","title":"TriplyETL"},{"location":"generics/Graphql/","text":"On this page: Graphql implementation Schema Object types Fields IDs Naming Renaming Queries Global Object identification Pagination Filtering Simple cases Language filtering Advanced filtering Graphql implementation \u00b6 Some TriplyDB instances expose a GraphQL endpoint. This endpoint uses information from user-provided SHACL shapes for the schema creation. The goal of this documentation is to inform users about Triply's implementation of the GraphQL endpoint. For more generic information about GraphQL, you can visit graphql.org or other resources. In order to understand this documentation, you have to be familiar with the SHACL language. Note: in order to avoid confusion we will use the noun object as a synonym for resource and triple object when referring to the third element of a triple. Schema \u00b6 Object types \u00b6 A basic element of the schema is object types, which represents the type of the resources that you can query. type Book { id:ID! title:[XsdString]! } This object type corresponds to the shape below: shp:Book a sh:NodeShape; sh:targetClass sdo:Book; sh:property [ sh:path dc:title; sh:datatype xsd:string ]. Fields \u00b6 Fields in object types, such as title , represent properties of nodes. By default, fields return arrays of values. The only exception is when the property has sh:maxCount: 1 , then the field returns a single value. Thus, for the shape: shp:Book a sh:NodeShape; sh:targetClass sdo:Book; sh:property [ sh:path dc:title; sh:maxCount 1; sh:datatype xsd:string ]. The object type will be: type Book { id:ID! title:XsdString } Additionally, following the best practices , fields can give null results, except for: IDs, which represents the IRI of the resource. Lists, but not their elements Properties that have sh:minCount 1 and sh:maxCount 1 Thus, for this shape: shp:Book a sh:NodeShape; sh:targetClass sdo:Book; sh:property [ sh:path dc:title; sh:maxCount 1; sh:minCount 1; sh:datatype xsd:string ]. The corresponding object type is: type Book { id:ID! title:XsdString! } If the property shape includes an sh:datatype , the field returns values of GraphQL scalar type (see example above). On the other hand, if the property shape has an sh:class pointing to a class that: - is the sh:targetClass of a node shape, the field returns values of the corresponding object type. - is not mentioned as a sh:targetClass in a node shape, then the type of the returned values is ExternalIri . Therefore, the shapes: shp:Book a sh:NodeShape; sh:targetClass sdo:Book; sh:property [ sh:path sdo:author; sh:class sdo:Person ]; [ sh:path sdo:audio; sh:class sdo:AudioObject ]. shp:Person a sh:NodeShape; sh:targetClass sdo:Person; sh:property [ sh:path sdo:name; sh:datatype xsd:string ]. correspond to the below graphql types: type Book { id:ID! author:[Person]! audio:[ExternalIri]! } type Person { id:ID! name:[XsdString]! } IDs \u00b6 The id field is of type ID, which represents the IRI of each resource. This ID is unique. For example: book:Odyssey a sdo:Book; dct:title \"Odyssey\". The id field of this resource would be https://example.org/book/Odyssey . You can read more information on the ID scalar in graphql.org . Also, the use of the id field is mentioned later in the section Object Global Identification . Naming \u00b6 In order to name the GraphQL types in correspondence to shapes, we follow the below conventions: - For object types, we use the sh:targetClass of the node shape. - For object type fields, we use the sh:path of the property shape. More specifically, the name comes from the part of the IRI after the last # or otherwise the last / , converted from kebab-case to camelCase. Notice that if the selected name is illegal or causes a name collision, we'll return an error informing the user about the problem and ignore this type or field. Renaming \u00b6 Shape designers are able use their custom names by using a special property: <https://triplydb.com/Triply/GraphQL/def/graphqlName> . More specifically, the designer has to add a triple with : - for object types, the class IRI - for fields, the IRI of the property shape as a subject, the above-mentioned predicate and a string literal with the custom name as triple object. If we wanted to rename using the first example of the section, we would do: shp:Book a sh:NodeShape; sh:targetClass sdo:Book; sh:property [ sh:path dc:title; triply:graphqlName \"name\"; # Rename the object type field sh:datatype xsd:string ] sdo:Book triply:graphqlName \"PieceOfArt\". # Rename the object type field. Then the corresponding object type would be: type PieceOfArt { id:ID! name:[XsdString]! } Queries \u00b6 The user can query for objects using their unique ID. Also, they can query for objects of a specific type along with fields, and get nested information. Last, the user can get information by filtering results. Let's see some important concepts. Global Object identification \u00b6 For reasons such as caching, the user should be able to query an object by their unique ID. This is possible using global object identification, using the node(id:ID) query. An example: { node(id: \"https://example.org/book/Odyssey\") { id } } For more information on global object identification, see graphql specification . Pagination \u00b6 A simple query would be: { BookConnection { edges { node { id title } } } } The results would include the IRIs of books together with their titles and would be paginated. In order to paginate through a large number of results, our GraphQL implementation supports cursor-based pagination using connections . For more information, please visit the Relay project's cursor-based connection pagination specification . Filtering \u00b6 When you query for objects, you might want to get back resources based on specific values in certain fields. You can do this by filtering. Simple cases \u00b6 For example, you can query for people with a specific id: { PersonConnection(filter: {id: \"https://example.org/person/Homer\"}) { edges { node { id name } } } } Another query would be to search for a person with a specific name: { PersonConnection(filter: {name: {eq: \"Homer\"}}) { edges { node { id name } } } } Notice that in the second example, there is a new field for filtering called eq . When we want to filter on a field with returns a scalar, meaning that its value is represented by a literal in linked data, we have to use comparison operators: eq , in for equality, and notEq and notIn for inequality. The operators in and notIn are refering to lists. On the other hand, when we are filtering based on IDs - or in linked data terms, based on the IRI - , as in the first example, we don't use comparison operators. The only idiomatic case is the literal with a language tag and rdf:langString as a datatype. This literal is represented as { value: \"example-string\", language: \"en\" } and the corresponding scalar is RdfsLangString . This means that in order to filter using a value of this scalar type, you have to execute the query below: { PersonConnection(filter: {name: {eq: {value: \"Odysseus\", language: \"en\"}}}) { edges { node { id name } } } } Language filtering \u00b6 Additionally, there is support for filtering results based on the language tag. An example is: Linked data: person:Odysseus a sdo:Person; sdo:name \"Odysseus\"@en, \"\u039f\u03b4\u03c5\u03c3\u03c3\u03b5\u03cd\u03c2\"@gr. shp:Person a sh:NodeShape; sh:targetClass sdo:Person; sh:property [ sh:path sdo:name; sh:datatype rdf:langString ]. GraphQL query: { PersonConnection { edges { node { id name(language:\"gr\") } } } } Results: { \"data\": { \"PersonConnection\": { \"edges\": [ { \"node\": { \"id\": \"https://example.org/person/Odysseus\", \"name\": [ { \"value\": \"\u039f\u03b4\u03c5\u03c3\u03c3\u03b5\u03cd\u03c2\", \"language\": \"gr\" } ] } } ] } } } Our implementation supports using the HTTP Accept-Language syntax, for filtering based on a language-tag. For example, GraphQL query: { PersonConnection { edges { node { id name(language:\"gr, en;q=.5\") } } } } Results: { \"data\": { \"PersonConnection\": { \"edges\": [ { \"node\": { \"id\": \"https://example.org/person/Odysseus\", \"name\": [ { \"value\": \"\u039f\u03b4\u03c5\u03c3\u03c3\u03b5\u03cd\u03c2\", \"language\": \"gr\" }, { \"value\": \"Odysseus\", \"language\": \"en\" }, ] } } ] } } } If the writer of the shapes includes the sh:uniqueLang constraint, then the result returned will be a single value, instead of an array. Thus, the example becomes: Linked data: person:Odysseus a sdo:Person; sdo:name \"Odysseus\"@en, \"\u039f\u03b4\u03c5\u03c3\u03c3\u03b5\u03cd\u03c2\"@gr. shp:Person a sh:NodeShape; sh:targetClass sdo:Person; sh:property [ sh:path sdo:name; sh:uniqueLang true; sh:datatype rdf:langString ]. GraphQL query: { PersonConnection { edges { node { id name(language:\"gr, en;q=.5\") } } } } Results: { \"data\": { \"PersonConnection\": { \"edges\": [ { \"node\": { \"id\": \"https://example.org/person/Odysseus\", \"name\": { \"value\": \"\u039f\u03b4\u03c5\u03c3\u03c3\u03b5\u03cd\u03c2\", \"language\": \"gr\" } } } ] } } } Advanced filtering \u00b6 Furthermore, there is possibility for nested filtering: { BookConnection( filter: {author: {name: {eq: \"Homer\"}}} ) { edges { node { id } } } } and for combination of filters: { BookConnection( filter: {author: {name: {eq: \"Homer\"}}, name: {eq: \"Odyssey\"}} ) { edges { node { id } } } } Note: The combination of filters is executed in an 'and' logic.","title":"GraphQL implementation information"},{"location":"generics/Graphql/#graphql-implementation","text":"Some TriplyDB instances expose a GraphQL endpoint. This endpoint uses information from user-provided SHACL shapes for the schema creation. The goal of this documentation is to inform users about Triply's implementation of the GraphQL endpoint. For more generic information about GraphQL, you can visit graphql.org or other resources. In order to understand this documentation, you have to be familiar with the SHACL language. Note: in order to avoid confusion we will use the noun object as a synonym for resource and triple object when referring to the third element of a triple.","title":"Graphql implementation"},{"location":"generics/Graphql/#schema","text":"","title":"Schema"},{"location":"generics/Graphql/#object-types","text":"A basic element of the schema is object types, which represents the type of the resources that you can query. type Book { id:ID! title:[XsdString]! } This object type corresponds to the shape below: shp:Book a sh:NodeShape; sh:targetClass sdo:Book; sh:property [ sh:path dc:title; sh:datatype xsd:string ].","title":"Object types"},{"location":"generics/Graphql/#fields","text":"Fields in object types, such as title , represent properties of nodes. By default, fields return arrays of values. The only exception is when the property has sh:maxCount: 1 , then the field returns a single value. Thus, for the shape: shp:Book a sh:NodeShape; sh:targetClass sdo:Book; sh:property [ sh:path dc:title; sh:maxCount 1; sh:datatype xsd:string ]. The object type will be: type Book { id:ID! title:XsdString } Additionally, following the best practices , fields can give null results, except for: IDs, which represents the IRI of the resource. Lists, but not their elements Properties that have sh:minCount 1 and sh:maxCount 1 Thus, for this shape: shp:Book a sh:NodeShape; sh:targetClass sdo:Book; sh:property [ sh:path dc:title; sh:maxCount 1; sh:minCount 1; sh:datatype xsd:string ]. The corresponding object type is: type Book { id:ID! title:XsdString! } If the property shape includes an sh:datatype , the field returns values of GraphQL scalar type (see example above). On the other hand, if the property shape has an sh:class pointing to a class that: - is the sh:targetClass of a node shape, the field returns values of the corresponding object type. - is not mentioned as a sh:targetClass in a node shape, then the type of the returned values is ExternalIri . Therefore, the shapes: shp:Book a sh:NodeShape; sh:targetClass sdo:Book; sh:property [ sh:path sdo:author; sh:class sdo:Person ]; [ sh:path sdo:audio; sh:class sdo:AudioObject ]. shp:Person a sh:NodeShape; sh:targetClass sdo:Person; sh:property [ sh:path sdo:name; sh:datatype xsd:string ]. correspond to the below graphql types: type Book { id:ID! author:[Person]! audio:[ExternalIri]! } type Person { id:ID! name:[XsdString]! }","title":"Fields"},{"location":"generics/Graphql/#ids","text":"The id field is of type ID, which represents the IRI of each resource. This ID is unique. For example: book:Odyssey a sdo:Book; dct:title \"Odyssey\". The id field of this resource would be https://example.org/book/Odyssey . You can read more information on the ID scalar in graphql.org . Also, the use of the id field is mentioned later in the section Object Global Identification .","title":"IDs"},{"location":"generics/Graphql/#naming","text":"In order to name the GraphQL types in correspondence to shapes, we follow the below conventions: - For object types, we use the sh:targetClass of the node shape. - For object type fields, we use the sh:path of the property shape. More specifically, the name comes from the part of the IRI after the last # or otherwise the last / , converted from kebab-case to camelCase. Notice that if the selected name is illegal or causes a name collision, we'll return an error informing the user about the problem and ignore this type or field.","title":"Naming"},{"location":"generics/Graphql/#renaming","text":"Shape designers are able use their custom names by using a special property: <https://triplydb.com/Triply/GraphQL/def/graphqlName> . More specifically, the designer has to add a triple with : - for object types, the class IRI - for fields, the IRI of the property shape as a subject, the above-mentioned predicate and a string literal with the custom name as triple object. If we wanted to rename using the first example of the section, we would do: shp:Book a sh:NodeShape; sh:targetClass sdo:Book; sh:property [ sh:path dc:title; triply:graphqlName \"name\"; # Rename the object type field sh:datatype xsd:string ] sdo:Book triply:graphqlName \"PieceOfArt\". # Rename the object type field. Then the corresponding object type would be: type PieceOfArt { id:ID! name:[XsdString]! }","title":"Renaming"},{"location":"generics/Graphql/#queries","text":"The user can query for objects using their unique ID. Also, they can query for objects of a specific type along with fields, and get nested information. Last, the user can get information by filtering results. Let's see some important concepts.","title":"Queries"},{"location":"generics/Graphql/#global-object-identification","text":"For reasons such as caching, the user should be able to query an object by their unique ID. This is possible using global object identification, using the node(id:ID) query. An example: { node(id: \"https://example.org/book/Odyssey\") { id } } For more information on global object identification, see graphql specification .","title":"Global Object identification"},{"location":"generics/Graphql/#pagination","text":"A simple query would be: { BookConnection { edges { node { id title } } } } The results would include the IRIs of books together with their titles and would be paginated. In order to paginate through a large number of results, our GraphQL implementation supports cursor-based pagination using connections . For more information, please visit the Relay project's cursor-based connection pagination specification .","title":"Pagination"},{"location":"generics/Graphql/#filtering","text":"When you query for objects, you might want to get back resources based on specific values in certain fields. You can do this by filtering.","title":"Filtering"},{"location":"generics/Graphql/#simple-cases","text":"For example, you can query for people with a specific id: { PersonConnection(filter: {id: \"https://example.org/person/Homer\"}) { edges { node { id name } } } } Another query would be to search for a person with a specific name: { PersonConnection(filter: {name: {eq: \"Homer\"}}) { edges { node { id name } } } } Notice that in the second example, there is a new field for filtering called eq . When we want to filter on a field with returns a scalar, meaning that its value is represented by a literal in linked data, we have to use comparison operators: eq , in for equality, and notEq and notIn for inequality. The operators in and notIn are refering to lists. On the other hand, when we are filtering based on IDs - or in linked data terms, based on the IRI - , as in the first example, we don't use comparison operators. The only idiomatic case is the literal with a language tag and rdf:langString as a datatype. This literal is represented as { value: \"example-string\", language: \"en\" } and the corresponding scalar is RdfsLangString . This means that in order to filter using a value of this scalar type, you have to execute the query below: { PersonConnection(filter: {name: {eq: {value: \"Odysseus\", language: \"en\"}}}) { edges { node { id name } } } }","title":"Simple cases"},{"location":"generics/Graphql/#language-filtering","text":"Additionally, there is support for filtering results based on the language tag. An example is: Linked data: person:Odysseus a sdo:Person; sdo:name \"Odysseus\"@en, \"\u039f\u03b4\u03c5\u03c3\u03c3\u03b5\u03cd\u03c2\"@gr. shp:Person a sh:NodeShape; sh:targetClass sdo:Person; sh:property [ sh:path sdo:name; sh:datatype rdf:langString ]. GraphQL query: { PersonConnection { edges { node { id name(language:\"gr\") } } } } Results: { \"data\": { \"PersonConnection\": { \"edges\": [ { \"node\": { \"id\": \"https://example.org/person/Odysseus\", \"name\": [ { \"value\": \"\u039f\u03b4\u03c5\u03c3\u03c3\u03b5\u03cd\u03c2\", \"language\": \"gr\" } ] } } ] } } } Our implementation supports using the HTTP Accept-Language syntax, for filtering based on a language-tag. For example, GraphQL query: { PersonConnection { edges { node { id name(language:\"gr, en;q=.5\") } } } } Results: { \"data\": { \"PersonConnection\": { \"edges\": [ { \"node\": { \"id\": \"https://example.org/person/Odysseus\", \"name\": [ { \"value\": \"\u039f\u03b4\u03c5\u03c3\u03c3\u03b5\u03cd\u03c2\", \"language\": \"gr\" }, { \"value\": \"Odysseus\", \"language\": \"en\" }, ] } } ] } } } If the writer of the shapes includes the sh:uniqueLang constraint, then the result returned will be a single value, instead of an array. Thus, the example becomes: Linked data: person:Odysseus a sdo:Person; sdo:name \"Odysseus\"@en, \"\u039f\u03b4\u03c5\u03c3\u03c3\u03b5\u03cd\u03c2\"@gr. shp:Person a sh:NodeShape; sh:targetClass sdo:Person; sh:property [ sh:path sdo:name; sh:uniqueLang true; sh:datatype rdf:langString ]. GraphQL query: { PersonConnection { edges { node { id name(language:\"gr, en;q=.5\") } } } } Results: { \"data\": { \"PersonConnection\": { \"edges\": [ { \"node\": { \"id\": \"https://example.org/person/Odysseus\", \"name\": { \"value\": \"\u039f\u03b4\u03c5\u03c3\u03c3\u03b5\u03cd\u03c2\", \"language\": \"gr\" } } } ] } } }","title":"Language filtering"},{"location":"generics/Graphql/#advanced-filtering","text":"Furthermore, there is possibility for nested filtering: { BookConnection( filter: {author: {name: {eq: \"Homer\"}}} ) { edges { node { id } } } } and for combination of filters: { BookConnection( filter: {author: {name: {eq: \"Homer\"}}, name: {eq: \"Odyssey\"}} ) { edges { node { id } } } } Note: The combination of filters is executed in an 'and' logic.","title":"Advanced filtering"},{"location":"generics/JSON-LD-frames/","text":"On this page: JSON-LD Framing Why JSON-LD Framing? The SPARQL Query The Frame Using SPARQL to create a frame JSON-LD Framing \u00b6 Why JSON-LD Framing? \u00b6 SPARQL Construct and SPARQL Describe queries can return results in the JSON-LD format. Here is an example: [ { \"@id\": \"john\", \"livesIn\": { \"@id\": \"amsterdam\" } }, { \"@id\": \"jane\", \"livesIn\": { \"@id\": \"berlin\" } }, { \"@id\": \"tim\", \"livesIn\": { \"@id\": \"berlin\" } } ] JSON-LD is one of the serialization formats for RDF, and encodes a graph structure. For example, the JSON-LD snippet above encodes the following graph: graph TB Tim -- livesIn --> Berlin John -- livesIn --> Amsterdam Jane -- livesIn --> Berlin The triples in a graphs do not have any specific order. In our graph picture, the triple about Tim is mentioned first, but this is arbitrary. A graph is a set of triples, so there is no 'first' or 'last' triple. Similarly, there is no 'primary' or 'secondary' element in a graph structure either. In our graph picture, persons occur on the left hand-side and cities occur on the right hand-side. In fact, the same information can be expressed with the following graph: Most RESTful APIs return data with a specific, often tree-shaped structure. For example: { \"amsterdam\": { \"inhabitants\": [ \"john\" ] }, \"berlin\": { \"inhabitants\": [ \"jane\", \"tim\" ] } } JSON-LD Framing is a standard that is used to assign additional structure to JSON-LD. With JSON-LD Framing, we can configure the extra structure that is needed to create RESTful APIs over SPARQL queries. JSON-LD Framing are a deterministic translation from a graph, which has an unordered set of triples where no node is \"first\" or \"special\", into a tree, which has ordered branches and exactly one \"root\" node. In other words, JSON-LD framing allows one to force a specific tree layout to a JSON-LD document. This makes it possible to translate SPARQL queries to REST-APIs. The TriplyDB API for saved queries has been equipped with a JSON-LD profiler which can apply a JSON-LD profile to a JSON-LD result, transforming the plain JSON-LD to framed JSON. To do this you need two things. A SPARQL construct query and a JSON-LD frame. When you have both of these, you can retrieve plain JSON from a SPARQL query. The cURL command when both the SPARQL query and JSON-LD frame are available is: curl -X POST [SAVED-QUERY-URL] \\ -H 'Accept: application/ld+json;profile=http://www.w3.org/ns/json-ld#framed' \\ -H 'Authorization: Bearer [YOUR_TOKEN]' \\ -H 'Content-type: application/json' \\ -d '[YOUR_FRAME]' When sending a curl request, a few things are important. First, the request needs to be a POST request. Only a POST request can accept a frame as a body. The Accept header needs to be set to a specific value. The Accept header needs to have both the expected returned content-type and the JSON-LD profile, e.g. application/ld+json;profile=http://www.w3.org/ns/json-ld#framed . When querying an internal or private query you need to add an authorization token. Finally, it is important to set the Content-type . It refers to the content-type of the input body and needs to be application/json , as the frame is of type application/json . The SPARQL Query \u00b6 Let's start with the SPARQL query. A JSON-LD frame query needs a SPARQL Construct query to create an RDF graph that is self contained and populated with relevant vocabulary and data. The graph in JSON-LD is used as input for the RESTful API call. The SPARQL Construct query can be designed with API variables. Do note that API variables with OPTIONAL s can sometimes behave a bit different than regular API variables. This is due to how SPARQL interprets OPTIONAL s. If an API variable is used in an OPTIONAL , the query will return false positives, as the OPTIONAL does not filter out results matching the API-variable. Also note that the use of UNION s can have unexpected effects on the SPARQL query. A union could split up the result set of the SPARQL query. Meaning that the SPARQL engine first exhausts the top part of the UNION and then starts with the second part of the UNION . This means that the first part of the result set can be disconnected from the second part. If the limit is set too small the result set is separated in two different JSON-LD documents. This could result in missing data in the response. Finally, please note that it can happen that you set a pageSize of 10 but the response contains less than 10 results, while the next page is not empty. This is possible as the result set of the WHERE clause is limited with a limit and not the Construct clause. This means that two rows of the resulting WHERE clause are condensed into a single result in the Construct clause. Thus the response of the API can differ from the pageSize . The result is a set of triples according to the query. Saving the SPARQL query will resolve in a saved query. The saved query has an API URL that we can now use in our cURL command. The URL most of the time starts with api and ends with run . The saved query url of an example query is: https://api.triplydb.com/queries/JD/JSON-LD-frame/run You could use API variables with a ? e.g. ?[queryVariable]=[value] The Frame \u00b6 The SPARQL query is not enough to provide the RDF data in a JSON serialization format. It requires additional syntactic conformities that cannot be defined in a SPARQL query. Thus the SPARQL query that was created needs a frame to restructure JSON-LD objects into JSON. The JSON-LD 1.1 standard allows for restructuring JSON-LD objects with a frame to JSON. A JSON-LD frame consists out of 2 parts. The @context of the response, and the structure of the response. The complete specification on JSON-LD frames can be found online The @context is the translation of the linked data to the JSON naming. In the @context all the IRIs that occur in the JSON-LD response are documented, with key-value pairs, where the key corresponds to a name the IRI will take in the REST-API response and the value corresponds to the IRI in the JSON-LD response. Most of the time the key-value pairs are one-to-one relations, where one key is mapped to a single string. Sometimes the value is an object. The object contains at least the @id , which is the IRI in the JSON-LD response. The object can also contain other modifiers, that change the REST-API response. Examples are, @type to define the datatype of the object value, or @container to define the container where the value in the REST-API response is stored in. The context can also hold references to vocabularies or prefixes. The second part of the JSON-LD frame is the structure of the data. The structure defines how the REST-API response will look like. Most of the time the structure starts with @type to denote the type that the root node should have. Setting the @type is the most straightforward way of selecting your root node. The structure is built outward from the root node. You can define a leaf node in the structure by adding an opening and closing bracket, as shown in the example. To define a nested node you first need to define the key that is a object property in the JSON-LD response that points to another IRI. Then from that IRI the node is created filling in the properties of that node. { \"@context\": { \"addresses\": \"ex:address\", \"Address\": \"ex:Address\", \"Object\": \"ex:Object\", \"street\": \"ex:street\", \"number\": { \"@id\": \"ex:number\", \"@type\": \"xsd:integer\" }, \"labels\": { \"@id\": \"ex:label\", \"@container\": \"@set\" }, \"ex\": \"https://triply.cc/example/\", \"xsd\": \"http://www.w3.org/2001/XMLSchema#\" }, \"@type\": \"Object\", \"labels\": {}, \"addresses\": { \"street\": {}, \"number\": {} } } The JSON-LD frame together with the SPARQL query will now result in a REST-API result: curl -X POST https://api.triplydb.com/queries/JD/JSON-LD-frame/run \\ -H 'Accept: application/ld+json;profile=http://www.w3.org/ns/json-ld#framed' \\ -H 'Content-type: application/json' \\ -d '{ \"@context\": { \"addresses\": \"ex:address\", \"Address\": \"ex:Address\", \"Object\": \"ex:Object\", \"street\": \"ex:street\", \"number\": { \"@id\": \"ex:number\", \"@type\": \"xsd:integer\" }, \"labels\": { \"@id\": \"ex:label\", \"@container\": \"@set\" }, \"ex\": \"https://triply.cc/example/\", \"xsd\": \"http://www.w3.org/2001/XMLSchema#\" }, \"@type\": \"Object\", \"labels\": {}, \"addresses\": { \"street\": {}, \"number\": {} } }' The JSON-LD frame turns SPARQL results for the query in step 1 into a format that is accepted as plain RESTful API request. Using SPARQL to create a frame \u00b6 Another way to create a frame is by using the SPARQL editor in TriplyDB. You can access the JSON-LD editor by clicking the three dots next to the SPARQL editor, and then selecting \"To JSON-LD frame editor\". Afterwards, the JSON script from above should be added to the JSON-LD Frame editor. Running the script results in the following REST-API result: This can also be accessed by the generated API Link above the SPARQL editor. Copying and pasting the generated link will direct you to a page where you can view the script:","title":"JSON-LD Framing"},{"location":"generics/JSON-LD-frames/#json-ld-framing","text":"","title":"JSON-LD Framing"},{"location":"generics/JSON-LD-frames/#why-json-ld-framing","text":"SPARQL Construct and SPARQL Describe queries can return results in the JSON-LD format. Here is an example: [ { \"@id\": \"john\", \"livesIn\": { \"@id\": \"amsterdam\" } }, { \"@id\": \"jane\", \"livesIn\": { \"@id\": \"berlin\" } }, { \"@id\": \"tim\", \"livesIn\": { \"@id\": \"berlin\" } } ] JSON-LD is one of the serialization formats for RDF, and encodes a graph structure. For example, the JSON-LD snippet above encodes the following graph: graph TB Tim -- livesIn --> Berlin John -- livesIn --> Amsterdam Jane -- livesIn --> Berlin The triples in a graphs do not have any specific order. In our graph picture, the triple about Tim is mentioned first, but this is arbitrary. A graph is a set of triples, so there is no 'first' or 'last' triple. Similarly, there is no 'primary' or 'secondary' element in a graph structure either. In our graph picture, persons occur on the left hand-side and cities occur on the right hand-side. In fact, the same information can be expressed with the following graph: Most RESTful APIs return data with a specific, often tree-shaped structure. For example: { \"amsterdam\": { \"inhabitants\": [ \"john\" ] }, \"berlin\": { \"inhabitants\": [ \"jane\", \"tim\" ] } } JSON-LD Framing is a standard that is used to assign additional structure to JSON-LD. With JSON-LD Framing, we can configure the extra structure that is needed to create RESTful APIs over SPARQL queries. JSON-LD Framing are a deterministic translation from a graph, which has an unordered set of triples where no node is \"first\" or \"special\", into a tree, which has ordered branches and exactly one \"root\" node. In other words, JSON-LD framing allows one to force a specific tree layout to a JSON-LD document. This makes it possible to translate SPARQL queries to REST-APIs. The TriplyDB API for saved queries has been equipped with a JSON-LD profiler which can apply a JSON-LD profile to a JSON-LD result, transforming the plain JSON-LD to framed JSON. To do this you need two things. A SPARQL construct query and a JSON-LD frame. When you have both of these, you can retrieve plain JSON from a SPARQL query. The cURL command when both the SPARQL query and JSON-LD frame are available is: curl -X POST [SAVED-QUERY-URL] \\ -H 'Accept: application/ld+json;profile=http://www.w3.org/ns/json-ld#framed' \\ -H 'Authorization: Bearer [YOUR_TOKEN]' \\ -H 'Content-type: application/json' \\ -d '[YOUR_FRAME]' When sending a curl request, a few things are important. First, the request needs to be a POST request. Only a POST request can accept a frame as a body. The Accept header needs to be set to a specific value. The Accept header needs to have both the expected returned content-type and the JSON-LD profile, e.g. application/ld+json;profile=http://www.w3.org/ns/json-ld#framed . When querying an internal or private query you need to add an authorization token. Finally, it is important to set the Content-type . It refers to the content-type of the input body and needs to be application/json , as the frame is of type application/json .","title":"Why JSON-LD Framing?"},{"location":"generics/JSON-LD-frames/#the-sparql-query","text":"Let's start with the SPARQL query. A JSON-LD frame query needs a SPARQL Construct query to create an RDF graph that is self contained and populated with relevant vocabulary and data. The graph in JSON-LD is used as input for the RESTful API call. The SPARQL Construct query can be designed with API variables. Do note that API variables with OPTIONAL s can sometimes behave a bit different than regular API variables. This is due to how SPARQL interprets OPTIONAL s. If an API variable is used in an OPTIONAL , the query will return false positives, as the OPTIONAL does not filter out results matching the API-variable. Also note that the use of UNION s can have unexpected effects on the SPARQL query. A union could split up the result set of the SPARQL query. Meaning that the SPARQL engine first exhausts the top part of the UNION and then starts with the second part of the UNION . This means that the first part of the result set can be disconnected from the second part. If the limit is set too small the result set is separated in two different JSON-LD documents. This could result in missing data in the response. Finally, please note that it can happen that you set a pageSize of 10 but the response contains less than 10 results, while the next page is not empty. This is possible as the result set of the WHERE clause is limited with a limit and not the Construct clause. This means that two rows of the resulting WHERE clause are condensed into a single result in the Construct clause. Thus the response of the API can differ from the pageSize . The result is a set of triples according to the query. Saving the SPARQL query will resolve in a saved query. The saved query has an API URL that we can now use in our cURL command. The URL most of the time starts with api and ends with run . The saved query url of an example query is: https://api.triplydb.com/queries/JD/JSON-LD-frame/run You could use API variables with a ? e.g. ?[queryVariable]=[value]","title":"The SPARQL Query"},{"location":"generics/JSON-LD-frames/#the-frame","text":"The SPARQL query is not enough to provide the RDF data in a JSON serialization format. It requires additional syntactic conformities that cannot be defined in a SPARQL query. Thus the SPARQL query that was created needs a frame to restructure JSON-LD objects into JSON. The JSON-LD 1.1 standard allows for restructuring JSON-LD objects with a frame to JSON. A JSON-LD frame consists out of 2 parts. The @context of the response, and the structure of the response. The complete specification on JSON-LD frames can be found online The @context is the translation of the linked data to the JSON naming. In the @context all the IRIs that occur in the JSON-LD response are documented, with key-value pairs, where the key corresponds to a name the IRI will take in the REST-API response and the value corresponds to the IRI in the JSON-LD response. Most of the time the key-value pairs are one-to-one relations, where one key is mapped to a single string. Sometimes the value is an object. The object contains at least the @id , which is the IRI in the JSON-LD response. The object can also contain other modifiers, that change the REST-API response. Examples are, @type to define the datatype of the object value, or @container to define the container where the value in the REST-API response is stored in. The context can also hold references to vocabularies or prefixes. The second part of the JSON-LD frame is the structure of the data. The structure defines how the REST-API response will look like. Most of the time the structure starts with @type to denote the type that the root node should have. Setting the @type is the most straightforward way of selecting your root node. The structure is built outward from the root node. You can define a leaf node in the structure by adding an opening and closing bracket, as shown in the example. To define a nested node you first need to define the key that is a object property in the JSON-LD response that points to another IRI. Then from that IRI the node is created filling in the properties of that node. { \"@context\": { \"addresses\": \"ex:address\", \"Address\": \"ex:Address\", \"Object\": \"ex:Object\", \"street\": \"ex:street\", \"number\": { \"@id\": \"ex:number\", \"@type\": \"xsd:integer\" }, \"labels\": { \"@id\": \"ex:label\", \"@container\": \"@set\" }, \"ex\": \"https://triply.cc/example/\", \"xsd\": \"http://www.w3.org/2001/XMLSchema#\" }, \"@type\": \"Object\", \"labels\": {}, \"addresses\": { \"street\": {}, \"number\": {} } } The JSON-LD frame together with the SPARQL query will now result in a REST-API result: curl -X POST https://api.triplydb.com/queries/JD/JSON-LD-frame/run \\ -H 'Accept: application/ld+json;profile=http://www.w3.org/ns/json-ld#framed' \\ -H 'Content-type: application/json' \\ -d '{ \"@context\": { \"addresses\": \"ex:address\", \"Address\": \"ex:Address\", \"Object\": \"ex:Object\", \"street\": \"ex:street\", \"number\": { \"@id\": \"ex:number\", \"@type\": \"xsd:integer\" }, \"labels\": { \"@id\": \"ex:label\", \"@container\": \"@set\" }, \"ex\": \"https://triply.cc/example/\", \"xsd\": \"http://www.w3.org/2001/XMLSchema#\" }, \"@type\": \"Object\", \"labels\": {}, \"addresses\": { \"street\": {}, \"number\": {} } }' The JSON-LD frame turns SPARQL results for the query in step 1 into a format that is accepted as plain RESTful API request.","title":"The Frame"},{"location":"generics/JSON-LD-frames/#using-sparql-to-create-a-frame","text":"Another way to create a frame is by using the SPARQL editor in TriplyDB. You can access the JSON-LD editor by clicking the three dots next to the SPARQL editor, and then selecting \"To JSON-LD frame editor\". Afterwards, the JSON script from above should be added to the JSON-LD Frame editor. Running the script results in the following REST-API result: This can also be accessed by the generated API Link above the SPARQL editor. Copying and pasting the generated link will direct you to a page where you can view the script:","title":"Using SPARQL to create a frame"},{"location":"generics/api-token/","text":"On this page: API Token API Token \u00b6 Applications (see TriplyDB.js ) and pipelines (see TriplyETL ) often require access rights to interact with TriplyDB instances. Specifically, reading non-public data and writing any (public or non-public) data requires setting an API token. The token ensures that only users that are specifically authorized for certain datasets are able to access and/or modify those datasets. The following steps must be performed in order to create an API token: Log into the web GUI of the TriplyDB server where you have an account and for which you want to obtain special access rights in your application or pipeline. Many organizations use their own TriplyDB server. If your organization does not yet have a TriplyDB server, you can also create a free account over at TriplyDB.com . Go to your user settings page. This page is reached by clicking on the user menu in the top-right corner and choosing \u201cUser settings\u201d. Go to the \u201cAPI tokens\u201d tab. Click on \u201cCreate token\u201d. Enter a name that describes the purpose of the token. This can be the name of the application or pipeline for which the API token will be used. You can use the name to manage the token later. For example, you can remove tokens for applications that are no longer used later on. It is good practice to create different API tokens for different applications. Choose the permission level that is sufficient for what you want to do with the API token. Notice that \u201cManagement access\u201d is often not needed. \u201cRead access\u201d is sufficient for read-only applications. \u201cWrite access\u201d is sufficient for most pipelines and applications that require write access. Management access: if your application must create or change organization accounts in the TriplyDB server. Write access: if your application must write (meta)data in the TriplyDB server. Read access: if your application must read public and/or private data from the TriplyDB server. Click the \u201cCreate\u201d button to create your token. The token (a long sequence of characters) will now appear in a dialog. For security reasons, the token will only be shown once. You can copy the token over to the application where you want to use it.","title":"API Token"},{"location":"generics/api-token/#api-token","text":"Applications (see TriplyDB.js ) and pipelines (see TriplyETL ) often require access rights to interact with TriplyDB instances. Specifically, reading non-public data and writing any (public or non-public) data requires setting an API token. The token ensures that only users that are specifically authorized for certain datasets are able to access and/or modify those datasets. The following steps must be performed in order to create an API token: Log into the web GUI of the TriplyDB server where you have an account and for which you want to obtain special access rights in your application or pipeline. Many organizations use their own TriplyDB server. If your organization does not yet have a TriplyDB server, you can also create a free account over at TriplyDB.com . Go to your user settings page. This page is reached by clicking on the user menu in the top-right corner and choosing \u201cUser settings\u201d. Go to the \u201cAPI tokens\u201d tab. Click on \u201cCreate token\u201d. Enter a name that describes the purpose of the token. This can be the name of the application or pipeline for which the API token will be used. You can use the name to manage the token later. For example, you can remove tokens for applications that are no longer used later on. It is good practice to create different API tokens for different applications. Choose the permission level that is sufficient for what you want to do with the API token. Notice that \u201cManagement access\u201d is often not needed. \u201cRead access\u201d is sufficient for read-only applications. \u201cWrite access\u201d is sufficient for most pipelines and applications that require write access. Management access: if your application must create or change organization accounts in the TriplyDB server. Write access: if your application must write (meta)data in the TriplyDB server. Read access: if your application must read public and/or private data from the TriplyDB server. Click the \u201cCreate\u201d button to create your token. The token (a long sequence of characters) will now appear in a dialog. For security reasons, the token will only be shown once. You can copy the token over to the application where you want to use it.","title":"API Token"},{"location":"generics/sparql-pagination/","text":"On this page: SPARQL Pagination Pagination with the saved query API Pagination with TriplyDB.js SPARQL Pagination \u00b6 This page explains how to retrieve all results from a SPARQL query using pagination. Often SPARQL queries can return more than 10.000 results, but due to limitations the result set will only consist out of the first 10.000 results. To retrieve more than 10.000 results you can use pagination. TriplyDB supports two methods to retrieve all results from a SPARQL query. Pagination with the saved query API or Pagination with TriplyDB.js. Pagination with the saved query API \u00b6 Each TriplyDB instance has a fully RESTful API. The TriplyDB RESTful API is extended for saved SPARQL queries. The API for saved queries is extended with two arguments that the query is able to process paginated result sets. The arguments are \u2018page\u2019 and \u2018pageSize\u2019. An example of a paginated saved SPARQL query request would look like: https://api.triplydb.com/queries/academy/pokemon-color/run?page=3&pageSize=100 The example request argument \u2018page\u2019 corresponds to the requested page. In the example request this would correspond to the third page of paginated SPARQL query, according to the \u2018pageSize\u2019. There is no maximum \u2018page\u2019 limit, as a SPARQL query could return an arbitrary number of results. When no results can be retrieved for the requested page an empty page will be returned. The argument \u2018pageSize\u2019 corresponds to how many results each page would contain. The \u2018pageSize\u2019 has a default of 100 returned results and a maximum \u2018pageSize\u2019 limit of 10.000 returned results. The request will return an error when the \u2018pageSize\u2019 is set higher than 10.000. The RESTful API for the saved SPARQL queries follows the RFC 8288 standard. The request will return a response body containing the result set and a response header. The response header contains a link header with the relative \"next\" request, the relative \"prev\" request, and the relative \"first\" request. By following the \"next\" link header request you can chain the pagination and retrieve all results. link: <https://api.triplydb.com/queries/academy/pokemon-color/run?page=4&pageSize=100>; rel=\"next\", <https://api.triplydb.com/queries/academy/pokemon-color/run?page=2&pageSize=100>; rel=\"prev\", <https://api.triplydb.com/queries/academy/pokemon-color/run?page=1&pageSize=100>; rel=\"first\" Pagination with TriplyDB.js \u00b6 TriplyDB.js is the official programming library for interacting with TriplyDB . TriplyDB.js allows the user to connect to a TriplyDB instance via the TypeScript language. TriplyDB.js has the advantage that it can handle pagination internally so it can reliably retrieve a large number of results. To get the output for a construct or select query, follow these steps: 1. Import the triplyDB.js library and set your parameters, regarding the TriplyDB instance and the account in which you have saved the query as well as the name of the query. Do not forget that we perform TriplyDB.js requests within an async context . import Client from '@triply/triplydb' async function run() { // Your code goes here. const client = Client.get({token: process.env.TRIPLYDB_TOKEN}) const account = await client.getAccount('account-name') const query = await account.getQuery('name-of-some-query') } run() 2. Get the results of a query by setting a results variable. More specifically, for construct queries you use the statements() call: const query = await account.getQuery('name-of-some-query') const results = query.results().statements() For select queries you use the bindings() call: const query = await account.getQuery('name-of-some-query') const results = query.results().bindings() Additionally, saved queries can have 'API variables' that allow you to specify variables that are used in the query. Thus, if you have query parameters, pass their values as the first argument to results as follows: // For SPARQL construct queries. const results = query.results({ someVariable: 'value of someVariable', anotherVariable: 'value of anotherVariable' }).statements() // For SPARQL select queries. const results = query.results({ someVariable: 'value of someVariable', anotherVariable: 'value of anotherVariable' }).bindings() 3. To iterate the results of your SPARQL query you have three options: 3.1. Iterate through the results per row in a for -loop: // Iterating over the results. for await (const row of results) { // execute something } Note: For select queries the for -loop iterates over the rows of the result set. For construct queries the for -loop iterates over the statements in the result set. 3.2. Save the results to a file. This is only supported for SPARQL construct queries: // Saving the results of a SPARQL construct query to a file. await results.toFile('my-file.nt') 3.3. Load all results into memory in the form of an Array. Note that this is almost never used. If you want to process results, then use the 3a option; if you want to persist results, then option 3b suits better. // Loading results for a SPARQL construct or SPARQL select query into memory. const array = await results.toArray()","title":"SPARQL pagination"},{"location":"generics/sparql-pagination/#sparql-pagination","text":"This page explains how to retrieve all results from a SPARQL query using pagination. Often SPARQL queries can return more than 10.000 results, but due to limitations the result set will only consist out of the first 10.000 results. To retrieve more than 10.000 results you can use pagination. TriplyDB supports two methods to retrieve all results from a SPARQL query. Pagination with the saved query API or Pagination with TriplyDB.js.","title":"SPARQL Pagination"},{"location":"generics/sparql-pagination/#pagination-with-the-saved-query-api","text":"Each TriplyDB instance has a fully RESTful API. The TriplyDB RESTful API is extended for saved SPARQL queries. The API for saved queries is extended with two arguments that the query is able to process paginated result sets. The arguments are \u2018page\u2019 and \u2018pageSize\u2019. An example of a paginated saved SPARQL query request would look like: https://api.triplydb.com/queries/academy/pokemon-color/run?page=3&pageSize=100 The example request argument \u2018page\u2019 corresponds to the requested page. In the example request this would correspond to the third page of paginated SPARQL query, according to the \u2018pageSize\u2019. There is no maximum \u2018page\u2019 limit, as a SPARQL query could return an arbitrary number of results. When no results can be retrieved for the requested page an empty page will be returned. The argument \u2018pageSize\u2019 corresponds to how many results each page would contain. The \u2018pageSize\u2019 has a default of 100 returned results and a maximum \u2018pageSize\u2019 limit of 10.000 returned results. The request will return an error when the \u2018pageSize\u2019 is set higher than 10.000. The RESTful API for the saved SPARQL queries follows the RFC 8288 standard. The request will return a response body containing the result set and a response header. The response header contains a link header with the relative \"next\" request, the relative \"prev\" request, and the relative \"first\" request. By following the \"next\" link header request you can chain the pagination and retrieve all results. link: <https://api.triplydb.com/queries/academy/pokemon-color/run?page=4&pageSize=100>; rel=\"next\", <https://api.triplydb.com/queries/academy/pokemon-color/run?page=2&pageSize=100>; rel=\"prev\", <https://api.triplydb.com/queries/academy/pokemon-color/run?page=1&pageSize=100>; rel=\"first\"","title":"Pagination with the saved query API"},{"location":"generics/sparql-pagination/#pagination-with-triplydbjs","text":"TriplyDB.js is the official programming library for interacting with TriplyDB . TriplyDB.js allows the user to connect to a TriplyDB instance via the TypeScript language. TriplyDB.js has the advantage that it can handle pagination internally so it can reliably retrieve a large number of results. To get the output for a construct or select query, follow these steps: 1. Import the triplyDB.js library and set your parameters, regarding the TriplyDB instance and the account in which you have saved the query as well as the name of the query. Do not forget that we perform TriplyDB.js requests within an async context . import Client from '@triply/triplydb' async function run() { // Your code goes here. const client = Client.get({token: process.env.TRIPLYDB_TOKEN}) const account = await client.getAccount('account-name') const query = await account.getQuery('name-of-some-query') } run() 2. Get the results of a query by setting a results variable. More specifically, for construct queries you use the statements() call: const query = await account.getQuery('name-of-some-query') const results = query.results().statements() For select queries you use the bindings() call: const query = await account.getQuery('name-of-some-query') const results = query.results().bindings() Additionally, saved queries can have 'API variables' that allow you to specify variables that are used in the query. Thus, if you have query parameters, pass their values as the first argument to results as follows: // For SPARQL construct queries. const results = query.results({ someVariable: 'value of someVariable', anotherVariable: 'value of anotherVariable' }).statements() // For SPARQL select queries. const results = query.results({ someVariable: 'value of someVariable', anotherVariable: 'value of anotherVariable' }).bindings() 3. To iterate the results of your SPARQL query you have three options: 3.1. Iterate through the results per row in a for -loop: // Iterating over the results. for await (const row of results) { // execute something } Note: For select queries the for -loop iterates over the rows of the result set. For construct queries the for -loop iterates over the statements in the result set. 3.2. Save the results to a file. This is only supported for SPARQL construct queries: // Saving the results of a SPARQL construct query to a file. await results.toFile('my-file.nt') 3.3. Load all results into memory in the form of an Array. Note that this is almost never used. If you want to process results, then use the 3a option; if you want to persist results, then option 3b suits better. // Loading results for a SPARQL construct or SPARQL select query into memory. const array = await results.toArray()","title":"Pagination with TriplyDB.js"},{"location":"triply-api/","text":"On this page: Triply API Authentication Creating an API token Using the API token Important Security Considerations Exporting linked data Datasets Create a dataset Upload linked data Upload assets Accounts Queries Query metadata (GRLC) LD Browser API Triple Pattern Fragments (TPF) URI path Reply format Query parameters Example request Exporting data Query parameters Example requests Services Create a service Synchronize a service SPARQL Sending a SPARQL Query request SPARQL Query result formats Examples of SPARQL Query requests GET request URL-encoded POST request Direct POST request SPARQL JSON SPARQL XML SPARQL tab-separated values SPARQL comma-separated values JSON-LD N-Quads N-Triples TriG Turtle GraphQL URI path Requests and Response Example Elasticsearch URI path Reply format Examples Simple search Custom search Count API Setting up index templates for ElasticSearch Index templates Component templates Triply API \u00b6 Each Triply instance has a fully RESTful API. All functionality, from managing the Triply instance to working with your data, is done through the API. This document describes the general setup of the API, contact support@triply.cc for more information. Authentication \u00b6 When a dataset is published publicly, most of the read operation on that dataset can be performed without authentication. Write operations and read operations on datasets that are published internally or privately require authentication. Creating an API token \u00b6 Authentication is implemented through API tokens. An API token can be created within the TriplyDB UI in the following way: Log into your TriplyDB instance. Click on the user menu in the top-right corner and click on \u201cUser settings\u201d. Go to the \u201cAPI tokens\u201d tab. Click the \u201cCreate token\u201d button, enter a description for the token (e.g., \u201ctest-token\u201d) and select the appropriate access rights. Click on \u201cCreate\u201d and copy the created API token (a lengthy string of characters). This string is only shown once, upon creation, and must not be shared with others. (Other users can create their own token in the here described way.) Using the API token \u00b6 API tokens are used by specifying them in an HTTP request header as follows: Authorization: Bearer TOKEN In the above, TOKEN should be replaced by your personal API token (a lengthy sequence of characters). See Creating an API token for information on how to create an API token. Important Security Considerations \u00b6 Do Not Commit Your Token to a Git Repository : Under no circumstances should you commit your TriplyDB token to a Git repository. This practice is not allowed according to our ISO standards. Do Not Share Your Token: Avoid sharing your TriplyDB token with anyone who should not have access to your TriplyDB resources . Tokens should be treated as sensitive information and shared only with trusted parties. Change Tokens Regularly : To enhance security, consider regularly generating a new token to replace the existing one especially if you suspect any compromise. Exporting linked data \u00b6 Every TriplyDB API path that returns linked data provides a number of serializations to choose from. We support the following serializations: Serialization Media type File extension TriG application/trig .trig N-Triples application/n-triples .nt N-Quads application/n-quads .nq Turtle text/turtle .ttl JSON-LD application/ld+json .jsonld To request a serialization, use one of the following mechanisms: Add an Accept header to the request. E.g. Accept: application/n-triples Add the extension to the URL path. E.g. https://api.triplydb.com/datasets/Triply/iris/download.nt Datasets \u00b6 Triply API requests are always directed towards a specific URI path. URI paths will often have the following form: https://api.INSTANCE/datasets/ACCOUNT/DATASET/ Upper-case letter words must be replaced by the following values: INSTANCE :: The host name of the TriplyDB instance that you want to use. ACCOUNT :: The name of a specific user or a specific organization. DATASET :: The name of a specific dataset. Here is an example of a URI path that points to the Triply API for the Pok\u00e9mon dataset: https://api.triplydb.com/datasets/academy/pokemon/ Create a dataset \u00b6 You can create a new dataset via the Triply API. You need to use the API Token and send an HTTP POST request with data specifying: name , accessLevel and displayName . The example of the URI: curl -H 'Authorization: Bearer TOKEN' -H 'Content-Type: application/json' -X POST https://api.INSTANCE/datasets/ACCOUNT/ -d '{\"name\": \"NAME\", \"accessLevel\": \"ACCESS_LEVEL\", \"displayName\": \"DISPLAY_NAME\"}' Upper-case letter words in json after -d must be replaced by the following values: NAME :: The name of the dataset in the url. ACCESS_LEVEL :: public , private or internal . For more information visit Access levels in TriplyDB . DISPLAY_NAME :: The display name of the dataset. Upload linked data \u00b6 You can upload linked data via the Triply API. You need to use the API Token and send an HTTP POST request with data specifying the local file path. The list of supported file extensions can be checked in Adding data: File upload documentation. The example of such a request: curl -H 'Authorization: Bearer TOKEN' -X POST https://api.INSTANCE/datasets/ACCOUNT/DATASET/jobs -F file=@FILENAME Upper-case letter words must be replaced by the following values: TOKEN :: Your TriplyDB token. INSTANCE :: The domain of your instance ACCOUNT :: Your account name DATASET :: The dataset name FILENAME :: The path to the file you want to upload A request looks like this: curl -H 'Authorization: Bearer xxxxxx' -X POST https://api.triplydb.com/datasets/my-account-name/my-dataset-name/jobs -F file=@./myfile.trig Limitations : We only support this API route for uploads less than 5MB. To upload more data, use: TriplyDB-JS : See the importFrom* methods under the Dataset class . TriplyDB Command-line Interface Upload assets \u00b6 You can upload assets via the Triply API. You need to use the API Token and send an HTTP POST request with data specifying the local file path. To add a new asset: curl -H \"Authorization: Bearer TOKEN\" -X POST https://api.INSTANCE/datasets/ACCOUNT/DATASET/assets -F file=@FILENAME To add a version to an existing asset: curl -H \"Authorization: Bearer TOKEN\" -X POST https://api.INSTANCE/datasets/ACCOUNT/DATASET/assets/IDENTIFIER -F file=@FILENAME Upper-case letter words must be replaced by the following values: TOKEN :: Your TriplyDB token. INSTANCE :: The domain of your instance ACCOUNT :: Your account name DATASET :: The dataset name IDENTIFIER :: The asset identifier where you'd like to add a new version to. FILENAME :: The path to the file you want to upload This request returns a JSON object, that includes (among other things) an identifier key, which can be used as a persistent identifier for this asset. A request to add a new asset looks like this: curl -H 'Authorization: Bearer xxxxxx' -X POST https://api.triplydb.com/datasets/my-account-name/my-dataset-name/assets -F file=@./myfile.txt A request to add a version to an existing asset looks like this: curl -H 'Authorization: Bearer xxxxxx' -X POST https://api.triplydb.com/datasets/my-account-name/my-dataset-name/assets/yyyyy -F file=@./myfile.txt Limitations : We only support this API route for uploads less than 5MB. To upload more data, use: TriplyDB-JS : See the uploadAsset methods under the Dataset class . TriplyDB Command-line Interface Accounts \u00b6 Information about TriplyDB accounts (organizations and users) can be retrieved from the following API path: https://api.INSTANCE/accounts Upper-case letter words must be replaced by the following values: INSTANCE :: The host name of the TriplyDB instance that you want to use. Here is an example of a URI path that points to the Triply API for the Triply organization account: https://api.triplydb.com/accounts/Triply Queries \u00b6 TriplyDB allows users to save SPARQL queries. The metadata for all saved query can be accessed as follows: https://api.triplydb.com/queries By adding an account name (for example: 'Triply'), metadata for all saved queries for that account can be accessed as follows: https://api.triplydb.com/queries/Triply By adding an account name and a query name (for example: 'Triply/flower-length'), metadata for one specific saved query can be accessed as follows: https://api.triplydb.com/queries/Triply/flower-length Query metadata (GRLC) \u00b6 You can retrieve a text-based version of each query, by requesting the text/plain content type: curl -vL -H 'Accept: text/plain' 'https://api.triplydb.com/queries/JD/pokemonNetwork' This returns the query string, together with metadata annotations. These metadata annotations use the GRLC format . For example: #+ description: This query shows a small subgraph from the Pokemon dataset. #+ endpoint: https://api.triplydb.com/datasets/academy/pokemon/services/pokemon/sparql #+ endpoint_in_url: false construct where { ?s ?p ?o. } limit 100 Notice that the GRLC annotations are encoded in SPARQL comments, i.e. lines that start with the hash character ( # ). This makes the result immediately usable as a SPARQL query. The above example includes the following GRLC annotations: description gives a human-readable description of the meaning of the query. This typically includes an explanation of the purpose or goal for which this query is used, the content returned, or the process or task in which this query is used. endpoint The URL of the SPARQL endpoint where queries are sent to. endpoint_in_url configures whether the URL of the SPARQL endpoint should be specified through the API. In TriplyDB, this configuration is by default set to false . (Users of the RESTful API typically expect domain parameters such as countryName or maximumAge , but they do not necessarily expect technical parameters like an endpoint URL.) LD Browser API \u00b6 Triply APIs provide a convenient way to access data used by LD Browser , which offers a comprehensive overview of a specific IRI. By using Triply API for a specific IRI, you can retrieve the associated 'document' in the .nt format that describes the IRI. To make an API request for a specific instance, you can use the following URI path: https://api.triplydb.com/datasets/ACCOUNT/DATASET/describe.nt?resource=RESOURCE To illustrate this, let's take the example of the DBpedia dataset and the specific instance of 'Mona Lisa' . If you use this URI path: https://api.triplydb.com/datasets/DBpedia-association/dbpedia/describe.nt?resource=http%3A%2F%2Fdbpedia.org%2Fresource%2FMona_Lisa In your browser, the .nt document describing the 'Mona Lisa' instance will be automatically downloaded. You can then upload this file to a dataset and visualize it in a graph . Figure 1 illustrates the retrieved graph for the \u2018Mona Lisa\u2019 instance. The requested resource will be displayed in the center of the graph, forming an 'ego graph'. It will include all direct properties, as well as some indirect properties that are also pulled in by LD Browser. The labels for all classes and properties will be included for easy human-readable display. In addition, this API also supports traversing blank node-replacing well-known IRIs (CBD style), and limits the number of objects per subject/property to manage the description size. This corresponds to the \"Show more\" button in the LD Browser GUI, ensuring a manageable and user-friendly experience. Triple Pattern Fragments (TPF) \u00b6 Triple Pattern Fragments (TPF) is a community standard that allows individual linked datasets to be queried for Triple Patterns (TP), a subset of the more complex SPARQL query language. The Triply API implements Triple Pattern Fragments version 2019-01-18 and Linked Data Fragments version 2016-06-05. The Triple Pattern Fragments (TPF) API is available for all datasets in Triply and does not require running a dedicated service. URI path \u00b6 TPF requests are sent to the following URI path: https://api.INSTANCE/datasets/ACCOUNT/DATATSET/fragments Reply format \u00b6 Since TPF replies distinguish between data and metadata that are stored in different graphs, it is recommended to request the TriG content type with the following HTTP request header: Accept: application/trig Query parameters \u00b6 Triple Pattern Fragments (TPF) uses the following query parameters in order to retrieve only those triples that adhere to a specified Triple Pattern: Key Value Purpose subject A URL-encoded IRI. Only return triples where the given IRI appears in the subject position. predicate A URL-encoded IRI. Only return triples where the given IRI appears in the predicate position. object A URL-encoded IRI or literal. Only return triples where the given IRI or literal appears in the object position. Example request \u00b6 curl -G \\ 'https://api.triplydb.com/datasets/academy/pokemon/fragments' \\ --data-urlencode 'predicate=http://www.w3.org/2000/01/rdf-schema#label' \\ -H 'Accept: application/trig' Exporting data \u00b6 To export the linked data, use the following path: https://api.INSTANCE/datasets/ACCOUNT/DATATSET/download Query parameters \u00b6 By default, an export includes all linked data graphs. Use a query argument to specify a particular graph. Key Value Purpose graph A URL-encoded IRI. Only download the export of the given graph IRI. Therefore, to export the linked data of a graph , use the following path: https://api.INSTANCE/datasets/ACCOUNT/DATATSET/download/?graph=GRAPH To find out which graphs are available, use the following path: https://api.INSTANCE/datasets/ACCOUNT/DATATSET/graphs Example requests \u00b6 Export a dataset: curl 'https://api.triplydb.com/datasets/academy/pokemon/download' \\ -H 'Accept: application/trig' > exportDataset.trig.gz Export a graph: First, find out which graphs are available: curl 'https://api.triplydb.com/datasets/academy/pokemon/graphs' Then, download one of the graph: curl 'curl 'https://api.triplydb.com/datasets/academy/pokemon/download?graph=https://triplydb.com/academy/pokemon/graphs/data' -H 'Accept: application/trig' > exportGraph.trig.gz Services \u00b6 Some API requests require the availability of a specific service over the dataset. These requests are directed towards a URI path of the following form: https://api.INSTANCE/datasets/ACCOUNT/DATASET/services/SERVICE/ Upper-case letter words must be replaced by the following values: SERVICE :: The name of a specific service that has been started for the corresponding dataset. See the previous section for Datasets to learn the meaning of INSTANCE , ACCOUNT , and DATASET . Here is an example of a URI path that points to a SPARQL endpoint over the Pok\u00e9mon dataset: https://api.triplydb.com/datasets/academy/pokemon/services/pokemon/ See the following sections for more information on how to query the endpoints provided by services: SPARQL Elasticsearch Create a service \u00b6 You can create a service for a dataset via TriplyDB API. You need to use the API Token and send an HTTP POST request with data specifying: \"type\" and \"name\" . The example of the URI: curl -H 'Authorization: Bearer TOKEN' -H 'Content-Type: application/json' -X POST https://api.INSTANCE/datasets/ACCOUNT/DATASET/services/ -d '{\"type\": \"TYPE\", \"name\": \"NAME\"}' Upper-case letter words in json after -d must be replaced by the following values: TYPE :: SPARQL ( virtuoso or jena ) or Elasticsearch NAME :: The name of the service Synchronize a service \u00b6 You can synchronize existing service for a dataset via TriplyDB API. You need to use the API Token and send an HTTP POST request with data: {\"sync\": \"true\"} The example of the URI: curl -H 'Authorization: Bearer TOKEN' -H 'Content-Type: application/json' -X POST https://api.INSTANCE/datasets/ACCOUNT/DATASET/services/SERVICE -d '{\"sync\": \"true\"}' SPARQL \u00b6 There are two service types in TriplyDB that expose the SPARQL 1.1 Query Language: \"Sparql\" and \"Jena\". The former works well for large quantities of instance data with a relatively small data model; the latter works well for smaller quantities of data with a richer data model. SPARQL services expose a generic endpoint URI at the following location (where ACCOUNT , DATASET and SERVICE are user-chosen names): https://api.triplydb.com/datasets/ACCOUNT/DATASET/services/SERVICE/sparql Everybody who has access to the dataset also has access to its services, including its SPARQL services: - For Public datasets, everybody on the Internet or Intranet can issue queries. - For Internal datasets, only users that are logged into the triple store can issue queries. - For Private datasets, only users that are logged into the triple store and are members of ACCOUNT can issue queries. Notice that for professional use it is easier and better to use saved queries . Saved queries have persistent URIs, descriptive metadata, versioning, and support for reliable large-scale pagination ( see how to use pagination with saved query API ). Still, if you do not have a saved query at your disposal and want to perform a custom SPARQL request against an accessible endpoint, you can do so. TriplyDB implements the SPARQL 1.1 Query Protocol standard for this purpose. Sending a SPARQL Query request \u00b6 According to the SPARQL 1.1 Protocol, queries can be send in the 3 different ways that are displayed in Table 1 . For small query strings it is possible to send an HTTP GET request (row 1 in Table 1 ). A benefit of this approach is that all information is stored in one URI. For public data, copy/pasting this URI in a web browser runs the query. For larger query strings it is required to send an HTTP POST request (rows 2 and 3 in Table 1 ). The reason for this is that longer query strings result in longer URIs when following the HTTP GET approach. Some applications do not support longer URIs, or they even silently truncate them resulting in an error down the line. The direct POST approach (row 3 in Table 1 ) is the best of these 3 variants, since it most clearly communicates that it is sending a SPARQL query request (see the Content-Type column). HTTP Method Query String Parameters Request Content-Type Request Message Body query via GET GET query (exactly 1) default-graph-uri (0 or more) named-graph-uri (0 or more) none none query via URL-encoded POST POST none application/x-www-form-urlencoded URL-encoded, ampersand-separated query parameters. query (exactly 1) default-graph-uri (0 or more) named-graph-uri (0 or more) query via POST directly POST default-graph-uri (0 or more) named-graph-uri (0 or more) application/sparql-query Unencoded SPARQL query string Table 1 - Overview of the three different ways in which SPARQL queries can be issues over HTTP. SPARQL Query result formats \u00b6 SPARQL services are able to return results in different formats. The user can specify the preferred format by specifying the corresponding Media Type in the HTTP Accept header. TriplyDB supports the Media Types in the following table. Notice that the chosen result format must be supported for your query form. Alternatively, it is possible (but not preferred) to specify the requested format as an URI path suffix; see the GET request section for an example. Result format Media Type Query forms Suffix CSV text/csv Select .csv JSON application/json Ask, Select .json JSON-LD application/ld+json Construct, Describe .jsonld N-Quads application/n-quads Construct, Describe .nq N-Triples application/n-triples Construct, Describe .nt SPARQL JSON application/sparql-results+json Ask, Select .srj SPARQL XML application/sparql-results+xml Ask, Select .srx TriG application/trig Construct, Describe .trig TSV text/tab-separated-values Select .tsv Turtle text/turtle Construct, Describe .ttl Examples of SPARQL Query requests \u00b6 This section contains examples of SPARQL HTTP requests. The requests run either of the following two SPARQL queries against a public SPARQL endpoint that contains data about Pokemon: select * { ?s ?p ?o. } limit 1 construct where { ?s ?p ?o. } limit 1 The examples made use of the popular command-line tool cURL . These examples should also work in any other HTTP client tool or library. GET request \u00b6 curl https://api.triplydb.com/datasets/academy/pokemon/services/pokemon/sparql?query=select%20%2A%20%7B%20%3Fs%20%3Fp%20%3Fo.%20%7D%20limit%201 Result: [ { \"s\": \"https://triplydb.com/academy/pokemon/vocab/\", \"p\": \"http://www.w3.org/1999/02/22-rdf-syntax-ns#type\", \"o\": \"http://www.w3.org/2002/07/owl#Ontology\" } ] The following request is identical to the previous one, but adds the \".srj\" suffix to the URI path (see /sparql.srj ). All suffixes from the table in Section SPARQL Query result formats are supported. curl https://api.triplydb.com/datasets/academy/pokemon/services/pokemon/sparql.srj?query=select%20%2A%20%7B%20%3Fs%20%3Fp%20%3Fo.%20%7D%20limit%201 This returns the official SPARQL Result Set JSON (SRJ) format. Notice that this official format is more verbose than the standard JSON format: { \"head\": { \"link\": [], \"vars\": [ \"s\", \"p\", \"o\" ] }, \"results\": { \"bindings\": [ { \"s\": { \"type\": \"uri\", \"value\": \"https://triplydb.com/academy/pokemon/\" }, \"p\": { \"type\": \"uri\", \"value\": \"http://www.w3.org/1999/02/22-rdf-syntax-ns#type\" }, \"o\": { \"type\": \"uri\", \"value\": \"http://rdfs.org/ns/void#Dataset\" } } ] } } URL-encoded POST request \u00b6 curl -X POST https://api.triplydb.com/datasets/academy/pokemon/services/pokemon/sparql \\ -H 'Content-Type: application/x-www-form-urlencoded' \\ --data query=select%20%2A%20%7B%20%3Fs%20%3Fp%20%3Fo.%20%7D%20limit%201 Result: [ { \"s\": \"https://triplydb.com/academy/pokemon/vocab/\", \"p\": \"http://www.w3.org/1999/02/22-rdf-syntax-ns#type\", \"o\": \"http://www.w3.org/2002/07/owl#Ontology\" } ] Direct POST request \u00b6 curl -X POST https://api.triplydb.com/datasets/academy/pokemon/services/pokemon/sparql \\ -H 'Content-Type: application/sparql-query' \\ -d 'select * { ?s ?p ?o } limit 1' Result: [ { \"s\": \"https://triplydb.com/academy/pokemon/vocab/\", \"p\": \"http://www.w3.org/1999/02/22-rdf-syntax-ns#type\", \"o\": \"http://www.w3.org/2002/07/owl#Ontology\" } ] SPARQL JSON \u00b6 Like the previous example, but with an Accept header that specifies Media Type application/sparql-results+json : curl -X POST https://api.triplydb.com/datasets/academy/pokemon/services/pokemon/sparql \\ -H 'Accept: application/sparql-results+json' \\ -H 'Content-Type: application/sparql-query' \\ -d 'select * { ?s ?p ?o } limit 1' Result: { \"head\": { \"vars\": [\"s\", \"p\", \"o\"] }, \"results\": { \"bindings\": [ { \"s\": { \"type\": \"uri\", \"value\": \"https://triplydb.com/academy/pokemon/vocab/\" }, \"p\": { \"type\": \"uri\", \"value\": \"http://www.w3.org/1999/02/22-rdf-syntax-ns#type\" }, \"o\": { \"type\": \"uri\", \"value\": \"http://www.w3.org/2002/07/owl#Ontology\" } } ] } } SPARQL XML \u00b6 Like the previous example, but with Media Type application/sparql-results+xml in the Accept header: curl -X POST https://api.triplydb.com/datasets/academy/pokemon/services/pokemon/sparql \\ -H 'Accept: application/sparql-results+xml' \\ -H 'Content-Type: application/sparql-query' \\ -d 'select * { ?s ?p ?o } limit 1' Result: <sparql xmlns=\"http://www.w3.org/2005/sparql-results#\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:schemaLocation=\"http://www.w3.org/2001/sw/DataAccess/rf1/result2.xsd\"> <head> <variable name=\"s\"/> <variable name=\"p\"/> <variable name=\"o\"/> </head> <results distinct=\"false\" ordered=\"true\"> <result> <binding name=\"s\"> <uri>https://triplydb.com/academy/pokemon/vocab/</uri> </binding> <binding name=\"p\"> <uri>http://www.w3.org/1999/02/22-rdf-syntax-ns#type</uri> </binding> <binding name=\"o\"> <uri>http://www.w3.org/2002/07/owl#Ontology</uri> </binding> </result> </results> </sparql> SPARQL tab-separated values \u00b6 Like the previous examples, but with Media Type text/tab-separated-values in the Accept header: curl -X POST https://api.triplydb.com/datasets/academy/pokemon/services/pokemon/sparql \\ -H 'Accept: text/tab-separated-values' \\ -H 'Content-Type: application/sparql-query' \\ -d 'select * { ?s ?p ?o } limit 1' \"s\" \"p\" \"o\" \"https://triplydb.com/academy/pokemon/vocab/\" \"http://www.w3.org/1999/02/22-rdf-syntax-ns#type\" \"http://www.w3.org/2002/07/owl#Ontology\" SPARQL comma-separated values \u00b6 Like the previous examples, but with Media Type text/csv in the Accept header: curl -X POST https://api.triplydb.com/datasets/academy/pokemon/services/pokemon/sparql \\ -H 'Accept: text/csv' \\ -H 'Content-Type: application/sparql-query' \\ -d 'select * { ?s ?p ?o } limit 1' Result: \"s\",\"p\",\"o\" \"https://triplydb.com/academy/pokemon/vocab/\",\"http://www.w3.org/1999/02/22-rdf-syntax-ns#type\",\"http://www.w3.org/2002/07/owl#Ontology\" JSON-LD \u00b6 Like the previous examples, but with a SPARQL construct query and Media Type application/ld+json in the Accept header: curl -X POST https://api.triplydb.com/datasets/academy/pokemon/services/pokemon/sparql \\ -H 'Accept: application/ld+json' \\ -H 'Content-Type: application/sparql-query' \\ -d 'construct where { ?s ?p ?o } limit 1' Result: { \"@graph\": [ { \"@id\": \"https://triplydb.com/academy/pokemon/vocab/\", \"@type\": \"http://www.w3.org/2002/07/owl#Ontology\" } ] } N-Quads \u00b6 Like the previous examples, but with Media Type application/n-quads in the Accept header: curl -X POST https://api.triplydb.com/datasets/academy/pokemon/services/pokemon/sparql \\ -H 'Accept: application/n-quads' \\ -H 'Content-Type: application/sparql-query' \\ -d 'construct where { ?s ?p ?o } limit 1' Result: { \"@graph\": [ { \"@id\": \"https://triplydb.com/academy/pokemon/vocab/\", \"@type\": \"http://www.w3.org/2002/07/owl#Ontology\" } ] } N-Triples \u00b6 Like the previous examples, but with Media Type application/n-triples in the Accept header: curl -X POST https://api.triplydb.com/datasets/academy/pokemon/services/pokemon/sparql \\ -H 'Accept: application/n-triples' \\ -H 'Content-Type: application/sparql-query' \\ -d 'construct where { ?s ?p ?o } limit 1' Result: <https://triplydb.com/academy/pokemon/vocab/> <http://www.w3.org/1999/02/22-rdf-syntax-ns#type> <http://www.w3.org/2002/07/owl#Ontology> . TriG \u00b6 Like the previous examples, but with Media Type application/trig in the Accept header: curl -X POST https://api.triplydb.com/datasets/academy/pokemon/services/pokemon/sparql \\ -H 'Accept: application/trig' \\ -H 'Content-Type: application/sparql-query' \\ -d 'construct where { ?s ?p ?o } limit 1' Result: @prefix rdf: <http://www.w3.org/1999/02/22-rdf-syntax-ns#> . @prefix owl: <http://www.w3.org/2002/07/owl#> . <https://triplydb.com/academy/pokemon/vocab/> rdf:type owl:Ontology . Turtle \u00b6 Like the previous examples, but with Media Type text/turtle in the Accept header: curl -X POST https://api.triplydb.com/datasets/academy/pokemon/services/pokemon/sparql \\ -H 'Accept: text/turtle' \\ -H 'Content-Type: application/sparql-query' \\ -d 'construct where { ?s ?p ?o } limit 1' Result: @prefix rdf: <http://www.w3.org/1999/02/22-rdf-syntax-ns#> . @prefix owl: <http://www.w3.org/2002/07/owl#> . <https://triplydb.com/academy/pokemon/vocab/> rdf:type owl:Ontology . GraphQL \u00b6 Some TriplyDB instances publish a GraphQL endpoint for every dataset. This endpoint can be used for GraphQL queries. It uses information from user-provided SHACL shapes to generate the GraphQL schema. See more information about this subject here . URI path \u00b6 GraphQL requests are sent to the following URI path: https://api.INSTANCE/datasets/ACCOUNT/DATASET/graphql Requests and Response \u00b6 The format of requests and corresponding responses are described by graphql.org Example \u00b6 Perform a search using the custom query: { \"query\": { \"{ CapitalConnection { edges { node { label } } } }\" } } This request is issued in the following way with the cURL command-line tool: curl -X POST https://api.triplydb.com/datasets/iish/cshapes/graphql \\ -d '{ \"query\":\"{ CapitalConnection { edges { node { label } } } }\"}' \\ -H \"Content-Type: application/json\" Elasticsearch \u00b6 The text search API returns a list of linked data entities based on a supplied text string. The text string is matched against the text in literals and IRIs that appear in the linked data description of the returned entities. The text search API is only available for a dataset after an Elasticsearch service has been created for that dataset. Two types of searches can be performed: a simple search, and a custom search. Simple searches require one search term for a fuzzy match. Custom searches accept a JSON object conforming to the Elasticsearch query DSL . URI path \u00b6 Text search requests are sent to the following URI path: https://api.INSTANCE/datasets/ACCOUNT/DATASET/services/SERVICE/search Reply format \u00b6 The reply format is a JSON object. Search results are returned in the JSON array that is stored under key sequence \"hits\"/\"hits\" . The order in which search results appear in the array is meaningful: better matches appear earlier. Every search result is represented by a JSON object. The name of the linked data entity is specified under key sequence \"_id\" . Properties of the linked data entity are stored as IRI keys. The values of these properties appear in a JSON array in order to allow more than one object term per predicate term (as is often the case in linked data). The following code snippet shows part of the reply for the below example request. The reply includes two results for search string \u201cmew\u201d, returning the Pok\u00e9mon Mew (higher ranked result) and Mewtwo (lower ranked result). { \"hits\": { \"hits\": [ { \"_id\": \"https://triply.cc/academy/pokemon/id/pokemon/mew\", \"http://open vocab org/terms/canonicalUri\": [ \"http://pokedex.dataincubator.org/pokemon/151\" ], \"https://triply cc/academy/pokemon/def/baseAttack\": [ 100 ], \"https://triply cc/academy/pokemon/def/name\": [ \"MEW\", \"MEW\", \"MEW\", \"MEW\", \"MEW\", \"\u30df\u30e5\u30a6\" ], \u2026 }, { \"_id\": \"https://triply.cc/academy/pokemon/id/pokemon/mewtwo\", \"http://open vocab org/terms/canonicalUri\": [ \"http://pokedex.dataincubator.org/pokemon/150\" ], \"https://triply cc/academy/pokemon/def/baseAttack\": [ 110 ], \"https://triply cc/academy/pokemon/def/name\": [ \"MEWTU\", \"MEWTWO\", \"MEWTWO\", \"MEWTWO\", \"MEWTWO\", \"\u30df\u30e5\u30a6\u30c4\u30fc\" ], \u2026 } ] }, \u2026 } Examples \u00b6 Simple search \u00b6 Perform a search for the string mew : curl 'https://api.triplydb.com/datasets/academy/pokemon/services/search/search?query=mew' Custom search \u00b6 Perform a search using the custom query: { \"query\": { \"simple_query_string\": { \"query\": \"pikachu\" } } } This request is issued in the following way with the cURL command-line tool: curl -X POST 'https://api.triplydb.com/datasets/academy/pokemon/services/search/search' \\ -d '{\"query\":{\"simple_query_string\":{\"query\":\"pikachu\"}}}' \\ -H 'content-type: application/json' Count API \u00b6 Elasticsearch allows the number of results to be determined without having to actually retrieve all these results. This is done with the \"Count API\". This API comes in handy when the number of results is shown in applications such as faceted search interfaces. The following two requests return the number of results for the search strings \"Iris\" and \"Setosa\". Notice that \"Iris\" occurs more often (184 times) than \"Setosa\" (52 times): curl 'https://api.triplydb.com/datasets/Triply/iris/services/iris-es/_count' -H 'Content-Type: application/json' --data-raw $'{\"query\": { \"simple_query_string\": { \"query\": \"Iris\" } } }' {\"count\":184,\"_shards\":{\"total\":1,\"successful\":1,\"skipped\":0,\"failed\":0}} and: curl 'https://api.triplydb.com/datasets/Triply/iris/services/iris-es/_count' -H 'Content-Type: application/json' --data-raw $'{\"query\": { \"simple_query_string\": { \"query\": \"Setosa\" } } }' {\"count\":52,\"_shards\":{\"total\":1,\"successful\":1,\"skipped\":0,\"failed\":0}} Setting up index templates for ElasticSearch \u00b6 TriplyDB allows you to configure a custom mapping for Elasticsearch services in TriplyDB using index templates. Index templates \u00b6 Index templates make it possible to create indices with user defined configuration, which an index can then pull from. A template will be defined with a name pattern and some configuration in it. If the name of the index matches the template\u2019s naming pattern, the new index will be created with the configuration defined in the template. Official documentation from ElasticSearch on how to use Index templates can be found here . Index templates on TriplyDB can be configured through either TriplyDB API or TriplyDB-JS . Index template can be created by making a POST request to the following URL: https://api.INSTANCE/datasets/ACCOUNT/DATASET/services/ with this body: { \"type\": \"elasticSearch\", \"name\": \"SERVICE_NAME\", \"config\": { \"indexTemplates\": [ { \"index_patterns\": \"index\", \"name\": \"TEMPLATE_NAME\", ... } ] } } index_patterns and name are obligatory fields to include in the body of index template. It's important that every index template has the field \"index_patterns\" equal \"index\" ! Below is the example of the post request: curl -H \"Authorization: Bearer TRIPLYDB_TOKEN\" -H \"Content-Type: application/json\" -d '{\"type\":\"elasticSearch\",\"name\":\"SERVICE_NAME\",\"config\":{\"indexTemplates\":[{\"index_patterns\":\"index\", \"name\": \"TEMPLATE_NAME\"}]}}' -X POST \"https://api.INSTANCE/datasets/ACCOUNT/DATASET/services/\" Component templates \u00b6 Component templates are building blocks for constructing index templates that specify index mappings, settings, and aliases. You can find the official documentation on their use in ElasticSearch here . They can be configured through either TriplyDB API or TriplyDB-JS . A component template can be created by making a POST request to the following URL: https://api.INSTANCE/datasets/ACCOUNT/DATASET/services/ with this body: { \"type\": \"elasticSearch\", \"name\": \"SERVICE_NAME\", \"config\": { \"componentTemplates\": [ { \"name\": \"TEMPLATE_NAME\", \"template\": { \"mappings\": { \"properties\": { ... } } } ... } ] } } name and template are obligatory fields to include in the body of component template. Component template can only be created together with an index template. In this case Index template needs to contain the field composed_of with the name of the component template. Below is an example of a POST request to create a component template for the property https://schema.org/dateCreated to be of type date . curl -H \"Authorization: Bearer TRIPLYDB_TOKEN\" -H \"Content-Type: application/json\" -d '{\"type\":\"elasticSearch\",\"name\":\"SERVICE_NAME\",\"config\":{\"indexTemplates\":[{\"index_patterns\":\"index\", \"name\": \"INDEX_TEMPLATE_NAME\",\"composed_of\":[\"COMPONENT_TEMPLATE_NAME\"]}], \"componentTemplates\":[{\"name\":\"COMPONENT_TEMPLATE_NAME\",\"template\":{\"mappings\":{\"properties\":{\"https://schema org/dateCreated\":{\"type\":\"date\"}}}}}]}}' -X POST \"https://api.INSTANCE/datasets/ACCOUNT/DATASET/services/\"","title":"API"},{"location":"triply-api/#triply-api","text":"Each Triply instance has a fully RESTful API. All functionality, from managing the Triply instance to working with your data, is done through the API. This document describes the general setup of the API, contact support@triply.cc for more information.","title":"Triply API"},{"location":"triply-api/#authentication","text":"When a dataset is published publicly, most of the read operation on that dataset can be performed without authentication. Write operations and read operations on datasets that are published internally or privately require authentication.","title":"Authentication"},{"location":"triply-api/#creating-an-api-token","text":"Authentication is implemented through API tokens. An API token can be created within the TriplyDB UI in the following way: Log into your TriplyDB instance. Click on the user menu in the top-right corner and click on \u201cUser settings\u201d. Go to the \u201cAPI tokens\u201d tab. Click the \u201cCreate token\u201d button, enter a description for the token (e.g., \u201ctest-token\u201d) and select the appropriate access rights. Click on \u201cCreate\u201d and copy the created API token (a lengthy string of characters). This string is only shown once, upon creation, and must not be shared with others. (Other users can create their own token in the here described way.)","title":"Creating an API token"},{"location":"triply-api/#using-the-api-token","text":"API tokens are used by specifying them in an HTTP request header as follows: Authorization: Bearer TOKEN In the above, TOKEN should be replaced by your personal API token (a lengthy sequence of characters). See Creating an API token for information on how to create an API token.","title":"Using the API token"},{"location":"triply-api/#important-security-considerations","text":"Do Not Commit Your Token to a Git Repository : Under no circumstances should you commit your TriplyDB token to a Git repository. This practice is not allowed according to our ISO standards. Do Not Share Your Token: Avoid sharing your TriplyDB token with anyone who should not have access to your TriplyDB resources . Tokens should be treated as sensitive information and shared only with trusted parties. Change Tokens Regularly : To enhance security, consider regularly generating a new token to replace the existing one especially if you suspect any compromise.","title":"Important Security Considerations"},{"location":"triply-api/#exporting-linked-data","text":"Every TriplyDB API path that returns linked data provides a number of serializations to choose from. We support the following serializations: Serialization Media type File extension TriG application/trig .trig N-Triples application/n-triples .nt N-Quads application/n-quads .nq Turtle text/turtle .ttl JSON-LD application/ld+json .jsonld To request a serialization, use one of the following mechanisms: Add an Accept header to the request. E.g. Accept: application/n-triples Add the extension to the URL path. E.g. https://api.triplydb.com/datasets/Triply/iris/download.nt","title":"Exporting linked data"},{"location":"triply-api/#datasets","text":"Triply API requests are always directed towards a specific URI path. URI paths will often have the following form: https://api.INSTANCE/datasets/ACCOUNT/DATASET/ Upper-case letter words must be replaced by the following values: INSTANCE :: The host name of the TriplyDB instance that you want to use. ACCOUNT :: The name of a specific user or a specific organization. DATASET :: The name of a specific dataset. Here is an example of a URI path that points to the Triply API for the Pok\u00e9mon dataset: https://api.triplydb.com/datasets/academy/pokemon/","title":"Datasets"},{"location":"triply-api/#create-a-dataset","text":"You can create a new dataset via the Triply API. You need to use the API Token and send an HTTP POST request with data specifying: name , accessLevel and displayName . The example of the URI: curl -H 'Authorization: Bearer TOKEN' -H 'Content-Type: application/json' -X POST https://api.INSTANCE/datasets/ACCOUNT/ -d '{\"name\": \"NAME\", \"accessLevel\": \"ACCESS_LEVEL\", \"displayName\": \"DISPLAY_NAME\"}' Upper-case letter words in json after -d must be replaced by the following values: NAME :: The name of the dataset in the url. ACCESS_LEVEL :: public , private or internal . For more information visit Access levels in TriplyDB . DISPLAY_NAME :: The display name of the dataset.","title":"Create a dataset"},{"location":"triply-api/#upload-linked-data","text":"You can upload linked data via the Triply API. You need to use the API Token and send an HTTP POST request with data specifying the local file path. The list of supported file extensions can be checked in Adding data: File upload documentation. The example of such a request: curl -H 'Authorization: Bearer TOKEN' -X POST https://api.INSTANCE/datasets/ACCOUNT/DATASET/jobs -F file=@FILENAME Upper-case letter words must be replaced by the following values: TOKEN :: Your TriplyDB token. INSTANCE :: The domain of your instance ACCOUNT :: Your account name DATASET :: The dataset name FILENAME :: The path to the file you want to upload A request looks like this: curl -H 'Authorization: Bearer xxxxxx' -X POST https://api.triplydb.com/datasets/my-account-name/my-dataset-name/jobs -F file=@./myfile.trig Limitations : We only support this API route for uploads less than 5MB. To upload more data, use: TriplyDB-JS : See the importFrom* methods under the Dataset class . TriplyDB Command-line Interface","title":"Upload linked data"},{"location":"triply-api/#upload-assets","text":"You can upload assets via the Triply API. You need to use the API Token and send an HTTP POST request with data specifying the local file path. To add a new asset: curl -H \"Authorization: Bearer TOKEN\" -X POST https://api.INSTANCE/datasets/ACCOUNT/DATASET/assets -F file=@FILENAME To add a version to an existing asset: curl -H \"Authorization: Bearer TOKEN\" -X POST https://api.INSTANCE/datasets/ACCOUNT/DATASET/assets/IDENTIFIER -F file=@FILENAME Upper-case letter words must be replaced by the following values: TOKEN :: Your TriplyDB token. INSTANCE :: The domain of your instance ACCOUNT :: Your account name DATASET :: The dataset name IDENTIFIER :: The asset identifier where you'd like to add a new version to. FILENAME :: The path to the file you want to upload This request returns a JSON object, that includes (among other things) an identifier key, which can be used as a persistent identifier for this asset. A request to add a new asset looks like this: curl -H 'Authorization: Bearer xxxxxx' -X POST https://api.triplydb.com/datasets/my-account-name/my-dataset-name/assets -F file=@./myfile.txt A request to add a version to an existing asset looks like this: curl -H 'Authorization: Bearer xxxxxx' -X POST https://api.triplydb.com/datasets/my-account-name/my-dataset-name/assets/yyyyy -F file=@./myfile.txt Limitations : We only support this API route for uploads less than 5MB. To upload more data, use: TriplyDB-JS : See the uploadAsset methods under the Dataset class . TriplyDB Command-line Interface","title":"Upload assets"},{"location":"triply-api/#accounts","text":"Information about TriplyDB accounts (organizations and users) can be retrieved from the following API path: https://api.INSTANCE/accounts Upper-case letter words must be replaced by the following values: INSTANCE :: The host name of the TriplyDB instance that you want to use. Here is an example of a URI path that points to the Triply API for the Triply organization account: https://api.triplydb.com/accounts/Triply","title":"Accounts"},{"location":"triply-api/#queries","text":"TriplyDB allows users to save SPARQL queries. The metadata for all saved query can be accessed as follows: https://api.triplydb.com/queries By adding an account name (for example: 'Triply'), metadata for all saved queries for that account can be accessed as follows: https://api.triplydb.com/queries/Triply By adding an account name and a query name (for example: 'Triply/flower-length'), metadata for one specific saved query can be accessed as follows: https://api.triplydb.com/queries/Triply/flower-length","title":"Queries"},{"location":"triply-api/#query-metadata-grlc","text":"You can retrieve a text-based version of each query, by requesting the text/plain content type: curl -vL -H 'Accept: text/plain' 'https://api.triplydb.com/queries/JD/pokemonNetwork' This returns the query string, together with metadata annotations. These metadata annotations use the GRLC format . For example: #+ description: This query shows a small subgraph from the Pokemon dataset. #+ endpoint: https://api.triplydb.com/datasets/academy/pokemon/services/pokemon/sparql #+ endpoint_in_url: false construct where { ?s ?p ?o. } limit 100 Notice that the GRLC annotations are encoded in SPARQL comments, i.e. lines that start with the hash character ( # ). This makes the result immediately usable as a SPARQL query. The above example includes the following GRLC annotations: description gives a human-readable description of the meaning of the query. This typically includes an explanation of the purpose or goal for which this query is used, the content returned, or the process or task in which this query is used. endpoint The URL of the SPARQL endpoint where queries are sent to. endpoint_in_url configures whether the URL of the SPARQL endpoint should be specified through the API. In TriplyDB, this configuration is by default set to false . (Users of the RESTful API typically expect domain parameters such as countryName or maximumAge , but they do not necessarily expect technical parameters like an endpoint URL.)","title":"Query metadata (GRLC)"},{"location":"triply-api/#ld-browser-api","text":"Triply APIs provide a convenient way to access data used by LD Browser , which offers a comprehensive overview of a specific IRI. By using Triply API for a specific IRI, you can retrieve the associated 'document' in the .nt format that describes the IRI. To make an API request for a specific instance, you can use the following URI path: https://api.triplydb.com/datasets/ACCOUNT/DATASET/describe.nt?resource=RESOURCE To illustrate this, let's take the example of the DBpedia dataset and the specific instance of 'Mona Lisa' . If you use this URI path: https://api.triplydb.com/datasets/DBpedia-association/dbpedia/describe.nt?resource=http%3A%2F%2Fdbpedia.org%2Fresource%2FMona_Lisa In your browser, the .nt document describing the 'Mona Lisa' instance will be automatically downloaded. You can then upload this file to a dataset and visualize it in a graph . Figure 1 illustrates the retrieved graph for the \u2018Mona Lisa\u2019 instance. The requested resource will be displayed in the center of the graph, forming an 'ego graph'. It will include all direct properties, as well as some indirect properties that are also pulled in by LD Browser. The labels for all classes and properties will be included for easy human-readable display. In addition, this API also supports traversing blank node-replacing well-known IRIs (CBD style), and limits the number of objects per subject/property to manage the description size. This corresponds to the \"Show more\" button in the LD Browser GUI, ensuring a manageable and user-friendly experience.","title":"LD Browser API"},{"location":"triply-api/#triple-pattern-fragments-tpf","text":"Triple Pattern Fragments (TPF) is a community standard that allows individual linked datasets to be queried for Triple Patterns (TP), a subset of the more complex SPARQL query language. The Triply API implements Triple Pattern Fragments version 2019-01-18 and Linked Data Fragments version 2016-06-05. The Triple Pattern Fragments (TPF) API is available for all datasets in Triply and does not require running a dedicated service.","title":"Triple Pattern Fragments (TPF)"},{"location":"triply-api/#uri-path","text":"TPF requests are sent to the following URI path: https://api.INSTANCE/datasets/ACCOUNT/DATATSET/fragments","title":"URI path"},{"location":"triply-api/#reply-format","text":"Since TPF replies distinguish between data and metadata that are stored in different graphs, it is recommended to request the TriG content type with the following HTTP request header: Accept: application/trig","title":"Reply format"},{"location":"triply-api/#query-parameters","text":"Triple Pattern Fragments (TPF) uses the following query parameters in order to retrieve only those triples that adhere to a specified Triple Pattern: Key Value Purpose subject A URL-encoded IRI. Only return triples where the given IRI appears in the subject position. predicate A URL-encoded IRI. Only return triples where the given IRI appears in the predicate position. object A URL-encoded IRI or literal. Only return triples where the given IRI or literal appears in the object position.","title":"Query parameters"},{"location":"triply-api/#example-request","text":"curl -G \\ 'https://api.triplydb.com/datasets/academy/pokemon/fragments' \\ --data-urlencode 'predicate=http://www.w3.org/2000/01/rdf-schema#label' \\ -H 'Accept: application/trig'","title":"Example request"},{"location":"triply-api/#exporting-data","text":"To export the linked data, use the following path: https://api.INSTANCE/datasets/ACCOUNT/DATATSET/download","title":"Exporting data"},{"location":"triply-api/#query-parameters_1","text":"By default, an export includes all linked data graphs. Use a query argument to specify a particular graph. Key Value Purpose graph A URL-encoded IRI. Only download the export of the given graph IRI. Therefore, to export the linked data of a graph , use the following path: https://api.INSTANCE/datasets/ACCOUNT/DATATSET/download/?graph=GRAPH To find out which graphs are available, use the following path: https://api.INSTANCE/datasets/ACCOUNT/DATATSET/graphs","title":"Query parameters"},{"location":"triply-api/#example-requests","text":"Export a dataset: curl 'https://api.triplydb.com/datasets/academy/pokemon/download' \\ -H 'Accept: application/trig' > exportDataset.trig.gz Export a graph: First, find out which graphs are available: curl 'https://api.triplydb.com/datasets/academy/pokemon/graphs' Then, download one of the graph: curl 'curl 'https://api.triplydb.com/datasets/academy/pokemon/download?graph=https://triplydb.com/academy/pokemon/graphs/data' -H 'Accept: application/trig' > exportGraph.trig.gz","title":"Example requests"},{"location":"triply-api/#services","text":"Some API requests require the availability of a specific service over the dataset. These requests are directed towards a URI path of the following form: https://api.INSTANCE/datasets/ACCOUNT/DATASET/services/SERVICE/ Upper-case letter words must be replaced by the following values: SERVICE :: The name of a specific service that has been started for the corresponding dataset. See the previous section for Datasets to learn the meaning of INSTANCE , ACCOUNT , and DATASET . Here is an example of a URI path that points to a SPARQL endpoint over the Pok\u00e9mon dataset: https://api.triplydb.com/datasets/academy/pokemon/services/pokemon/ See the following sections for more information on how to query the endpoints provided by services: SPARQL Elasticsearch","title":"Services"},{"location":"triply-api/#create-a-service","text":"You can create a service for a dataset via TriplyDB API. You need to use the API Token and send an HTTP POST request with data specifying: \"type\" and \"name\" . The example of the URI: curl -H 'Authorization: Bearer TOKEN' -H 'Content-Type: application/json' -X POST https://api.INSTANCE/datasets/ACCOUNT/DATASET/services/ -d '{\"type\": \"TYPE\", \"name\": \"NAME\"}' Upper-case letter words in json after -d must be replaced by the following values: TYPE :: SPARQL ( virtuoso or jena ) or Elasticsearch NAME :: The name of the service","title":"Create a service"},{"location":"triply-api/#synchronize-a-service","text":"You can synchronize existing service for a dataset via TriplyDB API. You need to use the API Token and send an HTTP POST request with data: {\"sync\": \"true\"} The example of the URI: curl -H 'Authorization: Bearer TOKEN' -H 'Content-Type: application/json' -X POST https://api.INSTANCE/datasets/ACCOUNT/DATASET/services/SERVICE -d '{\"sync\": \"true\"}'","title":"Synchronize a service"},{"location":"triply-api/#sparql","text":"There are two service types in TriplyDB that expose the SPARQL 1.1 Query Language: \"Sparql\" and \"Jena\". The former works well for large quantities of instance data with a relatively small data model; the latter works well for smaller quantities of data with a richer data model. SPARQL services expose a generic endpoint URI at the following location (where ACCOUNT , DATASET and SERVICE are user-chosen names): https://api.triplydb.com/datasets/ACCOUNT/DATASET/services/SERVICE/sparql Everybody who has access to the dataset also has access to its services, including its SPARQL services: - For Public datasets, everybody on the Internet or Intranet can issue queries. - For Internal datasets, only users that are logged into the triple store can issue queries. - For Private datasets, only users that are logged into the triple store and are members of ACCOUNT can issue queries. Notice that for professional use it is easier and better to use saved queries . Saved queries have persistent URIs, descriptive metadata, versioning, and support for reliable large-scale pagination ( see how to use pagination with saved query API ). Still, if you do not have a saved query at your disposal and want to perform a custom SPARQL request against an accessible endpoint, you can do so. TriplyDB implements the SPARQL 1.1 Query Protocol standard for this purpose.","title":"SPARQL"},{"location":"triply-api/#sending-a-sparql-query-request","text":"According to the SPARQL 1.1 Protocol, queries can be send in the 3 different ways that are displayed in Table 1 . For small query strings it is possible to send an HTTP GET request (row 1 in Table 1 ). A benefit of this approach is that all information is stored in one URI. For public data, copy/pasting this URI in a web browser runs the query. For larger query strings it is required to send an HTTP POST request (rows 2 and 3 in Table 1 ). The reason for this is that longer query strings result in longer URIs when following the HTTP GET approach. Some applications do not support longer URIs, or they even silently truncate them resulting in an error down the line. The direct POST approach (row 3 in Table 1 ) is the best of these 3 variants, since it most clearly communicates that it is sending a SPARQL query request (see the Content-Type column). HTTP Method Query String Parameters Request Content-Type Request Message Body query via GET GET query (exactly 1) default-graph-uri (0 or more) named-graph-uri (0 or more) none none query via URL-encoded POST POST none application/x-www-form-urlencoded URL-encoded, ampersand-separated query parameters. query (exactly 1) default-graph-uri (0 or more) named-graph-uri (0 or more) query via POST directly POST default-graph-uri (0 or more) named-graph-uri (0 or more) application/sparql-query Unencoded SPARQL query string Table 1 - Overview of the three different ways in which SPARQL queries can be issues over HTTP.","title":"Sending a SPARQL Query request"},{"location":"triply-api/#sparql-query-result-formats","text":"SPARQL services are able to return results in different formats. The user can specify the preferred format by specifying the corresponding Media Type in the HTTP Accept header. TriplyDB supports the Media Types in the following table. Notice that the chosen result format must be supported for your query form. Alternatively, it is possible (but not preferred) to specify the requested format as an URI path suffix; see the GET request section for an example. Result format Media Type Query forms Suffix CSV text/csv Select .csv JSON application/json Ask, Select .json JSON-LD application/ld+json Construct, Describe .jsonld N-Quads application/n-quads Construct, Describe .nq N-Triples application/n-triples Construct, Describe .nt SPARQL JSON application/sparql-results+json Ask, Select .srj SPARQL XML application/sparql-results+xml Ask, Select .srx TriG application/trig Construct, Describe .trig TSV text/tab-separated-values Select .tsv Turtle text/turtle Construct, Describe .ttl","title":"SPARQL Query result formats"},{"location":"triply-api/#examples-of-sparql-query-requests","text":"This section contains examples of SPARQL HTTP requests. The requests run either of the following two SPARQL queries against a public SPARQL endpoint that contains data about Pokemon: select * { ?s ?p ?o. } limit 1 construct where { ?s ?p ?o. } limit 1 The examples made use of the popular command-line tool cURL . These examples should also work in any other HTTP client tool or library.","title":"Examples of SPARQL Query requests"},{"location":"triply-api/#get-request","text":"curl https://api.triplydb.com/datasets/academy/pokemon/services/pokemon/sparql?query=select%20%2A%20%7B%20%3Fs%20%3Fp%20%3Fo.%20%7D%20limit%201 Result: [ { \"s\": \"https://triplydb.com/academy/pokemon/vocab/\", \"p\": \"http://www.w3.org/1999/02/22-rdf-syntax-ns#type\", \"o\": \"http://www.w3.org/2002/07/owl#Ontology\" } ] The following request is identical to the previous one, but adds the \".srj\" suffix to the URI path (see /sparql.srj ). All suffixes from the table in Section SPARQL Query result formats are supported. curl https://api.triplydb.com/datasets/academy/pokemon/services/pokemon/sparql.srj?query=select%20%2A%20%7B%20%3Fs%20%3Fp%20%3Fo.%20%7D%20limit%201 This returns the official SPARQL Result Set JSON (SRJ) format. Notice that this official format is more verbose than the standard JSON format: { \"head\": { \"link\": [], \"vars\": [ \"s\", \"p\", \"o\" ] }, \"results\": { \"bindings\": [ { \"s\": { \"type\": \"uri\", \"value\": \"https://triplydb.com/academy/pokemon/\" }, \"p\": { \"type\": \"uri\", \"value\": \"http://www.w3.org/1999/02/22-rdf-syntax-ns#type\" }, \"o\": { \"type\": \"uri\", \"value\": \"http://rdfs.org/ns/void#Dataset\" } } ] } }","title":"GET request"},{"location":"triply-api/#url-encoded-post-request","text":"curl -X POST https://api.triplydb.com/datasets/academy/pokemon/services/pokemon/sparql \\ -H 'Content-Type: application/x-www-form-urlencoded' \\ --data query=select%20%2A%20%7B%20%3Fs%20%3Fp%20%3Fo.%20%7D%20limit%201 Result: [ { \"s\": \"https://triplydb.com/academy/pokemon/vocab/\", \"p\": \"http://www.w3.org/1999/02/22-rdf-syntax-ns#type\", \"o\": \"http://www.w3.org/2002/07/owl#Ontology\" } ]","title":"URL-encoded POST request"},{"location":"triply-api/#direct-post-request","text":"curl -X POST https://api.triplydb.com/datasets/academy/pokemon/services/pokemon/sparql \\ -H 'Content-Type: application/sparql-query' \\ -d 'select * { ?s ?p ?o } limit 1' Result: [ { \"s\": \"https://triplydb.com/academy/pokemon/vocab/\", \"p\": \"http://www.w3.org/1999/02/22-rdf-syntax-ns#type\", \"o\": \"http://www.w3.org/2002/07/owl#Ontology\" } ]","title":"Direct POST request"},{"location":"triply-api/#sparql-json","text":"Like the previous example, but with an Accept header that specifies Media Type application/sparql-results+json : curl -X POST https://api.triplydb.com/datasets/academy/pokemon/services/pokemon/sparql \\ -H 'Accept: application/sparql-results+json' \\ -H 'Content-Type: application/sparql-query' \\ -d 'select * { ?s ?p ?o } limit 1' Result: { \"head\": { \"vars\": [\"s\", \"p\", \"o\"] }, \"results\": { \"bindings\": [ { \"s\": { \"type\": \"uri\", \"value\": \"https://triplydb.com/academy/pokemon/vocab/\" }, \"p\": { \"type\": \"uri\", \"value\": \"http://www.w3.org/1999/02/22-rdf-syntax-ns#type\" }, \"o\": { \"type\": \"uri\", \"value\": \"http://www.w3.org/2002/07/owl#Ontology\" } } ] } }","title":"SPARQL JSON"},{"location":"triply-api/#sparql-xml","text":"Like the previous example, but with Media Type application/sparql-results+xml in the Accept header: curl -X POST https://api.triplydb.com/datasets/academy/pokemon/services/pokemon/sparql \\ -H 'Accept: application/sparql-results+xml' \\ -H 'Content-Type: application/sparql-query' \\ -d 'select * { ?s ?p ?o } limit 1' Result: <sparql xmlns=\"http://www.w3.org/2005/sparql-results#\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:schemaLocation=\"http://www.w3.org/2001/sw/DataAccess/rf1/result2.xsd\"> <head> <variable name=\"s\"/> <variable name=\"p\"/> <variable name=\"o\"/> </head> <results distinct=\"false\" ordered=\"true\"> <result> <binding name=\"s\"> <uri>https://triplydb.com/academy/pokemon/vocab/</uri> </binding> <binding name=\"p\"> <uri>http://www.w3.org/1999/02/22-rdf-syntax-ns#type</uri> </binding> <binding name=\"o\"> <uri>http://www.w3.org/2002/07/owl#Ontology</uri> </binding> </result> </results> </sparql>","title":"SPARQL XML"},{"location":"triply-api/#sparql-tab-separated-values","text":"Like the previous examples, but with Media Type text/tab-separated-values in the Accept header: curl -X POST https://api.triplydb.com/datasets/academy/pokemon/services/pokemon/sparql \\ -H 'Accept: text/tab-separated-values' \\ -H 'Content-Type: application/sparql-query' \\ -d 'select * { ?s ?p ?o } limit 1' \"s\" \"p\" \"o\" \"https://triplydb.com/academy/pokemon/vocab/\" \"http://www.w3.org/1999/02/22-rdf-syntax-ns#type\" \"http://www.w3.org/2002/07/owl#Ontology\"","title":"SPARQL tab-separated values"},{"location":"triply-api/#sparql-comma-separated-values","text":"Like the previous examples, but with Media Type text/csv in the Accept header: curl -X POST https://api.triplydb.com/datasets/academy/pokemon/services/pokemon/sparql \\ -H 'Accept: text/csv' \\ -H 'Content-Type: application/sparql-query' \\ -d 'select * { ?s ?p ?o } limit 1' Result: \"s\",\"p\",\"o\" \"https://triplydb.com/academy/pokemon/vocab/\",\"http://www.w3.org/1999/02/22-rdf-syntax-ns#type\",\"http://www.w3.org/2002/07/owl#Ontology\"","title":"SPARQL comma-separated values"},{"location":"triply-api/#json-ld","text":"Like the previous examples, but with a SPARQL construct query and Media Type application/ld+json in the Accept header: curl -X POST https://api.triplydb.com/datasets/academy/pokemon/services/pokemon/sparql \\ -H 'Accept: application/ld+json' \\ -H 'Content-Type: application/sparql-query' \\ -d 'construct where { ?s ?p ?o } limit 1' Result: { \"@graph\": [ { \"@id\": \"https://triplydb.com/academy/pokemon/vocab/\", \"@type\": \"http://www.w3.org/2002/07/owl#Ontology\" } ] }","title":"JSON-LD"},{"location":"triply-api/#n-quads","text":"Like the previous examples, but with Media Type application/n-quads in the Accept header: curl -X POST https://api.triplydb.com/datasets/academy/pokemon/services/pokemon/sparql \\ -H 'Accept: application/n-quads' \\ -H 'Content-Type: application/sparql-query' \\ -d 'construct where { ?s ?p ?o } limit 1' Result: { \"@graph\": [ { \"@id\": \"https://triplydb.com/academy/pokemon/vocab/\", \"@type\": \"http://www.w3.org/2002/07/owl#Ontology\" } ] }","title":"N-Quads"},{"location":"triply-api/#n-triples","text":"Like the previous examples, but with Media Type application/n-triples in the Accept header: curl -X POST https://api.triplydb.com/datasets/academy/pokemon/services/pokemon/sparql \\ -H 'Accept: application/n-triples' \\ -H 'Content-Type: application/sparql-query' \\ -d 'construct where { ?s ?p ?o } limit 1' Result: <https://triplydb.com/academy/pokemon/vocab/> <http://www.w3.org/1999/02/22-rdf-syntax-ns#type> <http://www.w3.org/2002/07/owl#Ontology> .","title":"N-Triples"},{"location":"triply-api/#trig","text":"Like the previous examples, but with Media Type application/trig in the Accept header: curl -X POST https://api.triplydb.com/datasets/academy/pokemon/services/pokemon/sparql \\ -H 'Accept: application/trig' \\ -H 'Content-Type: application/sparql-query' \\ -d 'construct where { ?s ?p ?o } limit 1' Result: @prefix rdf: <http://www.w3.org/1999/02/22-rdf-syntax-ns#> . @prefix owl: <http://www.w3.org/2002/07/owl#> . <https://triplydb.com/academy/pokemon/vocab/> rdf:type owl:Ontology .","title":"TriG"},{"location":"triply-api/#turtle","text":"Like the previous examples, but with Media Type text/turtle in the Accept header: curl -X POST https://api.triplydb.com/datasets/academy/pokemon/services/pokemon/sparql \\ -H 'Accept: text/turtle' \\ -H 'Content-Type: application/sparql-query' \\ -d 'construct where { ?s ?p ?o } limit 1' Result: @prefix rdf: <http://www.w3.org/1999/02/22-rdf-syntax-ns#> . @prefix owl: <http://www.w3.org/2002/07/owl#> . <https://triplydb.com/academy/pokemon/vocab/> rdf:type owl:Ontology .","title":"Turtle"},{"location":"triply-api/#graphql","text":"Some TriplyDB instances publish a GraphQL endpoint for every dataset. This endpoint can be used for GraphQL queries. It uses information from user-provided SHACL shapes to generate the GraphQL schema. See more information about this subject here .","title":"GraphQL"},{"location":"triply-api/#uri-path_1","text":"GraphQL requests are sent to the following URI path: https://api.INSTANCE/datasets/ACCOUNT/DATASET/graphql","title":"URI path"},{"location":"triply-api/#requests-and-response","text":"The format of requests and corresponding responses are described by graphql.org","title":"Requests and Response"},{"location":"triply-api/#example","text":"Perform a search using the custom query: { \"query\": { \"{ CapitalConnection { edges { node { label } } } }\" } } This request is issued in the following way with the cURL command-line tool: curl -X POST https://api.triplydb.com/datasets/iish/cshapes/graphql \\ -d '{ \"query\":\"{ CapitalConnection { edges { node { label } } } }\"}' \\ -H \"Content-Type: application/json\"","title":"Example"},{"location":"triply-api/#elasticsearch","text":"The text search API returns a list of linked data entities based on a supplied text string. The text string is matched against the text in literals and IRIs that appear in the linked data description of the returned entities. The text search API is only available for a dataset after an Elasticsearch service has been created for that dataset. Two types of searches can be performed: a simple search, and a custom search. Simple searches require one search term for a fuzzy match. Custom searches accept a JSON object conforming to the Elasticsearch query DSL .","title":"Elasticsearch"},{"location":"triply-api/#uri-path_2","text":"Text search requests are sent to the following URI path: https://api.INSTANCE/datasets/ACCOUNT/DATASET/services/SERVICE/search","title":"URI path"},{"location":"triply-api/#reply-format_1","text":"The reply format is a JSON object. Search results are returned in the JSON array that is stored under key sequence \"hits\"/\"hits\" . The order in which search results appear in the array is meaningful: better matches appear earlier. Every search result is represented by a JSON object. The name of the linked data entity is specified under key sequence \"_id\" . Properties of the linked data entity are stored as IRI keys. The values of these properties appear in a JSON array in order to allow more than one object term per predicate term (as is often the case in linked data). The following code snippet shows part of the reply for the below example request. The reply includes two results for search string \u201cmew\u201d, returning the Pok\u00e9mon Mew (higher ranked result) and Mewtwo (lower ranked result). { \"hits\": { \"hits\": [ { \"_id\": \"https://triply.cc/academy/pokemon/id/pokemon/mew\", \"http://open vocab org/terms/canonicalUri\": [ \"http://pokedex.dataincubator.org/pokemon/151\" ], \"https://triply cc/academy/pokemon/def/baseAttack\": [ 100 ], \"https://triply cc/academy/pokemon/def/name\": [ \"MEW\", \"MEW\", \"MEW\", \"MEW\", \"MEW\", \"\u30df\u30e5\u30a6\" ], \u2026 }, { \"_id\": \"https://triply.cc/academy/pokemon/id/pokemon/mewtwo\", \"http://open vocab org/terms/canonicalUri\": [ \"http://pokedex.dataincubator.org/pokemon/150\" ], \"https://triply cc/academy/pokemon/def/baseAttack\": [ 110 ], \"https://triply cc/academy/pokemon/def/name\": [ \"MEWTU\", \"MEWTWO\", \"MEWTWO\", \"MEWTWO\", \"MEWTWO\", \"\u30df\u30e5\u30a6\u30c4\u30fc\" ], \u2026 } ] }, \u2026 }","title":"Reply format"},{"location":"triply-api/#examples","text":"","title":"Examples"},{"location":"triply-api/#simple-search","text":"Perform a search for the string mew : curl 'https://api.triplydb.com/datasets/academy/pokemon/services/search/search?query=mew'","title":"Simple search"},{"location":"triply-api/#custom-search","text":"Perform a search using the custom query: { \"query\": { \"simple_query_string\": { \"query\": \"pikachu\" } } } This request is issued in the following way with the cURL command-line tool: curl -X POST 'https://api.triplydb.com/datasets/academy/pokemon/services/search/search' \\ -d '{\"query\":{\"simple_query_string\":{\"query\":\"pikachu\"}}}' \\ -H 'content-type: application/json'","title":"Custom search"},{"location":"triply-api/#count-api","text":"Elasticsearch allows the number of results to be determined without having to actually retrieve all these results. This is done with the \"Count API\". This API comes in handy when the number of results is shown in applications such as faceted search interfaces. The following two requests return the number of results for the search strings \"Iris\" and \"Setosa\". Notice that \"Iris\" occurs more often (184 times) than \"Setosa\" (52 times): curl 'https://api.triplydb.com/datasets/Triply/iris/services/iris-es/_count' -H 'Content-Type: application/json' --data-raw $'{\"query\": { \"simple_query_string\": { \"query\": \"Iris\" } } }' {\"count\":184,\"_shards\":{\"total\":1,\"successful\":1,\"skipped\":0,\"failed\":0}} and: curl 'https://api.triplydb.com/datasets/Triply/iris/services/iris-es/_count' -H 'Content-Type: application/json' --data-raw $'{\"query\": { \"simple_query_string\": { \"query\": \"Setosa\" } } }' {\"count\":52,\"_shards\":{\"total\":1,\"successful\":1,\"skipped\":0,\"failed\":0}}","title":"Count API"},{"location":"triply-api/#setting-up-index-templates-for-elasticsearch","text":"TriplyDB allows you to configure a custom mapping for Elasticsearch services in TriplyDB using index templates.","title":"Setting up index templates for ElasticSearch"},{"location":"triply-api/#index-templates","text":"Index templates make it possible to create indices with user defined configuration, which an index can then pull from. A template will be defined with a name pattern and some configuration in it. If the name of the index matches the template\u2019s naming pattern, the new index will be created with the configuration defined in the template. Official documentation from ElasticSearch on how to use Index templates can be found here . Index templates on TriplyDB can be configured through either TriplyDB API or TriplyDB-JS . Index template can be created by making a POST request to the following URL: https://api.INSTANCE/datasets/ACCOUNT/DATASET/services/ with this body: { \"type\": \"elasticSearch\", \"name\": \"SERVICE_NAME\", \"config\": { \"indexTemplates\": [ { \"index_patterns\": \"index\", \"name\": \"TEMPLATE_NAME\", ... } ] } } index_patterns and name are obligatory fields to include in the body of index template. It's important that every index template has the field \"index_patterns\" equal \"index\" ! Below is the example of the post request: curl -H \"Authorization: Bearer TRIPLYDB_TOKEN\" -H \"Content-Type: application/json\" -d '{\"type\":\"elasticSearch\",\"name\":\"SERVICE_NAME\",\"config\":{\"indexTemplates\":[{\"index_patterns\":\"index\", \"name\": \"TEMPLATE_NAME\"}]}}' -X POST \"https://api.INSTANCE/datasets/ACCOUNT/DATASET/services/\"","title":"Index templates"},{"location":"triply-api/#component-templates","text":"Component templates are building blocks for constructing index templates that specify index mappings, settings, and aliases. You can find the official documentation on their use in ElasticSearch here . They can be configured through either TriplyDB API or TriplyDB-JS . A component template can be created by making a POST request to the following URL: https://api.INSTANCE/datasets/ACCOUNT/DATASET/services/ with this body: { \"type\": \"elasticSearch\", \"name\": \"SERVICE_NAME\", \"config\": { \"componentTemplates\": [ { \"name\": \"TEMPLATE_NAME\", \"template\": { \"mappings\": { \"properties\": { ... } } } ... } ] } } name and template are obligatory fields to include in the body of component template. Component template can only be created together with an index template. In this case Index template needs to contain the field composed_of with the name of the component template. Below is an example of a POST request to create a component template for the property https://schema.org/dateCreated to be of type date . curl -H \"Authorization: Bearer TRIPLYDB_TOKEN\" -H \"Content-Type: application/json\" -d '{\"type\":\"elasticSearch\",\"name\":\"SERVICE_NAME\",\"config\":{\"indexTemplates\":[{\"index_patterns\":\"index\", \"name\": \"INDEX_TEMPLATE_NAME\",\"composed_of\":[\"COMPONENT_TEMPLATE_NAME\"]}], \"componentTemplates\":[{\"name\":\"COMPONENT_TEMPLATE_NAME\",\"template\":{\"mappings\":{\"properties\":{\"https://schema org/dateCreated\":{\"type\":\"date\"}}}}}]}}' -X POST \"https://api.INSTANCE/datasets/ACCOUNT/DATASET/services/\"","title":"Component templates"},{"location":"triply-cli/","text":"On this page: TriplyDB Command-line Interface (CLI) TriplyDB Command-line Interface (CLI) \u00b6 The TriplyDB Command-line Interface (CLI) offers a more convenient way to upload files and assets to a TriplyDB instance. The latest version of the CLI for the respective OS can be found here: Linux Windows MacOS For more information about how to use the CLI, execute it with the --help argument. Please contact support@triply.cc for more information.","title":"Command-line Interface"},{"location":"triply-cli/#triplydb-command-line-interface-cli","text":"The TriplyDB Command-line Interface (CLI) offers a more convenient way to upload files and assets to a TriplyDB instance. The latest version of the CLI for the respective OS can be found here: Linux Windows MacOS For more information about how to use the CLI, execute it with the --help argument. Please contact support@triply.cc for more information.","title":"TriplyDB Command-line Interface (CLI)"},{"location":"triply-db-getting-started/","text":"On this page: TriplyDB Overview TriplyDB Overview \u00b6 TriplyDB allows you to store, share, and use linked data knowledge graphs. TriplyDB makes it easy to upload linked data and expose it through various APIs, including SPARQL, GraphQL, Elasticsearch, Linked Data Fragments, and REST. Learn more about the following features: Uploading data Sharing data Viewing data Exporting data Saved queries Data stories Admin settings","title":"Overview"},{"location":"triply-db-getting-started/#triplydb-overview","text":"TriplyDB allows you to store, share, and use linked data knowledge graphs. TriplyDB makes it easy to upload linked data and expose it through various APIs, including SPARQL, GraphQL, Elasticsearch, Linked Data Fragments, and REST. Learn more about the following features: Uploading data Sharing data Viewing data Exporting data Saved queries Data stories Admin settings","title":"TriplyDB Overview"},{"location":"triply-db-getting-started/admin-settings-pages/","text":"On this page: Admin settings Pages Overview page General overview Accounts overview Data overview Services overview Settings page Set logos and banner Setting metadata Setting contact email Setting example datasets Setting Starter dataset Setting Authentication Setting Site-wide prefixes Account overview page Add new user(s) Create a new user Datasets page Services page Redirects page How to setup a redirects for dereferencing Admin settings Pages \u00b6 You can use the console to perform administrator tasks. The administrator tasks are performed within the admin settings page. The admin settings pages are accessible by clicking on the user menu in the top-right corner and selecting the \u201cAdmin settings\u201d menu item. You must have administrator privileges to access these pages and perform administrator tasks. Overview page \u00b6 The first page that comes into view when opening the admin settings pages is the overview page. This page contains an overview of all the important statistics of the instance. The page also shows how close the instance is to hitting one or more limits. If no limit is set, the statistics are shown as a counter. If a limit is set a gauge is shown with a green, orange or red bar. The colors denote how far that statistic of the instance is to the limit. Green means not close to the limit, Orange means close to the limit, Red means over the limit. General overview \u00b6 The general overview gives an insight into the software version of the instance. Each instance consists of a console and an API. The console is the web interface of the instance and has a build date corresponding to the build date of the docker image of the console and a version number corresponding to the version of the docker image. The API is the layer between the console and the data. The API is separate from the console and is a different docker image. The API also has a version and build date of the docker image. Also contains a starting time, and an updated time, the moments when the docker image is started for this instance or when the docker image is updated for the instance. Accounts overview \u00b6 The accounts overview shows how many organizations and users are in this instance. The organizations and users are shown in a counter if no limit is set. If a limit is set on the number of organizations and/or users of the instance a gauge is shown. Data overview \u00b6 The data overview shows multiple statistics about datasets. The first counter shows the amount of datasets on the instance. The second and third counters show the amount of graphs and statements in all graphs. The fourth and fifth counters show the amount of unique graphs and statements. When a graph is copied from one dataset to another, the data in that graph does not change. The amount of unique data does not change either. The amount of unique data is a more representative way of calculating the amount of data in the instance. All statistics are shown in a counter, if no limit is set. If a limit is set on one of the statistics of the instance a gauge is shown. Services overview \u00b6 The data overview shows how multiple statistics about services. The first counter shows the total amount of services on the instance, The second counter shows the total amount of statements in all the services. Then for each of our service types a specific counter is created. Each containing the amount of services and the amount of statements in that service. All statistics are shown in a counter if no limit is set. If a limit is set on one of the statistics of the instance a gauge is shown. Settings page \u00b6 The settings page is the main page for administrators to institute instance wide changes. An administrator can change the site logo's here, change the contact email or update site wide prefixes. Set logos and banner \u00b6 For changing the logos and the banner follow the next steps: Click on the \u201cAdmin settings\u201d link in the user menu (top-right corner) and click the \"Settings\" tab. This brings up an overview of all the settings an admin can set for the TriplyDB instance. Under \"Site logos\" and \"Site banner\" you can upload a site logo (square and landscape) or a banner. The logo (preferably squared) is required to be SVG. In the UI the 30x30px image is displayed. The banner can be of any format, however, the WEBP is preferred. The image's resolution should be between 1920x500 and up to 4000x500. The banner is displayed at a height 500px. If the image is smaller than the browser screen size, the image will be stretched. Make sure you use files with a maximum size of 5 MB. Setting metadata \u00b6 For changing the metadata follow the next steps: 1. Click on the \u201cAdmin settings\u201d link in the user menu (top-right corner) and click the \"Settings\" tab. This brings up an overview of all the settings an admin can set for the TriplyDB instance. On that page navigate to \"Site metadata\", it looks as follows: Here you can set the name, tag line, description and welcome text. The name of your website appears in your browser tab. The welcome text appears on the homepage of your TriplyDB instance. The tagline and description are for metadata purposes (e.g. findability and website previews). Setting contact email \u00b6 For changing the contact email follow the next steps: 1. Click on the \u201cAdmin settings\u201d link in the user menu (top-right corner) and click the \"Settings\" tab. This brings up an overview of all the settings an admin can set for the TriplyDB instance. On that page navigate to \"Contact Email\". Here, you can change the contact email to a new contact email for the instance. Setting example datasets \u00b6 Example datasets are introduction datasets on the frontpage of your instance. The Example datasets are datasets that are interesting for people that visit your page to see and interact with. Most often you'll use open datasets to show them off on the frontpage. You can also use internal or private datasets, but these will only be visible if the person seeing them has the right access rights. For editing the example datasets follow the next steps: 1. Click on the \u201cAdmin settings\u201d link in the user menu (top-right corner) and click the \"Settings\" tab. This brings up an overview of all the settings an admin can set for the TriplyDB instance. On that page, navigate to \"Example datasets\". Here, you can execute the following changes: You can move datasets up and down in the order by clicking and holding your left mouse button over the three horizontal lines in front of the dataset name. You can then drag the selected dataset to their new spot. In the search field below the already added datasets you can add a new example dataset by typing in the search field and selecting the correct dataset. You can remove datasets by pressing the x on the right side of the dataset name to remove it from the example dataset list. Setting Starter dataset \u00b6 The starter dataset is a beginner-friendly linked dataset that can be an introduction into linked data when a user creates an account for the first time. The starter dataset is visible for a user when the user has not yet created a dataset on its own account. For editing the example datasets follow the next steps: 1. Click on the \u201cAdmin settings\u201d link in the user menu (top-right corner) and click the \"Settings\" tab. This brings up an overview of all the settings an admin can set for the TriplyDB instance. On that page navigate to \"Starter dataset\". Here you can change the starter dataset to a new starter dataset for the instance by typing in the search bar a name of an existing dataset to replace the started dataset. This dataset then will be presented to users on their account page, with an option to import(copy) them immediately. This needs to be a public dataset! If it's not public, new users will have to create a dataset. The starter dataset is only shown if the user currently has no datasets. Setting Authentication \u00b6 One of the roles of an administrator is to make sure only the right people will sign up for the TriplyDB instance. To do this, an administrator can set up authentication protocols. The authentication protocols can block people from signing up to instances where they are not allowed to sign up to. For changing the authentication protocols follow the next steps: 1. Click on the \u201cAdmin settings\u201d link in the user menu (top-right corner) and click the \"Settings\" tab. This brings up an overview of all the settings an admin can set for the TriplyDB instance. On that page navigate to \"Authentication\". Now you can change the password sign up. Allowing people to only register with a password or they are only allowed to register with a google or Github account. When password signup is enabled, the administrator can also set the permitted signup domains. Only users with e-mail addresses that match these domains are allowed to sign-up. Wildcards are allowed and domains are comma separated, for example: mydomain.com,*.mydomain.com. Setting Site-wide prefixes \u00b6 One of the advantages of using TriplyDB is that you can set site-wide prefixes once and use them everywhere on the instance. Site-wide prefixes are prefixes defined in the admin settings and can be used for all datasets that contain the IRIs matching the prefixes. For editing the side-wide prefixes follow the next steps: 1. Click on the \u201cAdmin settings\u201d link in the user menu (top-right corner) and click the \"Settings\" tab. This brings up an overview of all the settings an admin can set for the TriplyDB instance. On that page navigate to \"Site-wide prefixes\". Here, you can execute the following changes: Each field of the already added site-wide prefixes you can edit. You can edit the prefix label by typing in the first field. You can edit the prefix IRI and in the second field. Pressing UPDATE PREFIXES updates the list. In the last field below the already added site-wide prefixes you can add a new site-wide prefix by typing in the first field the prefix label, and in the second field the prefix IRI. Pressing UPDATE PREFIXES updates the list. You can remove prefixes by pressing the x on the right side of the prefixes name to remove it from the site-wide prefixes list. Account overview page \u00b6 The account page governs all the accounts of an instance. The paginated table shows all the accounts of the instance. The table is equipped with filters and sorting mechanisms to quickly search for and locate specific accounts according to search and filter criteria. The table also has a search field at the right side to quickly do wildcard searching. The table sorts all accounts automatically on the created at date with the latest created at date accounts first. The filters on top of the table can be used to filter the following columns: Name The name of the account, you can order the table based on the alphanumeric ordering, and filter based on the sequence of characters appearing in the filter. The name is also a URL that brings you to the location of the account. Type Type of the account, this can either be 'Organization' or 'User'. In the filter you can select a specific account type or 'All' account types. Display name The display name of the account, often an account has both a name and a display name. The display name is not limited to a specific set of characters, as it is not used as an URL. You can order the table based on the alphanumeric ordering, and filter based on the sequence of characters appearing in the filter. Email The email address of the account. You can order the table based on the alphanumeric ordering, and filter based on the sequence of characters appearing in the filter. Created at How long ago an account was created. When you hover over the text you can see the precise moment an account is created. You can order accounts based on the moment of creation. Updated at How long ago an account has been updated with new metadata such as display name or password. When you hover over the text you can see the precise moment an account is last updated. You can order accounts based on the moment of updated at time. Last activity How long ago the account has been last active. When you hover over the text you can see the precise moment an account was last active. You can order the accounts based on the moment of last time the account was active. Role Role of the account, this can either be 'light', 'regular' or 'administrator'. In the filter you can select a specific role or 'All' roles. Verified An account can be verified or not, to verify an account, the user needs to click on the verify button in the email. Or an administrator has verified the account in the account settings of that account. Only 'users' need to be verified. Disabled An account can be disabled or not, to disabled an account, the user needs to click on the disabled button in their user settings. Or an administrator has disabled the account in the account settings of that account. legal consent An account can have accepted the legal consent or not, to accept legal consent, the user needs to click on the accept legal consent either when creating an account or by checking it in the user settings. Only 'users' need to have accepted legal consent. For each account you can execute the following actions: Open account settings For each account, there is a button such that the administrator can directly go to the account settings of the user or organization. The account settings are behind the `cogwheel` button. Add new user(s) \u00b6 Go to the \u201cAccounts tab\u201d to receive an overview of all accounts on the TriplyDB instance. The type of account can be observed based on the following icons: Icon Account type organization user Create a new user \u00b6 New users can only be created by administrators by performing the following steps: Click on the \u201cAdmin settings\u201d link in the user menu (top-right corner) and click the \u201cAccounts\u201d tab. This brings up an overview of all users and organizations on the TriplyDB instance. Click the \u201cAdd user\u201d button. Fill in the user name and email address of the prospective user. The user name must consist of alphanumeric characters ( A-Za-z ) and hyphens ( - ). Click the \u201cAdd user\u201d button. This sends an account creation email to the prospective user, containing a link that allows them to log in. In addition to the above default procedure, the following two options are provided for user account creation: Temporary account : By default, user accounts do not expire. Sometimes it is useful to create a temporary account by specifying a concrete date in the \u201cAccount expiration date\u201d widget. Preset password : By default, a user can set her password after logging in for the first time by clicking on the link in the account creation email. When a password is entered in the \u201cPassword\u201d field, the user must enter this password in order to log in for the first time. Datasets page \u00b6 The account page governs all the datasets of an instance. The paginated table shows all the datasets of the instance. The table is equipped with filters and sorting mechanisms to quickly search for and locate specific datasets according to search and filter criteria. The table also has a search field at the right side to quickly do wildcard searching. The table sorts all datasets automatically on the created at date with the latest created at date datasets first. The filters on top of the table can be used to filter the following columns: Name The name of the dataset, you can order the table based on the alphanumeric ordering, and filter based on the sequence of characters appearing in the filter. The name is also a URL that brings you to the location of the dataset. Access level Access level of the dataset, this can either be 'Public', 'Internal' or 'Private'. In the filter you can select a specific access level or 'All' access levels. Display name The display name of the dataset, often a dataset has both a name and a display name. The display name is not limited to a specific set of characters, as it is not used as an URL. You can order the table based on the alphanumeric ordering, and filter based on the sequence of characters appearing in the filter. Owner The owner of the dataset. The owner is a URL and brings you to the overview page of the owner. The owners can be filtered based on the sequence of characters appearing in the filter. Graph count The amount of graphs in a dataset. These are all the total amount of graphs in a dataset, and can be filtered with the slider. Statement count The amount of statements in a dataset. These are all the statements of all the graphs, and can be filtered with the slider. Service count The amount of services in a dataset. These can be filtered with the slider. Asset count The amount of assets in a dataset. These can be filtered with the slider. Created at How long ago a dataset has been created. When you hover over the text you can see the precise moment a dataset is created. You can order datasets based on the moment of creation. Updated at How long ago a dataset has been updated with new metadata such as display name or new data. When you hover over the text you can see the precise moment an account is last updated. You can order dataset based on the moment of updated at time. Last graph edit How long ago the last graph has been edited, either new data is uploaded or removed, or the graph names changed. When you hover over the text you can see the precise moment a dataset was edited. You can order the accounts based on the moment of last time the dataset was last edited. For each dataset you can execute the following actions: Open dataset settings For each dataset there is button such that the administrator can directly go to the dataset settings of the dataset. The dataset settings are behind the `cogwheel` button. Services page \u00b6 The services page governs all the services of an instance. The paginated table shows all the services of the instance. The table is equipped with filters and sorting mechanisms to quickly search for and locate specific services according to search and filter criteria. The table also has a search field at the right side to quickly do wildcard searching. The table sorts all services automatically if a service is in an error state or not. All services that are in error state will be shown at the top of the table. This way immediate action can be taken to check the service. The filters on top of the table can be used to filter the following columns: Name The name of the SPARQL service, you can order the table based on the alphanumeric ordering, and filter based on the sequence of characters appearing in the filter. The name is also a URL that brings you to the location of the service. Type Type of the service, this can either be 'Virtuoso', 'Jena', 'Blazegraph', 'Prolog' or 'Elasticsearch'. In the filter you can select a specific service type or 'All' service types. Status The status of the service, can be 'Starting', 'Running', 'Stopped', 'Updating' or 'Error'. In the filter you can select a specific service status or 'All' services statuses Statements The amount of statements in a service. These are all the loaded statements in the service, and can be filtered with the slider. Loaded graphs Amount of graphs loaded in the service. All the statements of all the graphs together will count up to the total amount of statements. Dataset The dataset the service belongs to. The dataset is clickable and brings you to the dataset page. The datasets can be filtered based on the sequence of characters appearing in the filter. Owner The owner of the dataset is also the owner of the service. The owner is a URL and brings you to the overview page of the owner. The owners can be filtered based on the sequence of characters appearing in the filter. Created How long ago a service has been created. When you hover over the text you can see the precise moment a service is created. You can order the services based on the moment of creation. Last queried How long ago the service has been last queried. When you hover over the text you can see the precise moment a service is last queried. You can order the services based on the moment of last time the service has been queried. Auto stops Some services are equipped with an auto stop feature. This feature reduces the memory resources when a service is not queried in a while. The column `Auto stops` shows how long it will take before a service is auto-stopped. You can order the services on when the auto-stop feature kicks in. Each time a service is used the timer is reset. Version A service always has a particular version. A service is not automatically updated as it could be that the service has possible down time. The owner of the service can update a service when they deem it necessary to update to the latest version. For each service you can execute the following actions: Update the service When a service can be updated an orange arrow will appear just below the service. When you press the update service button the service is automatically updated to the latest service version. Open additional information For each service there is additional information available. The additional information is behind the `i` button. The additional information contains information about the graphs in the dataset and a raw information view of the service metadata. Inspect the logs For each service there is a log available. The logs are behind the `text` button. The logs contain information Synchronize the service The service can be outdated. This happens when the data in the dataset does not corresponds with the data in the service. When this happens the service can be synchronized from here to make it up to date with the latest version of the data. Remove the service When a service is no longer necessary or there needs to be made some space on the instance a service can be removed from here. Some of these actions can be cumbersome when you need to do them one at a time. To help with this, on the left side of the table you can click on the tickbox. This will select all the services that match search criteria if there search criteria and all tables when there are no search criteria. When pressed you can now remove all selected services or update all selected services to a new software version. Redirects page \u00b6 The great thing about linked data is that IRIs are used to define objects in linked data. Then when you visit the IRIs you find useful information about the object. But sometimes the data is not on the location where the IRI is pointing towards. You have the IRI: https://example.org/resource/Amsterdam but the information about the object is located in the dataset https://api.triplydb.com/MyAccount/myCities. This is a problem as the IRI is pointing to a location that does not contain the data, and the data is at a location that is not found without the correct IRI. This is where you can use redirects to redirect the user from the IRI to the location where the data is found. How to setup a redirects for dereferencing \u00b6 Redirects enable easy dereferencing of resources. For example, you can dereference a resource https://example.org/resource/Amsterdam into dataset https://api.triplydb.com/MyAccount/myCities by following these steps: First update the web server of where the IRI is originally pointing towards the redirect API. In this example all subpaths of /resource are to be redirected from https://example.org to https://api.triplydb.com/redirect/$requestUri . this means that when a request for https://example.org/resource/Amsterdam comes to the web server of https://example.org it will be redirected to https://api.triplydb.com/redirect/https://example.org/resource/Amsterdam . Now that the external web server is set up to redirect to TriplyDB, TriplyDB needs to be configured to accept the request and redirect it to the correct dataset. This is done by adding a rule on the administrator redirects page. To add a rule, press the ADD RULE button to begin with the creation of a new rule. For this example we want to add a prefix rule with the pattern to match https://example.org/resource/City/ . The prefix rule needs a dataset to redirect to. This will be the dataset https://api.triplydb.com/myAccount/myCities . Press CREATE RULE to create the rule. Each rule is evaluated when a request comes in https://api.triplydb.com/redirect/$requestUri and mapping rules are evaluated from top (highest priority) to bottom (lowest priority). When a match is found the requestUri is then redirected to that location. TriplyDB supports two types of mapping rules: Prefix Prefix rules trigger when the start of a resource matches the specified string. Regex Regular Expression rules trigger when a resource matches a Regular Expression.","title":"Admin Settings Pages"},{"location":"triply-db-getting-started/admin-settings-pages/#admin-settings-pages","text":"You can use the console to perform administrator tasks. The administrator tasks are performed within the admin settings page. The admin settings pages are accessible by clicking on the user menu in the top-right corner and selecting the \u201cAdmin settings\u201d menu item. You must have administrator privileges to access these pages and perform administrator tasks.","title":"Admin settings Pages"},{"location":"triply-db-getting-started/admin-settings-pages/#overview-page","text":"The first page that comes into view when opening the admin settings pages is the overview page. This page contains an overview of all the important statistics of the instance. The page also shows how close the instance is to hitting one or more limits. If no limit is set, the statistics are shown as a counter. If a limit is set a gauge is shown with a green, orange or red bar. The colors denote how far that statistic of the instance is to the limit. Green means not close to the limit, Orange means close to the limit, Red means over the limit.","title":"Overview page"},{"location":"triply-db-getting-started/admin-settings-pages/#general-overview","text":"The general overview gives an insight into the software version of the instance. Each instance consists of a console and an API. The console is the web interface of the instance and has a build date corresponding to the build date of the docker image of the console and a version number corresponding to the version of the docker image. The API is the layer between the console and the data. The API is separate from the console and is a different docker image. The API also has a version and build date of the docker image. Also contains a starting time, and an updated time, the moments when the docker image is started for this instance or when the docker image is updated for the instance.","title":"General overview"},{"location":"triply-db-getting-started/admin-settings-pages/#accounts-overview","text":"The accounts overview shows how many organizations and users are in this instance. The organizations and users are shown in a counter if no limit is set. If a limit is set on the number of organizations and/or users of the instance a gauge is shown.","title":"Accounts overview"},{"location":"triply-db-getting-started/admin-settings-pages/#data-overview","text":"The data overview shows multiple statistics about datasets. The first counter shows the amount of datasets on the instance. The second and third counters show the amount of graphs and statements in all graphs. The fourth and fifth counters show the amount of unique graphs and statements. When a graph is copied from one dataset to another, the data in that graph does not change. The amount of unique data does not change either. The amount of unique data is a more representative way of calculating the amount of data in the instance. All statistics are shown in a counter, if no limit is set. If a limit is set on one of the statistics of the instance a gauge is shown.","title":"Data overview"},{"location":"triply-db-getting-started/admin-settings-pages/#services-overview","text":"The data overview shows how multiple statistics about services. The first counter shows the total amount of services on the instance, The second counter shows the total amount of statements in all the services. Then for each of our service types a specific counter is created. Each containing the amount of services and the amount of statements in that service. All statistics are shown in a counter if no limit is set. If a limit is set on one of the statistics of the instance a gauge is shown.","title":"Services overview"},{"location":"triply-db-getting-started/admin-settings-pages/#settings-page","text":"The settings page is the main page for administrators to institute instance wide changes. An administrator can change the site logo's here, change the contact email or update site wide prefixes.","title":"Settings page"},{"location":"triply-db-getting-started/admin-settings-pages/#set-logos-and-banner","text":"For changing the logos and the banner follow the next steps: Click on the \u201cAdmin settings\u201d link in the user menu (top-right corner) and click the \"Settings\" tab. This brings up an overview of all the settings an admin can set for the TriplyDB instance. Under \"Site logos\" and \"Site banner\" you can upload a site logo (square and landscape) or a banner. The logo (preferably squared) is required to be SVG. In the UI the 30x30px image is displayed. The banner can be of any format, however, the WEBP is preferred. The image's resolution should be between 1920x500 and up to 4000x500. The banner is displayed at a height 500px. If the image is smaller than the browser screen size, the image will be stretched. Make sure you use files with a maximum size of 5 MB.","title":"Set logos and banner"},{"location":"triply-db-getting-started/admin-settings-pages/#setting-metadata","text":"For changing the metadata follow the next steps: 1. Click on the \u201cAdmin settings\u201d link in the user menu (top-right corner) and click the \"Settings\" tab. This brings up an overview of all the settings an admin can set for the TriplyDB instance. On that page navigate to \"Site metadata\", it looks as follows: Here you can set the name, tag line, description and welcome text. The name of your website appears in your browser tab. The welcome text appears on the homepage of your TriplyDB instance. The tagline and description are for metadata purposes (e.g. findability and website previews).","title":"Setting metadata"},{"location":"triply-db-getting-started/admin-settings-pages/#setting-contact-email","text":"For changing the contact email follow the next steps: 1. Click on the \u201cAdmin settings\u201d link in the user menu (top-right corner) and click the \"Settings\" tab. This brings up an overview of all the settings an admin can set for the TriplyDB instance. On that page navigate to \"Contact Email\". Here, you can change the contact email to a new contact email for the instance.","title":"Setting contact email"},{"location":"triply-db-getting-started/admin-settings-pages/#setting-example-datasets","text":"Example datasets are introduction datasets on the frontpage of your instance. The Example datasets are datasets that are interesting for people that visit your page to see and interact with. Most often you'll use open datasets to show them off on the frontpage. You can also use internal or private datasets, but these will only be visible if the person seeing them has the right access rights. For editing the example datasets follow the next steps: 1. Click on the \u201cAdmin settings\u201d link in the user menu (top-right corner) and click the \"Settings\" tab. This brings up an overview of all the settings an admin can set for the TriplyDB instance. On that page, navigate to \"Example datasets\". Here, you can execute the following changes: You can move datasets up and down in the order by clicking and holding your left mouse button over the three horizontal lines in front of the dataset name. You can then drag the selected dataset to their new spot. In the search field below the already added datasets you can add a new example dataset by typing in the search field and selecting the correct dataset. You can remove datasets by pressing the x on the right side of the dataset name to remove it from the example dataset list.","title":"Setting example datasets"},{"location":"triply-db-getting-started/admin-settings-pages/#setting-starter-dataset","text":"The starter dataset is a beginner-friendly linked dataset that can be an introduction into linked data when a user creates an account for the first time. The starter dataset is visible for a user when the user has not yet created a dataset on its own account. For editing the example datasets follow the next steps: 1. Click on the \u201cAdmin settings\u201d link in the user menu (top-right corner) and click the \"Settings\" tab. This brings up an overview of all the settings an admin can set for the TriplyDB instance. On that page navigate to \"Starter dataset\". Here you can change the starter dataset to a new starter dataset for the instance by typing in the search bar a name of an existing dataset to replace the started dataset. This dataset then will be presented to users on their account page, with an option to import(copy) them immediately. This needs to be a public dataset! If it's not public, new users will have to create a dataset. The starter dataset is only shown if the user currently has no datasets.","title":"Setting Starter dataset"},{"location":"triply-db-getting-started/admin-settings-pages/#setting-authentication","text":"One of the roles of an administrator is to make sure only the right people will sign up for the TriplyDB instance. To do this, an administrator can set up authentication protocols. The authentication protocols can block people from signing up to instances where they are not allowed to sign up to. For changing the authentication protocols follow the next steps: 1. Click on the \u201cAdmin settings\u201d link in the user menu (top-right corner) and click the \"Settings\" tab. This brings up an overview of all the settings an admin can set for the TriplyDB instance. On that page navigate to \"Authentication\". Now you can change the password sign up. Allowing people to only register with a password or they are only allowed to register with a google or Github account. When password signup is enabled, the administrator can also set the permitted signup domains. Only users with e-mail addresses that match these domains are allowed to sign-up. Wildcards are allowed and domains are comma separated, for example: mydomain.com,*.mydomain.com.","title":"Setting Authentication"},{"location":"triply-db-getting-started/admin-settings-pages/#setting-site-wide-prefixes","text":"One of the advantages of using TriplyDB is that you can set site-wide prefixes once and use them everywhere on the instance. Site-wide prefixes are prefixes defined in the admin settings and can be used for all datasets that contain the IRIs matching the prefixes. For editing the side-wide prefixes follow the next steps: 1. Click on the \u201cAdmin settings\u201d link in the user menu (top-right corner) and click the \"Settings\" tab. This brings up an overview of all the settings an admin can set for the TriplyDB instance. On that page navigate to \"Site-wide prefixes\". Here, you can execute the following changes: Each field of the already added site-wide prefixes you can edit. You can edit the prefix label by typing in the first field. You can edit the prefix IRI and in the second field. Pressing UPDATE PREFIXES updates the list. In the last field below the already added site-wide prefixes you can add a new site-wide prefix by typing in the first field the prefix label, and in the second field the prefix IRI. Pressing UPDATE PREFIXES updates the list. You can remove prefixes by pressing the x on the right side of the prefixes name to remove it from the site-wide prefixes list.","title":"Setting Site-wide prefixes"},{"location":"triply-db-getting-started/admin-settings-pages/#account-overview-page","text":"The account page governs all the accounts of an instance. The paginated table shows all the accounts of the instance. The table is equipped with filters and sorting mechanisms to quickly search for and locate specific accounts according to search and filter criteria. The table also has a search field at the right side to quickly do wildcard searching. The table sorts all accounts automatically on the created at date with the latest created at date accounts first. The filters on top of the table can be used to filter the following columns: Name The name of the account, you can order the table based on the alphanumeric ordering, and filter based on the sequence of characters appearing in the filter. The name is also a URL that brings you to the location of the account. Type Type of the account, this can either be 'Organization' or 'User'. In the filter you can select a specific account type or 'All' account types. Display name The display name of the account, often an account has both a name and a display name. The display name is not limited to a specific set of characters, as it is not used as an URL. You can order the table based on the alphanumeric ordering, and filter based on the sequence of characters appearing in the filter. Email The email address of the account. You can order the table based on the alphanumeric ordering, and filter based on the sequence of characters appearing in the filter. Created at How long ago an account was created. When you hover over the text you can see the precise moment an account is created. You can order accounts based on the moment of creation. Updated at How long ago an account has been updated with new metadata such as display name or password. When you hover over the text you can see the precise moment an account is last updated. You can order accounts based on the moment of updated at time. Last activity How long ago the account has been last active. When you hover over the text you can see the precise moment an account was last active. You can order the accounts based on the moment of last time the account was active. Role Role of the account, this can either be 'light', 'regular' or 'administrator'. In the filter you can select a specific role or 'All' roles. Verified An account can be verified or not, to verify an account, the user needs to click on the verify button in the email. Or an administrator has verified the account in the account settings of that account. Only 'users' need to be verified. Disabled An account can be disabled or not, to disabled an account, the user needs to click on the disabled button in their user settings. Or an administrator has disabled the account in the account settings of that account. legal consent An account can have accepted the legal consent or not, to accept legal consent, the user needs to click on the accept legal consent either when creating an account or by checking it in the user settings. Only 'users' need to have accepted legal consent. For each account you can execute the following actions: Open account settings For each account, there is a button such that the administrator can directly go to the account settings of the user or organization. The account settings are behind the `cogwheel` button.","title":"Account overview page"},{"location":"triply-db-getting-started/admin-settings-pages/#add-new-users","text":"Go to the \u201cAccounts tab\u201d to receive an overview of all accounts on the TriplyDB instance. The type of account can be observed based on the following icons: Icon Account type organization user","title":"Add new user(s)"},{"location":"triply-db-getting-started/admin-settings-pages/#create-a-new-user","text":"New users can only be created by administrators by performing the following steps: Click on the \u201cAdmin settings\u201d link in the user menu (top-right corner) and click the \u201cAccounts\u201d tab. This brings up an overview of all users and organizations on the TriplyDB instance. Click the \u201cAdd user\u201d button. Fill in the user name and email address of the prospective user. The user name must consist of alphanumeric characters ( A-Za-z ) and hyphens ( - ). Click the \u201cAdd user\u201d button. This sends an account creation email to the prospective user, containing a link that allows them to log in. In addition to the above default procedure, the following two options are provided for user account creation: Temporary account : By default, user accounts do not expire. Sometimes it is useful to create a temporary account by specifying a concrete date in the \u201cAccount expiration date\u201d widget. Preset password : By default, a user can set her password after logging in for the first time by clicking on the link in the account creation email. When a password is entered in the \u201cPassword\u201d field, the user must enter this password in order to log in for the first time.","title":"Create a new user"},{"location":"triply-db-getting-started/admin-settings-pages/#datasets-page","text":"The account page governs all the datasets of an instance. The paginated table shows all the datasets of the instance. The table is equipped with filters and sorting mechanisms to quickly search for and locate specific datasets according to search and filter criteria. The table also has a search field at the right side to quickly do wildcard searching. The table sorts all datasets automatically on the created at date with the latest created at date datasets first. The filters on top of the table can be used to filter the following columns: Name The name of the dataset, you can order the table based on the alphanumeric ordering, and filter based on the sequence of characters appearing in the filter. The name is also a URL that brings you to the location of the dataset. Access level Access level of the dataset, this can either be 'Public', 'Internal' or 'Private'. In the filter you can select a specific access level or 'All' access levels. Display name The display name of the dataset, often a dataset has both a name and a display name. The display name is not limited to a specific set of characters, as it is not used as an URL. You can order the table based on the alphanumeric ordering, and filter based on the sequence of characters appearing in the filter. Owner The owner of the dataset. The owner is a URL and brings you to the overview page of the owner. The owners can be filtered based on the sequence of characters appearing in the filter. Graph count The amount of graphs in a dataset. These are all the total amount of graphs in a dataset, and can be filtered with the slider. Statement count The amount of statements in a dataset. These are all the statements of all the graphs, and can be filtered with the slider. Service count The amount of services in a dataset. These can be filtered with the slider. Asset count The amount of assets in a dataset. These can be filtered with the slider. Created at How long ago a dataset has been created. When you hover over the text you can see the precise moment a dataset is created. You can order datasets based on the moment of creation. Updated at How long ago a dataset has been updated with new metadata such as display name or new data. When you hover over the text you can see the precise moment an account is last updated. You can order dataset based on the moment of updated at time. Last graph edit How long ago the last graph has been edited, either new data is uploaded or removed, or the graph names changed. When you hover over the text you can see the precise moment a dataset was edited. You can order the accounts based on the moment of last time the dataset was last edited. For each dataset you can execute the following actions: Open dataset settings For each dataset there is button such that the administrator can directly go to the dataset settings of the dataset. The dataset settings are behind the `cogwheel` button.","title":"Datasets page"},{"location":"triply-db-getting-started/admin-settings-pages/#services-page","text":"The services page governs all the services of an instance. The paginated table shows all the services of the instance. The table is equipped with filters and sorting mechanisms to quickly search for and locate specific services according to search and filter criteria. The table also has a search field at the right side to quickly do wildcard searching. The table sorts all services automatically if a service is in an error state or not. All services that are in error state will be shown at the top of the table. This way immediate action can be taken to check the service. The filters on top of the table can be used to filter the following columns: Name The name of the SPARQL service, you can order the table based on the alphanumeric ordering, and filter based on the sequence of characters appearing in the filter. The name is also a URL that brings you to the location of the service. Type Type of the service, this can either be 'Virtuoso', 'Jena', 'Blazegraph', 'Prolog' or 'Elasticsearch'. In the filter you can select a specific service type or 'All' service types. Status The status of the service, can be 'Starting', 'Running', 'Stopped', 'Updating' or 'Error'. In the filter you can select a specific service status or 'All' services statuses Statements The amount of statements in a service. These are all the loaded statements in the service, and can be filtered with the slider. Loaded graphs Amount of graphs loaded in the service. All the statements of all the graphs together will count up to the total amount of statements. Dataset The dataset the service belongs to. The dataset is clickable and brings you to the dataset page. The datasets can be filtered based on the sequence of characters appearing in the filter. Owner The owner of the dataset is also the owner of the service. The owner is a URL and brings you to the overview page of the owner. The owners can be filtered based on the sequence of characters appearing in the filter. Created How long ago a service has been created. When you hover over the text you can see the precise moment a service is created. You can order the services based on the moment of creation. Last queried How long ago the service has been last queried. When you hover over the text you can see the precise moment a service is last queried. You can order the services based on the moment of last time the service has been queried. Auto stops Some services are equipped with an auto stop feature. This feature reduces the memory resources when a service is not queried in a while. The column `Auto stops` shows how long it will take before a service is auto-stopped. You can order the services on when the auto-stop feature kicks in. Each time a service is used the timer is reset. Version A service always has a particular version. A service is not automatically updated as it could be that the service has possible down time. The owner of the service can update a service when they deem it necessary to update to the latest version. For each service you can execute the following actions: Update the service When a service can be updated an orange arrow will appear just below the service. When you press the update service button the service is automatically updated to the latest service version. Open additional information For each service there is additional information available. The additional information is behind the `i` button. The additional information contains information about the graphs in the dataset and a raw information view of the service metadata. Inspect the logs For each service there is a log available. The logs are behind the `text` button. The logs contain information Synchronize the service The service can be outdated. This happens when the data in the dataset does not corresponds with the data in the service. When this happens the service can be synchronized from here to make it up to date with the latest version of the data. Remove the service When a service is no longer necessary or there needs to be made some space on the instance a service can be removed from here. Some of these actions can be cumbersome when you need to do them one at a time. To help with this, on the left side of the table you can click on the tickbox. This will select all the services that match search criteria if there search criteria and all tables when there are no search criteria. When pressed you can now remove all selected services or update all selected services to a new software version.","title":"Services page"},{"location":"triply-db-getting-started/admin-settings-pages/#redirects-page","text":"The great thing about linked data is that IRIs are used to define objects in linked data. Then when you visit the IRIs you find useful information about the object. But sometimes the data is not on the location where the IRI is pointing towards. You have the IRI: https://example.org/resource/Amsterdam but the information about the object is located in the dataset https://api.triplydb.com/MyAccount/myCities. This is a problem as the IRI is pointing to a location that does not contain the data, and the data is at a location that is not found without the correct IRI. This is where you can use redirects to redirect the user from the IRI to the location where the data is found.","title":"Redirects page"},{"location":"triply-db-getting-started/admin-settings-pages/#how-to-setup-a-redirects-for-dereferencing","text":"Redirects enable easy dereferencing of resources. For example, you can dereference a resource https://example.org/resource/Amsterdam into dataset https://api.triplydb.com/MyAccount/myCities by following these steps: First update the web server of where the IRI is originally pointing towards the redirect API. In this example all subpaths of /resource are to be redirected from https://example.org to https://api.triplydb.com/redirect/$requestUri . this means that when a request for https://example.org/resource/Amsterdam comes to the web server of https://example.org it will be redirected to https://api.triplydb.com/redirect/https://example.org/resource/Amsterdam . Now that the external web server is set up to redirect to TriplyDB, TriplyDB needs to be configured to accept the request and redirect it to the correct dataset. This is done by adding a rule on the administrator redirects page. To add a rule, press the ADD RULE button to begin with the creation of a new rule. For this example we want to add a prefix rule with the pattern to match https://example.org/resource/City/ . The prefix rule needs a dataset to redirect to. This will be the dataset https://api.triplydb.com/myAccount/myCities . Press CREATE RULE to create the rule. Each rule is evaluated when a request comes in https://api.triplydb.com/redirect/$requestUri and mapping rules are evaluated from top (highest priority) to bottom (lowest priority). When a match is found the requestUri is then redirected to that location. TriplyDB supports two types of mapping rules: Prefix Prefix rules trigger when the start of a resource matches the specified string. Regex Regular Expression rules trigger when a resource matches a Regular Expression.","title":"How to setup a redirects for dereferencing"},{"location":"triply-db-getting-started/data-stories/","text":"On this page: Data stories Creating a data story Editing a data story Adding elements Existing query Paragraph Sharing and embedding Data stories \u00b6 A TriplyDB data story is a way of communicating information about your linked data along with explanatory text while also being able to integrate query results. Creating a data story \u00b6 You can create your own data story via the stories tab on TriplyDB. If this is your first time creating a data story, your view will look something like the image below. If you already are a proud owner of a data story, you will find it here. To create a new one, you can click the orange \"Create story\" button and you\u2019ll see the same form. In this form, you can fill in the title and set the access level for a data story. When everything is set, press the \"Create story\" button.This will take you to a new page where you can customize the data story. Editing a data story \u00b6 As shown in the image below, in the top right corner of the page, there is a menu button. Here you will find the following: Story settings : Here you can change the title and the access level of your story. Change banner : Here you can change the banner, just choose an image that you want as your banner (wide images work best). Copy : To copy the story to a different user or organization. Transfer : To transfer the story to a different user or organization. Embed : HTML to embed the story in a web page using an iFrame. Print : Dialog and print options to print the story. Delete : To delete the story. In the right lower corner you see a button with a notepad. With this button, you can toggle between the edit view, which allows you to edit the story, and the reader view, which is how readers of your story will perceive this page. Adding elements \u00b6 To create your first element press \"+ Add new element\". This will open a new form as shown in the images below. Here you can select what kind of element you want to add to your data story; you\u2019ll have the option to write text, to select an already existing SPARQL query, or even to create a new SPARQL query. Existing query \u00b6 Let\u2019s start by selecting a query for our data story. Maybe you have already created one, but if you haven\u2019t, you can select one of the queries available to you. You can search in the Query search bar and select the one you want, for example \"our-first-select-query\". Optionally you can select the version of the query and set the caption. When everything is set, press \"Create story element\". And look, we have added our first element to our story! Paragraph \u00b6 Data sometimes needs accompanying text to be completely understandable. TriplyDB not only supports writing plain text, but TriplyDB paragraphs are also markdown compliant. The markdown that you\u2019ll add in the paragraph text box will be rendered as HTML and can be previewed. TriplyDB also supports images, and even code blocks with highlighting for the most common linked data and programming languages. Sharing and embedding \u00b6 Before you know it, you will have created your first data story. Congratulations! Now it is time to share it with the world, but don\u2019t forget to set the access level to \u201cpublic\u201d. Then you have two options: 1. You can simply share the URL in TriplyDB. 2. You can embed the Data Story on your own webpage. Scroll all the way to the end of your Data Story and click the \u201c Embed\u201d button. This brings up a code snippet that you can copy/paste into your own HTML web page.","title":"Data Stories"},{"location":"triply-db-getting-started/data-stories/#data-stories","text":"A TriplyDB data story is a way of communicating information about your linked data along with explanatory text while also being able to integrate query results.","title":"Data stories"},{"location":"triply-db-getting-started/data-stories/#creating-a-data-story","text":"You can create your own data story via the stories tab on TriplyDB. If this is your first time creating a data story, your view will look something like the image below. If you already are a proud owner of a data story, you will find it here. To create a new one, you can click the orange \"Create story\" button and you\u2019ll see the same form. In this form, you can fill in the title and set the access level for a data story. When everything is set, press the \"Create story\" button.This will take you to a new page where you can customize the data story.","title":"Creating a data story"},{"location":"triply-db-getting-started/data-stories/#editing-a-data-story","text":"As shown in the image below, in the top right corner of the page, there is a menu button. Here you will find the following: Story settings : Here you can change the title and the access level of your story. Change banner : Here you can change the banner, just choose an image that you want as your banner (wide images work best). Copy : To copy the story to a different user or organization. Transfer : To transfer the story to a different user or organization. Embed : HTML to embed the story in a web page using an iFrame. Print : Dialog and print options to print the story. Delete : To delete the story. In the right lower corner you see a button with a notepad. With this button, you can toggle between the edit view, which allows you to edit the story, and the reader view, which is how readers of your story will perceive this page.","title":"Editing a data story"},{"location":"triply-db-getting-started/data-stories/#adding-elements","text":"To create your first element press \"+ Add new element\". This will open a new form as shown in the images below. Here you can select what kind of element you want to add to your data story; you\u2019ll have the option to write text, to select an already existing SPARQL query, or even to create a new SPARQL query.","title":"Adding elements"},{"location":"triply-db-getting-started/data-stories/#existing-query","text":"Let\u2019s start by selecting a query for our data story. Maybe you have already created one, but if you haven\u2019t, you can select one of the queries available to you. You can search in the Query search bar and select the one you want, for example \"our-first-select-query\". Optionally you can select the version of the query and set the caption. When everything is set, press \"Create story element\". And look, we have added our first element to our story!","title":"Existing query"},{"location":"triply-db-getting-started/data-stories/#paragraph","text":"Data sometimes needs accompanying text to be completely understandable. TriplyDB not only supports writing plain text, but TriplyDB paragraphs are also markdown compliant. The markdown that you\u2019ll add in the paragraph text box will be rendered as HTML and can be previewed. TriplyDB also supports images, and even code blocks with highlighting for the most common linked data and programming languages.","title":"Paragraph"},{"location":"triply-db-getting-started/data-stories/#sharing-and-embedding","text":"Before you know it, you will have created your first data story. Congratulations! Now it is time to share it with the world, but don\u2019t forget to set the access level to \u201cpublic\u201d. Then you have two options: 1. You can simply share the URL in TriplyDB. 2. You can embed the Data Story on your own webpage. Scroll all the way to the end of your Data Story and click the \u201c Embed\u201d button. This brings up a code snippet that you can copy/paste into your own HTML web page.","title":"Sharing and embedding"},{"location":"triply-db-getting-started/editing-data/","text":"On this page: Editing Data About Editor forms Choosing an Editor view Creating a new instance Finding an instance SKOS: selecting concept schemes SKOS: navigating concept schemes Instance details Copying an instance Editing an instance Deleting an instance Editing Data \u00b6 The Editor is available in specific instances of TriplyDB. It allows creating, updating or deleting data that have a structure that is defined in a SHACL shapes graph. For example, a shapes graph could define the structure of a concept scheme based on SKOS , or it could define the structure of a data catalogue based on DCAT . This section describes using the Editor to edit SKOS concept schemes as an example. The Editor works in a similar way for other views. In order to open the Editor, select the dataset that you want to work with. If your instance provides the feature, you will see \"Editor\" on the left-hand side. Pressing there will open the Editor pane. Alternatively, instance hyperlinks can be dragged and dropped on the \"Editor\" menu entry, to directly view the instance in the Editor. About Editor forms \u00b6 The Editor uses forms to create or edit instances of a class. The image below shows elements of a form. In Editor forms, an asterisk (*) is used to mark mandatory properties of an instance. It is not possible to save resources that do not have a value for all mandatory properties. In order to add a property, press the plus symbol (+). Some properties can occur multiple times, with different values. In order to remove a property, press the waste basket symbol. Input fields for values support dropdown lists where possible. Alternatively, it is possible to start typing to bring up a list of matching values. The availabilty of dropdown lists is indicated by a small downward facing triangle on the right of the input field. Choosing an Editor view \u00b6 In the top left corner of the Editor pane, the view button can be used to select a different view, if multiple views are configured. The image below shows the SKOS view being selected. Creating a new instance \u00b6 In the top right corner of the Editor pane you can find the create button. It can be used to create a new instance of a class that is defined in the shapes graph. Pressing the create button will open a form that allows picking the class for the new instance, and setting other properties. By default, each resource needs to have an IRI (Internationalized Resource Identifier). The Editor generates an IRI, which can be changed, if needed. Finding an instance \u00b6 Next to the create button there is an input box that can be used to find an instance based on its label. All data that are configured to be used by the Editor are included in the search. You can start typing to populate a dropdown list with resources that have a matching sequence of characters somewhere. The matching characters are displayed in boldface. SKOS: selecting concept schemes \u00b6 In the SKOS view, it is possible to select a concept scheme to work with. Press the concept scheme(s) input field to choose a concept scheme from a dropdown list. Alternatively, start typing to find a concept scheme with a matching character sequence. SKOS concept schemes are allowed to be chained, by means of skos:narrowMatch and skos:broadMatch relations between concepts. Once a concept scheme is found, you can press in the space behind its name to find and select nested concept schemes. Concept schemes in the chain will be assigned different colours, as shown in the image below. SKOS: navigating concept schemes \u00b6 Once one or more concept schemes have been selected, the hierarchy of concepts and concept schemes is displayed in a tree view: The symbol > in front of a concept means that the concept has underlying elements. Pressing > will expand the concept. The symbol will then point downward to show that the concept has been expanded. When a concept has no underlying concepts, its name is preceded by a dot (\u2022). As concepts can be defined in different nested concept schemes, the coloured label of the concept scheme is displayed for each concept. To search within the concept hierarchy, use the Search in hierarchy input field. When a concept in the concept hierarchy is selected, detailled information is displayed on the right-hand side of the Editor pane. Instance details \u00b6 When an instance (for example, a SKOS concept) is selected on the left side of the Editor pane, a description of the instance is shown on the right: Below the name of the instance, its IRI is displayed as a hyperlink (blue underlined). The hyperlink can be dragged and dropped on other visualisation in TriplyDB, like Browser or Triples . To the right of the instance hyperlink its class is shown. For instances that have been edited in the Editor, a history of changes is recorded. Details of the last change are shown by Modified by and Modified . The symbol of the backward turning clock can be pressed to show the complete modification history of the selected instance. The image below shows an example. Copying an instance \u00b6 The copy button can be used to copy an instance. That can be useful if a new instance is needed that is similar to an existing one. Editing an instance \u00b6 The edit button can be used to edit the properties of an instance. Deleting an instance \u00b6 To remove an instance, press the delete button. You will be asked for a confirmation before the instance is really deleted.","title":"Editing (SKOS) Data"},{"location":"triply-db-getting-started/editing-data/#editing-data","text":"The Editor is available in specific instances of TriplyDB. It allows creating, updating or deleting data that have a structure that is defined in a SHACL shapes graph. For example, a shapes graph could define the structure of a concept scheme based on SKOS , or it could define the structure of a data catalogue based on DCAT . This section describes using the Editor to edit SKOS concept schemes as an example. The Editor works in a similar way for other views. In order to open the Editor, select the dataset that you want to work with. If your instance provides the feature, you will see \"Editor\" on the left-hand side. Pressing there will open the Editor pane. Alternatively, instance hyperlinks can be dragged and dropped on the \"Editor\" menu entry, to directly view the instance in the Editor.","title":"Editing Data"},{"location":"triply-db-getting-started/editing-data/#about-editor-forms","text":"The Editor uses forms to create or edit instances of a class. The image below shows elements of a form. In Editor forms, an asterisk (*) is used to mark mandatory properties of an instance. It is not possible to save resources that do not have a value for all mandatory properties. In order to add a property, press the plus symbol (+). Some properties can occur multiple times, with different values. In order to remove a property, press the waste basket symbol. Input fields for values support dropdown lists where possible. Alternatively, it is possible to start typing to bring up a list of matching values. The availabilty of dropdown lists is indicated by a small downward facing triangle on the right of the input field.","title":"About Editor forms"},{"location":"triply-db-getting-started/editing-data/#choosing-an-editor-view","text":"In the top left corner of the Editor pane, the view button can be used to select a different view, if multiple views are configured. The image below shows the SKOS view being selected.","title":"Choosing an Editor view"},{"location":"triply-db-getting-started/editing-data/#creating-a-new-instance","text":"In the top right corner of the Editor pane you can find the create button. It can be used to create a new instance of a class that is defined in the shapes graph. Pressing the create button will open a form that allows picking the class for the new instance, and setting other properties. By default, each resource needs to have an IRI (Internationalized Resource Identifier). The Editor generates an IRI, which can be changed, if needed.","title":"Creating a new instance"},{"location":"triply-db-getting-started/editing-data/#finding-an-instance","text":"Next to the create button there is an input box that can be used to find an instance based on its label. All data that are configured to be used by the Editor are included in the search. You can start typing to populate a dropdown list with resources that have a matching sequence of characters somewhere. The matching characters are displayed in boldface.","title":"Finding an instance"},{"location":"triply-db-getting-started/editing-data/#skos-selecting-concept-schemes","text":"In the SKOS view, it is possible to select a concept scheme to work with. Press the concept scheme(s) input field to choose a concept scheme from a dropdown list. Alternatively, start typing to find a concept scheme with a matching character sequence. SKOS concept schemes are allowed to be chained, by means of skos:narrowMatch and skos:broadMatch relations between concepts. Once a concept scheme is found, you can press in the space behind its name to find and select nested concept schemes. Concept schemes in the chain will be assigned different colours, as shown in the image below.","title":"SKOS: selecting concept schemes"},{"location":"triply-db-getting-started/editing-data/#skos-navigating-concept-schemes","text":"Once one or more concept schemes have been selected, the hierarchy of concepts and concept schemes is displayed in a tree view: The symbol > in front of a concept means that the concept has underlying elements. Pressing > will expand the concept. The symbol will then point downward to show that the concept has been expanded. When a concept has no underlying concepts, its name is preceded by a dot (\u2022). As concepts can be defined in different nested concept schemes, the coloured label of the concept scheme is displayed for each concept. To search within the concept hierarchy, use the Search in hierarchy input field. When a concept in the concept hierarchy is selected, detailled information is displayed on the right-hand side of the Editor pane.","title":"SKOS: navigating concept schemes"},{"location":"triply-db-getting-started/editing-data/#instance-details","text":"When an instance (for example, a SKOS concept) is selected on the left side of the Editor pane, a description of the instance is shown on the right: Below the name of the instance, its IRI is displayed as a hyperlink (blue underlined). The hyperlink can be dragged and dropped on other visualisation in TriplyDB, like Browser or Triples . To the right of the instance hyperlink its class is shown. For instances that have been edited in the Editor, a history of changes is recorded. Details of the last change are shown by Modified by and Modified . The symbol of the backward turning clock can be pressed to show the complete modification history of the selected instance. The image below shows an example.","title":"Instance details"},{"location":"triply-db-getting-started/editing-data/#copying-an-instance","text":"The copy button can be used to copy an instance. That can be useful if a new instance is needed that is similar to an existing one.","title":"Copying an instance"},{"location":"triply-db-getting-started/editing-data/#editing-an-instance","text":"The edit button can be used to edit the properties of an instance.","title":"Editing an instance"},{"location":"triply-db-getting-started/editing-data/#deleting-an-instance","text":"To remove an instance, press the delete button. You will be asked for a confirmation before the instance is really deleted.","title":"Deleting an instance"},{"location":"triply-db-getting-started/exporting-data/","text":"On this page: Exporting Data Export Datasets Export Graphs Extract Exporting Data \u00b6 This section explains how a user of TriplyDB can export linked data stored in the triple store. Export Datasets \u00b6 The data stored on TriplyDB is stored in two different containers: datasets and graphs. Each triple contained in a dataset is part of exactly one graph. A graph is always a part of a dataset and a dataset can have multiple graphs. The following screenshot shows the dataset \"Pok\u00e9mon\" that contains three graphs: \"data\" and \"vocab\". The graph \"data\" contains 28.588 triples and the graph \"vocab\" 185 triples. By summing up the amount of triples contained in the two graphs the dataset \"Pok\u00e9mon\" contains 28.773 triples in total. To export the dataset users can click on the downwards facing arrow. In our example, the dataset is automatically downloaded as the file \"pokemon.trig\" and compressed with .gz. The name of the file is the name of the dataset. The used serialization format is \".trig\" because that is the standard format to store triples that are appended to graphs. It is also possible to export the whole dataset on the graphs interface. Select \"Graphs\" and \"EXPORT ALL GRAPHS\". Export Graphs \u00b6 To export only one graph select \"Graphs\" and the arrow next to the graph that should be exported. In this case the downloaded file \"https___triplydb.com_academy_pokemon_graphs_data.trig.gz\" is named after the graph and also compressed with \"gz\". Extract \u00b6 The process of extracting the compressed file is the same for exporting graphs and datasets. The downloaded and compressed file is automatically stored in the \"Downloads\" folder. Select the file with the ending \".gz\" and open it with a double click. This opens an application that looks similar to the following screenshot: Select the file that should be extracted. In this case select \"pokemon.trig\" and click on \"Extract\". In the following step choose the location where the extracted file should be stored.","title":"Exporting Data"},{"location":"triply-db-getting-started/exporting-data/#exporting-data","text":"This section explains how a user of TriplyDB can export linked data stored in the triple store.","title":"Exporting Data"},{"location":"triply-db-getting-started/exporting-data/#export-datasets","text":"The data stored on TriplyDB is stored in two different containers: datasets and graphs. Each triple contained in a dataset is part of exactly one graph. A graph is always a part of a dataset and a dataset can have multiple graphs. The following screenshot shows the dataset \"Pok\u00e9mon\" that contains three graphs: \"data\" and \"vocab\". The graph \"data\" contains 28.588 triples and the graph \"vocab\" 185 triples. By summing up the amount of triples contained in the two graphs the dataset \"Pok\u00e9mon\" contains 28.773 triples in total. To export the dataset users can click on the downwards facing arrow. In our example, the dataset is automatically downloaded as the file \"pokemon.trig\" and compressed with .gz. The name of the file is the name of the dataset. The used serialization format is \".trig\" because that is the standard format to store triples that are appended to graphs. It is also possible to export the whole dataset on the graphs interface. Select \"Graphs\" and \"EXPORT ALL GRAPHS\".","title":"Export Datasets"},{"location":"triply-db-getting-started/exporting-data/#export-graphs","text":"To export only one graph select \"Graphs\" and the arrow next to the graph that should be exported. In this case the downloaded file \"https___triplydb.com_academy_pokemon_graphs_data.trig.gz\" is named after the graph and also compressed with \"gz\".","title":"Export Graphs"},{"location":"triply-db-getting-started/exporting-data/#extract","text":"The process of extracting the compressed file is the same for exporting graphs and datasets. The downloaded and compressed file is automatically stored in the \"Downloads\" folder. Select the file with the ending \".gz\" and open it with a double click. This opens an application that looks similar to the following screenshot: Select the file that should be extracted. In this case select \"pokemon.trig\" and click on \"Extract\". In the following step choose the location where the extracted file should be stored.","title":"Extract"},{"location":"triply-db-getting-started/publishing-data/","text":"On this page: Sharing data Sharing your dataset Dataset settings page Update dataset profile Dataset metadata Starting services Existing services Webhooks Sharing data \u00b6 With TriplyDB you can easily make your data available to the outside world. Sharing your dataset \u00b6 You can share your dataset by setting the visibility to \u201cPublic\u201d in the dataset settings menu. Making a dataset public in TriplyDB has the following consequences: The dataset can be searched for and visited by anybody on the web. The dataset will be indexed by web search engines such as Google Dataset Search. Any services that are started for that dataset will be available to anybody on the web. This includes SPARQL, Text Search, and Linked Data Fragments. Dataset settings page \u00b6 The dataset settings page can be accessed from any dataset page. It appears as the last item in the menu to the left (see Figure 1 ). Figure 1. The homepage of a dataset. The dataset settings page contains the following items: Update dataset profile Prefixes Transfer ownership Delete dataset Webhooks Update dataset profile \u00b6 The \"Update dataset profile\" pane (see Figure 2 ) allows the following things to be configured: Dataset Access Level, see Section Access Levels . Dataset metadata, see Section Dataset metadata . Figure 2. The \"Update dataset profile\" pane. Dataset metadata \u00b6 Adding metadata for your datasets is important. This makes it easier to find your dataset later and also allows search engines and social media applications to understand your dataset. The \"Update dataset profile\" pane allows the following metadata to be configured: Dataset name Dataset slug Dataset description Example resources License Avatar Within the TriplyDB instance your dataset is now more findable for users. Whenever a user searches on one of the topics of your dataset, or types in a word that is present in the description of your dataset, the dataset will be shown as a search result. The metadata will allow TriplyDB to give a better impression of your dataset when a user visits. Figure 3 shows the dataset homepage of a dataset for which metadata has been configured. Figure 3. The dataset homepage of a dataset with configured metadata. Search engines and social media applications can recognize the metadata that is entered for datasets in TriplyDB. Figure 4 shows an example of a Social Media widget in the Slack chat application. Such widgets are automatically generated upon entering the link to a public dataset in TriplyDB. The chat application in Figure 4 understands the metadata properties: title, description, and image. Different Social Media applications may make use of different metadata properties. Figure 4. A dataset widget in the Slack chat application. Starting services \u00b6 By default, datasets in TriplyDB can be queried through TriplyDB-js as well as through the Linked Data Fragments API. In order to allow additional query paradigms, specific services can be started from the \u201cCreate service\u201d page. This page is accessed by clicking on the \u201cServices\u201d icon in the left-hand sidebar. TriplyDB instances can be configured with different types of services. The below screenshot shows the \u201cCreate service\u201d page for a TriplyDB instance that allows SPARQL, Jena SPARQL, and Elasticsearch services to be created. Notice that three different types of services can be created. It is possible to create multiple services for one dataset. Existing services \u00b6 Existing services are displayed on service widgets (see screenshot). From these widgets, services can be created or deleted. Datasets can change whenever a graph is added, deleted or renamed. When this happens, the data in a service is out of sync with the data in the dataset and a synchronization button will appear in the service widget. By clicking the button, the service will be synchronized with the current state of the dataset. Webhooks \u00b6 If you want to be notified or trigger an event when anything changes in your dataset, you can set up a webhook. The webhook page can be found under dataset's settings on the right, as shown in the image below. To create a webhook, you will need to provide the following information: Payload target : The URL to which the webhook message should be sent. Payload format : The format of the message. Trigger events : Select for which event you wish to trigger the webhook. The options are: Graph import : Happens when data is imported from a different dataset and where the data is already stored on the instance. Linked data upload : Happens when a person uploads data to the instance. The data did not exist on the instance before. Asset upload : Happens when an asset is uploaded. You can activate or deactivate the webhook with the slider after the Webhook is active message. After filling in everything, you can click on the SUBMIT button and the new webhook will be activated. For example, if you wish to trigger a pipeline on gitlab every time you upload an asset to your dataset, you can use the below snippet as a payload target, as described on the official gitlab documentation and select Asset upload as a trigger event. https://gitlab.example.com/api/v4/projects/<project_id>/trigger/pipeline?token=<token>&ref=<ref_name> When your webhook is created and active, you can see every occasion the webhook was called in the webhook trigger history.","title":"Sharing Data"},{"location":"triply-db-getting-started/publishing-data/#sharing-data","text":"With TriplyDB you can easily make your data available to the outside world.","title":"Sharing data"},{"location":"triply-db-getting-started/publishing-data/#sharing-your-dataset","text":"You can share your dataset by setting the visibility to \u201cPublic\u201d in the dataset settings menu. Making a dataset public in TriplyDB has the following consequences: The dataset can be searched for and visited by anybody on the web. The dataset will be indexed by web search engines such as Google Dataset Search. Any services that are started for that dataset will be available to anybody on the web. This includes SPARQL, Text Search, and Linked Data Fragments.","title":"Sharing your dataset"},{"location":"triply-db-getting-started/publishing-data/#dataset-settings-page","text":"The dataset settings page can be accessed from any dataset page. It appears as the last item in the menu to the left (see Figure 1 ). Figure 1. The homepage of a dataset. The dataset settings page contains the following items: Update dataset profile Prefixes Transfer ownership Delete dataset Webhooks","title":"Dataset settings page"},{"location":"triply-db-getting-started/publishing-data/#update-dataset-profile","text":"The \"Update dataset profile\" pane (see Figure 2 ) allows the following things to be configured: Dataset Access Level, see Section Access Levels . Dataset metadata, see Section Dataset metadata . Figure 2. The \"Update dataset profile\" pane.","title":"Update dataset profile"},{"location":"triply-db-getting-started/publishing-data/#dataset-metadata","text":"Adding metadata for your datasets is important. This makes it easier to find your dataset later and also allows search engines and social media applications to understand your dataset. The \"Update dataset profile\" pane allows the following metadata to be configured: Dataset name Dataset slug Dataset description Example resources License Avatar Within the TriplyDB instance your dataset is now more findable for users. Whenever a user searches on one of the topics of your dataset, or types in a word that is present in the description of your dataset, the dataset will be shown as a search result. The metadata will allow TriplyDB to give a better impression of your dataset when a user visits. Figure 3 shows the dataset homepage of a dataset for which metadata has been configured. Figure 3. The dataset homepage of a dataset with configured metadata. Search engines and social media applications can recognize the metadata that is entered for datasets in TriplyDB. Figure 4 shows an example of a Social Media widget in the Slack chat application. Such widgets are automatically generated upon entering the link to a public dataset in TriplyDB. The chat application in Figure 4 understands the metadata properties: title, description, and image. Different Social Media applications may make use of different metadata properties. Figure 4. A dataset widget in the Slack chat application.","title":"Dataset metadata"},{"location":"triply-db-getting-started/publishing-data/#starting-services","text":"By default, datasets in TriplyDB can be queried through TriplyDB-js as well as through the Linked Data Fragments API. In order to allow additional query paradigms, specific services can be started from the \u201cCreate service\u201d page. This page is accessed by clicking on the \u201cServices\u201d icon in the left-hand sidebar. TriplyDB instances can be configured with different types of services. The below screenshot shows the \u201cCreate service\u201d page for a TriplyDB instance that allows SPARQL, Jena SPARQL, and Elasticsearch services to be created. Notice that three different types of services can be created. It is possible to create multiple services for one dataset.","title":"Starting services"},{"location":"triply-db-getting-started/publishing-data/#existing-services","text":"Existing services are displayed on service widgets (see screenshot). From these widgets, services can be created or deleted. Datasets can change whenever a graph is added, deleted or renamed. When this happens, the data in a service is out of sync with the data in the dataset and a synchronization button will appear in the service widget. By clicking the button, the service will be synchronized with the current state of the dataset.","title":"Existing services"},{"location":"triply-db-getting-started/publishing-data/#webhooks","text":"If you want to be notified or trigger an event when anything changes in your dataset, you can set up a webhook. The webhook page can be found under dataset's settings on the right, as shown in the image below. To create a webhook, you will need to provide the following information: Payload target : The URL to which the webhook message should be sent. Payload format : The format of the message. Trigger events : Select for which event you wish to trigger the webhook. The options are: Graph import : Happens when data is imported from a different dataset and where the data is already stored on the instance. Linked data upload : Happens when a person uploads data to the instance. The data did not exist on the instance before. Asset upload : Happens when an asset is uploaded. You can activate or deactivate the webhook with the slider after the Webhook is active message. After filling in everything, you can click on the SUBMIT button and the new webhook will be activated. For example, if you wish to trigger a pipeline on gitlab every time you upload an asset to your dataset, you can use the below snippet as a payload target, as described on the official gitlab documentation and select Asset upload as a trigger event. https://gitlab.example.com/api/v4/projects/<project_id>/trigger/pipeline?token=<token>&ref=<ref_name> When your webhook is created and active, you can see every occasion the webhook was called in the webhook trigger history.","title":"Webhooks"},{"location":"triply-db-getting-started/reference/","text":"On this page: Reference Access Levels Access level control Access Level meaning Access Level dependencies Access levels and workflows Markdown support Headings Text styling Hyperlinks Code In-line code Multi-line code blocks Code language Introspection Reference \u00b6 Access Levels \u00b6 TriplyDB uses Access Levels that determine who can access content. Access Levels can be specified for the following content: Datasets, including everything that exist at the dataset level, such as metadata, settings, graphs, and services. Queries Stories Access level control \u00b6 The Access Level control (see Figure 1 ) is available on the settings page for these content types. The Access Level control also appears on the create dialog for these content types. The standard Access Level is always \"Private\". An explicit user action is needed to set the Access Level to \"Internal\" or \"Public\". Figure 1. The Access Level control for content in TriplyDB. Access Level meaning \u00b6 What an Access Level means, depends on whether content belongs to a user or to an organization. The following table contains the meaning of the Access Levels for content that belongs to a user: Icon Access Level Meaning Private Content is only accessible to you. Internal Content is accessible to anyone who is logged into the same TriplyDB environment. Public Content is accessible to anyone on the Internet. The following table contains the meaning of the Access Levels for content that belongs to an organization: Icon Access Level Meaning Private Content is only accessible to organization members. Internal Content is accessible to anyone who is logged into the same TriplyDB environment. Public Content is accessible to anyone on the Internet. Access Levels cannot be specified for the following content. This means that this content is always publicly accessible: Organizations, including their metadata and members. Users, including their metadata. Access Level dependencies \u00b6 The Access Levels for datasets, queries, and stories may affect each other. For example, a public query may use a private dataset. This means that visitors who are not logged in, can see the query, its metadata, and its query string; however, such visitors will never receive query results from the private dataset. This ensures that private content always stays private, as intended. A warning is shown to the user when a dependency is introduced to content with a stricter Access Level (see Figure 2 ). This allows the user to change the Access Levels to a consistent state. Figure 2. A public query over a private dataset. Access levels and workflows \u00b6 These access levels are often used for the following workflow: You create a new dataset/query/story starts with access level \u2018Private\u2019. As the dataset/query/story progresses, give it access level \u2018Internal\u2019 to receive feedback from other users. Once the dataset/query/story is ready, give it access level \u2018Public\u2019 to publish it to the world. Markdown support \u00b6 Triply allows rich text formatting to be used in the following places: Dataset description Account description Saved Query description Data Story elements Site welcome message The following Markdown elements are supported: Headings \u00b6 Headings are used to divide a text into different sections. The hash character ( # ) at the beginning of a line indicates a heading is used. Multiple hash characters indicate nested headings. # Heading 1 ## Heading 2 ### Heading 3 #### Heading 4 ##### Heading 5 ###### Heading 6 Text styling \u00b6 Style Syntax Output Bold **bold** bold Italic _italic_ italic Strikethrough ~~strikethrough~~ ~~strikethrough~~ Hyperlinks \u00b6 Style Syntax Output Raw URL <https://triply.cc> https://triply.cc Labeled URL [label](https://triply.cc) label Notice that URLs can also be relative. This allows you to refer to other datasets, saved queries, etc. by using relative paths. Code \u00b6 There are options for formatting in-line code as well as multi-line code blocks. In-line code \u00b6 Code can also be used in-line with single backticks: Use `code` inside a sentence. Multi-line code blocks \u00b6 Multi-line code blocks start and end with three consecutive backticks. The following Markdown denotes two lines of Turtle: select * { graph ?g { ?s ?p ?o. } } The above is rendered as follows: select * { graph ?g { ?s ?p ?o. } } Code language \u00b6 The opening backticks are optionally following by the name of the code language. The following code languages are supported: Language Syntax SPARQL sparql Turtle ttl TypeScript typescript R r Python python The other supported languages are: Bash ( bash ), C ( c ), C++ ( cpp ), C# ( csharp ), Extended Backus-Naur Form ( ebnf ), Go ( go ), Haskell ( haskell ), Java ( java ), JavaScript ( javascript ), LaTeX ( latex ), Makefile ( makefile ), Markdown ( markdown ), Objective C ( objectivec ), Pascal ( pascal ), Perl ( perl ), Powershell ( powershell ), Prolog ( prolog ), Regular Expression ( regex ), Ruby ( ruby ), Scala ( scala ), SQL ( sql ), Yaml ( yaml ). Introspection \u00b6 TriplyDB provides SPARQL functions that can be used to obtain data about the current working environment. Examples are the name and URL of the current dataset, or the name and URL of the current user. This concept is called introspection . Introspection is supported for TriplyDB's default SPARQL Engine, Speedy. This means that in order to use introspection, service should be set to \"Speedy\" for (saved) SPARQL queries. All the introspection functions are identified by IRIs in the namespace https://triplydb.com/Triply/function/ . In the rest of this section, the following prefix is assumed: prefix tf: <https://triplydb.com/Triply/function/> The table below lists the introspection functions and the datatype of their result. The introspection functions do not need input parameters. Function Returns datatype Explanation tf:instance_url() xsd:anyURI The URL of the TriplyDB instance tf:authenticated_user_url() xsd:anyURI The URL of the user executing the SPARQL query tf:authenticated_user_name() xsd:string The name of the user executing the SPARQL query tf:queried_dataset_url() xsd:anyURI The URL of the dataset that is being queried tf:queried_dataset_name() xsd:string The name of the dataset that is being queried tf:queried_dataset_owner_url() xsd:anyURI The URL of the owner of the dataset that is being queried tf:queried_dataset_owner_name() xsd:string The name of the owner of the dataset that is being queried One way in which introspection functions can be put to use is in making dynamic overviews in data stories . The example below shows how the URL of the current user can be matched to provenance data in the Editor to create a personalised overview of Editor operations: prefix tf: <https://triplydb.com/Triply/function/> prefix editor: <https://triplydb.com/Triply/TriplyDB-instance-editor-vocabulary/> select ?action ?time where { bind (iri(tf:authenticated_user_url()) as ?userIri) ?event editor:actor ?userIri ; editor:action ?action ; editor:time ?time . }","title":"Reference"},{"location":"triply-db-getting-started/reference/#reference","text":"","title":"Reference"},{"location":"triply-db-getting-started/reference/#access-levels","text":"TriplyDB uses Access Levels that determine who can access content. Access Levels can be specified for the following content: Datasets, including everything that exist at the dataset level, such as metadata, settings, graphs, and services. Queries Stories","title":"Access Levels"},{"location":"triply-db-getting-started/reference/#access-level-control","text":"The Access Level control (see Figure 1 ) is available on the settings page for these content types. The Access Level control also appears on the create dialog for these content types. The standard Access Level is always \"Private\". An explicit user action is needed to set the Access Level to \"Internal\" or \"Public\". Figure 1. The Access Level control for content in TriplyDB.","title":"Access level control"},{"location":"triply-db-getting-started/reference/#access-level-meaning","text":"What an Access Level means, depends on whether content belongs to a user or to an organization. The following table contains the meaning of the Access Levels for content that belongs to a user: Icon Access Level Meaning Private Content is only accessible to you. Internal Content is accessible to anyone who is logged into the same TriplyDB environment. Public Content is accessible to anyone on the Internet. The following table contains the meaning of the Access Levels for content that belongs to an organization: Icon Access Level Meaning Private Content is only accessible to organization members. Internal Content is accessible to anyone who is logged into the same TriplyDB environment. Public Content is accessible to anyone on the Internet. Access Levels cannot be specified for the following content. This means that this content is always publicly accessible: Organizations, including their metadata and members. Users, including their metadata.","title":"Access Level meaning"},{"location":"triply-db-getting-started/reference/#access-level-dependencies","text":"The Access Levels for datasets, queries, and stories may affect each other. For example, a public query may use a private dataset. This means that visitors who are not logged in, can see the query, its metadata, and its query string; however, such visitors will never receive query results from the private dataset. This ensures that private content always stays private, as intended. A warning is shown to the user when a dependency is introduced to content with a stricter Access Level (see Figure 2 ). This allows the user to change the Access Levels to a consistent state. Figure 2. A public query over a private dataset.","title":"Access Level dependencies"},{"location":"triply-db-getting-started/reference/#access-levels-and-workflows","text":"These access levels are often used for the following workflow: You create a new dataset/query/story starts with access level \u2018Private\u2019. As the dataset/query/story progresses, give it access level \u2018Internal\u2019 to receive feedback from other users. Once the dataset/query/story is ready, give it access level \u2018Public\u2019 to publish it to the world.","title":"Access levels and workflows"},{"location":"triply-db-getting-started/reference/#markdown-support","text":"Triply allows rich text formatting to be used in the following places: Dataset description Account description Saved Query description Data Story elements Site welcome message The following Markdown elements are supported:","title":"Markdown support"},{"location":"triply-db-getting-started/reference/#headings","text":"Headings are used to divide a text into different sections. The hash character ( # ) at the beginning of a line indicates a heading is used. Multiple hash characters indicate nested headings. # Heading 1 ## Heading 2 ### Heading 3 #### Heading 4 ##### Heading 5 ###### Heading 6","title":"Headings"},{"location":"triply-db-getting-started/reference/#text-styling","text":"Style Syntax Output Bold **bold** bold Italic _italic_ italic Strikethrough ~~strikethrough~~ ~~strikethrough~~","title":"Text styling"},{"location":"triply-db-getting-started/reference/#hyperlinks","text":"Style Syntax Output Raw URL <https://triply.cc> https://triply.cc Labeled URL [label](https://triply.cc) label Notice that URLs can also be relative. This allows you to refer to other datasets, saved queries, etc. by using relative paths.","title":"Hyperlinks"},{"location":"triply-db-getting-started/reference/#code","text":"There are options for formatting in-line code as well as multi-line code blocks.","title":"Code"},{"location":"triply-db-getting-started/reference/#in-line-code","text":"Code can also be used in-line with single backticks: Use `code` inside a sentence.","title":"In-line code"},{"location":"triply-db-getting-started/reference/#multi-line-code-blocks","text":"Multi-line code blocks start and end with three consecutive backticks. The following Markdown denotes two lines of Turtle: select * { graph ?g { ?s ?p ?o. } } The above is rendered as follows: select * { graph ?g { ?s ?p ?o. } }","title":"Multi-line code blocks"},{"location":"triply-db-getting-started/reference/#code-language","text":"The opening backticks are optionally following by the name of the code language. The following code languages are supported: Language Syntax SPARQL sparql Turtle ttl TypeScript typescript R r Python python The other supported languages are: Bash ( bash ), C ( c ), C++ ( cpp ), C# ( csharp ), Extended Backus-Naur Form ( ebnf ), Go ( go ), Haskell ( haskell ), Java ( java ), JavaScript ( javascript ), LaTeX ( latex ), Makefile ( makefile ), Markdown ( markdown ), Objective C ( objectivec ), Pascal ( pascal ), Perl ( perl ), Powershell ( powershell ), Prolog ( prolog ), Regular Expression ( regex ), Ruby ( ruby ), Scala ( scala ), SQL ( sql ), Yaml ( yaml ).","title":"Code language"},{"location":"triply-db-getting-started/reference/#introspection","text":"TriplyDB provides SPARQL functions that can be used to obtain data about the current working environment. Examples are the name and URL of the current dataset, or the name and URL of the current user. This concept is called introspection . Introspection is supported for TriplyDB's default SPARQL Engine, Speedy. This means that in order to use introspection, service should be set to \"Speedy\" for (saved) SPARQL queries. All the introspection functions are identified by IRIs in the namespace https://triplydb.com/Triply/function/ . In the rest of this section, the following prefix is assumed: prefix tf: <https://triplydb.com/Triply/function/> The table below lists the introspection functions and the datatype of their result. The introspection functions do not need input parameters. Function Returns datatype Explanation tf:instance_url() xsd:anyURI The URL of the TriplyDB instance tf:authenticated_user_url() xsd:anyURI The URL of the user executing the SPARQL query tf:authenticated_user_name() xsd:string The name of the user executing the SPARQL query tf:queried_dataset_url() xsd:anyURI The URL of the dataset that is being queried tf:queried_dataset_name() xsd:string The name of the dataset that is being queried tf:queried_dataset_owner_url() xsd:anyURI The URL of the owner of the dataset that is being queried tf:queried_dataset_owner_name() xsd:string The name of the owner of the dataset that is being queried One way in which introspection functions can be put to use is in making dynamic overviews in data stories . The example below shows how the URL of the current user can be matched to provenance data in the Editor to create a personalised overview of Editor operations: prefix tf: <https://triplydb.com/Triply/function/> prefix editor: <https://triplydb.com/Triply/TriplyDB-instance-editor-vocabulary/> select ?action ?time where { bind (iri(tf:authenticated_user_url()) as ?userIri) ?event editor:actor ?userIri ; editor:action ?action ; editor:time ?time . }","title":"Introspection"},{"location":"triply-db-getting-started/saved-queries/","text":"On this page: Saved Queries How to save a query Creating a new version Deleting a saved query Using a saved query Sharing a saved query Downloading a query result Download more than 10 000 query results - SPARQL pagination Pagination with the saved query API Pagination with TriplyDB.js Using a saved query as RESTful API Using a saved query in Python or R notebooks Query metadata Saved Queries \u00b6 A Saved Query is a versioned SPARQL query with its own URL. Using this URL, users are able to view any version of the query and its results. It can also be used to run the query and retrieve the results from a browser or a program, removing the hassle of figuring out how to run a SPARQL query. How to save a query \u00b6 There are two ways to create a saved query. You need to be logged in and have authorization rights on the dataset to use this feature When working from the SPARQL IDE Using the Saved Queries tab in a dataset Creating a saved query with the SPARQL IDE is done by writing a query/visualization and hitting the save button Creating a new version \u00b6 Updating the saved query can be done by clicking a query in the Saved Queries tab and editing the query or the visualization. Hit the save button to save it as a new version. Deleting a saved query \u00b6 If you want to delete a saved query, you can do so by clicking the three dots on the top right corner of the query, as shown in the image below, and then clicking Delete . Using a saved query \u00b6 Sharing a saved query \u00b6 To share a saved query, for example in Data Stories , you can copy the link that is used when you open the query in TriplyDB. Let's say you have a query called Timelined-Cars-BETA in the dataset core under the account dbpedia and you want to use version 9. Then the following link would be used: https://triplydb.com/DBpedia-association/-/queries/timeline-cars/9 If you want to always use the latest query, you can simply omit the version number like so: https://triplydb.com/DBpedia-association/-/queries/timeline-cars Downloading a query result \u00b6 The result of a query can be downloaded via the TriplyDB interface. After saving the query, open it in TriplyDB. e.g. https://triplydb.com/DBpedia-association/-/queries/timeline-cars/ . You can download results in different data format, depending on which visualization option you use. For example, if you want to download the results in a .json format, you can choose the option Response and click on the download icon or scroll down and click on Download result . The downloaded file is automatically stored in the Downloads -folder and has the name of the query. In our example, the file is called timeline-cars.json . The downloaded file contains the query result as a json-object. TriplyDB also displays the json-object when selecting the option Response . Below is a table of all supported visualizations and what format of results they produce. Visualization option Result data format Table .csv Response .json Gallery Download not supported Chart .svg Geo Download not supported Geo-3D Download not supported Geo events Download not supported Markup .svg , .html Network .png Timeline Download not supported As another example, to download the query result in CSV-format, select the option Table and click on the download icon. The downloaded file is named after the query with the suffix .csv . Download more than 10 000 query results - SPARQL pagination \u00b6 This section explains how to retrieve all results from a SPARQL query using pagination. Often SPARQL queries can return more than 10.000 results, but due to limitations the result set will only consist out of the first 10.000 results. To retrieve more than 10.000 results you can use pagination. TriplyDB supports two methods to retrieve all results from a SPARQL query. Pagination with the saved query API or Pagination with TriplyDB.js. Pagination with the saved query API \u00b6 Each TriplyDB instance has a fully RESTful API. The TriplyDB RESTful API is extended for saved SPARQL queries. The API for saved queries is extended with two arguments that the query is able to process paginated result sets. The arguments are \u2018page\u2019 and \u2018pageSize\u2019. An example of a paginated saved SPARQL query request would look like: https://api.triplydb.com/queries/academy/pokemon-color/run?page=3&pageSize=100 The example request argument \u2018page\u2019 corresponds to the requested page. In the example request this would correspond to the third page of paginated SPARQL query, according to the \u2018pageSize\u2019. There is no maximum \u2018page\u2019 limit, as a SPARQL query could return an arbitrary number of results. When no results can be retrieved for the requested page an empty page will be returned. The argument \u2018pageSize\u2019 corresponds to how many results each page would contain. The \u2018pageSize\u2019 has a default of 100 returned results and a maximum \u2018pageSize\u2019 limit of 10.000 returned results. The request will return an error when the \u2018pageSize\u2019 is set higher than 10.000. The RESTful API for the saved SPARQL queries follows the RFC 8288 standard. The request will return an response body containing the result set and a response header. The response header contains a link header with the relative \"next\" request, the relative \"prev\" request, and the relative \"first\" request. By following the \"next\" link header request you can chain the pagination and retrieve all results. link: <https://api.triplydb.com/queries/academy/pokemon-color/run?page=4&pageSize=100>; rel=\"next\", <https://api.triplydb.com/queries/academy/pokemon-color/run?page=2&pageSize=100>; rel=\"prev\", <https://api.triplydb.com/queries/academy/pokemon-color/run?page=1&pageSize=100>; rel=\"first\" Pagination with TriplyDB.js \u00b6 TriplyDB.js is the official programming library for interacting with TriplyDB . TriplyDB.js allows the user to connect to a TriplyDB instance via the TypeScript language. TriplyDB.js has the advantage that it can handle pagination internally so it can reliably retrieve a large number of results. To get the output for a construct or select query, follow these steps: 1. Import the TriplyDB library and set your parameters, regarding the TriplyDB instance and the account in which you have saved the query as well as the name of the query. Do not forget that we perform TriplyDB.js requests within an async context. import Client from '@triply/triplydb' async function run() { // Your code goes here. const client = Client.get({token: process.env.TRIPLYDB_TOKEN}) const account = await client.getAccount('account-name') const query = await account.getQuery('name-of-some-query') } run() 2. Get the results of a query by setting a results variable. More specifically, for construct queries you use the statements() call: const query = await account.getQuery('name-of-some-query') const results = query.results().statements() For select queries you use the bindings() call: const query = await account.getQuery('name-of-some-query') const results = query.results().bindings() Additionally, saved queries can have 'API variables' that allow you to specify variables that are used in the query. Thus, if you have query parameters, pass their values as the first argument to results as follows: // For SPARQL construct queries. const results = query.results({ someVariable: 'value of someVariable', anotherVariable: 'value of anotherVariable' }).statements() // For SPARQL select queries. const results = query.results({ someVariable: 'value of someVariable', anotherVariable: 'value of anotherVariable' }).bindings() 3. To iterate the results of your SPARQL query you have three options: a. Iterate through the results per row in a for -loop: // Iterating over the results. for await (const row of results) { // execute something } Note: For select queries the for -loop iterates over the rows of the result set. For construct queries the for -loop iterates over the statements in the result set. b. Save the results to a file. This is only supported for SPARQL construct queries: // Saving the results of a SPARQL construct query to a file. await results.toFile('my-file.nt') c. Load all results into memory in the form of an Array. Note that this is almost never used. If you want to process results, then use the 3a option; if you want to persist results, then option 3b suits better. // Loading results for a SPARQL construct or SPARQL select query into memory. const array = await results.toArray() Using a saved query as RESTful API \u00b6 Each TriplyDB instance has a fully RESTful API. The TriplyDB RESTful API is extended for saved SPARQL queries. A saved query can be used as a RESTful API to retrieve data from your linked dataset. The URL next to the keywork API is the RESTful API URL and can be used with RESTful API libraries. You can copy the RESTful API by pressing the copy button just behind the URL. Pressing the copy button from the above query will result in the following run url: https://api.triplydb.com/queries/DBpedia-association/timeline-cars/run When you copy this URL in your browser or fetch the URL with curl, you will get a get request to a RESTful API and get a JSON representation of the data in your browser or command window. Using a saved query in Python or R notebooks \u00b6 SPARQL queries as a RESTful API, also means you can transport your data to your Python script, R script or Jupyter notebook. To use the result set from your SPARQL query you need to connect your script to the saved SPARQL query. To do this you will need to write a small connector. To help you out TriplyDB has added a code snippet generator for Python and R. This snippet contains the code to retrieve the data from the SPARQL query into your script or notebook. You can open the code snippet generator by clicking on the '' button on the right side of the screen. Clicking the '' button opens the code snippet screen. Here you select the snippet in the language you want to have, either Python or R. You can then copy the snippet, by clicking the 'copy to clipboard' button or selecting the snippet and pressing ctrl-c . Now you can paste the code in the location you want to use the data. The data is stored in the data variable in JSON format. When the SPARQL query is not public, but instead either private or internal, you will need to add an authorization header to the get request. Without the authorization header the request will return an incorrect response. Checkout Creating your API token about creating your API-token for the authorization header. Check out the SPARQL pagination page when you want to query a SPARQL query that holds more than 10.000 results. The SPARQL pagination page will explain how you can retrieve the complete set. Query metadata \u00b6 Every Saved Query has a metadata section. This metadata section includes the following two links: A link to the dataset over which the query is executed. Clicking this links navigates to the dataset homepage. A link to the service by which the query is executed. Clicking this link navigates to the services page that includes that service. Users can specify a query title and description, both of which are included as metadata. The access level and version of the query are also exposed as metadata. See the following screenshot for how the metadata fields are shown in TriplyDB: Users can specify additional metadata inside the query string, by using the GRLC annotation format. GRLC annotations start with the hash and plus sign characters ( #+ ). Visit the GRLC project to learn more about this format. For example, the following GRLC annotation could indicate to a software application that the query should be repeated every hour: #+ frequency: hourly See the Triply API documentation for how to retrieve query metadata, including how to retrieve GRLC annotations.","title":"Saved Queries"},{"location":"triply-db-getting-started/saved-queries/#saved-queries","text":"A Saved Query is a versioned SPARQL query with its own URL. Using this URL, users are able to view any version of the query and its results. It can also be used to run the query and retrieve the results from a browser or a program, removing the hassle of figuring out how to run a SPARQL query.","title":"Saved Queries"},{"location":"triply-db-getting-started/saved-queries/#how-to-save-a-query","text":"There are two ways to create a saved query. You need to be logged in and have authorization rights on the dataset to use this feature When working from the SPARQL IDE Using the Saved Queries tab in a dataset Creating a saved query with the SPARQL IDE is done by writing a query/visualization and hitting the save button","title":"How to save a query"},{"location":"triply-db-getting-started/saved-queries/#creating-a-new-version","text":"Updating the saved query can be done by clicking a query in the Saved Queries tab and editing the query or the visualization. Hit the save button to save it as a new version.","title":"Creating a new version"},{"location":"triply-db-getting-started/saved-queries/#deleting-a-saved-query","text":"If you want to delete a saved query, you can do so by clicking the three dots on the top right corner of the query, as shown in the image below, and then clicking Delete .","title":"Deleting a saved query"},{"location":"triply-db-getting-started/saved-queries/#using-a-saved-query","text":"","title":"Using a saved query"},{"location":"triply-db-getting-started/saved-queries/#sharing-a-saved-query","text":"To share a saved query, for example in Data Stories , you can copy the link that is used when you open the query in TriplyDB. Let's say you have a query called Timelined-Cars-BETA in the dataset core under the account dbpedia and you want to use version 9. Then the following link would be used: https://triplydb.com/DBpedia-association/-/queries/timeline-cars/9 If you want to always use the latest query, you can simply omit the version number like so: https://triplydb.com/DBpedia-association/-/queries/timeline-cars","title":"Sharing a saved query"},{"location":"triply-db-getting-started/saved-queries/#downloading-a-query-result","text":"The result of a query can be downloaded via the TriplyDB interface. After saving the query, open it in TriplyDB. e.g. https://triplydb.com/DBpedia-association/-/queries/timeline-cars/ . You can download results in different data format, depending on which visualization option you use. For example, if you want to download the results in a .json format, you can choose the option Response and click on the download icon or scroll down and click on Download result . The downloaded file is automatically stored in the Downloads -folder and has the name of the query. In our example, the file is called timeline-cars.json . The downloaded file contains the query result as a json-object. TriplyDB also displays the json-object when selecting the option Response . Below is a table of all supported visualizations and what format of results they produce. Visualization option Result data format Table .csv Response .json Gallery Download not supported Chart .svg Geo Download not supported Geo-3D Download not supported Geo events Download not supported Markup .svg , .html Network .png Timeline Download not supported As another example, to download the query result in CSV-format, select the option Table and click on the download icon. The downloaded file is named after the query with the suffix .csv .","title":"Downloading a query result"},{"location":"triply-db-getting-started/saved-queries/#download-more-than-10-000-query-results-sparql-pagination","text":"This section explains how to retrieve all results from a SPARQL query using pagination. Often SPARQL queries can return more than 10.000 results, but due to limitations the result set will only consist out of the first 10.000 results. To retrieve more than 10.000 results you can use pagination. TriplyDB supports two methods to retrieve all results from a SPARQL query. Pagination with the saved query API or Pagination with TriplyDB.js.","title":"Download more than 10 000 query results - SPARQL pagination"},{"location":"triply-db-getting-started/saved-queries/#pagination-with-the-saved-query-api","text":"Each TriplyDB instance has a fully RESTful API. The TriplyDB RESTful API is extended for saved SPARQL queries. The API for saved queries is extended with two arguments that the query is able to process paginated result sets. The arguments are \u2018page\u2019 and \u2018pageSize\u2019. An example of a paginated saved SPARQL query request would look like: https://api.triplydb.com/queries/academy/pokemon-color/run?page=3&pageSize=100 The example request argument \u2018page\u2019 corresponds to the requested page. In the example request this would correspond to the third page of paginated SPARQL query, according to the \u2018pageSize\u2019. There is no maximum \u2018page\u2019 limit, as a SPARQL query could return an arbitrary number of results. When no results can be retrieved for the requested page an empty page will be returned. The argument \u2018pageSize\u2019 corresponds to how many results each page would contain. The \u2018pageSize\u2019 has a default of 100 returned results and a maximum \u2018pageSize\u2019 limit of 10.000 returned results. The request will return an error when the \u2018pageSize\u2019 is set higher than 10.000. The RESTful API for the saved SPARQL queries follows the RFC 8288 standard. The request will return an response body containing the result set and a response header. The response header contains a link header with the relative \"next\" request, the relative \"prev\" request, and the relative \"first\" request. By following the \"next\" link header request you can chain the pagination and retrieve all results. link: <https://api.triplydb.com/queries/academy/pokemon-color/run?page=4&pageSize=100>; rel=\"next\", <https://api.triplydb.com/queries/academy/pokemon-color/run?page=2&pageSize=100>; rel=\"prev\", <https://api.triplydb.com/queries/academy/pokemon-color/run?page=1&pageSize=100>; rel=\"first\"","title":"Pagination with the saved query API"},{"location":"triply-db-getting-started/saved-queries/#pagination-with-triplydbjs","text":"TriplyDB.js is the official programming library for interacting with TriplyDB . TriplyDB.js allows the user to connect to a TriplyDB instance via the TypeScript language. TriplyDB.js has the advantage that it can handle pagination internally so it can reliably retrieve a large number of results. To get the output for a construct or select query, follow these steps: 1. Import the TriplyDB library and set your parameters, regarding the TriplyDB instance and the account in which you have saved the query as well as the name of the query. Do not forget that we perform TriplyDB.js requests within an async context. import Client from '@triply/triplydb' async function run() { // Your code goes here. const client = Client.get({token: process.env.TRIPLYDB_TOKEN}) const account = await client.getAccount('account-name') const query = await account.getQuery('name-of-some-query') } run() 2. Get the results of a query by setting a results variable. More specifically, for construct queries you use the statements() call: const query = await account.getQuery('name-of-some-query') const results = query.results().statements() For select queries you use the bindings() call: const query = await account.getQuery('name-of-some-query') const results = query.results().bindings() Additionally, saved queries can have 'API variables' that allow you to specify variables that are used in the query. Thus, if you have query parameters, pass their values as the first argument to results as follows: // For SPARQL construct queries. const results = query.results({ someVariable: 'value of someVariable', anotherVariable: 'value of anotherVariable' }).statements() // For SPARQL select queries. const results = query.results({ someVariable: 'value of someVariable', anotherVariable: 'value of anotherVariable' }).bindings() 3. To iterate the results of your SPARQL query you have three options: a. Iterate through the results per row in a for -loop: // Iterating over the results. for await (const row of results) { // execute something } Note: For select queries the for -loop iterates over the rows of the result set. For construct queries the for -loop iterates over the statements in the result set. b. Save the results to a file. This is only supported for SPARQL construct queries: // Saving the results of a SPARQL construct query to a file. await results.toFile('my-file.nt') c. Load all results into memory in the form of an Array. Note that this is almost never used. If you want to process results, then use the 3a option; if you want to persist results, then option 3b suits better. // Loading results for a SPARQL construct or SPARQL select query into memory. const array = await results.toArray()","title":"Pagination with TriplyDB.js"},{"location":"triply-db-getting-started/saved-queries/#using-a-saved-query-as-restful-api","text":"Each TriplyDB instance has a fully RESTful API. The TriplyDB RESTful API is extended for saved SPARQL queries. A saved query can be used as a RESTful API to retrieve data from your linked dataset. The URL next to the keywork API is the RESTful API URL and can be used with RESTful API libraries. You can copy the RESTful API by pressing the copy button just behind the URL. Pressing the copy button from the above query will result in the following run url: https://api.triplydb.com/queries/DBpedia-association/timeline-cars/run When you copy this URL in your browser or fetch the URL with curl, you will get a get request to a RESTful API and get a JSON representation of the data in your browser or command window.","title":"Using a saved query as RESTful API"},{"location":"triply-db-getting-started/saved-queries/#using-a-saved-query-in-python-or-r-notebooks","text":"SPARQL queries as a RESTful API, also means you can transport your data to your Python script, R script or Jupyter notebook. To use the result set from your SPARQL query you need to connect your script to the saved SPARQL query. To do this you will need to write a small connector. To help you out TriplyDB has added a code snippet generator for Python and R. This snippet contains the code to retrieve the data from the SPARQL query into your script or notebook. You can open the code snippet generator by clicking on the '' button on the right side of the screen. Clicking the '' button opens the code snippet screen. Here you select the snippet in the language you want to have, either Python or R. You can then copy the snippet, by clicking the 'copy to clipboard' button or selecting the snippet and pressing ctrl-c . Now you can paste the code in the location you want to use the data. The data is stored in the data variable in JSON format. When the SPARQL query is not public, but instead either private or internal, you will need to add an authorization header to the get request. Without the authorization header the request will return an incorrect response. Checkout Creating your API token about creating your API-token for the authorization header. Check out the SPARQL pagination page when you want to query a SPARQL query that holds more than 10.000 results. The SPARQL pagination page will explain how you can retrieve the complete set.","title":"Using a saved query in Python or R notebooks"},{"location":"triply-db-getting-started/saved-queries/#query-metadata","text":"Every Saved Query has a metadata section. This metadata section includes the following two links: A link to the dataset over which the query is executed. Clicking this links navigates to the dataset homepage. A link to the service by which the query is executed. Clicking this link navigates to the services page that includes that service. Users can specify a query title and description, both of which are included as metadata. The access level and version of the query are also exposed as metadata. See the following screenshot for how the metadata fields are shown in TriplyDB: Users can specify additional metadata inside the query string, by using the GRLC annotation format. GRLC annotations start with the hash and plus sign characters ( #+ ). Visit the GRLC project to learn more about this format. For example, the following GRLC annotation could indicate to a software application that the query should be repeated every hour: #+ frequency: hourly See the Triply API documentation for how to retrieve query metadata, including how to retrieve GRLC annotations.","title":"Query metadata"},{"location":"triply-db-getting-started/uploading-data/","text":"On this page: Uploading Data Creating a new dataset Opening the \u201cCreate dataset\u201d dialog Inside the \u201cCreate dataset\u201d dialog Adding data Opening the \u201cAdd data\u201d pane Inside the \u201cAdd data\u201d pane Add data from an existing dataset Add data from URL Add data from files Supported data formats CSV and TSV format XML format Adding malformed data Assets: binary data Uploading Data \u00b6 This section explains how to create a linked dataset in TriplyDB. Creating a new dataset \u00b6 You must be logged in before you can create a new dataset. Opening the \u201cCreate dataset\u201d dialog \u00b6 You can create a new dataset in either of the following two ways: From the home screen (see Figure 1a ), click on the + button next to \"Your datasets\", on the right-hand side of the screen. From the user screen (see Figure 1b ), click on the \u201cCreate dataset\u201d button on the right-hand side. Figure 1a. The home screen for a logged in user. Figure 1b. The user screen for a logged in user. Inside the \u201cCreate dataset\u201d dialog \u00b6 This opens the \u201cCreate dataset\u201d dialog (see Figure 2 ). Figure 2. The \u201cCreate dataset\u201d dialog In the \u201cCreate dataset\u201d dialog, perform the following steps: Required: Enter a dataset name. A dataset name can contain letters, number, and hyphens. Optional: Enter a dataset display name. The display name will be shown in the GUI and will be included in dataset metadata. Optional: Enter a dataset description. This description will be shown in the GUI, and will be included in dataset metadata. The description can be formatted with Markdown. See Section Markdown for details. Optional: Change the access level of the dataset. The standard access level is \u201cPrivate\u201d. See Section Dataset Access Levels for more information. This creates a new dataset, and displays the \u201cAdd data\u201d page (see Section Adding data ). Adding data \u00b6 You must first have a dataset, before you can add data. See Section Creating a new dataset for more information. Opening the \u201cAdd data\u201d pane \u00b6 You can open the \u201cAdd data\u201d pane in either of the following two ways: From the Graphs page, click on the \"Import a new graph\" button (see Figure 3a ). This opens the \"Add data\" pane. When a dataset does not have any data yet, a message is displayed on the dataset homepage (see Figure 3b ) that can be clicked. This opens the \"Add data\" pane. After creating a new dataset, the \"Add data\" pane is automatically opened. Figure 3a. The Graphs page of a dataset. Figure 3b. The Graphs page of a dataset. Inside the \u201cAdd data\u201d pane \u00b6 The \u201cAdd data\u201d pane is now displayed (see Figure 4 ). Figure 4. The \u201cAdd data\u201d pane. In the \"Add data\" pane, choose one of the following approaches for adding data: \"Add data from an existing dataset\" Search for data from a dataset that you have access to in the TriplyDB system. After you have found a dataset, you can choose which graphs to add. See Section Add data from an existing dataset for more details. \"Add data from URL\" Enter a URL to a data file that is published online. The URL must be publicly accessible. The URL must point to a file that contains RDF or CSV data. See Section Add data from a URL for more details. \"Add data from files\" Click the cloud icon to open a file explorer window, in which you can select one or more files from your computer. Alternatively, drag-and-drop the local files from your computer onto the cloud icon with the upward pointing arrow. Files must contain RDF or CSV data. See Section Add data from files for more details. Add data from an existing dataset \u00b6 The first option for adding data is to add it from datasets that are already published in the same TriplyDB instance. This is done with the \u201cAdd data from an existing dataset\u201d field. By typing in this field, a dropdown list of existing datasets is shown (see Figure 5 ). Figure 5. The dropdown list that shows existing datasets. Once the correct dataset appears in the dropdown list, click it to select it. This will open the \"Import from dataset\" pane (see Figure 6 ). You can choose which graphs to import from the existing dataset. Click \"Import graphs\" to start importing from an existing dataset. Moments later, the graphs are added to your dataset. Figure 6. The \"Import from dataset\" pane. Add data from URL \u00b6 The second option for adding data is to add it from an online URL. This is done by entering the URL inside the \u201cAdd data from a URL\u201d text field (see Figure 7 ). After you have entered the URL, click the orange button on the right to start adding data. The data is now being downloaded to your dataset. How long this takes depends on the size of the data and the speed of the remote server where the data is retrieved from. Figure 7. The \"Add data from URL\" field. Only URLs that contain supported data formats will be added. See Section Supported data formats for more information. Add data from files \u00b6 The third option for adding data is to add it from files that are on your computer. This can be done in two ways: Click the cloud icon to open a file finder dialog. Here you can select one or more files from your computer (see Figure 8 ). The file finder dialog that opens, depends on your Operating System. In Figure 8 , the Windows file finder dialog is shown. Drag-and-drop one or more files from your computer onto the cloud icon with the upward pointing arrow. Figure 8. The file finder dialog that is opened when adding data from files. After you have added one of more files, a list of uploaded appears (see Figure 9 ). You can add or remove more files, until you have the complete upload job configured. Once the list of files is complete, you can click \"Import from files\" to start adding data from files. How long this takes depends on the size of the data. Figure 9. The list of uploaded files in the \"Add data from files\" pane. Only files that contain supported data formats will be added. See Section Supported data formats for more information. Supported data formats \u00b6 Files must contain RDF, CSV, TSV or XML data, and must use one of the supported file name extensions: Data Format File name extension Comma-Separated Values (CSV) .csv Tab-Separated Values (CSV) .tsv XML .xml JSON-LD .jsonld , .json N-Quads .nq N-Triples .nt RDF/XML .rdf , .owl , .owx TriG .trig Turtle .ttl , .n3 It is possible to upload up to 1,000 separate files in this way. When you have a lot of files and/or large files, it is better to compress them into an archive format. This allows an any number of files of any size to be uploaded. The following archive/compression formats are supported: Archive format File name extension gzip .gz bzip2 .bz2 tar tar XZ .xz ZIP .zip CSV and TSV format \u00b6 When you upload CSV (Comma-Separated Values) or TSV (Tab-Separated Values) files to TriplyDB, they are automatically converted to RDF and stored in two linked data representations: Facade-X representation : An expressive RDF model that preserves the full structure of the tabular data. The model is documented in detail here . Simple representation : A straightforward row-based model for easier querying Simple representation: The simple representation provides a more opinionated and direct mapping, and is suitable for tables with a simple structure: A table resource is created that links to all rows via rdfs:member Each row gets a unique IRI based on the row number (e.g., .../row/1 , .../row/2 , etc.) Column headers become properties in the https://triplydb.com/table/triply/def/ namespace Each property has an rdfs:label with the original column header name Cell values are stored as string literals Take for example this CSV file: product_id,name,category,price,in_stock P001,Laptop Stand,Office Equipment,49.99,true P002,Ergonomic Keyboard,Peripherals,89.50,true This can be queried as such: prefix table: <https://triplydb.com/table/triply/def/> prefix rdfs: <http://www.w3.org/2000/01/rdf-schema#> select ?name ?category ?price where { ?table rdfs:member ?row . ?row table:name ?name ; table:category ?category ; table:price ?price . } XML format \u00b6 When you upload XML files to TriplyDB, they are automatically converted to RDF using the Facade-X data model. This preserves the hierarchical structure of the XML document, making it queryable via SPARQL. See here for more details on the Facade-X XML data model. Adding malformed data \u00b6 TriplyDB only allows valid RDF data to be added. If data is malformed, TriplyDB will show an error message that indicates which part of the RDF data is malformed (see screenshot). If such malformed data is encountered, the RDF file must first be corrected and uploaded again. TriplyDB follows the linked data standards strictly. Many triple stores allow incorrect RDF data to be added. This may seem convenient during the loading phase, but often results in errors when standards-compliant tools start using the data. Assets: binary data \u00b6 Not all data can be stored as RDF data. For example images and video files use a binary format. Such files can also be stored in TriplyDB and can be integrated into the Knowledge Graph.","title":"Uploading Data"},{"location":"triply-db-getting-started/uploading-data/#uploading-data","text":"This section explains how to create a linked dataset in TriplyDB.","title":"Uploading Data"},{"location":"triply-db-getting-started/uploading-data/#creating-a-new-dataset","text":"You must be logged in before you can create a new dataset.","title":"Creating a new dataset"},{"location":"triply-db-getting-started/uploading-data/#opening-the-create-dataset-dialog","text":"You can create a new dataset in either of the following two ways: From the home screen (see Figure 1a ), click on the + button next to \"Your datasets\", on the right-hand side of the screen. From the user screen (see Figure 1b ), click on the \u201cCreate dataset\u201d button on the right-hand side. Figure 1a. The home screen for a logged in user. Figure 1b. The user screen for a logged in user.","title":"Opening the \u201cCreate dataset\u201d dialog"},{"location":"triply-db-getting-started/uploading-data/#inside-the-create-dataset-dialog","text":"This opens the \u201cCreate dataset\u201d dialog (see Figure 2 ). Figure 2. The \u201cCreate dataset\u201d dialog In the \u201cCreate dataset\u201d dialog, perform the following steps: Required: Enter a dataset name. A dataset name can contain letters, number, and hyphens. Optional: Enter a dataset display name. The display name will be shown in the GUI and will be included in dataset metadata. Optional: Enter a dataset description. This description will be shown in the GUI, and will be included in dataset metadata. The description can be formatted with Markdown. See Section Markdown for details. Optional: Change the access level of the dataset. The standard access level is \u201cPrivate\u201d. See Section Dataset Access Levels for more information. This creates a new dataset, and displays the \u201cAdd data\u201d page (see Section Adding data ).","title":"Inside the \u201cCreate dataset\u201d dialog"},{"location":"triply-db-getting-started/uploading-data/#adding-data","text":"You must first have a dataset, before you can add data. See Section Creating a new dataset for more information.","title":"Adding data"},{"location":"triply-db-getting-started/uploading-data/#opening-the-add-data-pane","text":"You can open the \u201cAdd data\u201d pane in either of the following two ways: From the Graphs page, click on the \"Import a new graph\" button (see Figure 3a ). This opens the \"Add data\" pane. When a dataset does not have any data yet, a message is displayed on the dataset homepage (see Figure 3b ) that can be clicked. This opens the \"Add data\" pane. After creating a new dataset, the \"Add data\" pane is automatically opened. Figure 3a. The Graphs page of a dataset. Figure 3b. The Graphs page of a dataset.","title":"Opening the \u201cAdd data\u201d pane"},{"location":"triply-db-getting-started/uploading-data/#inside-the-add-data-pane","text":"The \u201cAdd data\u201d pane is now displayed (see Figure 4 ). Figure 4. The \u201cAdd data\u201d pane. In the \"Add data\" pane, choose one of the following approaches for adding data: \"Add data from an existing dataset\" Search for data from a dataset that you have access to in the TriplyDB system. After you have found a dataset, you can choose which graphs to add. See Section Add data from an existing dataset for more details. \"Add data from URL\" Enter a URL to a data file that is published online. The URL must be publicly accessible. The URL must point to a file that contains RDF or CSV data. See Section Add data from a URL for more details. \"Add data from files\" Click the cloud icon to open a file explorer window, in which you can select one or more files from your computer. Alternatively, drag-and-drop the local files from your computer onto the cloud icon with the upward pointing arrow. Files must contain RDF or CSV data. See Section Add data from files for more details.","title":"Inside the \u201cAdd data\u201d pane"},{"location":"triply-db-getting-started/uploading-data/#add-data-from-an-existing-dataset","text":"The first option for adding data is to add it from datasets that are already published in the same TriplyDB instance. This is done with the \u201cAdd data from an existing dataset\u201d field. By typing in this field, a dropdown list of existing datasets is shown (see Figure 5 ). Figure 5. The dropdown list that shows existing datasets. Once the correct dataset appears in the dropdown list, click it to select it. This will open the \"Import from dataset\" pane (see Figure 6 ). You can choose which graphs to import from the existing dataset. Click \"Import graphs\" to start importing from an existing dataset. Moments later, the graphs are added to your dataset. Figure 6. The \"Import from dataset\" pane.","title":"Add data from an existing dataset"},{"location":"triply-db-getting-started/uploading-data/#add-data-from-url","text":"The second option for adding data is to add it from an online URL. This is done by entering the URL inside the \u201cAdd data from a URL\u201d text field (see Figure 7 ). After you have entered the URL, click the orange button on the right to start adding data. The data is now being downloaded to your dataset. How long this takes depends on the size of the data and the speed of the remote server where the data is retrieved from. Figure 7. The \"Add data from URL\" field. Only URLs that contain supported data formats will be added. See Section Supported data formats for more information.","title":"Add data from URL"},{"location":"triply-db-getting-started/uploading-data/#add-data-from-files","text":"The third option for adding data is to add it from files that are on your computer. This can be done in two ways: Click the cloud icon to open a file finder dialog. Here you can select one or more files from your computer (see Figure 8 ). The file finder dialog that opens, depends on your Operating System. In Figure 8 , the Windows file finder dialog is shown. Drag-and-drop one or more files from your computer onto the cloud icon with the upward pointing arrow. Figure 8. The file finder dialog that is opened when adding data from files. After you have added one of more files, a list of uploaded appears (see Figure 9 ). You can add or remove more files, until you have the complete upload job configured. Once the list of files is complete, you can click \"Import from files\" to start adding data from files. How long this takes depends on the size of the data. Figure 9. The list of uploaded files in the \"Add data from files\" pane. Only files that contain supported data formats will be added. See Section Supported data formats for more information.","title":"Add data from files"},{"location":"triply-db-getting-started/uploading-data/#supported-data-formats","text":"Files must contain RDF, CSV, TSV or XML data, and must use one of the supported file name extensions: Data Format File name extension Comma-Separated Values (CSV) .csv Tab-Separated Values (CSV) .tsv XML .xml JSON-LD .jsonld , .json N-Quads .nq N-Triples .nt RDF/XML .rdf , .owl , .owx TriG .trig Turtle .ttl , .n3 It is possible to upload up to 1,000 separate files in this way. When you have a lot of files and/or large files, it is better to compress them into an archive format. This allows an any number of files of any size to be uploaded. The following archive/compression formats are supported: Archive format File name extension gzip .gz bzip2 .bz2 tar tar XZ .xz ZIP .zip","title":"Supported data formats"},{"location":"triply-db-getting-started/uploading-data/#csv-and-tsv-format","text":"When you upload CSV (Comma-Separated Values) or TSV (Tab-Separated Values) files to TriplyDB, they are automatically converted to RDF and stored in two linked data representations: Facade-X representation : An expressive RDF model that preserves the full structure of the tabular data. The model is documented in detail here . Simple representation : A straightforward row-based model for easier querying Simple representation: The simple representation provides a more opinionated and direct mapping, and is suitable for tables with a simple structure: A table resource is created that links to all rows via rdfs:member Each row gets a unique IRI based on the row number (e.g., .../row/1 , .../row/2 , etc.) Column headers become properties in the https://triplydb.com/table/triply/def/ namespace Each property has an rdfs:label with the original column header name Cell values are stored as string literals Take for example this CSV file: product_id,name,category,price,in_stock P001,Laptop Stand,Office Equipment,49.99,true P002,Ergonomic Keyboard,Peripherals,89.50,true This can be queried as such: prefix table: <https://triplydb.com/table/triply/def/> prefix rdfs: <http://www.w3.org/2000/01/rdf-schema#> select ?name ?category ?price where { ?table rdfs:member ?row . ?row table:name ?name ; table:category ?category ; table:price ?price . }","title":"CSV and TSV format"},{"location":"triply-db-getting-started/uploading-data/#xml-format","text":"When you upload XML files to TriplyDB, they are automatically converted to RDF using the Facade-X data model. This preserves the hierarchical structure of the XML document, making it queryable via SPARQL. See here for more details on the Facade-X XML data model.","title":"XML format"},{"location":"triply-db-getting-started/uploading-data/#adding-malformed-data","text":"TriplyDB only allows valid RDF data to be added. If data is malformed, TriplyDB will show an error message that indicates which part of the RDF data is malformed (see screenshot). If such malformed data is encountered, the RDF file must first be corrected and uploaded again. TriplyDB follows the linked data standards strictly. Many triple stores allow incorrect RDF data to be added. This may seem convenient during the loading phase, but often results in errors when standards-compliant tools start using the data.","title":"Adding malformed data"},{"location":"triply-db-getting-started/uploading-data/#assets-binary-data","text":"Not all data can be stored as RDF data. For example images and video files use a binary format. Such files can also be stored in TriplyDB and can be integrated into the Knowledge Graph.","title":"Assets: binary data"},{"location":"triply-db-getting-started/viewing-data/","text":"On this page: Viewing Data Linked Data Browser Types Labels Descriptions Geo Images Audio Video Linked Data Table SPARQL IDE Saving a SPARQL query Sharing a SPARQL query Transfer a SPARQL query Copy a SPARQL query ElasticSearch GraphQL Insights Class frequency Class hierarchy When does the class hierarchy show? Viewing Data \u00b6 TriplyDB offers several ways to explore your datasets. Linked Data Browser \u00b6 The linked data browser offers to traverse the data by focusing on one node at the time. The node is describe using it's properties, which can be followed to other nodes in the graph. The following properties provide additional information about your linked data, enabling the LD-browser to display visualizations and provide a better user experience. Types \u00b6 The predicate rdf:type allows you to specify the type or class of a resource in your linked data: By using rdf:type , you can indicate the category or classification of the resource, which can help the LD-browser understand the nature of the data and display it appropriately. In the example below , you can see that \"Iris setosa\" is the type of flowering plant due to the usage of the rdf:type property. Labels \u00b6 Labels are typically used to provide a concise and meaningful title or display name for a resource, making it easier for users to understand the content of your linked data. These predicates allow you to provide human-readable labels or names for your resources: The property rdfs:label The property skos:prefLabel In the example below , the rdfs:label property was used to denote the label(name) of the Pokemon, resulting in the display of \"Pikachu\" above its corresponding image. Descriptions \u00b6 Descriptions can provide additional context or information about a resource, helping users understand its purpose, content, or significance. These predicates allow you to provide textual descriptions or comments about your resources: The property sdo:description The property rdfs:comment In the following example rdfs:comment was used to provide additional information on Iris Setosa. Geo \u00b6 These are some of the predicates used for representing geographic information in your LD-browser: The property geo:asWKT : This property allows you to specify the geometries of geographic features using the Well-Known Text (WKT) format, which can be visualized on a map in the LD-browser. The property geo:hasGeometry : This property is used to link a geographic feature with its corresponding geometry. The property geo:location : This property is used to indicate the location of a resource using geographic coordinates, such as latitude and longitude, in your linked data. In the following example geo:hasGeometry property was used to showcase a map depicting the location of Instituut voor Beeld en Geluid. Images \u00b6 These predicates allow you to associate images or visual representations with your resources: Class sdo:ImageObject The property foaf:depiction The property foaf:thumbnail The property foaf:img The property sdo:image The property sdo:contentUrl By using these predicates, you can provide URLs or references to images that can be displayed alongside your linked data in the LD-browser. In the example below , foaf:depiction was used to display picture of Pikachu in the LD-browser: Audio \u00b6 These predicates allow you to associate audio content with your resources: The class sdo:AudioObject The property sdo:audio The property sdo:contentUrl You can use these predicates to provide URLs or references to audio files that can be played or streamed within the LD-browser. In the following example , sdo:audio was used to showcase audio content of the Carnival Festival within the LD-browser. Video \u00b6 These predicates allow you to associate video content with your resources: Class sdo:VideoObject Property sdo:video Property sdo:contentUrl You can use these predicates to provide URLs or references to video files that can be played or streamed within the LD-browser.The video formats that are included in this dataset are \".mp4\", \".webm\", \".ogg\". In the following example , sdo:contentUrl was used to showcase video content of the Kleine Piep within the LD-browser. Linked Data Table \u00b6 The linked data Table shows a dataset at the triple level. The first three columns represent the subject, predicate, and object position of the triple. The fourth column represents the graph to which the triple belongs. The linked data Table can be used to perform simple queries by filling in the subject, predicate, object, and/or graph using the text field. Terms in the linked data Table can be dragged and dropped between columns. This allows a simple form of graph navigation. For example, an object term can be dragged to the subject column in order to show the triples in which that term appears in the subject position. Queries in the linked data Table can also be performed automatically through the Statements API and the Triple Pattern Fragments API . SPARQL IDE \u00b6 When a dataset has a running SPARQL service, the data can be queried from the SPARQL IDE. The SPARQL IDE is an extended version of the Open Source Yasgui query editor. Saving a SPARQL query \u00b6 It is often useful to save a SPARQL query for later use. This is achieved by clicking on the save icon in the top-right corner of the SPARQL Editor. Doing so will create a Saved Query . Sharing a SPARQL query \u00b6 It is sometimes useful to share a SPARQL query with somebody else, or to have a cURL command that can be used to run the same SPARQL query from a command line. This is achieved by clicking on the share icon in the top-right corner of the SPARQL Editor. This brings us a dialog from which the SPARQL query can be copied in the following three forms: The URL-encoded SPARQL query. This is a long URL that includes the endpoint, the query, and visualization settings. Notice that this URL can be quite long for complex queries and/or visualizations. Long URLs are not supported by some application that cut off a URL after a maximum length (often 1,024 characters). Use one of the other two options or use Saved Queries to avoid such restrictions. A short URL that redirects to the full URL-encoded SPARQL query. A cURL command that can be copy/pasted into a terminal application that supports this command. cURL is often used by programmers to test HTTP(S) requests. Saved Queries are a more modern way of sharing SPARQL queries. They do not have any of the technical limitations that occur with URL-encoded queries. Transfer a SPARQL query \u00b6 The SPARQL queries could be transferred to another account or an organization. To do that, go to the setting field at the query page, choose transfer: and then choose where the SPARQL query should be moved to: After the destination is set you would be redirected to the SPARQL query new page. The SPARQL query could be transferred from an account to an organization and vice versa. Copy a SPARQL query \u00b6 Users can copy SPARQL queries to another account or an organization. To do that, click on three dots in the upper right corner of the query and choose the copy option: Then, choose where the SPARQL query should be moved to: After setting the destination, you will be redirected to the SPARQL query new page. The SPARQL query can be copied from an account to an organization and vice versa. ElasticSearch \u00b6 When a dataset has a running Elasticsearch service, textual searches can be performed over the entire dataset. Text search with Elasticsearch works like a search engine and returns any node that contains your search term, or contains the search term in any of its properties. It is also possible to write a custom query using the Elasticsearch Query DSL (Domain Specific Language) . GraphQL \u00b6 Some TriplyDB instances also support querying using the GraphQL language . For more information on the schema and possible queries, also read this document . Insights \u00b6 The insights page has been developed to give you a succinct overview of the linked data at hand. It holds two views: the class frequency and the class hierarchy view. Class frequency \u00b6 The class frequency diagram shows how often classes and properties appear in a graph. The drop-down on the top of the visualization selects the graph for which the class frequency is drawn. The visualization shows the 10 most frequent classes in the selected graph. The exact number of occurrences can be seen when hovering over the bar of a class, also showing the complete IRI/prefixed IRI. When clicking on the bar of a class the node will expand and show the 10 most frequent predicates of that class. Class hierarchy \u00b6 The class hierarchy diagram shows the hierarchy of the dataset in three different visualizations. Each of the diagrams are created by the rdfs:subClassOf relations and the classes in the dataset. TriplyDB has three different visualization methods for the classHierarchy: Bubbles visualization Treemap visualization Sunburst visualization All three visualizations are interactive in two ways. It is possible to hover over them, which will show information about the layer the mouse is on, or to click on them, so the visualization zooms in one or more layers. For each visualization it is also possible to zoom out: Bubbles visualization: click the outside of the bubble Treemap visualization: use the breadcrumbs trail shown above the visualization Sunburst visualization: click the innermost circle of the visualization When does the class hierarchy show? \u00b6 A class only appears in the class hierarchy tab if it has instances (connected to the class via rdf:type ). The class hierarchy cannot be shown if it contains a cycle, meaning that some class is (indirectly) its own subclass.","title":"Viewing Data"},{"location":"triply-db-getting-started/viewing-data/#viewing-data","text":"TriplyDB offers several ways to explore your datasets.","title":"Viewing Data"},{"location":"triply-db-getting-started/viewing-data/#linked-data-browser","text":"The linked data browser offers to traverse the data by focusing on one node at the time. The node is describe using it's properties, which can be followed to other nodes in the graph. The following properties provide additional information about your linked data, enabling the LD-browser to display visualizations and provide a better user experience.","title":"Linked Data Browser"},{"location":"triply-db-getting-started/viewing-data/#types","text":"The predicate rdf:type allows you to specify the type or class of a resource in your linked data: By using rdf:type , you can indicate the category or classification of the resource, which can help the LD-browser understand the nature of the data and display it appropriately. In the example below , you can see that \"Iris setosa\" is the type of flowering plant due to the usage of the rdf:type property.","title":"Types"},{"location":"triply-db-getting-started/viewing-data/#labels","text":"Labels are typically used to provide a concise and meaningful title or display name for a resource, making it easier for users to understand the content of your linked data. These predicates allow you to provide human-readable labels or names for your resources: The property rdfs:label The property skos:prefLabel In the example below , the rdfs:label property was used to denote the label(name) of the Pokemon, resulting in the display of \"Pikachu\" above its corresponding image.","title":"Labels"},{"location":"triply-db-getting-started/viewing-data/#descriptions","text":"Descriptions can provide additional context or information about a resource, helping users understand its purpose, content, or significance. These predicates allow you to provide textual descriptions or comments about your resources: The property sdo:description The property rdfs:comment In the following example rdfs:comment was used to provide additional information on Iris Setosa.","title":"Descriptions"},{"location":"triply-db-getting-started/viewing-data/#geo","text":"These are some of the predicates used for representing geographic information in your LD-browser: The property geo:asWKT : This property allows you to specify the geometries of geographic features using the Well-Known Text (WKT) format, which can be visualized on a map in the LD-browser. The property geo:hasGeometry : This property is used to link a geographic feature with its corresponding geometry. The property geo:location : This property is used to indicate the location of a resource using geographic coordinates, such as latitude and longitude, in your linked data. In the following example geo:hasGeometry property was used to showcase a map depicting the location of Instituut voor Beeld en Geluid.","title":"Geo"},{"location":"triply-db-getting-started/viewing-data/#images","text":"These predicates allow you to associate images or visual representations with your resources: Class sdo:ImageObject The property foaf:depiction The property foaf:thumbnail The property foaf:img The property sdo:image The property sdo:contentUrl By using these predicates, you can provide URLs or references to images that can be displayed alongside your linked data in the LD-browser. In the example below , foaf:depiction was used to display picture of Pikachu in the LD-browser:","title":"Images"},{"location":"triply-db-getting-started/viewing-data/#audio","text":"These predicates allow you to associate audio content with your resources: The class sdo:AudioObject The property sdo:audio The property sdo:contentUrl You can use these predicates to provide URLs or references to audio files that can be played or streamed within the LD-browser. In the following example , sdo:audio was used to showcase audio content of the Carnival Festival within the LD-browser.","title":"Audio"},{"location":"triply-db-getting-started/viewing-data/#video","text":"These predicates allow you to associate video content with your resources: Class sdo:VideoObject Property sdo:video Property sdo:contentUrl You can use these predicates to provide URLs or references to video files that can be played or streamed within the LD-browser.The video formats that are included in this dataset are \".mp4\", \".webm\", \".ogg\". In the following example , sdo:contentUrl was used to showcase video content of the Kleine Piep within the LD-browser.","title":"Video"},{"location":"triply-db-getting-started/viewing-data/#linked-data-table","text":"The linked data Table shows a dataset at the triple level. The first three columns represent the subject, predicate, and object position of the triple. The fourth column represents the graph to which the triple belongs. The linked data Table can be used to perform simple queries by filling in the subject, predicate, object, and/or graph using the text field. Terms in the linked data Table can be dragged and dropped between columns. This allows a simple form of graph navigation. For example, an object term can be dragged to the subject column in order to show the triples in which that term appears in the subject position. Queries in the linked data Table can also be performed automatically through the Statements API and the Triple Pattern Fragments API .","title":"Linked Data Table"},{"location":"triply-db-getting-started/viewing-data/#sparql-ide","text":"When a dataset has a running SPARQL service, the data can be queried from the SPARQL IDE. The SPARQL IDE is an extended version of the Open Source Yasgui query editor.","title":"SPARQL IDE"},{"location":"triply-db-getting-started/viewing-data/#saving-a-sparql-query","text":"It is often useful to save a SPARQL query for later use. This is achieved by clicking on the save icon in the top-right corner of the SPARQL Editor. Doing so will create a Saved Query .","title":"Saving a SPARQL query"},{"location":"triply-db-getting-started/viewing-data/#sharing-a-sparql-query","text":"It is sometimes useful to share a SPARQL query with somebody else, or to have a cURL command that can be used to run the same SPARQL query from a command line. This is achieved by clicking on the share icon in the top-right corner of the SPARQL Editor. This brings us a dialog from which the SPARQL query can be copied in the following three forms: The URL-encoded SPARQL query. This is a long URL that includes the endpoint, the query, and visualization settings. Notice that this URL can be quite long for complex queries and/or visualizations. Long URLs are not supported by some application that cut off a URL after a maximum length (often 1,024 characters). Use one of the other two options or use Saved Queries to avoid such restrictions. A short URL that redirects to the full URL-encoded SPARQL query. A cURL command that can be copy/pasted into a terminal application that supports this command. cURL is often used by programmers to test HTTP(S) requests. Saved Queries are a more modern way of sharing SPARQL queries. They do not have any of the technical limitations that occur with URL-encoded queries.","title":"Sharing a SPARQL query"},{"location":"triply-db-getting-started/viewing-data/#transfer-a-sparql-query","text":"The SPARQL queries could be transferred to another account or an organization. To do that, go to the setting field at the query page, choose transfer: and then choose where the SPARQL query should be moved to: After the destination is set you would be redirected to the SPARQL query new page. The SPARQL query could be transferred from an account to an organization and vice versa.","title":"Transfer a SPARQL query"},{"location":"triply-db-getting-started/viewing-data/#copy-a-sparql-query","text":"Users can copy SPARQL queries to another account or an organization. To do that, click on three dots in the upper right corner of the query and choose the copy option: Then, choose where the SPARQL query should be moved to: After setting the destination, you will be redirected to the SPARQL query new page. The SPARQL query can be copied from an account to an organization and vice versa.","title":"Copy a SPARQL query"},{"location":"triply-db-getting-started/viewing-data/#elasticsearch","text":"When a dataset has a running Elasticsearch service, textual searches can be performed over the entire dataset. Text search with Elasticsearch works like a search engine and returns any node that contains your search term, or contains the search term in any of its properties. It is also possible to write a custom query using the Elasticsearch Query DSL (Domain Specific Language) .","title":"ElasticSearch"},{"location":"triply-db-getting-started/viewing-data/#graphql","text":"Some TriplyDB instances also support querying using the GraphQL language . For more information on the schema and possible queries, also read this document .","title":"GraphQL"},{"location":"triply-db-getting-started/viewing-data/#insights","text":"The insights page has been developed to give you a succinct overview of the linked data at hand. It holds two views: the class frequency and the class hierarchy view.","title":"Insights"},{"location":"triply-db-getting-started/viewing-data/#class-frequency","text":"The class frequency diagram shows how often classes and properties appear in a graph. The drop-down on the top of the visualization selects the graph for which the class frequency is drawn. The visualization shows the 10 most frequent classes in the selected graph. The exact number of occurrences can be seen when hovering over the bar of a class, also showing the complete IRI/prefixed IRI. When clicking on the bar of a class the node will expand and show the 10 most frequent predicates of that class.","title":"Class frequency"},{"location":"triply-db-getting-started/viewing-data/#class-hierarchy","text":"The class hierarchy diagram shows the hierarchy of the dataset in three different visualizations. Each of the diagrams are created by the rdfs:subClassOf relations and the classes in the dataset. TriplyDB has three different visualization methods for the classHierarchy: Bubbles visualization Treemap visualization Sunburst visualization All three visualizations are interactive in two ways. It is possible to hover over them, which will show information about the layer the mouse is on, or to click on them, so the visualization zooms in one or more layers. For each visualization it is also possible to zoom out: Bubbles visualization: click the outside of the bubble Treemap visualization: use the breadcrumbs trail shown above the visualization Sunburst visualization: click the innermost circle of the visualization","title":"Class hierarchy"},{"location":"triply-db-getting-started/viewing-data/#when-does-the-class-hierarchy-show","text":"A class only appears in the class hierarchy tab if it has instances (connected to the class via rdf:type ). The class hierarchy cannot be shown if it contains a cycle, meaning that some class is (indirectly) its own subclass.","title":"When does the class hierarchy show?"},{"location":"triply-db-on-premise/","text":"TriplyDB: On-Premise Deployment \u00b6 This document guides you through deploying TriplyDB on your own Kubernetes cluster. It is intended for someone with Kubernetes experience who has permission to create resources in the target namespace. Before you begin \u00b6 Make sure you have received the following from Triply: A license file ( key.license ) to activate your TriplyDB instance. The license is linked to a specific API domain \u2014 the api.domain value in your configuration must match the domain for which the license was created. A registry access token to pull Helm charts and container images. Make sure the following is available on your cluster: A Kubernetes namespace in which TriplyDB will run. An Ingress controller (see Prerequisites ). HTTPS egress to 77.73.225.176/28 from the namespace, for periodic license validation. Overview \u00b6 TriplyDB consists of several components running inside a Kubernetes namespace: The Console provides the web UI. It proxies API requests through the /_api path. The API Server handles all application logic, stores metadata in MongoDB, and uses Redis for event signaling between worker processes. API data (linked data and assets) is stored on persistent volumes. MongoDB stores instance metadata. It can be deployed as a Bitnami subchart, as an externally managed instance, or using the MongoDB community operator. Redis handles event signaling between API worker processes and other pods. The Orchestrator interacts with the Kubernetes API to dynamically create and remove TDB-Services (e.g. Virtuoso, Jena, or ElasticSearch) and to manage background jobs such as long-running SPARQL queries. Several CronJobs run periodically for maintenance (cleanup, graph optimization, query cache refresh, and license validation). Optional TDB services are pods created by the API pods whenever a user starts a service (i.e. Virtuoso or Elastic) for a dataset in TriplyDB. These pods are not part of the helm deployment but will be created dynamically when requested by the user and permitted by the license. Prerequisites \u00b6 RBAC \u00b6 You need at least an Admin role within the namespace where TriplyDB will be deployed. Ingress Controller \u00b6 TriplyDB requires an Ingress controller to be available on the cluster. TriplyDB strives to support any Ingress controller. The Ingress manifests have been tested with Ingress NGINX and HAProxy , but other controllers should work as well. The request methods used by TriplyDB are GET, POST, PATCH, and DELETE. Network \u00b6 In case of network or firewall restrictions, you will need to allow HTTPS egress traffic from the namespace to the IP range 77.73.225.176/28 . This is required for TriplyDB to periodically validate your license key. Configuration \u00b6 Before installing TriplyDB, you need to prepare a values.yaml file. Below is a minimal starting point showing the values you will need to customize for your environment: # The api.domain must match the domain for which your license was created api: domain: api.example.com # Domain where the API server is accessed persistentVolumeClaim: storageClassName: my-storage-class size: 10Gi storagePersistentVolumeClaim: storageClassName: my-storage-class size: 10Gi console: domain: example.com # Domain where the Console is accessed redis: persistentVolumeClaim: storageClassName: my-storage-class size: 1Gi mongodb: deploymentStrategy: subChart persistentVolumeClaim: storageClassName: my-storage-class size: 3Gi mongodbSubChart: enabled: true tdbServices: persistentVolumeClaim: storageClassName: my-storage-class kubernetesApi: cidr: 10.96.0.1/32 # CIDR of your cluster's Kubernetes API port: 443 ingress: controller: nginx # \"nginx\" or \"haproxy\" sslMode: behind # \"behind\", \"terminate\", or \"none\" The sections below describe each configuration area in detail. For a complete reference of all available options, run: helm show readme triply/triplydb --version <version> Domains (required) \u00b6 You must configure the domain names for both the API server and the Console: api: domain: api.example.com console: domain: example.com The api.domain value must match the domain for which your TriplyDB license was created. If the domain does not match, the license validation will fail. Ensure that DNS records for both domains point to your Ingress controller. Persistent storage (required) \u00b6 TriplyDB requires persistent storage for API data, Redis, MongoDB, and TDB-Services. You must specify a storage class and size for each persistent volume claim. The sizes of the PVCs used by the TDB-Services are determined dynamically based on the data that will be loaded by the service. api: persistentVolumeClaim: storageClassName: my-storage-class size: 10Gi storagePersistentVolumeClaim: storageClassName: my-storage-class size: 10Gi redis: persistentVolumeClaim: storageClassName: my-storage-class size: 1Gi mongodb: persistentVolumeClaim: storageClassName: my-storage-class size: 3Gi tdbServices: persistentVolumeClaim: storageClassName: my-storage-class Ingress \u00b6 The ingress configuration controls how external traffic reaches TriplyDB: ingress: controller: nginx # Ingress controller type: \"nginx\" or \"haproxy\" className: my-ingress # Ingress class name (if required by your cluster) sslMode: behind # \"behind\" (proxy terminates SSL), # \"terminate\" (TriplyDB terminates SSL), or \"none\" annotations: {} # Custom annotations, e.g. for cert-manager If you use a TLS certificate manager such as cert-manager, you can specify the issuer through annotations: ingress: annotations: cert-manager.io/issuer: my-issuer MongoDB \u00b6 TriplyDB requires MongoDB as its database. There are three deployment strategies: subChart (recommended): Deploys MongoDB using the Bitnami MongoDB Helm chart as a subchart. When using this strategy, also set mongodbSubChart.enabled: true . external (recommended): Uses an externally managed MongoDB instance. This is ideal if your organization already operates a managed MongoDB service. You must manually create a Kubernetes secret containing the connection string and reference it in the values. useOperator : Uses the MongoDB community operator to deploy and manage a MongoDB replica set. If the operator is not already installed on your cluster, you can deploy it as part of this chart by setting mongodb-operator.enabled: true . Example for the subchart strategy: mongodb: deploymentStrategy: subChart mongodbSubChart: enabled: true Example for an external MongoDB: mongodb: deploymentStrategy: external secret: name: my-mongodb-secret key: connectionString.standard Create the secret with: kubectl -n <namespace> create secret generic my-mongodb-secret \\ --from-literal=connectionString.standard=\"mongodb://<user>:<password>@<host>:<port>/triply?replicaSet=<rs>&authSource=admin\" Email (SMTP) \u00b6 To enable TriplyDB to send emails (e.g. password resets and notifications), configure an SMTP server: email: host: smtp.example.com port: 587 secure: false name: example.com # Domain used in the \"From\" address of outgoing emails ip: 10.0.0.50 # IP of the SMTP server, used for network policy egress rules auth: username: smtp-user password: smtp-password The name value is the domain used in the sender address of outgoing emails (e.g. noreply@example.com ). Set this to the domain that your SMTP server is configured to send from. The secure option controls TLS behavior: when true , the connection uses TLS directly (port 465). When false , the connection starts in plaintext and upgrades via STARTTLS if the server supports it (port 587). HTTP Proxy \u00b6 If your environment requires an HTTP proxy for access to external networks (e.g. for license validation or SPARQL federation), configure the proxy settings: httpProxy: ip: 10.0.0.1 port: 8080 protocol: http # \"http\" or \"https\" Network policies \u00b6 TriplyDB deploys Kubernetes network policies by default ( networkPolicies.enabled: true ), which include a default-deny-all rule and specific allow rules for internal communication. If your cluster uses a custom DNS setup or the Kubernetes API is not reachable from the default configuration, you may need to configure: kubernetesDns: mode: pod # \"pod\" or \"address\" pod: namespaceSelector: {} podSelector: matchLabels: k8s-app: kube-dns To disable network policies entirely (not recommended), set networkPolicies.enabled: false . Kubernetes API (required) \u00b6 Several TriplyDB components need access to the Kubernetes API, including the API server and the Orchestrator. You must specify the CIDR and port of your cluster's Kubernetes API server: kubernetesApi: cidr: 10.96.0.1/32 port: 443 Installation \u00b6 1. Create the license secret \u00b6 Replace <namespace> with your namespace and /path/to/key.license with the path to the license file: kubectl -n <namespace> create secret generic triply-license-secret \\ --from-literal=TRIPLY__LICENSE=\"$(cat /path/to/key.license)\" 2. Create the registry secret \u00b6 Replace <token> with the registry access token provided by Triply: kubectl -n <namespace> create secret docker-registry triply-registry-secret \\ --docker-username=oauth2 \\ --docker-password=<token> \\ --docker-server=registry.triply.cc 3. Add the Helm chart repository \u00b6 helm repo add triply \\ https://git.triply.cc/api/v4/projects/276/packages/helm/stable \\ --username oauth2 \\ --password <token> 4. Install TriplyDB \u00b6 helm -n <namespace> install triplydb triply/triplydb -f /path/to/values.yaml Verifying the deployment \u00b6 Check pod status \u00b6 List the pods in the namespace and verify that all pods are running and all containers are ready: kubectl -n <namespace> get pods The output should look something like this: NAME READY STATUS RESTARTS api-84445bcc86-ckshd 1/1 Running 0 console-647fd854b6-j5zwh 1/1 Running 0 mongodb-0 2/2 Running 0 redis-6bdbd4d968-lsb8l 1/1 Running 0 orchestrator-68ddbdb4f6-ljb54 1/1 Running 0 Retrieve initial credentials \u00b6 The API init job creates an initial admin account during installation. Retrieve the credentials from the job logs: kubectl -n <namespace> logs job/<init-job-name> The job name can be found by running kubectl -n <namespace> get jobs . Save these credentials \u2014 you will need them to log in for the first time. Test API access \u00b6 Verify that the API server is reachable by sending a request to the configured API domain: curl https://api.example.com The following response indicates that the API is accessible: { \"message\": \"Not found. For developer API documentation, go to https://triply.cc/docs/triply-api.\" } Test Console access \u00b6 Visit the Console domain in your browser (e.g. https://example.com ). On a fresh instance, you should see a mostly empty page with a blue banner, a search field and a login button. First login \u00b6 Log in with the credentials retrieved from the init job logs. You will be prompted to set an email address. You can then change the username and password through the user settings page, accessible through the drop-down menu in the upper right corner of the Console. If you encounter any issues during verification, please contact support@triply.cc . Upgrading \u00b6 To see all available versions of the TriplyDB Helm chart: helm search repo triply -l Functional changes (e.g. feature X has been added in version Y) can be found in the changelog . Technical changes (e.g. helm value X is no longer supported in version Y) can be found in the technical changelog . To deploy a new version: helm -n <namespace> upgrade triplydb triply/triplydb --version <new_version> -f /path/to/values.yaml Authentication (optional) \u00b6 TriplyDB supports SAML and OAuth2 for single sign-on. Both can be configured through Helm values. Authentication is optional \u2014 without it, TriplyDB uses built-in username/password authentication. SAML \u00b6 Run the following command in the API pod to generate the SAML service provider metadata: sh kubectl -n <namespace> exec <api_pod> -- yarn run bin getSamlServiceProviderMetadata --idp <idp_id> Use the outputted XML to configure a SAML application at the IDP side. Create a Kubernetes secret containing the IDP signing certificate: sh kubectl -n <namespace> create secret generic saml-idp-cert \\ --from-literal=certificate=\"$(cat /path/to/idp-certificate.pem | base64 -w 0)\" Add the SAML configuration to your values file (replacing okta with your IDP identifier): yaml auth: saml: okta: label: \"Okta\" entryPoint: \"https://example.okta.com/app/triplydb/some_random_id/sso/saml\" issuer: \"https://example.com\" idpCertSecret: name: saml-idp-cert key: certificate Upgrade the Helm deployment: sh helm -n <namespace> upgrade triplydb triply/triplydb -f /path/to/values.yaml Persisting SAML certificates \u00b6 By default, TriplyDB generates certificates for SAML communication during the initial deployment. These certificates are included in backups, so SAML does not need to be reconfigured when a backup is restored. To persist the certificates independently (e.g. for recreating the instance from scratch), you can store them in a Kubernetes TLS secret: Copy the certificates from the API container: sh kubectl cp -n <namespace> <api_pod>:/home/triply/data/.samlCertificate.crt samlCertificate.crt kubectl cp -n <namespace> <api_pod>:/home/triply/data/.samlPrivateKey.key samlPrivateKey.key Create a TLS secret: sh kubectl create secret tls -n <namespace> saml-sp-certs \\ --cert=samlCertificate.crt \\ --key=samlPrivateKey.key Reference the secret in your values file: yaml auth: saml: okta: spCertSecret: name: saml-sp-certs Upgrade the Helm deployment. Additional SAML configuration \u00b6 TriplyDB uses node-saml for SAML support. Additional node-saml configuration parameters can be passed through the additionalConfig value. See the node-saml documentation for all available options. auth: saml: okta: additionalConfig: wantAuthnResponseSigned: false OAuth2 \u00b6 TriplyDB can authenticate users through an OAuth2 provider: auth: oauth2: label: \"Login with SSO\" authorizationURL: \"https://idp.example.com/oauth2/authorize\" tokenURL: \"https://idp.example.com/oauth2/token\" clientID: \"my-client-id\" clientSecret: \"my-client-secret\" scope: - openid - email - profile Backups (highly recommended) \u00b6 Regular backups are highly recommended to protect your TriplyDB instance against data loss caused by storage failures or other unforeseen issues. Backups include all assets used and managed by the API, and all service metadata stored in MongoDB. Since every infrastructure setup is different, we provide a template rather than a full backup solution. The sections below describe the general approach to performing backups and restorations. Making a backup \u00b6 Below is an example of a Kubernetes CronJob for periodically backing up a TriplyDB instance. Some parts are left open, since these depend on your infrastructure setup. The two init containers copy the required data to a temporary backup volume: one for MongoDB and one for the relevant files on the API persistent volumes. The CronJob should be deployed in the same namespace as TriplyDB, as it depends on the mongodb-triply-triplydb secret for the MongoDB connection string and on the persistent volumes used by the API pods. The main container then syncs the data from the temporary backup volume to your backup storage. This could be a container that syncs to an S3 bucket, copies to an NFS share, or any other approach that fits your infrastructure. apiVersion: batch/v1 kind: CronJob metadata: name: backup labels: cc.triply.type: api # if network policies are used, this label is required to access mongo ... spec: jobTemplate: spec: template: metadata: labels: cc.triply.type: api # if network policies are used, this label is required to access mongo spec: initContainers: - name: mongo-dump image: 'some.registry/mongo:<same_mongo_version>' command: ['sh', '-c', 'set -e; mongodump ${MONGODB_URI} --archive > /backup/mongo.db'] env: - name: MONGODB_URI valueFrom: secretKeyRef: name: mongodb-triply-triplydb key: connectionString.standard volumeMounts: - name: shared-backup-volume mountPath: /backup ... - name: api-files-copy image: <image_containing_rsync> command: - /bin/sh - -c - >- rsync -avzr --files-from /home/triply/data-storage/.backup.include --exclude-from /home/triply/data-storage/.backup.exclude / /backup volumeMounts: - name: primary mountPath: /home/triply/data - name: storage mountPath: /home/triply/data-storage - name: shared-backup-volume mountPath: /backup containers: - name: sync-to-backup-location # Container that copies the contents of /backup to your backup storage ... volumeMounts: - name: shared-backup-volume mountPath: /backup volumes: - name: primary persistentVolumeClaim: claimName: primary - name: storage persistentVolumeClaim: claimName: storage - name: shared-backup-volume emptyDir: {} restartPolicy: Never Restoring a backup \u00b6 To restore a backup, prepare a Kubernetes Job that reverses the backup process. The first container should download the backup from your backup storage to a temporary backup volume. A second container can then restore the MongoDB database: mongorestore --drop --archive=/backup/mongo.db --nsFrom=triply.* --nsTo=triply.* ${MONGODB_URI} A third container restores the disk files by copying them from the backup to their original locations within the API persistent volumes. We recommend scheduling backups to run regularly (e.g. daily). Contact support@triply.cc for detailed guidance on setting up backups for your infrastructure. Reference \u00b6 User roles \u00b6 TriplyDB has three kinds of users: regular users, site administrators, and super administrators. Feature Regular user Site admin Super admin Read any public or internal dataset, query or story \u2713 \u2713 \u2713 Create, update and delete own datasets, queries or stories \u2713 \u2713 \u2713 Manage organizations that you own \u2713 \u2713 \u2713 Create organizations \u2713 \u2713 \u2713 Change your own account details \u2713 \u2713 \u2713 Change other users' details and content \u274c \u2713 \u2713 Assign site administrators \u274c \u2713 \u2713 Access admin pages (accounts, datasets, services overview) \u274c \u2713 \u2713 Configure security policies, site profile, prefixes, redirects \u274c \u2713 \u2713 Impersonate users, manage cache, assign super admins \u274c \u274c \u2713 Enable/disable features, configure debug flags \u274c \u274c \u2713 Helm chart documentation \u00b6 For a complete reference of all available Helm values: helm show readme triply/triplydb --version <version>","title":"TriplyDB: On-Premise Deployment"},{"location":"triply-db-on-premise/#triplydb-on-premise-deployment","text":"This document guides you through deploying TriplyDB on your own Kubernetes cluster. It is intended for someone with Kubernetes experience who has permission to create resources in the target namespace.","title":"TriplyDB: On-Premise Deployment"},{"location":"triply-db-on-premise/#before-you-begin","text":"Make sure you have received the following from Triply: A license file ( key.license ) to activate your TriplyDB instance. The license is linked to a specific API domain \u2014 the api.domain value in your configuration must match the domain for which the license was created. A registry access token to pull Helm charts and container images. Make sure the following is available on your cluster: A Kubernetes namespace in which TriplyDB will run. An Ingress controller (see Prerequisites ). HTTPS egress to 77.73.225.176/28 from the namespace, for periodic license validation.","title":"Before you begin"},{"location":"triply-db-on-premise/#overview","text":"TriplyDB consists of several components running inside a Kubernetes namespace: The Console provides the web UI. It proxies API requests through the /_api path. The API Server handles all application logic, stores metadata in MongoDB, and uses Redis for event signaling between worker processes. API data (linked data and assets) is stored on persistent volumes. MongoDB stores instance metadata. It can be deployed as a Bitnami subchart, as an externally managed instance, or using the MongoDB community operator. Redis handles event signaling between API worker processes and other pods. The Orchestrator interacts with the Kubernetes API to dynamically create and remove TDB-Services (e.g. Virtuoso, Jena, or ElasticSearch) and to manage background jobs such as long-running SPARQL queries. Several CronJobs run periodically for maintenance (cleanup, graph optimization, query cache refresh, and license validation). Optional TDB services are pods created by the API pods whenever a user starts a service (i.e. Virtuoso or Elastic) for a dataset in TriplyDB. These pods are not part of the helm deployment but will be created dynamically when requested by the user and permitted by the license.","title":"Overview"},{"location":"triply-db-on-premise/#prerequisites","text":"","title":"Prerequisites"},{"location":"triply-db-on-premise/#rbac","text":"You need at least an Admin role within the namespace where TriplyDB will be deployed.","title":"RBAC"},{"location":"triply-db-on-premise/#ingress-controller","text":"TriplyDB requires an Ingress controller to be available on the cluster. TriplyDB strives to support any Ingress controller. The Ingress manifests have been tested with Ingress NGINX and HAProxy , but other controllers should work as well. The request methods used by TriplyDB are GET, POST, PATCH, and DELETE.","title":"Ingress Controller"},{"location":"triply-db-on-premise/#network","text":"In case of network or firewall restrictions, you will need to allow HTTPS egress traffic from the namespace to the IP range 77.73.225.176/28 . This is required for TriplyDB to periodically validate your license key.","title":"Network"},{"location":"triply-db-on-premise/#configuration","text":"Before installing TriplyDB, you need to prepare a values.yaml file. Below is a minimal starting point showing the values you will need to customize for your environment: # The api.domain must match the domain for which your license was created api: domain: api.example.com # Domain where the API server is accessed persistentVolumeClaim: storageClassName: my-storage-class size: 10Gi storagePersistentVolumeClaim: storageClassName: my-storage-class size: 10Gi console: domain: example.com # Domain where the Console is accessed redis: persistentVolumeClaim: storageClassName: my-storage-class size: 1Gi mongodb: deploymentStrategy: subChart persistentVolumeClaim: storageClassName: my-storage-class size: 3Gi mongodbSubChart: enabled: true tdbServices: persistentVolumeClaim: storageClassName: my-storage-class kubernetesApi: cidr: 10.96.0.1/32 # CIDR of your cluster's Kubernetes API port: 443 ingress: controller: nginx # \"nginx\" or \"haproxy\" sslMode: behind # \"behind\", \"terminate\", or \"none\" The sections below describe each configuration area in detail. For a complete reference of all available options, run: helm show readme triply/triplydb --version <version>","title":"Configuration"},{"location":"triply-db-on-premise/#domains-required","text":"You must configure the domain names for both the API server and the Console: api: domain: api.example.com console: domain: example.com The api.domain value must match the domain for which your TriplyDB license was created. If the domain does not match, the license validation will fail. Ensure that DNS records for both domains point to your Ingress controller.","title":"Domains (required)"},{"location":"triply-db-on-premise/#persistent-storage-required","text":"TriplyDB requires persistent storage for API data, Redis, MongoDB, and TDB-Services. You must specify a storage class and size for each persistent volume claim. The sizes of the PVCs used by the TDB-Services are determined dynamically based on the data that will be loaded by the service. api: persistentVolumeClaim: storageClassName: my-storage-class size: 10Gi storagePersistentVolumeClaim: storageClassName: my-storage-class size: 10Gi redis: persistentVolumeClaim: storageClassName: my-storage-class size: 1Gi mongodb: persistentVolumeClaim: storageClassName: my-storage-class size: 3Gi tdbServices: persistentVolumeClaim: storageClassName: my-storage-class","title":"Persistent storage (required)"},{"location":"triply-db-on-premise/#ingress","text":"The ingress configuration controls how external traffic reaches TriplyDB: ingress: controller: nginx # Ingress controller type: \"nginx\" or \"haproxy\" className: my-ingress # Ingress class name (if required by your cluster) sslMode: behind # \"behind\" (proxy terminates SSL), # \"terminate\" (TriplyDB terminates SSL), or \"none\" annotations: {} # Custom annotations, e.g. for cert-manager If you use a TLS certificate manager such as cert-manager, you can specify the issuer through annotations: ingress: annotations: cert-manager.io/issuer: my-issuer","title":"Ingress"},{"location":"triply-db-on-premise/#mongodb","text":"TriplyDB requires MongoDB as its database. There are three deployment strategies: subChart (recommended): Deploys MongoDB using the Bitnami MongoDB Helm chart as a subchart. When using this strategy, also set mongodbSubChart.enabled: true . external (recommended): Uses an externally managed MongoDB instance. This is ideal if your organization already operates a managed MongoDB service. You must manually create a Kubernetes secret containing the connection string and reference it in the values. useOperator : Uses the MongoDB community operator to deploy and manage a MongoDB replica set. If the operator is not already installed on your cluster, you can deploy it as part of this chart by setting mongodb-operator.enabled: true . Example for the subchart strategy: mongodb: deploymentStrategy: subChart mongodbSubChart: enabled: true Example for an external MongoDB: mongodb: deploymentStrategy: external secret: name: my-mongodb-secret key: connectionString.standard Create the secret with: kubectl -n <namespace> create secret generic my-mongodb-secret \\ --from-literal=connectionString.standard=\"mongodb://<user>:<password>@<host>:<port>/triply?replicaSet=<rs>&authSource=admin\"","title":"MongoDB"},{"location":"triply-db-on-premise/#email-smtp","text":"To enable TriplyDB to send emails (e.g. password resets and notifications), configure an SMTP server: email: host: smtp.example.com port: 587 secure: false name: example.com # Domain used in the \"From\" address of outgoing emails ip: 10.0.0.50 # IP of the SMTP server, used for network policy egress rules auth: username: smtp-user password: smtp-password The name value is the domain used in the sender address of outgoing emails (e.g. noreply@example.com ). Set this to the domain that your SMTP server is configured to send from. The secure option controls TLS behavior: when true , the connection uses TLS directly (port 465). When false , the connection starts in plaintext and upgrades via STARTTLS if the server supports it (port 587).","title":"Email (SMTP)"},{"location":"triply-db-on-premise/#http-proxy","text":"If your environment requires an HTTP proxy for access to external networks (e.g. for license validation or SPARQL federation), configure the proxy settings: httpProxy: ip: 10.0.0.1 port: 8080 protocol: http # \"http\" or \"https\"","title":"HTTP Proxy"},{"location":"triply-db-on-premise/#network-policies","text":"TriplyDB deploys Kubernetes network policies by default ( networkPolicies.enabled: true ), which include a default-deny-all rule and specific allow rules for internal communication. If your cluster uses a custom DNS setup or the Kubernetes API is not reachable from the default configuration, you may need to configure: kubernetesDns: mode: pod # \"pod\" or \"address\" pod: namespaceSelector: {} podSelector: matchLabels: k8s-app: kube-dns To disable network policies entirely (not recommended), set networkPolicies.enabled: false .","title":"Network policies"},{"location":"triply-db-on-premise/#kubernetes-api-required","text":"Several TriplyDB components need access to the Kubernetes API, including the API server and the Orchestrator. You must specify the CIDR and port of your cluster's Kubernetes API server: kubernetesApi: cidr: 10.96.0.1/32 port: 443","title":"Kubernetes API (required)"},{"location":"triply-db-on-premise/#installation","text":"","title":"Installation"},{"location":"triply-db-on-premise/#1-create-the-license-secret","text":"Replace <namespace> with your namespace and /path/to/key.license with the path to the license file: kubectl -n <namespace> create secret generic triply-license-secret \\ --from-literal=TRIPLY__LICENSE=\"$(cat /path/to/key.license)\"","title":"1. Create the license secret"},{"location":"triply-db-on-premise/#2-create-the-registry-secret","text":"Replace <token> with the registry access token provided by Triply: kubectl -n <namespace> create secret docker-registry triply-registry-secret \\ --docker-username=oauth2 \\ --docker-password=<token> \\ --docker-server=registry.triply.cc","title":"2. Create the registry secret"},{"location":"triply-db-on-premise/#3-add-the-helm-chart-repository","text":"helm repo add triply \\ https://git.triply.cc/api/v4/projects/276/packages/helm/stable \\ --username oauth2 \\ --password <token>","title":"3. Add the Helm chart repository"},{"location":"triply-db-on-premise/#4-install-triplydb","text":"helm -n <namespace> install triplydb triply/triplydb -f /path/to/values.yaml","title":"4. Install TriplyDB"},{"location":"triply-db-on-premise/#verifying-the-deployment","text":"","title":"Verifying the deployment"},{"location":"triply-db-on-premise/#check-pod-status","text":"List the pods in the namespace and verify that all pods are running and all containers are ready: kubectl -n <namespace> get pods The output should look something like this: NAME READY STATUS RESTARTS api-84445bcc86-ckshd 1/1 Running 0 console-647fd854b6-j5zwh 1/1 Running 0 mongodb-0 2/2 Running 0 redis-6bdbd4d968-lsb8l 1/1 Running 0 orchestrator-68ddbdb4f6-ljb54 1/1 Running 0","title":"Check pod status"},{"location":"triply-db-on-premise/#retrieve-initial-credentials","text":"The API init job creates an initial admin account during installation. Retrieve the credentials from the job logs: kubectl -n <namespace> logs job/<init-job-name> The job name can be found by running kubectl -n <namespace> get jobs . Save these credentials \u2014 you will need them to log in for the first time.","title":"Retrieve initial credentials"},{"location":"triply-db-on-premise/#test-api-access","text":"Verify that the API server is reachable by sending a request to the configured API domain: curl https://api.example.com The following response indicates that the API is accessible: { \"message\": \"Not found. For developer API documentation, go to https://triply.cc/docs/triply-api.\" }","title":"Test API access"},{"location":"triply-db-on-premise/#test-console-access","text":"Visit the Console domain in your browser (e.g. https://example.com ). On a fresh instance, you should see a mostly empty page with a blue banner, a search field and a login button.","title":"Test Console access"},{"location":"triply-db-on-premise/#first-login","text":"Log in with the credentials retrieved from the init job logs. You will be prompted to set an email address. You can then change the username and password through the user settings page, accessible through the drop-down menu in the upper right corner of the Console. If you encounter any issues during verification, please contact support@triply.cc .","title":"First login"},{"location":"triply-db-on-premise/#upgrading","text":"To see all available versions of the TriplyDB Helm chart: helm search repo triply -l Functional changes (e.g. feature X has been added in version Y) can be found in the changelog . Technical changes (e.g. helm value X is no longer supported in version Y) can be found in the technical changelog . To deploy a new version: helm -n <namespace> upgrade triplydb triply/triplydb --version <new_version> -f /path/to/values.yaml","title":"Upgrading"},{"location":"triply-db-on-premise/#authentication-optional","text":"TriplyDB supports SAML and OAuth2 for single sign-on. Both can be configured through Helm values. Authentication is optional \u2014 without it, TriplyDB uses built-in username/password authentication.","title":"Authentication (optional)"},{"location":"triply-db-on-premise/#saml","text":"Run the following command in the API pod to generate the SAML service provider metadata: sh kubectl -n <namespace> exec <api_pod> -- yarn run bin getSamlServiceProviderMetadata --idp <idp_id> Use the outputted XML to configure a SAML application at the IDP side. Create a Kubernetes secret containing the IDP signing certificate: sh kubectl -n <namespace> create secret generic saml-idp-cert \\ --from-literal=certificate=\"$(cat /path/to/idp-certificate.pem | base64 -w 0)\" Add the SAML configuration to your values file (replacing okta with your IDP identifier): yaml auth: saml: okta: label: \"Okta\" entryPoint: \"https://example.okta.com/app/triplydb/some_random_id/sso/saml\" issuer: \"https://example.com\" idpCertSecret: name: saml-idp-cert key: certificate Upgrade the Helm deployment: sh helm -n <namespace> upgrade triplydb triply/triplydb -f /path/to/values.yaml","title":"SAML"},{"location":"triply-db-on-premise/#persisting-saml-certificates","text":"By default, TriplyDB generates certificates for SAML communication during the initial deployment. These certificates are included in backups, so SAML does not need to be reconfigured when a backup is restored. To persist the certificates independently (e.g. for recreating the instance from scratch), you can store them in a Kubernetes TLS secret: Copy the certificates from the API container: sh kubectl cp -n <namespace> <api_pod>:/home/triply/data/.samlCertificate.crt samlCertificate.crt kubectl cp -n <namespace> <api_pod>:/home/triply/data/.samlPrivateKey.key samlPrivateKey.key Create a TLS secret: sh kubectl create secret tls -n <namespace> saml-sp-certs \\ --cert=samlCertificate.crt \\ --key=samlPrivateKey.key Reference the secret in your values file: yaml auth: saml: okta: spCertSecret: name: saml-sp-certs Upgrade the Helm deployment.","title":"Persisting SAML certificates"},{"location":"triply-db-on-premise/#additional-saml-configuration","text":"TriplyDB uses node-saml for SAML support. Additional node-saml configuration parameters can be passed through the additionalConfig value. See the node-saml documentation for all available options. auth: saml: okta: additionalConfig: wantAuthnResponseSigned: false","title":"Additional SAML configuration"},{"location":"triply-db-on-premise/#oauth2","text":"TriplyDB can authenticate users through an OAuth2 provider: auth: oauth2: label: \"Login with SSO\" authorizationURL: \"https://idp.example.com/oauth2/authorize\" tokenURL: \"https://idp.example.com/oauth2/token\" clientID: \"my-client-id\" clientSecret: \"my-client-secret\" scope: - openid - email - profile","title":"OAuth2"},{"location":"triply-db-on-premise/#backups-highly-recommended","text":"Regular backups are highly recommended to protect your TriplyDB instance against data loss caused by storage failures or other unforeseen issues. Backups include all assets used and managed by the API, and all service metadata stored in MongoDB. Since every infrastructure setup is different, we provide a template rather than a full backup solution. The sections below describe the general approach to performing backups and restorations.","title":"Backups (highly recommended)"},{"location":"triply-db-on-premise/#making-a-backup","text":"Below is an example of a Kubernetes CronJob for periodically backing up a TriplyDB instance. Some parts are left open, since these depend on your infrastructure setup. The two init containers copy the required data to a temporary backup volume: one for MongoDB and one for the relevant files on the API persistent volumes. The CronJob should be deployed in the same namespace as TriplyDB, as it depends on the mongodb-triply-triplydb secret for the MongoDB connection string and on the persistent volumes used by the API pods. The main container then syncs the data from the temporary backup volume to your backup storage. This could be a container that syncs to an S3 bucket, copies to an NFS share, or any other approach that fits your infrastructure. apiVersion: batch/v1 kind: CronJob metadata: name: backup labels: cc.triply.type: api # if network policies are used, this label is required to access mongo ... spec: jobTemplate: spec: template: metadata: labels: cc.triply.type: api # if network policies are used, this label is required to access mongo spec: initContainers: - name: mongo-dump image: 'some.registry/mongo:<same_mongo_version>' command: ['sh', '-c', 'set -e; mongodump ${MONGODB_URI} --archive > /backup/mongo.db'] env: - name: MONGODB_URI valueFrom: secretKeyRef: name: mongodb-triply-triplydb key: connectionString.standard volumeMounts: - name: shared-backup-volume mountPath: /backup ... - name: api-files-copy image: <image_containing_rsync> command: - /bin/sh - -c - >- rsync -avzr --files-from /home/triply/data-storage/.backup.include --exclude-from /home/triply/data-storage/.backup.exclude / /backup volumeMounts: - name: primary mountPath: /home/triply/data - name: storage mountPath: /home/triply/data-storage - name: shared-backup-volume mountPath: /backup containers: - name: sync-to-backup-location # Container that copies the contents of /backup to your backup storage ... volumeMounts: - name: shared-backup-volume mountPath: /backup volumes: - name: primary persistentVolumeClaim: claimName: primary - name: storage persistentVolumeClaim: claimName: storage - name: shared-backup-volume emptyDir: {} restartPolicy: Never","title":"Making a backup"},{"location":"triply-db-on-premise/#restoring-a-backup","text":"To restore a backup, prepare a Kubernetes Job that reverses the backup process. The first container should download the backup from your backup storage to a temporary backup volume. A second container can then restore the MongoDB database: mongorestore --drop --archive=/backup/mongo.db --nsFrom=triply.* --nsTo=triply.* ${MONGODB_URI} A third container restores the disk files by copying them from the backup to their original locations within the API persistent volumes. We recommend scheduling backups to run regularly (e.g. daily). Contact support@triply.cc for detailed guidance on setting up backups for your infrastructure.","title":"Restoring a backup"},{"location":"triply-db-on-premise/#reference","text":"","title":"Reference"},{"location":"triply-db-on-premise/#user-roles","text":"TriplyDB has three kinds of users: regular users, site administrators, and super administrators. Feature Regular user Site admin Super admin Read any public or internal dataset, query or story \u2713 \u2713 \u2713 Create, update and delete own datasets, queries or stories \u2713 \u2713 \u2713 Manage organizations that you own \u2713 \u2713 \u2713 Create organizations \u2713 \u2713 \u2713 Change your own account details \u2713 \u2713 \u2713 Change other users' details and content \u274c \u2713 \u2713 Assign site administrators \u274c \u2713 \u2713 Access admin pages (accounts, datasets, services overview) \u274c \u2713 \u2713 Configure security policies, site profile, prefixes, redirects \u274c \u2713 \u2713 Impersonate users, manage cache, assign super admins \u274c \u274c \u2713 Enable/disable features, configure debug flags \u274c \u274c \u2713","title":"User roles"},{"location":"triply-db-on-premise/#helm-chart-documentation","text":"For a complete reference of all available Helm values: helm show readme triply/triplydb --version <version>","title":"Helm chart documentation"},{"location":"triply-etl/","text":"On this page: TriplyETL Overview Supported standards and formats Supported data formats Why TriplyETL? TriplyETL Overview \u00b6 TriplyETL allows you to create and maintain production-grade linked data pipelines. Getting Started explains how TriplyETL can be used for the first time. CLI explains the commands that are used to install, compile, and run TriplyETL pipelines. The Changelog documents the changes that are introduced in new TriplyETL version. Maintenance explains how TriplyETL can be updated and can be configured to run in automated pipelines. TriplyETL uses the following unique approach: graph LR sources -- 1. Extract --> record record -- 2. Transform --> record record -- 3. Assert --> ld ld -- 4. Enrich --> ld ld -- 5. Validate --> ld ld -- 6. Publish --> destinations destinations[(\"D. Destinations\\n(TriplyDB)\")] ld[C. Internal Store] record[B. Record] sources[A. Data Sources] This approach consists of the following six steps (see diagram): Step 1. Extract : extracts a stream of records from one or more data sources . Step 2. Transform : cleans, combines, and extends data in the record . Step 3. Assert : uses data from the record to make linked data assertions in the internal store . Step 4. Enrich : improves and extends linked data in the internal store . Step 5. Validate ensures that linked data in the internal store meets the specified quality criteria. Step 6. Publish : takes the linked data from the internal store , and publishes it to a destination such as TriplyDB . TriplyETL uses the following data storage stages, to connect the six steps in the approach (see diagram): Stage A. Sources : the data inputs to the pipeline. Stage B. Record : provides a uniform representation for data from any source system. Stage C. Internal Store : temporarily holds linked data generated in the pipeline. Stage D. Destinations : places where output from the pipeline is published to, for example TriplyDB . In addition, the following configuration tools are used throughout the six TriplyETL steps: Declarations : introduce constants are reuse throughout the TriplyETL configuration. Control structures : make parts of the TriplyETL configuration optional or repeating (loops). Debug functions : give insights into TriplyETL internals for the purpose of finding issues and performing maintenance. Supported standards and formats \u00b6 TriplyETL follows a multi-paradigm approach. This means that TriplyETL seeks to support a wide variety of data formats, configuration languages, and linked data standards. This allows users to most optimally combine the formats, languages, and standards that they wish to use. Other ETL approaches focus on one format/language/standard, which severely limits what users that use those approaches can do. Supported data formats \u00b6 TriplyETL supports the following data formats through its extractors : CSV (Comma-Separated Values) JSON (JavaScript Object Notation) OAI-PMH (Open Archives Initiative, Protocol for Metadata Harvesting) PostgreSQL (Postgres, SQL) RDF 1.1 (Resource Description Language) TSV (Tab-Separated Values) XLSX (Office Open XML Workbook, Microsoft Excel) XML 1.1 (Extensible Markup Language) TriplyETL implements the latest versions of the linked data standards and best practices: RDF 1.1, SHACL Core, SHACL Advanced, XML Schema Datatypes 1.1, IETF RFC3987 (IRIs), IETF RFC5646 (Language Tags), SPARQL 1.1 Query Languahge, SPARQL 1.1 Update, SPARQL 1.1 Federation, N-Triples 1.1, N-Quads 1.1, Turtle 1.1, TriG 1.1, RDF/XML 1.1, JSON-LD 1.1 (TBA), JSON-LD Framing (TBA), and JSON-LD Algorithms (TBA). Why TriplyETL? \u00b6 TriplyETL has the following core features, that set it apart from other data pipeline products: Backend-agnostic : TriplyETL supports a large number of data source formats and types. Source data is processed in a unified record. This decouples configuration from source format specific. In TriplyETL, changing the source system often only requires changing the extractor. Multi-paradigm : TriplyETL supports all major paradigms for transforming and asserting linked data: SPARQL, SHACL, RML, JSON-LD, XSLT, and RATT (RDF All The Things). You can also write your own transformations in TypeScript for optimal extensibility. Scalable : TriplyETL processes data in a stream of self-contained records. This allows TriplyETL pipelines to run in parallel, ensuring a high pipeline throughput. High Quality : The output of TriplyETL pipelines is automatically validated against the specified data model, and/or against a set of preconfigured 'gold records'. Production-grade : TriplyETL pipelines run in GitLab CI/CD, and support the four DTAP environments that are often used in production systems: Development, Testing, Acceptance, Production.","title":"Overview"},{"location":"triply-etl/#triplyetl-overview","text":"TriplyETL allows you to create and maintain production-grade linked data pipelines. Getting Started explains how TriplyETL can be used for the first time. CLI explains the commands that are used to install, compile, and run TriplyETL pipelines. The Changelog documents the changes that are introduced in new TriplyETL version. Maintenance explains how TriplyETL can be updated and can be configured to run in automated pipelines. TriplyETL uses the following unique approach: graph LR sources -- 1. Extract --> record record -- 2. Transform --> record record -- 3. Assert --> ld ld -- 4. Enrich --> ld ld -- 5. Validate --> ld ld -- 6. Publish --> destinations destinations[(\"D. Destinations\\n(TriplyDB)\")] ld[C. Internal Store] record[B. Record] sources[A. Data Sources] This approach consists of the following six steps (see diagram): Step 1. Extract : extracts a stream of records from one or more data sources . Step 2. Transform : cleans, combines, and extends data in the record . Step 3. Assert : uses data from the record to make linked data assertions in the internal store . Step 4. Enrich : improves and extends linked data in the internal store . Step 5. Validate ensures that linked data in the internal store meets the specified quality criteria. Step 6. Publish : takes the linked data from the internal store , and publishes it to a destination such as TriplyDB . TriplyETL uses the following data storage stages, to connect the six steps in the approach (see diagram): Stage A. Sources : the data inputs to the pipeline. Stage B. Record : provides a uniform representation for data from any source system. Stage C. Internal Store : temporarily holds linked data generated in the pipeline. Stage D. Destinations : places where output from the pipeline is published to, for example TriplyDB . In addition, the following configuration tools are used throughout the six TriplyETL steps: Declarations : introduce constants are reuse throughout the TriplyETL configuration. Control structures : make parts of the TriplyETL configuration optional or repeating (loops). Debug functions : give insights into TriplyETL internals for the purpose of finding issues and performing maintenance.","title":"TriplyETL Overview"},{"location":"triply-etl/#supported-standards-and-formats","text":"TriplyETL follows a multi-paradigm approach. This means that TriplyETL seeks to support a wide variety of data formats, configuration languages, and linked data standards. This allows users to most optimally combine the formats, languages, and standards that they wish to use. Other ETL approaches focus on one format/language/standard, which severely limits what users that use those approaches can do.","title":"Supported standards and formats"},{"location":"triply-etl/#supported-data-formats","text":"TriplyETL supports the following data formats through its extractors : CSV (Comma-Separated Values) JSON (JavaScript Object Notation) OAI-PMH (Open Archives Initiative, Protocol for Metadata Harvesting) PostgreSQL (Postgres, SQL) RDF 1.1 (Resource Description Language) TSV (Tab-Separated Values) XLSX (Office Open XML Workbook, Microsoft Excel) XML 1.1 (Extensible Markup Language) TriplyETL implements the latest versions of the linked data standards and best practices: RDF 1.1, SHACL Core, SHACL Advanced, XML Schema Datatypes 1.1, IETF RFC3987 (IRIs), IETF RFC5646 (Language Tags), SPARQL 1.1 Query Languahge, SPARQL 1.1 Update, SPARQL 1.1 Federation, N-Triples 1.1, N-Quads 1.1, Turtle 1.1, TriG 1.1, RDF/XML 1.1, JSON-LD 1.1 (TBA), JSON-LD Framing (TBA), and JSON-LD Algorithms (TBA).","title":"Supported data formats"},{"location":"triply-etl/#why-triplyetl","text":"TriplyETL has the following core features, that set it apart from other data pipeline products: Backend-agnostic : TriplyETL supports a large number of data source formats and types. Source data is processed in a unified record. This decouples configuration from source format specific. In TriplyETL, changing the source system often only requires changing the extractor. Multi-paradigm : TriplyETL supports all major paradigms for transforming and asserting linked data: SPARQL, SHACL, RML, JSON-LD, XSLT, and RATT (RDF All The Things). You can also write your own transformations in TypeScript for optimal extensibility. Scalable : TriplyETL processes data in a stream of self-contained records. This allows TriplyETL pipelines to run in parallel, ensuring a high pipeline throughput. High Quality : The output of TriplyETL pipelines is automatically validated against the specified data model, and/or against a set of preconfigured 'gold records'. Production-grade : TriplyETL pipelines run in GitLab CI/CD, and support the four DTAP environments that are often used in production systems: Development, Testing, Acceptance, Production.","title":"Why TriplyETL?"},{"location":"triply-etl/assert/","text":"On this page: Assert Next steps Assert \u00b6 The Assert step uses data from the Record to add linked data to the Internal Store. graph LR sources -- 1. Extract --> record record -- 2. Transform --> record record -- 3. Assert --> ld ld -- 4. Enrich --> ld ld -- 5. Validate --> ld ld -- 6. Publish --> destinations linkStyle 2 stroke:red,stroke-width:3px; destinations[(\"D. Destinations\\n(TriplyDB)\")] ld[C. Internal Store] record[B. Record] sources[A. Data Sources] Assertion are statements of fact. In linked data, assertions are commonly called 'triples' or 'quads'. A triple is composed of three parts: a subject term, a predicate term, and an object term. A quad or quadruple also has a fourth graph term. TriplyETL supports the following languages for making linked data assertions: JSON-LD can be used to assert data according to a JSON-LD Context. RATT contains a core set of TypeScript functions for making linked data assertions: Term Assertions : functions that are used to assert terms (IRIs or literals). Statement Assertions : functions that are used to assert statements (triples or quads). RML inserts the data that has been transformed (from a non-RDF format into RDF triples) into the store. XSLT inserts the data that has been transformed (from XML to XML or RDF) using stylesheet parameter in loadRdf() function into the store Next steps \u00b6 After linked data has been asserted into the internal store, the following steps can be preformed: Step 4. Enrich : improves and extends linked data in the internal store. Step 5. Validate ensures that linked data in the internal store meets the specified quality criteria. Step 6. Publish : takes the linked data from the internal store, and publishes it to a destination such as TriplyDB .","title":"Overview"},{"location":"triply-etl/assert/#assert","text":"The Assert step uses data from the Record to add linked data to the Internal Store. graph LR sources -- 1. Extract --> record record -- 2. Transform --> record record -- 3. Assert --> ld ld -- 4. Enrich --> ld ld -- 5. Validate --> ld ld -- 6. Publish --> destinations linkStyle 2 stroke:red,stroke-width:3px; destinations[(\"D. Destinations\\n(TriplyDB)\")] ld[C. Internal Store] record[B. Record] sources[A. Data Sources] Assertion are statements of fact. In linked data, assertions are commonly called 'triples' or 'quads'. A triple is composed of three parts: a subject term, a predicate term, and an object term. A quad or quadruple also has a fourth graph term. TriplyETL supports the following languages for making linked data assertions: JSON-LD can be used to assert data according to a JSON-LD Context. RATT contains a core set of TypeScript functions for making linked data assertions: Term Assertions : functions that are used to assert terms (IRIs or literals). Statement Assertions : functions that are used to assert statements (triples or quads). RML inserts the data that has been transformed (from a non-RDF format into RDF triples) into the store. XSLT inserts the data that has been transformed (from XML to XML or RDF) using stylesheet parameter in loadRdf() function into the store","title":"Assert"},{"location":"triply-etl/assert/#next-steps","text":"After linked data has been asserted into the internal store, the following steps can be preformed: Step 4. Enrich : improves and extends linked data in the internal store. Step 5. Validate ensures that linked data in the internal store meets the specified quality criteria. Step 6. Publish : takes the linked data from the internal store, and publishes it to a destination such as TriplyDB .","title":"Next steps"},{"location":"triply-etl/assert/json-ld/","text":"On this page: JSON-LD Assert JSON-LD Assert \u00b6 The JSON-LD standard includes the following algorithms that allow linked data to be added to the internal store: The Expansion algorithm allows a JSON-LD context to be applied to the record. The Deserialization algorithm allows linked data to be generated based on the expanded record.","title":"JSON-LD"},{"location":"triply-etl/assert/json-ld/#json-ld-assert","text":"The JSON-LD standard includes the following algorithms that allow linked data to be added to the internal store: The Expansion algorithm allows a JSON-LD context to be applied to the record. The Deserialization algorithm allows linked data to be generated based on the expanded record.","title":"JSON-LD Assert"},{"location":"triply-etl/assert/rml/","text":"On this page: RML Assert RML Assert \u00b6","title":"RML"},{"location":"triply-etl/assert/rml/#rml-assert","text":"","title":"RML Assert"},{"location":"triply-etl/assert/xslt/","text":"On this page: XSLT Assert XSLT Assert \u00b6","title":"XSLT"},{"location":"triply-etl/assert/xslt/#xslt-assert","text":"","title":"XSLT Assert"},{"location":"triply-etl/assert/ratt/statements/","text":"On this page: RATT Statement Assertion nestedPairs() Signature Parameters Example: Unit of measure Example: Geometry Maintenance impact Relation to standards objects() Signature Parameters Example: Alternative labels Maintenance impact Relation to standards pairs() Signature Parameters Example: Alternative and preferred label Maintenance impact Relation to standards quad() Signature Parameters Example: Data and metadata See also quads() Signature Parameters Example: Data and metadata See also triple() Signature Parameters Example: 1 Example: 2 Example: 3 triples() Signature Parameters Example Maintenance impact Relation to standards Implicit casts Relation to standards RATT Statement Assertion \u00b6 This page documents the functions that make linked data statement assertions (triples and quads). The statement assertion functions are imported as follows: import { nestedPairs, objects, pairs, quad, quads, triple, triples } from '@triplyetl/etl/ratt' nestedPairs() \u00b6 Creates a nested node and makes multiple assertions about that node. Since linked data is composed of triples, more complex n-ary information must often be asserted by using a nested node. Since they must appear in both the subject and object term position, nested nodes are required to be IRIs. Signature \u00b6 This function has the following signature: nestedPairs(subject, predicate, ...pairs) // [1] nestedPairs(subject, predicate, nestedNode, ...pairs) // [2] nestedPairs(graph, subject, predicate, nestedNode, ...pairs) // [3] When Signature 1 is used, the nested node is automatically generated by TriplyETL. This automatically generated nested node is a Skolem IRI (see the documentation on Skolem IRIs for more information). When Signature 2 is used, the nested node must be specified by the user. This allows the nested node to be created in a more structured way, for example by using iri() , addIri() , or addHashedIri() . When Signature 3 is used, a graph name must be specified, resulting in quad statements. Parameters \u00b6 graph is a graph term; this must be an IRI. subject is a subject term; this must be an IRI. predicate is a predicate term; this must be an IRI. nestedNode is the nested node; this must be an IRI. pairs is one or more pairs that make assertions about the nested node. Every pair consists of a predicate term and an object term (in that order). Example: Unit of measure \u00b6 The following example asserts a value together with a unit of measure. Since Signature 1 is used, a Skolem IRI is used as the nested node. fromJson([{ id: '1', height: 15 }]), nestedPairs(iri(prefix.product, 'id'), sdo.height, [qudt.unit, unit.CentiM], [rdf.value, 'height'], ), This makes the following linked data assertions: product:1 sdo:height [ qudt:unit unit:CentiM; rdf:value 15 ]. Or diagrammatically: graph LR product -- sdo:height --> skolem skolem -- qudt:unit --> centim skolem -- rdf:value --> 15 product[product:1]:::data skolem[_:1]:::data centim[unit:CentiM]:::model 15:::data classDef data fill:yellow classDef model fill:lightblue classDef meta fill:sandybrown Example: Geometry \u00b6 The following example asserts a GeoSPARQL geometry. The geometry is created as a separate node. fromJson([{ id: '1', geometry: 'Point(1.1 2.2)' }]), nestedPairs(iri(prefix.feature, 'id'), geo.hasGeometry, iri(prefix.geometry, 'id'), [a, geo.Geometry], [geo.asWKT, literal('geometry', geo.wktLiteral)], ), This generates the following linked data: feature:1 geo:hasGeometry geometry:1. geometry:1 a geo:Geometry; geo:asWKT 'Point(1.1 2.2)'^^geo:wktLiteral. Or diagrammatically: graph LR feature -- geo:hasGeometry --> geometry geometry -- a --> Geometry geometry -- geo:asWKT --> wkt feature[feature:1]:::data geometry[geometry:1]:::data Geometry[geo:Geometry]:::model wkt[\"'Point(1.1 2.2)'^^geo:wktLiteral\"]:::data classDef data fill:yellow classDef model fill:lightblue classDef meta fill:sandybrown Maintenance impact \u00b6 Every use of nestedPairs() can be replaced by multiple uses of other assertion functions, like triple() , quad() , pairs() , and addSkolemIri() . For example, when the geometry example is rewritten to not use nestedPairs() , the nested node must be specified twice, which is a maintenance burden: fromJson([{ id: '1', geometry: 'Point(1.1 2.2)' }]), triple(iri(prefix.feature, 'id'), geo.hasGeometry, iri(prefix.geometry, 'id')), pairs(iri(prefix.geometry, 'id'), [a, geo.Geometry], [geo.asWKT, literal('geometry', geo.wktLiteral)], ), Notice that the use of nestedPairs() results in configuration that is shorter and easier to maintain. For example, nestedPairs() does not need to repeat the specification of the nested node iri(prefix.geometry, 'id') . Relation to standards \u00b6 The functionality of nestedPairs() is similar to anonymous node notation and predicate list notation in the linked data standards TriG, Turtle, and SPARQL. Notice that the following notation in these standards: feature:id geo:hasGeometry [ a geo:Geometry; geo:asWKT 'Point(1.1 2.2)'^^geo:wktLiteral ]. is structurally similar to the following code snippet that uses nestedPairs() : nestedPairs(iri(prefix.feature, 'id'), geo.hasGeometry, [a, geo.Geometry], [geo.asWKT, literal('geometry', geo.wktLiteral)], ), objects() \u00b6 Asserts multiple triples that share the same subject and predicate term. Signature \u00b6 This function has the following signature: objects(subject, predicate, ...objects) Parameters \u00b6 subject is a subject term; this must be an IRI. predicate is a predicate term; this must be an IRI. objects is one or more object terms; these must be IRIs and/or literals. Example: Alternative labels \u00b6 The following snippet asserts multiple alternative labels for a city: fromJson([{ name: 'Ney York', alt1: 'The Big Apple', alt2: 'The Capital of the World', alt3: 'The City of Dreams', alt4: 'The City That Never Sleeps', }]), objects(iri(prefix.city, 'name'), skos.altLabel, 'alt1', 'alt2', 'alt3', 'alt4', ), This results in the following 4 linked data assertions: city:New%20York skos:altLabel 'The Big Apple'@en. 'The Capital of the World'@en, 'The City of Dreams'@en, 'The City That Never Sleeps'@en. Or diagrammatically: graph LR newYork -- skos:altLabel --> a & b & c & d newYork[city:New%20York]:::data a[\"'The Big Apple'@en\"]:::data b[\"'The Capital of the World'@en\"]:::data c[\"'The City of Dreams'@en\"]:::data d[\"'The City That Never Sleeps'@en\"]:::data classDef data fill:yellow Maintenance impact \u00b6 Every use of objects() can be replaced by multiple uses of the triple() assertion function. However, doing so requires the subject and predicate terms to be repeated for each use of triple() . This is why the use of objects() results in configuration that is shorter and easier to maintain. With the release of 4.0.0 version, it is no longer allowed to have less than 2 objects, otherwise refer to transformation triple() . Relation to standards \u00b6 The functionality of objects() is similar to predicate-object list notation in the linked data standards TriG, Turtle, and SPARQL. Notice that the following notation in these standards: city:New%20York skos:altLabel 'The Big Apple', 'The Capital of the World', 'The City of Dreams', 'The City That Never Sleeps'. is structurally similar to the following code snippet that uses objects() : objects(iri(prefix.city, 'name'), skos.altLabel, 'alt1', 'alt2', 'alt3', 'alt4', ), pairs() \u00b6 Asserts multiple triples that share the same subject term. Signature \u00b6 This function has the following signature: pairs(subject, ...pairs) // [1] pairs(graph, subject, ...pairs) // [2] Signature 1 asserts triples, while Signature 2 asserts quads. Parameters \u00b6 graph is a graph term; this must be an IRI subject is a subject term; this must be an IRI. pairs is one or more pairs that make assertions about the subject term. Every pair consists of a predicate term and an object term (in that order). Example: Alternative and preferred label \u00b6 The following snippet asserts a preferred label and an alternative label for cities: fromJson([ { name: 'London', alt: 'Home of the Big Ben' }, { name: 'Ney York', alt: 'The Big Apple' }, ]), pairs(iri(prefix.city, 'name'), [skos.prefLabel, literal('name', lang.en)], [skos.altLabel, literal('alt', lang.en)], ), This results in the following 4 linked data assertions: city:London skos:prefLabel 'London'@en; skos:altLabel 'Home of the Big Ben'@en. city:New%20York skos:prefLabel 'New York'@en; skos:altLabel 'The Big Apple'@en. Or diagrammatically: graph LR london -- skos:altLabel --> a london -- skos:prefLabel --> b newYork -- skos:altLabel --> c newYork -- skos:prefLabel --> d london[city:London]:::data newYork[city:New%20York]:::data a[\"'Home of the Big Ben'@en\"]:::data b[\"'London'@en\"]:::data c[\"'The Big Apple'@en\"]:::data d[\"'New York'@en\"]:::data classDef data fill:yellow Maintenance impact \u00b6 This function provides a shorthand notation for assertions that can also be made with multiple uses of assertion triple() . Relation to standards \u00b6 The notational convenience of this middleware is similar to predicate lists in TriG, Turtle, and SPARQL. Notice that the following notation in these standards: city:New%20York skos:prefLabel 'New York'@en; skos:altLabel 'The Big Apple'@en. is structurally similar to the following code snippet that uses pairs() : pairs(iri(prefix.city, 'name'), [skos.prefLabel, literal('name', lang.en)], [skos.altLabel, literal('alt', lang.en)], ), quad() \u00b6 Asserts a linked data statement that consists of four terms: subject, predicate, object, and graph (in that order). A quadruple or 'quad' is a triple to which a graph name is added. Signature \u00b6 This function has the following signature: quad(subject, predicate, object, graph) Parameters \u00b6 subject is a subject term; this must be an IRI. predicate is a predicate term; this must be an IRI. object is an object term; this must be an IRI or literal. graph is a graph term; this must be an IRI. Example: Data and metadata \u00b6 A dataset may distinguish between data statements and metadata statements. Such a distinction can be implemented by placing statements into different graphs. The following code snippet makes one statements assertion in a metadata graph and one statement assertion in a data graph: quad(iri(prefix.dataset, 'flowers'), a, dcat.Dataset, graph.metadata), quad(iri(prefix.flower, '_id'), a, def.Flower, graph.data), See also \u00b6 Use function quads() to make multiple quad assertions. quads() \u00b6 Asserts multiple linked data statements that consists of four terms: subject, predicate, object, and graph (in that order). A quadruple or 'quad' is a triple to which a graph name is added. Signature \u00b6 This function has the following signature: quads(...quads) Parameters \u00b6 quads is one or more quads, represented by arrays that contain four terms: subject, predicate, object, and graph (in that order). Example: Data and metadata \u00b6 An ETL can distinguish between data and metadata assertions. Both may be placed into distinct graphs. The following snippet makes assertions in a metadata graph and assertions in a data graph. quads( [iri(prefix.dataset, 'flowers'), a, dcat.Dataset, graph.metadata], ..., ), quads( [iri(prefix.flower, '_id'), a, def.Flower, graph.data], ..., ), See also \u00b6 Use function quad() for asserting a single quad. triple() \u00b6 Asserts a linked data statement that consists of three terms: subject, predicate, and object (in that order). A triple asserts a factual statement, claiming that the thing denoted by the subject term and the thing denotes by the object term are related to one another according to the relationship denoted by the predicate term. A triple is the smallest unit of meaning in linked data. Signature \u00b6 This function has the following signature: triple(subject, predicate, object) Parameters \u00b6 subject is a subject term; this must be an IRI. predicate is a predicate term; this must be an IRI. object is an object term; this must be an IRI or literal. Example: 1 \u00b6 The following triple asserts that someone is a person: triple(iri(prefix.person, 'id'), a, foaf.Person), The subject term is an IRI that is constructed from an IRI prefix ( prefix.person ) and a key that contains the IRI local name ( 'id' ). The predicate and object terms are IRIs that are imported from external vocabularies . Example: 2 \u00b6 The following triple asserts that someone has an age that is derived from the 'age' key in the record: triple('_person', foaf.age, literal('age', xsd.nonNegativeInteger)), The subject term is an IRI that is stored in the '_person' key of the record. This term was created previously, for example by using the addIri() function. The predicate term is imported from an external vocabulary . The object term is a typed literal that is constructed from a key ( 'age' ) that contains the lexical form, and a datatype IRI that is imported from an external vocabulary. Example: 3 \u00b6 The following triple uses three static IRIs: triple(Iri('https://example.com/id/123'), a, sdo.Product), triples() \u00b6 Asserts multiple linked data statements that consists of three terms: subject, predicate, and object (in that order). Signature \u00b6 This function has the following signature: triples(graph, ...triples) Parameters \u00b6 graph is a graph term; this must be an IRI. triples is one or more triples, represented by arrays that contain three terms: subject, predicate, and object (in that order). Example \u00b6 Suppose that we want to distinguish between data and metadata assertions. We can do so by asserting them in distinct graphs. The following makes multiple metadata assertions in the metadata graph, followed by multiple data assertions in the data graph. triples(graph.metadata, [iri(prefix.dataset, str('flowers')), a, dcat.Dataset], ... ), triples(graph.data, [iri(prefix.flower, '_id'), a, def.Flower], ... ), Maintenance impact \u00b6 It is common for multiple statements to occur in the same graph. In such cases, it is position to use the quad() function multiple times, but this requires repeating the graph term. This is why the use of triples() results in configuration that is shorter and easier to maintain. Relation to standards \u00b6 The functionality of triples() is conceptually similar to graph notation in the linked data standard TriG. Notice that the following notation in TriG: graph:metadata { dataset:flowers a dcat:Dataset. ... } graph:data { flower:123 a def:Flower. ... } is structurally similar to the following code snippet that uses triples() : triples(graph.metadata, [iri(prefix.dataset, str('flowers')), a, dcat.Dataset], ... ), triples(graph.data, [iri(prefix.flower, '_id'), a, def.Flower], ... ), Implicit casts \u00b6 The statement assertion functions use implicit casting from strings to IRIs or literals. The rules for this are as follows: If a string value that encodes a valid IRI is specified in the subject, predicate, or graph position, that string is implicitly cast to an IRI. If a string value is specified in the object position, that string is implicitly cast to a literal. The following code snippet uses implicit casts for all fours terms in the quad() assertion: fromJson([{ url: 'https://example.com/123' }]), quad('url', 'url', 'url', 'url'), This results in the following linked data: <https://example.com/123> { <https://example.com/123> <https://example.com/123> 'https://example.com/123'. } Notice that the code snippet can be rewritten to make use of explicit casts: fromJson([{ url: 'https://example.com/123' }]), quad(iri('url'), iri('url'), literal('url'), iri('url')), Relation to standards \u00b6 The functionality of implicit casts for literals is conceptually similar to shorthand notation for xsd:string literals in the linked data standards Turtle, TriG, and SPARQL. Notice that the following notation in Turtle: city:amsterdam dct:identifier '0200'. is structurally similar to the following code snippet that uses an implicit cast for the string literal: triple('_city', dct.identifier, 'id'),","title":"RATT Statements"},{"location":"triply-etl/assert/ratt/statements/#ratt-statement-assertion","text":"This page documents the functions that make linked data statement assertions (triples and quads). The statement assertion functions are imported as follows: import { nestedPairs, objects, pairs, quad, quads, triple, triples } from '@triplyetl/etl/ratt'","title":"RATT Statement Assertion"},{"location":"triply-etl/assert/ratt/statements/#nestedpairs","text":"Creates a nested node and makes multiple assertions about that node. Since linked data is composed of triples, more complex n-ary information must often be asserted by using a nested node. Since they must appear in both the subject and object term position, nested nodes are required to be IRIs.","title":"nestedPairs()"},{"location":"triply-etl/assert/ratt/statements/#signature","text":"This function has the following signature: nestedPairs(subject, predicate, ...pairs) // [1] nestedPairs(subject, predicate, nestedNode, ...pairs) // [2] nestedPairs(graph, subject, predicate, nestedNode, ...pairs) // [3] When Signature 1 is used, the nested node is automatically generated by TriplyETL. This automatically generated nested node is a Skolem IRI (see the documentation on Skolem IRIs for more information). When Signature 2 is used, the nested node must be specified by the user. This allows the nested node to be created in a more structured way, for example by using iri() , addIri() , or addHashedIri() . When Signature 3 is used, a graph name must be specified, resulting in quad statements.","title":"Signature"},{"location":"triply-etl/assert/ratt/statements/#parameters","text":"graph is a graph term; this must be an IRI. subject is a subject term; this must be an IRI. predicate is a predicate term; this must be an IRI. nestedNode is the nested node; this must be an IRI. pairs is one or more pairs that make assertions about the nested node. Every pair consists of a predicate term and an object term (in that order).","title":"Parameters"},{"location":"triply-etl/assert/ratt/statements/#example-unit-of-measure","text":"The following example asserts a value together with a unit of measure. Since Signature 1 is used, a Skolem IRI is used as the nested node. fromJson([{ id: '1', height: 15 }]), nestedPairs(iri(prefix.product, 'id'), sdo.height, [qudt.unit, unit.CentiM], [rdf.value, 'height'], ), This makes the following linked data assertions: product:1 sdo:height [ qudt:unit unit:CentiM; rdf:value 15 ]. Or diagrammatically: graph LR product -- sdo:height --> skolem skolem -- qudt:unit --> centim skolem -- rdf:value --> 15 product[product:1]:::data skolem[_:1]:::data centim[unit:CentiM]:::model 15:::data classDef data fill:yellow classDef model fill:lightblue classDef meta fill:sandybrown","title":"Example: Unit of measure"},{"location":"triply-etl/assert/ratt/statements/#example-geometry","text":"The following example asserts a GeoSPARQL geometry. The geometry is created as a separate node. fromJson([{ id: '1', geometry: 'Point(1.1 2.2)' }]), nestedPairs(iri(prefix.feature, 'id'), geo.hasGeometry, iri(prefix.geometry, 'id'), [a, geo.Geometry], [geo.asWKT, literal('geometry', geo.wktLiteral)], ), This generates the following linked data: feature:1 geo:hasGeometry geometry:1. geometry:1 a geo:Geometry; geo:asWKT 'Point(1.1 2.2)'^^geo:wktLiteral. Or diagrammatically: graph LR feature -- geo:hasGeometry --> geometry geometry -- a --> Geometry geometry -- geo:asWKT --> wkt feature[feature:1]:::data geometry[geometry:1]:::data Geometry[geo:Geometry]:::model wkt[\"'Point(1.1 2.2)'^^geo:wktLiteral\"]:::data classDef data fill:yellow classDef model fill:lightblue classDef meta fill:sandybrown","title":"Example: Geometry"},{"location":"triply-etl/assert/ratt/statements/#maintenance-impact","text":"Every use of nestedPairs() can be replaced by multiple uses of other assertion functions, like triple() , quad() , pairs() , and addSkolemIri() . For example, when the geometry example is rewritten to not use nestedPairs() , the nested node must be specified twice, which is a maintenance burden: fromJson([{ id: '1', geometry: 'Point(1.1 2.2)' }]), triple(iri(prefix.feature, 'id'), geo.hasGeometry, iri(prefix.geometry, 'id')), pairs(iri(prefix.geometry, 'id'), [a, geo.Geometry], [geo.asWKT, literal('geometry', geo.wktLiteral)], ), Notice that the use of nestedPairs() results in configuration that is shorter and easier to maintain. For example, nestedPairs() does not need to repeat the specification of the nested node iri(prefix.geometry, 'id') .","title":"Maintenance impact"},{"location":"triply-etl/assert/ratt/statements/#relation-to-standards","text":"The functionality of nestedPairs() is similar to anonymous node notation and predicate list notation in the linked data standards TriG, Turtle, and SPARQL. Notice that the following notation in these standards: feature:id geo:hasGeometry [ a geo:Geometry; geo:asWKT 'Point(1.1 2.2)'^^geo:wktLiteral ]. is structurally similar to the following code snippet that uses nestedPairs() : nestedPairs(iri(prefix.feature, 'id'), geo.hasGeometry, [a, geo.Geometry], [geo.asWKT, literal('geometry', geo.wktLiteral)], ),","title":"Relation to standards"},{"location":"triply-etl/assert/ratt/statements/#objects","text":"Asserts multiple triples that share the same subject and predicate term.","title":"objects()"},{"location":"triply-etl/assert/ratt/statements/#signature_1","text":"This function has the following signature: objects(subject, predicate, ...objects)","title":"Signature"},{"location":"triply-etl/assert/ratt/statements/#parameters_1","text":"subject is a subject term; this must be an IRI. predicate is a predicate term; this must be an IRI. objects is one or more object terms; these must be IRIs and/or literals.","title":"Parameters"},{"location":"triply-etl/assert/ratt/statements/#example-alternative-labels","text":"The following snippet asserts multiple alternative labels for a city: fromJson([{ name: 'Ney York', alt1: 'The Big Apple', alt2: 'The Capital of the World', alt3: 'The City of Dreams', alt4: 'The City That Never Sleeps', }]), objects(iri(prefix.city, 'name'), skos.altLabel, 'alt1', 'alt2', 'alt3', 'alt4', ), This results in the following 4 linked data assertions: city:New%20York skos:altLabel 'The Big Apple'@en. 'The Capital of the World'@en, 'The City of Dreams'@en, 'The City That Never Sleeps'@en. Or diagrammatically: graph LR newYork -- skos:altLabel --> a & b & c & d newYork[city:New%20York]:::data a[\"'The Big Apple'@en\"]:::data b[\"'The Capital of the World'@en\"]:::data c[\"'The City of Dreams'@en\"]:::data d[\"'The City That Never Sleeps'@en\"]:::data classDef data fill:yellow","title":"Example: Alternative labels"},{"location":"triply-etl/assert/ratt/statements/#maintenance-impact_1","text":"Every use of objects() can be replaced by multiple uses of the triple() assertion function. However, doing so requires the subject and predicate terms to be repeated for each use of triple() . This is why the use of objects() results in configuration that is shorter and easier to maintain. With the release of 4.0.0 version, it is no longer allowed to have less than 2 objects, otherwise refer to transformation triple() .","title":"Maintenance impact"},{"location":"triply-etl/assert/ratt/statements/#relation-to-standards_1","text":"The functionality of objects() is similar to predicate-object list notation in the linked data standards TriG, Turtle, and SPARQL. Notice that the following notation in these standards: city:New%20York skos:altLabel 'The Big Apple', 'The Capital of the World', 'The City of Dreams', 'The City That Never Sleeps'. is structurally similar to the following code snippet that uses objects() : objects(iri(prefix.city, 'name'), skos.altLabel, 'alt1', 'alt2', 'alt3', 'alt4', ),","title":"Relation to standards"},{"location":"triply-etl/assert/ratt/statements/#pairs","text":"Asserts multiple triples that share the same subject term.","title":"pairs()"},{"location":"triply-etl/assert/ratt/statements/#signature_2","text":"This function has the following signature: pairs(subject, ...pairs) // [1] pairs(graph, subject, ...pairs) // [2] Signature 1 asserts triples, while Signature 2 asserts quads.","title":"Signature"},{"location":"triply-etl/assert/ratt/statements/#parameters_2","text":"graph is a graph term; this must be an IRI subject is a subject term; this must be an IRI. pairs is one or more pairs that make assertions about the subject term. Every pair consists of a predicate term and an object term (in that order).","title":"Parameters"},{"location":"triply-etl/assert/ratt/statements/#example-alternative-and-preferred-label","text":"The following snippet asserts a preferred label and an alternative label for cities: fromJson([ { name: 'London', alt: 'Home of the Big Ben' }, { name: 'Ney York', alt: 'The Big Apple' }, ]), pairs(iri(prefix.city, 'name'), [skos.prefLabel, literal('name', lang.en)], [skos.altLabel, literal('alt', lang.en)], ), This results in the following 4 linked data assertions: city:London skos:prefLabel 'London'@en; skos:altLabel 'Home of the Big Ben'@en. city:New%20York skos:prefLabel 'New York'@en; skos:altLabel 'The Big Apple'@en. Or diagrammatically: graph LR london -- skos:altLabel --> a london -- skos:prefLabel --> b newYork -- skos:altLabel --> c newYork -- skos:prefLabel --> d london[city:London]:::data newYork[city:New%20York]:::data a[\"'Home of the Big Ben'@en\"]:::data b[\"'London'@en\"]:::data c[\"'The Big Apple'@en\"]:::data d[\"'New York'@en\"]:::data classDef data fill:yellow","title":"Example: Alternative and preferred label"},{"location":"triply-etl/assert/ratt/statements/#maintenance-impact_2","text":"This function provides a shorthand notation for assertions that can also be made with multiple uses of assertion triple() .","title":"Maintenance impact"},{"location":"triply-etl/assert/ratt/statements/#relation-to-standards_2","text":"The notational convenience of this middleware is similar to predicate lists in TriG, Turtle, and SPARQL. Notice that the following notation in these standards: city:New%20York skos:prefLabel 'New York'@en; skos:altLabel 'The Big Apple'@en. is structurally similar to the following code snippet that uses pairs() : pairs(iri(prefix.city, 'name'), [skos.prefLabel, literal('name', lang.en)], [skos.altLabel, literal('alt', lang.en)], ),","title":"Relation to standards"},{"location":"triply-etl/assert/ratt/statements/#quad","text":"Asserts a linked data statement that consists of four terms: subject, predicate, object, and graph (in that order). A quadruple or 'quad' is a triple to which a graph name is added.","title":"quad()"},{"location":"triply-etl/assert/ratt/statements/#signature_3","text":"This function has the following signature: quad(subject, predicate, object, graph)","title":"Signature"},{"location":"triply-etl/assert/ratt/statements/#parameters_3","text":"subject is a subject term; this must be an IRI. predicate is a predicate term; this must be an IRI. object is an object term; this must be an IRI or literal. graph is a graph term; this must be an IRI.","title":"Parameters"},{"location":"triply-etl/assert/ratt/statements/#example-data-and-metadata","text":"A dataset may distinguish between data statements and metadata statements. Such a distinction can be implemented by placing statements into different graphs. The following code snippet makes one statements assertion in a metadata graph and one statement assertion in a data graph: quad(iri(prefix.dataset, 'flowers'), a, dcat.Dataset, graph.metadata), quad(iri(prefix.flower, '_id'), a, def.Flower, graph.data),","title":"Example: Data and metadata"},{"location":"triply-etl/assert/ratt/statements/#see-also","text":"Use function quads() to make multiple quad assertions.","title":"See also"},{"location":"triply-etl/assert/ratt/statements/#quads","text":"Asserts multiple linked data statements that consists of four terms: subject, predicate, object, and graph (in that order). A quadruple or 'quad' is a triple to which a graph name is added.","title":"quads()"},{"location":"triply-etl/assert/ratt/statements/#signature_4","text":"This function has the following signature: quads(...quads)","title":"Signature"},{"location":"triply-etl/assert/ratt/statements/#parameters_4","text":"quads is one or more quads, represented by arrays that contain four terms: subject, predicate, object, and graph (in that order).","title":"Parameters"},{"location":"triply-etl/assert/ratt/statements/#example-data-and-metadata_1","text":"An ETL can distinguish between data and metadata assertions. Both may be placed into distinct graphs. The following snippet makes assertions in a metadata graph and assertions in a data graph. quads( [iri(prefix.dataset, 'flowers'), a, dcat.Dataset, graph.metadata], ..., ), quads( [iri(prefix.flower, '_id'), a, def.Flower, graph.data], ..., ),","title":"Example: Data and metadata"},{"location":"triply-etl/assert/ratt/statements/#see-also_1","text":"Use function quad() for asserting a single quad.","title":"See also"},{"location":"triply-etl/assert/ratt/statements/#triple","text":"Asserts a linked data statement that consists of three terms: subject, predicate, and object (in that order). A triple asserts a factual statement, claiming that the thing denoted by the subject term and the thing denotes by the object term are related to one another according to the relationship denoted by the predicate term. A triple is the smallest unit of meaning in linked data.","title":"triple()"},{"location":"triply-etl/assert/ratt/statements/#signature_5","text":"This function has the following signature: triple(subject, predicate, object)","title":"Signature"},{"location":"triply-etl/assert/ratt/statements/#parameters_5","text":"subject is a subject term; this must be an IRI. predicate is a predicate term; this must be an IRI. object is an object term; this must be an IRI or literal.","title":"Parameters"},{"location":"triply-etl/assert/ratt/statements/#example-1","text":"The following triple asserts that someone is a person: triple(iri(prefix.person, 'id'), a, foaf.Person), The subject term is an IRI that is constructed from an IRI prefix ( prefix.person ) and a key that contains the IRI local name ( 'id' ). The predicate and object terms are IRIs that are imported from external vocabularies .","title":"Example: 1"},{"location":"triply-etl/assert/ratt/statements/#example-2","text":"The following triple asserts that someone has an age that is derived from the 'age' key in the record: triple('_person', foaf.age, literal('age', xsd.nonNegativeInteger)), The subject term is an IRI that is stored in the '_person' key of the record. This term was created previously, for example by using the addIri() function. The predicate term is imported from an external vocabulary . The object term is a typed literal that is constructed from a key ( 'age' ) that contains the lexical form, and a datatype IRI that is imported from an external vocabulary.","title":"Example: 2"},{"location":"triply-etl/assert/ratt/statements/#example-3","text":"The following triple uses three static IRIs: triple(Iri('https://example.com/id/123'), a, sdo.Product),","title":"Example: 3"},{"location":"triply-etl/assert/ratt/statements/#triples","text":"Asserts multiple linked data statements that consists of three terms: subject, predicate, and object (in that order).","title":"triples()"},{"location":"triply-etl/assert/ratt/statements/#signature_6","text":"This function has the following signature: triples(graph, ...triples)","title":"Signature"},{"location":"triply-etl/assert/ratt/statements/#parameters_6","text":"graph is a graph term; this must be an IRI. triples is one or more triples, represented by arrays that contain three terms: subject, predicate, and object (in that order).","title":"Parameters"},{"location":"triply-etl/assert/ratt/statements/#example","text":"Suppose that we want to distinguish between data and metadata assertions. We can do so by asserting them in distinct graphs. The following makes multiple metadata assertions in the metadata graph, followed by multiple data assertions in the data graph. triples(graph.metadata, [iri(prefix.dataset, str('flowers')), a, dcat.Dataset], ... ), triples(graph.data, [iri(prefix.flower, '_id'), a, def.Flower], ... ),","title":"Example"},{"location":"triply-etl/assert/ratt/statements/#maintenance-impact_3","text":"It is common for multiple statements to occur in the same graph. In such cases, it is position to use the quad() function multiple times, but this requires repeating the graph term. This is why the use of triples() results in configuration that is shorter and easier to maintain.","title":"Maintenance impact"},{"location":"triply-etl/assert/ratt/statements/#relation-to-standards_3","text":"The functionality of triples() is conceptually similar to graph notation in the linked data standard TriG. Notice that the following notation in TriG: graph:metadata { dataset:flowers a dcat:Dataset. ... } graph:data { flower:123 a def:Flower. ... } is structurally similar to the following code snippet that uses triples() : triples(graph.metadata, [iri(prefix.dataset, str('flowers')), a, dcat.Dataset], ... ), triples(graph.data, [iri(prefix.flower, '_id'), a, def.Flower], ... ),","title":"Relation to standards"},{"location":"triply-etl/assert/ratt/statements/#implicit-casts","text":"The statement assertion functions use implicit casting from strings to IRIs or literals. The rules for this are as follows: If a string value that encodes a valid IRI is specified in the subject, predicate, or graph position, that string is implicitly cast to an IRI. If a string value is specified in the object position, that string is implicitly cast to a literal. The following code snippet uses implicit casts for all fours terms in the quad() assertion: fromJson([{ url: 'https://example.com/123' }]), quad('url', 'url', 'url', 'url'), This results in the following linked data: <https://example.com/123> { <https://example.com/123> <https://example.com/123> 'https://example.com/123'. } Notice that the code snippet can be rewritten to make use of explicit casts: fromJson([{ url: 'https://example.com/123' }]), quad(iri('url'), iri('url'), literal('url'), iri('url')),","title":"Implicit casts"},{"location":"triply-etl/assert/ratt/statements/#relation-to-standards_4","text":"The functionality of implicit casts for literals is conceptually similar to shorthand notation for xsd:string literals in the linked data standards Turtle, TriG, and SPARQL. Notice that the following notation in Turtle: city:amsterdam dct:identifier '0200'. is structurally similar to the following code snippet that uses an implicit cast for the string literal: triple('_city', dct.identifier, 'id'),","title":"Relation to standards"},{"location":"triply-etl/assert/ratt/terms/","text":"On this page: RATT Term Assertion Iri() constructor Signature Parameters Example: IRI declaration Example: in-line IRI Example: IRI concatenation iri() function Signature Parameters Example: explicit cast to IRI Example: dynamic IRI Example: static IRI See also iris() Signature Parameters Example list() Signature Parameters Example: fruit basket Example: children Maintenance impact Relation to standards literal() Signature Parameters Example: language-tagged string Example: typed literal Example: string literal See also literals() Signature Parameters Example: fruit basket Example: string literals str() Signature Parameters Example Relation to standards RATT Term Assertion \u00b6 This page documents RATT functions that are used to create RDF terms. These RDF terms are used in statement assertions . The term assertion functions are imported in the following way: import { iri, iris, literal, literals, str } from '@triplyetl/etl/ratt' Iri() constructor \u00b6 Creates static IRIs. Signature \u00b6 The signature of this constructor is as follows: Iri(string): Iri Parameters \u00b6 string is a string that encodes an absolute IRI. Once an IRI object is constructed, the concat() member function can be used to create new IRIs according to the following signature: Iri.concat(string): Iri Example: IRI declaration \u00b6 The following code snippet creates a static IRI by using the Iri() constructor: const subject = Iri('https://example.com/123') etl.use( triple(subject, a, sdo.Product), ) Example: in-line IRI \u00b6 It is also possible to add the static IRI in-line, without declaring a constant: etl.use( triple(Iri('https://example.com/123'), a, sdo.Product), ) Example: IRI concatenation \u00b6 The following code snippet uses a static IRI that is created by applying the concat() member function const prefix = Iri('https://example.com/') etl.use( triple(prefix.concat('123'), a, sdo.Product), ) iri() function \u00b6 Creates a static or dynamic IRI that can be used in statement assertions. Notice that this function is more powerful than the Iri() constructor , which can only create static Signature \u00b6 This function has the following signature: iri(fullIri) // 1 iri(prefix, localName) // 2 Signature 1 is used to explicitly cast a strings that encodes an absolute IRI to an IRI. Signature 2 is used to create IRIs based on an IRI prefix and multiple local names. Parameters \u00b6 fullIri is either a key that contains a dynamic string that encodes an absolute IRI, or a static string that encodes an absolute IRI. prefix is an IRI prefix that is declared with the Iri() constructor . localName is ither a key that contains a dynamic string, or a static string. This string is used as the local name of the IRI. The local name is suffixed to the given IRI prefix. Example: explicit cast to IRI \u00b6 The following code snippets casts strings that encode IRIs in the source data to subject and object IRIs that are used in triple assertions: fromJson([ { url: 'https://example.com/id/person/Jane' }, { url: 'https://example.com/id/person/John' }, ]), triple(iri('url'), owl:sameAs, iri('url')), This results in the following linked data: <https://example.com/id/person/Jane> owl:sameAs <https://example.com/id/person/Jane>. <https://example.com/id/person/John> owl:sameAs <https://example.com/id/person/John>. Notice that the use of iri() it is not required in the subject position, but is required in the object position. The following code snippet results in the same linked data, but uses an implicit cast for the subject term: fromJson([ { url: 'https://example.com/id/person/Jane' }, { url: 'https://example.com/id/person/John' }, ]), triple('url', owl:sameAs, iri('url')), See the section on automatic casts for more information. Example: dynamic IRI \u00b6 The following code snippet asserts an IRI based on a declared prefix ( prefix.ex ) and the string stored in key ( 'firstName' ): fromJson([{ firstName: 'Jane' }, { firstName: 'John' }]), triple(iri(prefix.person, 'fistName'), a, sdo.Person), This creates a dynamic IRI . This means that the asserted IRI depends on the content of the 'firstName' key in each record. For the first record, IRI person:Jane is created. For the second record, IRI person:John is created. Example: static IRI \u00b6 The following asserts an IRI based on a declared prefix ( prefix.ex ) and a static string (see function str() ): triple(iri(prefix.person, str('John')), a, sdo.Person), This creates a static IRI . This means that the same IRI is used for each record (always person:John ). Notice that the same triple assertion can be made by using the Iri() instead of the iri() function: triple(prefix.person.concat('John'), a, sdo.Person), See also \u00b6 If the same IRI is used in multiple statements, repeating the same assertion multiple times may impose a maintenance burden. In such cases, it is possible to first add the IRI to the record by using the addIri() function, and refer to that one IRI in multiple statement assertions. Use function iris() to create multiple IRIs at once. iris() \u00b6 Creates multiple dynamic or static IRIs, one for each entry in an array of strings. Signature \u00b6 This function has the following signature: iris(fullIris) // 1 iris(prefix, localNames) // 2 Signature 1 is used to explicitly cast strings that encode IRIs to IRIs. Signature 2 is used to create an IRI based on an IRI prefix and a local name. Parameters \u00b6 fullIri is either a key that contains a dynamic string that encodes an absolute IRI, or a static string that encodes an absolute IRI. prefix is an IRI prefix that is declared with the Iri() constructor . localNames is either a key that contains an array of strings, or an array of keys that store dynamic strings and static strings. These string are used as the local names of the IRIs that are created. These local names are suffixed to the given IRI prefix. Example \u00b6 The following code snippet asserts one IRI for each entry in record key 'children' : fromJson([{ parent: 'John', children: ['Joe', 'Jane'] }]), triple(iri(prefix.person, 'parent'), sdo.children, iris(prefix.person, 'children')), This makes the following linked data assertions: person:John sdo:children person:Joe, person:Jane. Or diagrammatically: graph LR john -- sdo:children --> joe john -- sdo:children --> jane john[person:John]:::data joe[person:Joe]:::data jane[person:Jane]:::data classDef data fill:yellow list() \u00b6 Creates an RDF collection or singly-linked list (class rdf:List ). See the Triply Data Story about collections for more information. Signature \u00b6 This function has the following signature: list(prefix, terms) Parameters \u00b6 prefix is an IRI prefix that is declared with the Iri() constructor . terms is an array of dynamic and/or static terms. Example: fruit basket \u00b6 The following code snippet creates linked lists (linked by rdf:rest ), where each value stored in the 'contents' key is rdf:first object: fromJson([{ id: 123, contents: ['apple', 'pear', 'banana'] }]), triple(iri(prefix.basket, 'id'), def.contains, list(prefix.basket, literals('contents', lang.en))), This results in the following linked data: basket:123 def:contains ( 'apple'@en 'pear'@en 'banana'@en ). When we do not make use of the collection notation ( ... ) , the asserted linked data looks as follows: basket:123 def:contains _:list1. _:list1 rdf:first 'apple'@en; rdf:rest _:list2. _:list2 rdf:first 'pear'@en; rdf:rest _:list3. _:list3 rdf:first 'banana'@en; rdf:rest rdf:nil. Or diagrammatically: graph LR basket -- def:contains --> list1 list1 -- rdf:first --> apple list1 -- rdf:rest --> list2 list2 -- rdf:first --> pear list2 -- rdf:rest --> list3 list3 -- rdf:first --> banana list3 -- rdf:rest --> rdf:nil apple[\"'apple'@en\"]:::data list1[_:list1]:::data list2[_:list2]:::data list3[_:list3]:::data banana[\"'banana'@en\"]:::data basket[basket:123]:::data pear[\"'pear'@en\"]:::data classDef data fill:yellow Example: children \u00b6 The following code snippet creates linked lists for the children of every parent: fromJson([{ parent: 'John', children: ['Joe', 'Jane'] }]), triple(iri(prefix.person, 'parent'), sdo.children, list(prefix.skolem, iris(prefix.person, 'children'))), This results in the following linked data: person:John sdo:children _:list1. _:list1 rdf:first person:Joe; rdf:rest _:list2. _:list2 rdf:first person:Jane; rdf:rest rdf:nil. Or diagrammatically: graph LR john -- sdo:children --> list1 list1 -- rdf:first --> joe list1 -- rdf:rest --> list2 list2 -- rdf:first --> jane list2 -- rdf:rest --> rdf:nil list1[_:list1]:::data list2[_:list2]:::data john[person:John]:::data joe[person:Joe]:::data jane[person:Jane]:::data classDef data fill:yellow The above diagram can be translated into the statement: \"John has two children, where Joe is his first child and Jane is his second child\". Maintenance impact \u00b6 Since RDF collections (or single-linked lists) require a large number of triple assertions to establish the structure of the singly-linked list, creating such collections by hand imposes a maintenance risk. It is very easy to forget one link, or to break an existing link later during maintenance. For this reason, it is always better to use the list() function in order to assert collections. Relation to standards \u00b6 The functionality of list() is similar to the collections notation in the linked data standards TriG, Turtle, and SPARQL. Notice that the following notation in these standards: person:John sdo:children ( person:Joe person:Jane ). is structurally similar to the following code snippet that uses list() : triple(iri(prefix.person, str('John')), sdo.children, list(prefix.skolem, [str('Joe'), str('Jane')])), literal() \u00b6 Creates a literal term, based on a lexical form and a datatype IRI or language tag. Signature \u00b6 This function has the following signature: literal(lexicalForm, languageTagOrDatatype) Parameters \u00b6 lexicalForm is a static string (see function str() ), or a key that contains a dynamic string. languageTagOrDatatype is a static language tag or datatype IRI, or a key that contains a dynamic language tag or datatype IRI. Example: language-tagged string \u00b6 The following code snippet uses a language-tagged string: triple('_city', sdo.name, literal('name', lang.nl)), This results in the following linked data: city:Amsterdam sdo:name 'Amsterdam'@nl. city:Berlin sdo:name 'Berlijn'@nl. The lang object contains declarations for all language tags. See the sector on language tag declarations for more information. Example: typed literal \u00b6 The following code snippet uses a typed literal: triple('_city', vocab.population, literal('population', xsd.nonNegativeInteger)), This results in the following linked data: city:Amsterdam vocab:population '1000000'^^xsd:nonNegativeInteger. city:Berlin vocab:population '2000000'^^xsd:nonNegativeInteger. New datatype IRIs can be declared and used, and existing datatype IRIs can be reused from external vocabularies. The following code snippet imports four external vocabularies that contain datatype IRIs: import { dbt, geo, rdf, xsd } from '@triplyetl/etl/generic' Here is one example of a datatype IRI for each of these four external vocabularies: dbt.kilogram geo.wktLiteral rdf.HTML xsd.dateTime Example: string literal \u00b6 Statement assertions support implicit casts from strings to string literals. This means that the following code snippet: triple('_city', dct.identifier, literal('id', xsd.string)), triple('_person', sdo.name, literal(str('John Doe'), xsd.string)), can also be expressed with the following code snippet, which does not use literal() : triple('_city', dct.identifier, 'id'), triple('_person', sdo.name, str('John Doe')), Both code snippets result in the following linked data: city:amsterdam dct:identifier '0200'. person:john-doe sdo:name 'John Doe'. See also \u00b6 If the same literal is used in multiple statements, repeating the same literal assertion multiple times may impose a maintenance burden. In such cases, it is possible to first add the literal to the record with transformation addLiteral() , and refer to that one literal in multiple statements. If multiple literals with the same language tag or datatype IRI are created, repeating the same language tag or datatype IRI may impose a maintenance burden. In such cases, the literals() assertion function can be sued instead. literals() \u00b6 Creates multiple literals, one for each lexical form that appears in an array. Signature \u00b6 The signature for this function is as follows: literals(lexicalForms, languageTagOrDatatype) Parameters \u00b6 lexicalForms is an array that contains string values, or a key that stores an array that contains string values. languageTagOrDatatype is a language tag or datatype IRI. Example: fruit basket \u00b6 The following code snippet creates one literal for each value in the array that is stored in the 'contents' key: fromJson([{ id: 123, contents: ['apple', 'pear', 'banana'] }]), triple(iri(prefix.basket, 'id'), rdfs.member, literals('contents', lang.en)), This results in the following linked data: basket:123 rdfs:member 'apple'@en, 'banana'@en, 'pear'@en. Or diagrammatically: graph LR basket -- rdfs:member --> apple basket -- rdfs:member --> banana basket -- rdfs:member --> pear apple[\"'apple'@en\"]:::data banana[\"'banana'@en\"]:::data basket[basket:123]:::data pear[\"'pear'@en\"]:::data classDef data fill:yellow Example: string literals \u00b6 String literals can be asserted directly from a key that stores an array of strings. The following code snippet asserts one string literal for each child: fromJson([{ parent: 'John', children: ['Joe', 'Jane'] }]), triple(iri(prefix.person, 'parent'), sdo.children, 'children'), This results in the following linked data assertions: person:John sdo:children 'Jane', 'Joe'. Or diagrammatically: graph LR john -- sdo:children --> jane john -- sdo:children --> joe jane['Jane']:::data joe['Joe']:::data john['John']:::data classDef data fill:yellow The same thing can be achieved by specifying an explicit datatype IRI: triple(iri(prefix.person, 'parent'), sdo.children, literals('children', xsd.string)), str() \u00b6 Creates a StaticString value. RATT uses strings to denote keys in the record. The used string denotes a key that results in dynamic values : a value that is different for each record. Sometimes we want to specify a static string instead: a string that is the same for each record. With the str() function we indicate that a string should not be processed as a key, but should be processed as a regular (static) string value. This is usefull in case we want to provide a regular string to a middleware, and not a key that could relate to the record. Signature \u00b6 This function has the following signature: str(string) Parameters \u00b6 string is a string value. Example \u00b6 In RATT, strings often denote keys in the record. For example, the string 'abc' in the following code snippet indicates that the value of key 'abc' should be used as the local name of the IRI in the subject position, and should be used as the lexical form of the literal in the object position: triple(iri(prefix.id, 'abc'), rdfs.label, 'abc'), If we want to assert the regular (static) string 'abc' , we must use the str() function. The following code snippet asserts the IRI id:abc and the literal 'abc' : triple(iri(prefix.id, str('abc')), rdfs.label, str('abc')), To illustrate the difference between a dynamic string 'key' and the static string str('key') in a more complex usecase, imagine we use the following ifElse condition in our ETL: etl.use( fromJson([ { parent: 'John', child: 'Jane', notchild: 'Joe' }, { parent: 'Lisa', child: 'James', notchild: 'Mike' } ]), ifElse({ if: ctx => ctx.getString('parent') === 'John', then: triple(iri(prefix.id, 'parent'), sdo.children, iri(prefix.id, 'child')) }), logQuads() ) In the context of the first record { parent: 'John', child: 'Jane', notchild: 'Joe' } , we grab the string in the key parent for string comparison to the string 'John' . This value is dynamic and will be 'John' for the first record (returning true for the string comparison) and Lisa for the second record (returning false for the string comparison). This results in the created triple: <https://example.com/id/John> sdo:children <https://example.com/id/Jane> If we would use str() in the ifElse for the string comparison, it would statically compare the two strings. This means that in the following example we compare string 'parent' === 'John' , which will return false for each record. ifElse({ if: ctx => ctx.getString(str('parent')) === 'John', then: triple(iri(prefix.id, 'parent'), sdo.children, iri(prefix.id, 'child')) However, if we would change the static string from str('parent') to str('John') , the string comparison will always return true for each record: etl.use( fromJson([ { parent: 'John', child: 'Jane', notchild: 'Joe' }, { parent: 'Lisa', child: 'James', notchild: 'Mike' } ]), ifElse({ if: ctx => ctx.getString(str('John')) === 'John', then: triple(iri(prefix.id, 'parent'), sdo.children, iri(prefix.id, 'child')) }), logQuads() ) This results in the created triples: <https://example.com/id/John> sdo:children <https://example.com/id/Jane> <https://example.com/id/Lisa> sdo:children <https://example.com/id/James> Relation to standards \u00b6 The functionality of str() is conceptually similar to the str function in SPARQL. In SPARQL, the str function is used to explicitly cast IRIs to their string value, and literals to their lexical form.","title":"RATT Terms"},{"location":"triply-etl/assert/ratt/terms/#ratt-term-assertion","text":"This page documents RATT functions that are used to create RDF terms. These RDF terms are used in statement assertions . The term assertion functions are imported in the following way: import { iri, iris, literal, literals, str } from '@triplyetl/etl/ratt'","title":"RATT Term Assertion"},{"location":"triply-etl/assert/ratt/terms/#iri-constructor","text":"Creates static IRIs.","title":"Iri() constructor"},{"location":"triply-etl/assert/ratt/terms/#signature","text":"The signature of this constructor is as follows: Iri(string): Iri","title":"Signature"},{"location":"triply-etl/assert/ratt/terms/#parameters","text":"string is a string that encodes an absolute IRI. Once an IRI object is constructed, the concat() member function can be used to create new IRIs according to the following signature: Iri.concat(string): Iri","title":"Parameters"},{"location":"triply-etl/assert/ratt/terms/#example-iri-declaration","text":"The following code snippet creates a static IRI by using the Iri() constructor: const subject = Iri('https://example.com/123') etl.use( triple(subject, a, sdo.Product), )","title":"Example: IRI declaration"},{"location":"triply-etl/assert/ratt/terms/#example-in-line-iri","text":"It is also possible to add the static IRI in-line, without declaring a constant: etl.use( triple(Iri('https://example.com/123'), a, sdo.Product), )","title":"Example: in-line IRI"},{"location":"triply-etl/assert/ratt/terms/#example-iri-concatenation","text":"The following code snippet uses a static IRI that is created by applying the concat() member function const prefix = Iri('https://example.com/') etl.use( triple(prefix.concat('123'), a, sdo.Product), )","title":"Example: IRI concatenation"},{"location":"triply-etl/assert/ratt/terms/#iri-function","text":"Creates a static or dynamic IRI that can be used in statement assertions. Notice that this function is more powerful than the Iri() constructor , which can only create static","title":"iri() function"},{"location":"triply-etl/assert/ratt/terms/#signature_1","text":"This function has the following signature: iri(fullIri) // 1 iri(prefix, localName) // 2 Signature 1 is used to explicitly cast a strings that encodes an absolute IRI to an IRI. Signature 2 is used to create IRIs based on an IRI prefix and multiple local names.","title":"Signature"},{"location":"triply-etl/assert/ratt/terms/#parameters_1","text":"fullIri is either a key that contains a dynamic string that encodes an absolute IRI, or a static string that encodes an absolute IRI. prefix is an IRI prefix that is declared with the Iri() constructor . localName is ither a key that contains a dynamic string, or a static string. This string is used as the local name of the IRI. The local name is suffixed to the given IRI prefix.","title":"Parameters"},{"location":"triply-etl/assert/ratt/terms/#example-explicit-cast-to-iri","text":"The following code snippets casts strings that encode IRIs in the source data to subject and object IRIs that are used in triple assertions: fromJson([ { url: 'https://example.com/id/person/Jane' }, { url: 'https://example.com/id/person/John' }, ]), triple(iri('url'), owl:sameAs, iri('url')), This results in the following linked data: <https://example.com/id/person/Jane> owl:sameAs <https://example.com/id/person/Jane>. <https://example.com/id/person/John> owl:sameAs <https://example.com/id/person/John>. Notice that the use of iri() it is not required in the subject position, but is required in the object position. The following code snippet results in the same linked data, but uses an implicit cast for the subject term: fromJson([ { url: 'https://example.com/id/person/Jane' }, { url: 'https://example.com/id/person/John' }, ]), triple('url', owl:sameAs, iri('url')), See the section on automatic casts for more information.","title":"Example: explicit cast to IRI"},{"location":"triply-etl/assert/ratt/terms/#example-dynamic-iri","text":"The following code snippet asserts an IRI based on a declared prefix ( prefix.ex ) and the string stored in key ( 'firstName' ): fromJson([{ firstName: 'Jane' }, { firstName: 'John' }]), triple(iri(prefix.person, 'fistName'), a, sdo.Person), This creates a dynamic IRI . This means that the asserted IRI depends on the content of the 'firstName' key in each record. For the first record, IRI person:Jane is created. For the second record, IRI person:John is created.","title":"Example: dynamic IRI"},{"location":"triply-etl/assert/ratt/terms/#example-static-iri","text":"The following asserts an IRI based on a declared prefix ( prefix.ex ) and a static string (see function str() ): triple(iri(prefix.person, str('John')), a, sdo.Person), This creates a static IRI . This means that the same IRI is used for each record (always person:John ). Notice that the same triple assertion can be made by using the Iri() instead of the iri() function: triple(prefix.person.concat('John'), a, sdo.Person),","title":"Example: static IRI"},{"location":"triply-etl/assert/ratt/terms/#see-also","text":"If the same IRI is used in multiple statements, repeating the same assertion multiple times may impose a maintenance burden. In such cases, it is possible to first add the IRI to the record by using the addIri() function, and refer to that one IRI in multiple statement assertions. Use function iris() to create multiple IRIs at once.","title":"See also"},{"location":"triply-etl/assert/ratt/terms/#iris","text":"Creates multiple dynamic or static IRIs, one for each entry in an array of strings.","title":"iris()"},{"location":"triply-etl/assert/ratt/terms/#signature_2","text":"This function has the following signature: iris(fullIris) // 1 iris(prefix, localNames) // 2 Signature 1 is used to explicitly cast strings that encode IRIs to IRIs. Signature 2 is used to create an IRI based on an IRI prefix and a local name.","title":"Signature"},{"location":"triply-etl/assert/ratt/terms/#parameters_2","text":"fullIri is either a key that contains a dynamic string that encodes an absolute IRI, or a static string that encodes an absolute IRI. prefix is an IRI prefix that is declared with the Iri() constructor . localNames is either a key that contains an array of strings, or an array of keys that store dynamic strings and static strings. These string are used as the local names of the IRIs that are created. These local names are suffixed to the given IRI prefix.","title":"Parameters"},{"location":"triply-etl/assert/ratt/terms/#example","text":"The following code snippet asserts one IRI for each entry in record key 'children' : fromJson([{ parent: 'John', children: ['Joe', 'Jane'] }]), triple(iri(prefix.person, 'parent'), sdo.children, iris(prefix.person, 'children')), This makes the following linked data assertions: person:John sdo:children person:Joe, person:Jane. Or diagrammatically: graph LR john -- sdo:children --> joe john -- sdo:children --> jane john[person:John]:::data joe[person:Joe]:::data jane[person:Jane]:::data classDef data fill:yellow","title":"Example"},{"location":"triply-etl/assert/ratt/terms/#list","text":"Creates an RDF collection or singly-linked list (class rdf:List ). See the Triply Data Story about collections for more information.","title":"list()"},{"location":"triply-etl/assert/ratt/terms/#signature_3","text":"This function has the following signature: list(prefix, terms)","title":"Signature"},{"location":"triply-etl/assert/ratt/terms/#parameters_3","text":"prefix is an IRI prefix that is declared with the Iri() constructor . terms is an array of dynamic and/or static terms.","title":"Parameters"},{"location":"triply-etl/assert/ratt/terms/#example-fruit-basket","text":"The following code snippet creates linked lists (linked by rdf:rest ), where each value stored in the 'contents' key is rdf:first object: fromJson([{ id: 123, contents: ['apple', 'pear', 'banana'] }]), triple(iri(prefix.basket, 'id'), def.contains, list(prefix.basket, literals('contents', lang.en))), This results in the following linked data: basket:123 def:contains ( 'apple'@en 'pear'@en 'banana'@en ). When we do not make use of the collection notation ( ... ) , the asserted linked data looks as follows: basket:123 def:contains _:list1. _:list1 rdf:first 'apple'@en; rdf:rest _:list2. _:list2 rdf:first 'pear'@en; rdf:rest _:list3. _:list3 rdf:first 'banana'@en; rdf:rest rdf:nil. Or diagrammatically: graph LR basket -- def:contains --> list1 list1 -- rdf:first --> apple list1 -- rdf:rest --> list2 list2 -- rdf:first --> pear list2 -- rdf:rest --> list3 list3 -- rdf:first --> banana list3 -- rdf:rest --> rdf:nil apple[\"'apple'@en\"]:::data list1[_:list1]:::data list2[_:list2]:::data list3[_:list3]:::data banana[\"'banana'@en\"]:::data basket[basket:123]:::data pear[\"'pear'@en\"]:::data classDef data fill:yellow","title":"Example: fruit basket"},{"location":"triply-etl/assert/ratt/terms/#example-children","text":"The following code snippet creates linked lists for the children of every parent: fromJson([{ parent: 'John', children: ['Joe', 'Jane'] }]), triple(iri(prefix.person, 'parent'), sdo.children, list(prefix.skolem, iris(prefix.person, 'children'))), This results in the following linked data: person:John sdo:children _:list1. _:list1 rdf:first person:Joe; rdf:rest _:list2. _:list2 rdf:first person:Jane; rdf:rest rdf:nil. Or diagrammatically: graph LR john -- sdo:children --> list1 list1 -- rdf:first --> joe list1 -- rdf:rest --> list2 list2 -- rdf:first --> jane list2 -- rdf:rest --> rdf:nil list1[_:list1]:::data list2[_:list2]:::data john[person:John]:::data joe[person:Joe]:::data jane[person:Jane]:::data classDef data fill:yellow The above diagram can be translated into the statement: \"John has two children, where Joe is his first child and Jane is his second child\".","title":"Example: children"},{"location":"triply-etl/assert/ratt/terms/#maintenance-impact","text":"Since RDF collections (or single-linked lists) require a large number of triple assertions to establish the structure of the singly-linked list, creating such collections by hand imposes a maintenance risk. It is very easy to forget one link, or to break an existing link later during maintenance. For this reason, it is always better to use the list() function in order to assert collections.","title":"Maintenance impact"},{"location":"triply-etl/assert/ratt/terms/#relation-to-standards","text":"The functionality of list() is similar to the collections notation in the linked data standards TriG, Turtle, and SPARQL. Notice that the following notation in these standards: person:John sdo:children ( person:Joe person:Jane ). is structurally similar to the following code snippet that uses list() : triple(iri(prefix.person, str('John')), sdo.children, list(prefix.skolem, [str('Joe'), str('Jane')])),","title":"Relation to standards"},{"location":"triply-etl/assert/ratt/terms/#literal","text":"Creates a literal term, based on a lexical form and a datatype IRI or language tag.","title":"literal()"},{"location":"triply-etl/assert/ratt/terms/#signature_4","text":"This function has the following signature: literal(lexicalForm, languageTagOrDatatype)","title":"Signature"},{"location":"triply-etl/assert/ratt/terms/#parameters_4","text":"lexicalForm is a static string (see function str() ), or a key that contains a dynamic string. languageTagOrDatatype is a static language tag or datatype IRI, or a key that contains a dynamic language tag or datatype IRI.","title":"Parameters"},{"location":"triply-etl/assert/ratt/terms/#example-language-tagged-string","text":"The following code snippet uses a language-tagged string: triple('_city', sdo.name, literal('name', lang.nl)), This results in the following linked data: city:Amsterdam sdo:name 'Amsterdam'@nl. city:Berlin sdo:name 'Berlijn'@nl. The lang object contains declarations for all language tags. See the sector on language tag declarations for more information.","title":"Example: language-tagged string"},{"location":"triply-etl/assert/ratt/terms/#example-typed-literal","text":"The following code snippet uses a typed literal: triple('_city', vocab.population, literal('population', xsd.nonNegativeInteger)), This results in the following linked data: city:Amsterdam vocab:population '1000000'^^xsd:nonNegativeInteger. city:Berlin vocab:population '2000000'^^xsd:nonNegativeInteger. New datatype IRIs can be declared and used, and existing datatype IRIs can be reused from external vocabularies. The following code snippet imports four external vocabularies that contain datatype IRIs: import { dbt, geo, rdf, xsd } from '@triplyetl/etl/generic' Here is one example of a datatype IRI for each of these four external vocabularies: dbt.kilogram geo.wktLiteral rdf.HTML xsd.dateTime","title":"Example: typed literal"},{"location":"triply-etl/assert/ratt/terms/#example-string-literal","text":"Statement assertions support implicit casts from strings to string literals. This means that the following code snippet: triple('_city', dct.identifier, literal('id', xsd.string)), triple('_person', sdo.name, literal(str('John Doe'), xsd.string)), can also be expressed with the following code snippet, which does not use literal() : triple('_city', dct.identifier, 'id'), triple('_person', sdo.name, str('John Doe')), Both code snippets result in the following linked data: city:amsterdam dct:identifier '0200'. person:john-doe sdo:name 'John Doe'.","title":"Example: string literal"},{"location":"triply-etl/assert/ratt/terms/#see-also_1","text":"If the same literal is used in multiple statements, repeating the same literal assertion multiple times may impose a maintenance burden. In such cases, it is possible to first add the literal to the record with transformation addLiteral() , and refer to that one literal in multiple statements. If multiple literals with the same language tag or datatype IRI are created, repeating the same language tag or datatype IRI may impose a maintenance burden. In such cases, the literals() assertion function can be sued instead.","title":"See also"},{"location":"triply-etl/assert/ratt/terms/#literals","text":"Creates multiple literals, one for each lexical form that appears in an array.","title":"literals()"},{"location":"triply-etl/assert/ratt/terms/#signature_5","text":"The signature for this function is as follows: literals(lexicalForms, languageTagOrDatatype)","title":"Signature"},{"location":"triply-etl/assert/ratt/terms/#parameters_5","text":"lexicalForms is an array that contains string values, or a key that stores an array that contains string values. languageTagOrDatatype is a language tag or datatype IRI.","title":"Parameters"},{"location":"triply-etl/assert/ratt/terms/#example-fruit-basket_1","text":"The following code snippet creates one literal for each value in the array that is stored in the 'contents' key: fromJson([{ id: 123, contents: ['apple', 'pear', 'banana'] }]), triple(iri(prefix.basket, 'id'), rdfs.member, literals('contents', lang.en)), This results in the following linked data: basket:123 rdfs:member 'apple'@en, 'banana'@en, 'pear'@en. Or diagrammatically: graph LR basket -- rdfs:member --> apple basket -- rdfs:member --> banana basket -- rdfs:member --> pear apple[\"'apple'@en\"]:::data banana[\"'banana'@en\"]:::data basket[basket:123]:::data pear[\"'pear'@en\"]:::data classDef data fill:yellow","title":"Example: fruit basket"},{"location":"triply-etl/assert/ratt/terms/#example-string-literals","text":"String literals can be asserted directly from a key that stores an array of strings. The following code snippet asserts one string literal for each child: fromJson([{ parent: 'John', children: ['Joe', 'Jane'] }]), triple(iri(prefix.person, 'parent'), sdo.children, 'children'), This results in the following linked data assertions: person:John sdo:children 'Jane', 'Joe'. Or diagrammatically: graph LR john -- sdo:children --> jane john -- sdo:children --> joe jane['Jane']:::data joe['Joe']:::data john['John']:::data classDef data fill:yellow The same thing can be achieved by specifying an explicit datatype IRI: triple(iri(prefix.person, 'parent'), sdo.children, literals('children', xsd.string)),","title":"Example: string literals"},{"location":"triply-etl/assert/ratt/terms/#str","text":"Creates a StaticString value. RATT uses strings to denote keys in the record. The used string denotes a key that results in dynamic values : a value that is different for each record. Sometimes we want to specify a static string instead: a string that is the same for each record. With the str() function we indicate that a string should not be processed as a key, but should be processed as a regular (static) string value. This is usefull in case we want to provide a regular string to a middleware, and not a key that could relate to the record.","title":"str()"},{"location":"triply-etl/assert/ratt/terms/#signature_6","text":"This function has the following signature: str(string)","title":"Signature"},{"location":"triply-etl/assert/ratt/terms/#parameters_6","text":"string is a string value.","title":"Parameters"},{"location":"triply-etl/assert/ratt/terms/#example_1","text":"In RATT, strings often denote keys in the record. For example, the string 'abc' in the following code snippet indicates that the value of key 'abc' should be used as the local name of the IRI in the subject position, and should be used as the lexical form of the literal in the object position: triple(iri(prefix.id, 'abc'), rdfs.label, 'abc'), If we want to assert the regular (static) string 'abc' , we must use the str() function. The following code snippet asserts the IRI id:abc and the literal 'abc' : triple(iri(prefix.id, str('abc')), rdfs.label, str('abc')), To illustrate the difference between a dynamic string 'key' and the static string str('key') in a more complex usecase, imagine we use the following ifElse condition in our ETL: etl.use( fromJson([ { parent: 'John', child: 'Jane', notchild: 'Joe' }, { parent: 'Lisa', child: 'James', notchild: 'Mike' } ]), ifElse({ if: ctx => ctx.getString('parent') === 'John', then: triple(iri(prefix.id, 'parent'), sdo.children, iri(prefix.id, 'child')) }), logQuads() ) In the context of the first record { parent: 'John', child: 'Jane', notchild: 'Joe' } , we grab the string in the key parent for string comparison to the string 'John' . This value is dynamic and will be 'John' for the first record (returning true for the string comparison) and Lisa for the second record (returning false for the string comparison). This results in the created triple: <https://example.com/id/John> sdo:children <https://example.com/id/Jane> If we would use str() in the ifElse for the string comparison, it would statically compare the two strings. This means that in the following example we compare string 'parent' === 'John' , which will return false for each record. ifElse({ if: ctx => ctx.getString(str('parent')) === 'John', then: triple(iri(prefix.id, 'parent'), sdo.children, iri(prefix.id, 'child')) However, if we would change the static string from str('parent') to str('John') , the string comparison will always return true for each record: etl.use( fromJson([ { parent: 'John', child: 'Jane', notchild: 'Joe' }, { parent: 'Lisa', child: 'James', notchild: 'Mike' } ]), ifElse({ if: ctx => ctx.getString(str('John')) === 'John', then: triple(iri(prefix.id, 'parent'), sdo.children, iri(prefix.id, 'child')) }), logQuads() ) This results in the created triples: <https://example.com/id/John> sdo:children <https://example.com/id/Jane> <https://example.com/id/Lisa> sdo:children <https://example.com/id/James>","title":"Example"},{"location":"triply-etl/assert/ratt/terms/#relation-to-standards_1","text":"The functionality of str() is conceptually similar to the str function in SPARQL. In SPARQL, the str function is used to explicitly cast IRIs to their string value, and literals to their lexical form.","title":"Relation to standards"},{"location":"triply-etl/enrich/","text":"On this page: Enrich See also Enrich \u00b6 The Enrich step uses linked data that is asserted in the internal store to derive new linked data. graph LR sources -- 1. Extract --> record record -- 2. Transform --> record record -- 3. Assert --> ld ld -- 4. Enrich --> ld ld -- 5. Validate --> ld ld -- 6. Publish --> destinations linkStyle 3 stroke:red,stroke-width:3px; destinations[(\"D. Destinations\\n(TriplyDB)\")] ld[C. Internal Store] record[B. Record] sources[A. Data Sources] TriplyETL supports the following languages for making enrichments: SHACL Rules allows the implementation of business rules that can add linked data to the internal store. TriplyETL supports the following forms of SHACL Rules: Triple Rules SPARQL Rules SPARQL is a query language that can also be used to add and remove data. This allows SPARQL to be used as an enrichment language. TriplyETL supports the following forms of SPARQL enrichment: SPARQL Construct allows linked data to be added to the internal store. SPARQL Update allows linked data to be added to and deleted from the internal store. See also \u00b6 If you have not loaded any linked data in your Internal Store yet, use one of the following approaches to do so: loadRdf() JSON-LD Expansion The RATT statement assertion functions.","title":"Overview"},{"location":"triply-etl/enrich/#enrich","text":"The Enrich step uses linked data that is asserted in the internal store to derive new linked data. graph LR sources -- 1. Extract --> record record -- 2. Transform --> record record -- 3. Assert --> ld ld -- 4. Enrich --> ld ld -- 5. Validate --> ld ld -- 6. Publish --> destinations linkStyle 3 stroke:red,stroke-width:3px; destinations[(\"D. Destinations\\n(TriplyDB)\")] ld[C. Internal Store] record[B. Record] sources[A. Data Sources] TriplyETL supports the following languages for making enrichments: SHACL Rules allows the implementation of business rules that can add linked data to the internal store. TriplyETL supports the following forms of SHACL Rules: Triple Rules SPARQL Rules SPARQL is a query language that can also be used to add and remove data. This allows SPARQL to be used as an enrichment language. TriplyETL supports the following forms of SPARQL enrichment: SPARQL Construct allows linked data to be added to the internal store. SPARQL Update allows linked data to be added to and deleted from the internal store.","title":"Enrich"},{"location":"triply-etl/enrich/#see-also","text":"If you have not loaded any linked data in your Internal Store yet, use one of the following approaches to do so: loadRdf() JSON-LD Expansion The RATT statement assertion functions.","title":"See also"},{"location":"triply-etl/enrich/shacl/","text":"SHACL Rules \u00b6 SHACL Rules allow new data to be added to the internal store, based on data that is already present. This makes SHACL Rules a great approach for data enrichment. Since SHACL Rules can be defined as part of the data model by using standardized SHACL properties and classes, it is one of the best approaches for creating and maintaining business rules in complex domains. The order in which rules are evaluated can be specified in terms of dynamic preconditions, or in terms of a predefined order. It is possible to execute rules iteratively, generating increasingly more data upon each iteration, steadily unlocking more rules as the process unfolds. Prerequisites \u00b6 SHACL Rules can be used when the following preconditions are met: You have a data model that has one or more SHACL Rules. You have some linked data in the internal store. If your internal store is still empty, you can read the Assert documentation on how to add linked data to the internal store. The function for executing SHACL Rules is imported as follows: import { executeRules } from '@triplyetl/etl/shacl' Typical ETL structure \u00b6 When SHACL Rules are used, the typical structure of the ETL script looks as follows: Assert some linked data into the internal store. This can be done by loading RDF directly with loadRdf() , or by using an extractor with transformations and assertions . Execute the SHACL rules with executeRules() from library @triplyetl/etl/shacl . Do something with the enriched linked data, e.g. publish it to TriplyDB with toTriplyDb() . import { Etl, Source } from '@triplyetl/etl/generic' import { executeRules } from '@triplyetl/etl/shacl' export default async function (): Promise<Etl> { const etl = new Etl() etl.use( // Step 1. Assert some linked data into the internal store. executeRules(Source.file('static/model.trig')), // Step 2 // Step 3. Do something with the linked data, e.g. publish it to TriplyDB. ) return etl } Formulating SHACL Rules \u00b6 The actual formulation of the SHACL Rules depends on the kind of SHACL Rule that is used. TriplyETL supports the following two kinds of SHACL Rules: Triple Rules SPARQL Rules SHACL Rules are typically included in the information model of the dataset. Notice that it is possible to combine different kinds of SHACL Rules in the same information model. Iterative rules \u00b6 It is possible to apply SHACL Rules multiple times. This is useful when one rule generates data that can be used by another rule, whose results can be used by another rule, etc. Since it is easy for rules to end up in an infinite loop, the user must set a maximum to the number of iterations that are allowed. This is done with the following options: maxIterations is the maximum number of iterations that are used by the SHACL Rules engine. By default, this value is set to 0, which means that no iterative behavior is used. By setting this value to 2 or higher, iterative behavior is enabled. errorOnMaxIterations is the behavior that is used when the last iteration produced new linked data assertions. This may indicate that more iterations are needed. The possible values for this option are: 'throw' which throws an exception, which terminates the ETL. 'warn' which emits a warning, after which the ETL continues. 'none' which is the default value.","title":"Overview"},{"location":"triply-etl/enrich/shacl/#shacl-rules","text":"SHACL Rules allow new data to be added to the internal store, based on data that is already present. This makes SHACL Rules a great approach for data enrichment. Since SHACL Rules can be defined as part of the data model by using standardized SHACL properties and classes, it is one of the best approaches for creating and maintaining business rules in complex domains. The order in which rules are evaluated can be specified in terms of dynamic preconditions, or in terms of a predefined order. It is possible to execute rules iteratively, generating increasingly more data upon each iteration, steadily unlocking more rules as the process unfolds.","title":"SHACL Rules"},{"location":"triply-etl/enrich/shacl/#prerequisites","text":"SHACL Rules can be used when the following preconditions are met: You have a data model that has one or more SHACL Rules. You have some linked data in the internal store. If your internal store is still empty, you can read the Assert documentation on how to add linked data to the internal store. The function for executing SHACL Rules is imported as follows: import { executeRules } from '@triplyetl/etl/shacl'","title":"Prerequisites"},{"location":"triply-etl/enrich/shacl/#typical-etl-structure","text":"When SHACL Rules are used, the typical structure of the ETL script looks as follows: Assert some linked data into the internal store. This can be done by loading RDF directly with loadRdf() , or by using an extractor with transformations and assertions . Execute the SHACL rules with executeRules() from library @triplyetl/etl/shacl . Do something with the enriched linked data, e.g. publish it to TriplyDB with toTriplyDb() . import { Etl, Source } from '@triplyetl/etl/generic' import { executeRules } from '@triplyetl/etl/shacl' export default async function (): Promise<Etl> { const etl = new Etl() etl.use( // Step 1. Assert some linked data into the internal store. executeRules(Source.file('static/model.trig')), // Step 2 // Step 3. Do something with the linked data, e.g. publish it to TriplyDB. ) return etl }","title":"Typical ETL structure"},{"location":"triply-etl/enrich/shacl/#formulating-shacl-rules","text":"The actual formulation of the SHACL Rules depends on the kind of SHACL Rule that is used. TriplyETL supports the following two kinds of SHACL Rules: Triple Rules SPARQL Rules SHACL Rules are typically included in the information model of the dataset. Notice that it is possible to combine different kinds of SHACL Rules in the same information model.","title":"Formulating SHACL Rules"},{"location":"triply-etl/enrich/shacl/#iterative-rules","text":"It is possible to apply SHACL Rules multiple times. This is useful when one rule generates data that can be used by another rule, whose results can be used by another rule, etc. Since it is easy for rules to end up in an infinite loop, the user must set a maximum to the number of iterations that are allowed. This is done with the following options: maxIterations is the maximum number of iterations that are used by the SHACL Rules engine. By default, this value is set to 0, which means that no iterative behavior is used. By setting this value to 2 or higher, iterative behavior is enabled. errorOnMaxIterations is the behavior that is used when the last iteration produced new linked data assertions. This may indicate that more iterations are needed. The possible values for this option are: 'throw' which throws an exception, which terminates the ETL. 'warn' which emits a warning, after which the ETL continues. 'none' which is the default value.","title":"Iterative rules"},{"location":"triply-etl/enrich/shacl/sparql-rules/","text":"On this page: SPARQL Rules Example 1: Deducing fatherhood Step 1A: Implement the SPARQL Construct query Step 1B: Create the node shape Step 1C: Write and run the script Step 1D: Using files (optional) SPARQL Rules \u00b6 SPARQL Rules are a form of SHACL Rules . SPARQL Rules can be arbitrarily complex, utilizing all features available in the SPARQL query language. SPARQL Rules have the following benefits and downsides. Benefits: Simple to use if you are familiar with SPARQL. Integrated with the information model. Can be used to assert any number of triples. Allows arbitrarily complex business rules to be formulated, e.g. using aggregation, filters, external data, property paths, function calls. Can use the prefix declarations that are represented in the information model ( sh:namespace and sh:prefix ). Downsides: No reflection: the rule is encoded in a literal, so its internal structure cannot be queried as RDF. No reflection: cannot use the prefix declarations that occur in the serialization format of the information model in which it occurs (e.g. TriG, Turtle). The rest of this page describes a examples that uses SPARQL Rules. Example 1: Deducing fatherhood \u00b6 This example uses the same data and rule as the corresponding Triple Rule example . Step 1A: Implement the SPARQL Construct query \u00b6 In natural language, we can define the following rule for deducing fatherhood: Persons with at least one child and the male gender, are fathers. We can implement this deduction with the following a SPARQL Construct query: base <https://triplydb.com/> prefix sdo: <https://schema.org/> construct { $this a <Father>. } where { $this a sdo:Person; sdo:children []; sdo:gender sdo:Male. } Notice the following details: The conditions are specified in the Where clause. The assertion is specified in the Construct template. We use the variable name $this to bind to the instances for which the rule will be executed. In the SPARQL query language, this name is only a convention, and has exactly the same behavior as using any other variable name such as ?person or ?x . We can run this query directly from TriplyETL, and this will result in the correct deductions. In fact, this is why SPARQL Construct is one of the enrichment configuration languages that are supported by TriplyETL. Step 1B: Create the node shape \u00b6 In Step 1A, we used a SPARQL Construct query to deduce new data.In this step, we will wrap that query inside a SPARQL Rule. This allows us to relate the rule to our information model. In the information model, rules are related to node shapes. When instance data conforms to the node shape, the SPARQL Rule is executed. Notice that this is different from calling SPARQL Construct queries directly, where we must determine when to run which query. SPARQL Rules are triggered by the information model instead. This has many benefits, especially for large collections of business rules, where the execution order may no longer be straightforward. In order for our SPARQL Construct query to be triggered by a node shape, we need to identify some target criterion that will allow the node shape to trigger the query. One target criterion for node shapes is sh:targetClass . We can trigger the SPARQL Construct query for every instance of the class sdo:Person . This means that we move the check of whether a resource is a person from the SPARQL Construct query into the node shape. This results in the following linked data snippet: base <https://triplydb.com/> prefix sdo: <https://schema.org/> prefix sh: <http://www.w3.org/ns/shacl#> <Person> a sh:NodeShape; sh:targetClass sdo:Person; sh:rule [ a sh:SPARQLRule; sh:construct ''' base <https://triplydb.com/> prefix sdo: <https://schema.org/> construct { $this a <Father>. } where { $this sdo:children []; sdo:gender sdo:Male. }''' ]. Notice the following details: We introduce a node shape that targets all instances of sdo:Person . The node shape is connected to a SPARQL Rule via the sh:rule property. The SPARQL Rule has its own RDF resource, and is connected to the query string via the sh:construct property. The SPARQL Construct query from Step 1 no longer include the a sdo:Person line. This line is no longer needed, since the node shape will only trigger for instances of sdo:Person in the first place. The SPARQL Construct query uses variable name $this to bind to the instances for which the rule will be executed. While this name is only a convention in the SPARQL query language, it has a special meaning in the SPARQL Rule. This variable will be bound for all targets of the node shape (i.e. for every person in the data). The literal that contains the SPARQL Construct query uses triple quoted literals notation ( '''...''' ). This notation allows us to use unescaped newlines inside the literal, which allows us to inline the query string in a readable way. Step 1C: Write and run the script \u00b6 The following script is completely self-contained. By copy/pasting it into TriplyETL, you can execute the rule over the instance data, and deduce the fact that John is a father. Notice that the script includes the following components: Load the instance data from Step 1A with loadRdf() . Execute the rule from Step 1B with executeRules() . Print the contents of the internal store with logQuads() . import { logQuads } from '@triplyetl/etl/debug' import { Etl, Source, loadRdf } from '@triplyetl/etl/generic' import { executeRules } from '@triplyetl/etl/shacl' export default async function (): Promise<Etl> { const etl = new Etl() etl.use( loadRdf(Source.string(` base <https://triplydb.com/> prefix sdo: <https://schema.org/> <john> a sdo:Person; sdo:children <mary>; sdo:gender sdo:Male.`)), executeRules(Source.string(` base <https://triplydb.com/> prefix sdo: <https://schema.org/> prefix sh: <http://www.w3.org/ns/shacl#> <Person> a sh:NodeShape; sh:targetClass sdo:Person; sh:rule [ a sh:SPARQLRule; sh:construct ''' base <https://triplydb.com/> prefix sdo: <https://schema.org/> construct { $this a <Father>. } where { $this sdo:children []; sdo:gender sdo:Male. }''' ].`)), logQuads(), ) return etl } When we run this script (command npx etl ), the following linked data is printed: <john> a sdo:Person, <Father>; sdo:children <mary>; sdo:gender sdo:Male. Notice that the fatherhood assertion was correctly added to the internal store, based on the Triple Rule in the data model. Step 1D: Using files (optional) \u00b6 The script in Step 1C includes both the instance data and the information model as inline strings, using Source.string() . This is great for creating a self-contained example, but not realistic when the number of rules increases. We therefore show the same script after these inline components have been stored in separate files: The instance data is stored in file static/instances.trig . The information model is stored in file static/model.trig . Now the instance data and information model can be edited in their own files, and the script stays concise: import { logQuads } from '@triplyetl/etl/debug' import { Etl, Source, loadRdf } from '@triplyetl/etl/generic' import { executeRules } from '@triplyetl/etl/shacl' export default async function (): Promise<Etl> { const etl = new Etl() etl.use( loadRdf(Source.file('static/instances.trig')), executeRules(Source.file('static/model.trig')), logQuads(), ) return etl }","title":"SPARQL Rules"},{"location":"triply-etl/enrich/shacl/sparql-rules/#sparql-rules","text":"SPARQL Rules are a form of SHACL Rules . SPARQL Rules can be arbitrarily complex, utilizing all features available in the SPARQL query language. SPARQL Rules have the following benefits and downsides. Benefits: Simple to use if you are familiar with SPARQL. Integrated with the information model. Can be used to assert any number of triples. Allows arbitrarily complex business rules to be formulated, e.g. using aggregation, filters, external data, property paths, function calls. Can use the prefix declarations that are represented in the information model ( sh:namespace and sh:prefix ). Downsides: No reflection: the rule is encoded in a literal, so its internal structure cannot be queried as RDF. No reflection: cannot use the prefix declarations that occur in the serialization format of the information model in which it occurs (e.g. TriG, Turtle). The rest of this page describes a examples that uses SPARQL Rules.","title":"SPARQL Rules"},{"location":"triply-etl/enrich/shacl/sparql-rules/#example-1-deducing-fatherhood","text":"This example uses the same data and rule as the corresponding Triple Rule example .","title":"Example 1: Deducing fatherhood"},{"location":"triply-etl/enrich/shacl/sparql-rules/#step-1a-implement-the-sparql-construct-query","text":"In natural language, we can define the following rule for deducing fatherhood: Persons with at least one child and the male gender, are fathers. We can implement this deduction with the following a SPARQL Construct query: base <https://triplydb.com/> prefix sdo: <https://schema.org/> construct { $this a <Father>. } where { $this a sdo:Person; sdo:children []; sdo:gender sdo:Male. } Notice the following details: The conditions are specified in the Where clause. The assertion is specified in the Construct template. We use the variable name $this to bind to the instances for which the rule will be executed. In the SPARQL query language, this name is only a convention, and has exactly the same behavior as using any other variable name such as ?person or ?x . We can run this query directly from TriplyETL, and this will result in the correct deductions. In fact, this is why SPARQL Construct is one of the enrichment configuration languages that are supported by TriplyETL.","title":"Step 1A: Implement the SPARQL Construct query"},{"location":"triply-etl/enrich/shacl/sparql-rules/#step-1b-create-the-node-shape","text":"In Step 1A, we used a SPARQL Construct query to deduce new data.In this step, we will wrap that query inside a SPARQL Rule. This allows us to relate the rule to our information model. In the information model, rules are related to node shapes. When instance data conforms to the node shape, the SPARQL Rule is executed. Notice that this is different from calling SPARQL Construct queries directly, where we must determine when to run which query. SPARQL Rules are triggered by the information model instead. This has many benefits, especially for large collections of business rules, where the execution order may no longer be straightforward. In order for our SPARQL Construct query to be triggered by a node shape, we need to identify some target criterion that will allow the node shape to trigger the query. One target criterion for node shapes is sh:targetClass . We can trigger the SPARQL Construct query for every instance of the class sdo:Person . This means that we move the check of whether a resource is a person from the SPARQL Construct query into the node shape. This results in the following linked data snippet: base <https://triplydb.com/> prefix sdo: <https://schema.org/> prefix sh: <http://www.w3.org/ns/shacl#> <Person> a sh:NodeShape; sh:targetClass sdo:Person; sh:rule [ a sh:SPARQLRule; sh:construct ''' base <https://triplydb.com/> prefix sdo: <https://schema.org/> construct { $this a <Father>. } where { $this sdo:children []; sdo:gender sdo:Male. }''' ]. Notice the following details: We introduce a node shape that targets all instances of sdo:Person . The node shape is connected to a SPARQL Rule via the sh:rule property. The SPARQL Rule has its own RDF resource, and is connected to the query string via the sh:construct property. The SPARQL Construct query from Step 1 no longer include the a sdo:Person line. This line is no longer needed, since the node shape will only trigger for instances of sdo:Person in the first place. The SPARQL Construct query uses variable name $this to bind to the instances for which the rule will be executed. While this name is only a convention in the SPARQL query language, it has a special meaning in the SPARQL Rule. This variable will be bound for all targets of the node shape (i.e. for every person in the data). The literal that contains the SPARQL Construct query uses triple quoted literals notation ( '''...''' ). This notation allows us to use unescaped newlines inside the literal, which allows us to inline the query string in a readable way.","title":"Step 1B: Create the node shape"},{"location":"triply-etl/enrich/shacl/sparql-rules/#step-1c-write-and-run-the-script","text":"The following script is completely self-contained. By copy/pasting it into TriplyETL, you can execute the rule over the instance data, and deduce the fact that John is a father. Notice that the script includes the following components: Load the instance data from Step 1A with loadRdf() . Execute the rule from Step 1B with executeRules() . Print the contents of the internal store with logQuads() . import { logQuads } from '@triplyetl/etl/debug' import { Etl, Source, loadRdf } from '@triplyetl/etl/generic' import { executeRules } from '@triplyetl/etl/shacl' export default async function (): Promise<Etl> { const etl = new Etl() etl.use( loadRdf(Source.string(` base <https://triplydb.com/> prefix sdo: <https://schema.org/> <john> a sdo:Person; sdo:children <mary>; sdo:gender sdo:Male.`)), executeRules(Source.string(` base <https://triplydb.com/> prefix sdo: <https://schema.org/> prefix sh: <http://www.w3.org/ns/shacl#> <Person> a sh:NodeShape; sh:targetClass sdo:Person; sh:rule [ a sh:SPARQLRule; sh:construct ''' base <https://triplydb.com/> prefix sdo: <https://schema.org/> construct { $this a <Father>. } where { $this sdo:children []; sdo:gender sdo:Male. }''' ].`)), logQuads(), ) return etl } When we run this script (command npx etl ), the following linked data is printed: <john> a sdo:Person, <Father>; sdo:children <mary>; sdo:gender sdo:Male. Notice that the fatherhood assertion was correctly added to the internal store, based on the Triple Rule in the data model.","title":"Step 1C: Write and run the script"},{"location":"triply-etl/enrich/shacl/sparql-rules/#step-1d-using-files-optional","text":"The script in Step 1C includes both the instance data and the information model as inline strings, using Source.string() . This is great for creating a self-contained example, but not realistic when the number of rules increases. We therefore show the same script after these inline components have been stored in separate files: The instance data is stored in file static/instances.trig . The information model is stored in file static/model.trig . Now the instance data and information model can be edited in their own files, and the script stays concise: import { logQuads } from '@triplyetl/etl/debug' import { Etl, Source, loadRdf } from '@triplyetl/etl/generic' import { executeRules } from '@triplyetl/etl/shacl' export default async function (): Promise<Etl> { const etl = new Etl() etl.use( loadRdf(Source.file('static/instances.trig')), executeRules(Source.file('static/model.trig')), logQuads(), ) return etl }","title":"Step 1D: Using files (optional)"},{"location":"triply-etl/enrich/shacl/triple-rules/","text":"On this page: Triple Rules Example: Deducing fatherhood Step A: Load instance data Step B: Formulate the SHACL rule Step C: Write and run the script Step D: Using files (optional) See also Triple Rules \u00b6 Triple Rules are a form of SHACL Rules . Triple Rules can only assert one single triple, but they are relatively easy to learn and apply. Triple Rules have the following benefits and downsides. Benefits: Simple to use if you are familiar with RDF. Does not require knowledge of the SPARQL language. Integrated with the information model. Reflection: the rule can itself be queried as RDF. Reflection: the rule can use the prefix declarations that occur in the serialization format of the information model in which it appears (e.g. TriG, Turtle). Downsides: Can only assert one single triple. Cannot use the prefix declarations that are represented in the information model (using sh:namespace and sh:prefix ). The rest of this page describes a complete example that uses Triple Rules. Example: Deducing fatherhood \u00b6 This section describes a full example of a Triple Rule that can deduce that somebody is a father, based on other facts that are known about the person. Step A: Load instance data \u00b6 We first need to load some instance data, so that we can apply a rule and enrich the loaded data with some new data. We start with linked data assertions that state that John is a male who has a child (Mary): base <https://triplydb.com/> prefix sdo: <https://schema.org/> <john> a sdo:Person; sdo:children <mary>; sdo:gender sdo:Male. Applying our knowledge of the world, we as humans can deduce that John is also a father. This deduction can also be expressed in linked data: base <https://triplydb.com/> <john> a <Father>. When we make this deduction, we are applying a (possibly implicit) rule. When we try to make the rule that we have applied explicit, we discover that a rule has the following two components: The condition is the criterion that must be met in order for the rule to become applicable. In our example, we must have instance data about a person. That person must have at least one child, and that person must be male. Notice that the condition can be made arbitrarily complex: we can add more criteria like age, nationality, etc. if we wanted to. The assertion is the new data that we can add to our internal store. In our example, this is the assertion that John is a father. We can show this principle in a diagram, where condition and assertion contain the two components of the rule: graph subgraph Condition id:john -- a --> sdo:Person id:john -- sdo:children --> id:mary id:john -- sdo:gender --> sdo:Male end subgraph Assertion id:john -- a --> def:Father end Step B: Formulate the SHACL rule \u00b6 In Step A, we applied a rule to the instance John. But our dataset may contain information about other people too: people with or without children, people with different genders, etc. Suppose our dataset contains information about Peter, who has two children and has the male gender. We can apply the same rule to deduce that Peter is also a father. When we apply the same rule to an arbitrary number of instances, we are applying a principle called 'generalization'. We replace information about instances like 'John' and 'Peter' with a generic class such as 'Person'. When we think about it, the generalized rule that we have applied to John and Peter, and that we can apply to any number of individuals, runs as follows: Persons with at least one child and the male gender, are fathers. We can formalize this generalized rule in the following SHACL snippet: base <https://triplydb.com/> prefix rdf: <http://www.w3.org/1999/02/22-rdf-syntax-ns#> prefix sdo: <https://schema.org/> prefix sh: <http://www.w3.org/ns/shacl#> <Person> sh:targetClass sdo:Person; sh:rule [ a sh:TripleRule; sh:condition [ sh:property [ sh:path sdo:children; sh:minCount 1 ] ], [ sh:property [ sh:path sdo:gender; sh:hasValue sdo:Male ] ]; sh:subject sh:this; sh:predicate rdf:type; sh:object <Father> ]. Notice the following details: The rule only applies to persons, i.e. instances of the class sdo:Person . This is expressed by the sh:targetClass property. The first condition of the rule is that the person must have at least one child. This is expressed by sh:condition and sh:minCount . The second condition of the rule is that the gender of the person is male. This is expressed by sh:condition and sh:hasValue . The assertion is that the person is a father. Since we use a Triple Rule, this is expressed by the properties sh:subject , sh:predicate , and sh:object . Notice that the term sh:this is used to refer to individuals for whom all conditions are met (in our example: John). Step C: Write and run the script \u00b6 The following script is completely self-contained. By copy/pasting it into TriplyETL, you can execute the rule over the instance data, and deduce the fact that John is a father. Notice that the script includes the following components: Load the instance data from Step A with loadRdf() . Execute the rule from Step B with executeRules() . Print the contents of the internal store with logQuads() . import { logQuads } from '@triplyetl/etl/debug' import { Etl, Source, loadRdf } from '@triplyetl/etl/generic' import { executeRules } from '@triplyetl/etl/shacl' export default async function (): Promise<Etl> { const etl = new Etl() etl.use( loadRdf(Source.string(` base <https://triplydb.com/> prefix sdo: <https://schema.org/> <john> a sdo:Person; sdo:children <mary>; sdo:gender sdo:Male.`)), executeRules(Source.string(` base <https://triplydb.com/> prefix rdf: <http://www.w3.org/1999/02/22-rdf-syntax-ns#> prefix sdo: <https://schema.org/> prefix sh: <http://www.w3.org/ns/shacl#> <Person> sh:targetClass sdo:Person; sh:rule [ a sh:TripleRule; sh:condition [ sh:property [ sh:path sdo:children; sh:minCount 1 ] ], [ sh:property [ sh:path sdo:gender; sh:hasValue sdo:Male ] ]; sh:subject sh:this; sh:predicate rdf:type; sh:object <Father> ].`)), logQuads(), ) return etl } When we run this script (command npx etl ), the following linked data is printed: <john> a sdo:Person, <Father>; sdo:children <mary>; sdo:gender sdo:Male. Notice that the fatherhood assertion was correctly added to the internal store, based on the Triple Rule in the data model. Step D: Using files (optional) \u00b6 The script in Step C includes both the instance data and the information model as inline strings, using Source.string() . This is great for creating a self-contained example, but not realistic when the number of rules increases. We therefore show the same script after these inline components have been stored in separate files: The instance data is stored in file static/instances.trig . The information model is stored in file static/model.trig . Now the instance data and information model can be edited in their own files, and the script stays concise: import { logQuads } from '@triplyetl/etl/debug' import { Etl, Source, loadRdf } from '@triplyetl/etl/generic' import { executeRules } from '@triplyetl/etl/shacl' export default async function (): Promise<Etl> { const etl = new Etl() etl.use( loadRdf(Source.file('static/instances.trig')), executeRules(Source.file('static/model.trig')), logQuads(), ) return etl } See also \u00b6 Use SPARQL Rules for rules that are more complex and that cannot be expressed by Triple Rules. Triple Rules are a form of SHACL Rules . SHACL Rules are documented in the SHACL Advanced Features Working Group Note . SHACL Rules are a form of data enrichment. Go to the enrichment overview page for information about other enrichment approaches.","title":"Triple Rules"},{"location":"triply-etl/enrich/shacl/triple-rules/#triple-rules","text":"Triple Rules are a form of SHACL Rules . Triple Rules can only assert one single triple, but they are relatively easy to learn and apply. Triple Rules have the following benefits and downsides. Benefits: Simple to use if you are familiar with RDF. Does not require knowledge of the SPARQL language. Integrated with the information model. Reflection: the rule can itself be queried as RDF. Reflection: the rule can use the prefix declarations that occur in the serialization format of the information model in which it appears (e.g. TriG, Turtle). Downsides: Can only assert one single triple. Cannot use the prefix declarations that are represented in the information model (using sh:namespace and sh:prefix ). The rest of this page describes a complete example that uses Triple Rules.","title":"Triple Rules"},{"location":"triply-etl/enrich/shacl/triple-rules/#example-deducing-fatherhood","text":"This section describes a full example of a Triple Rule that can deduce that somebody is a father, based on other facts that are known about the person.","title":"Example: Deducing fatherhood"},{"location":"triply-etl/enrich/shacl/triple-rules/#step-a-load-instance-data","text":"We first need to load some instance data, so that we can apply a rule and enrich the loaded data with some new data. We start with linked data assertions that state that John is a male who has a child (Mary): base <https://triplydb.com/> prefix sdo: <https://schema.org/> <john> a sdo:Person; sdo:children <mary>; sdo:gender sdo:Male. Applying our knowledge of the world, we as humans can deduce that John is also a father. This deduction can also be expressed in linked data: base <https://triplydb.com/> <john> a <Father>. When we make this deduction, we are applying a (possibly implicit) rule. When we try to make the rule that we have applied explicit, we discover that a rule has the following two components: The condition is the criterion that must be met in order for the rule to become applicable. In our example, we must have instance data about a person. That person must have at least one child, and that person must be male. Notice that the condition can be made arbitrarily complex: we can add more criteria like age, nationality, etc. if we wanted to. The assertion is the new data that we can add to our internal store. In our example, this is the assertion that John is a father. We can show this principle in a diagram, where condition and assertion contain the two components of the rule: graph subgraph Condition id:john -- a --> sdo:Person id:john -- sdo:children --> id:mary id:john -- sdo:gender --> sdo:Male end subgraph Assertion id:john -- a --> def:Father end","title":"Step A: Load instance data"},{"location":"triply-etl/enrich/shacl/triple-rules/#step-b-formulate-the-shacl-rule","text":"In Step A, we applied a rule to the instance John. But our dataset may contain information about other people too: people with or without children, people with different genders, etc. Suppose our dataset contains information about Peter, who has two children and has the male gender. We can apply the same rule to deduce that Peter is also a father. When we apply the same rule to an arbitrary number of instances, we are applying a principle called 'generalization'. We replace information about instances like 'John' and 'Peter' with a generic class such as 'Person'. When we think about it, the generalized rule that we have applied to John and Peter, and that we can apply to any number of individuals, runs as follows: Persons with at least one child and the male gender, are fathers. We can formalize this generalized rule in the following SHACL snippet: base <https://triplydb.com/> prefix rdf: <http://www.w3.org/1999/02/22-rdf-syntax-ns#> prefix sdo: <https://schema.org/> prefix sh: <http://www.w3.org/ns/shacl#> <Person> sh:targetClass sdo:Person; sh:rule [ a sh:TripleRule; sh:condition [ sh:property [ sh:path sdo:children; sh:minCount 1 ] ], [ sh:property [ sh:path sdo:gender; sh:hasValue sdo:Male ] ]; sh:subject sh:this; sh:predicate rdf:type; sh:object <Father> ]. Notice the following details: The rule only applies to persons, i.e. instances of the class sdo:Person . This is expressed by the sh:targetClass property. The first condition of the rule is that the person must have at least one child. This is expressed by sh:condition and sh:minCount . The second condition of the rule is that the gender of the person is male. This is expressed by sh:condition and sh:hasValue . The assertion is that the person is a father. Since we use a Triple Rule, this is expressed by the properties sh:subject , sh:predicate , and sh:object . Notice that the term sh:this is used to refer to individuals for whom all conditions are met (in our example: John).","title":"Step B: Formulate the SHACL rule"},{"location":"triply-etl/enrich/shacl/triple-rules/#step-c-write-and-run-the-script","text":"The following script is completely self-contained. By copy/pasting it into TriplyETL, you can execute the rule over the instance data, and deduce the fact that John is a father. Notice that the script includes the following components: Load the instance data from Step A with loadRdf() . Execute the rule from Step B with executeRules() . Print the contents of the internal store with logQuads() . import { logQuads } from '@triplyetl/etl/debug' import { Etl, Source, loadRdf } from '@triplyetl/etl/generic' import { executeRules } from '@triplyetl/etl/shacl' export default async function (): Promise<Etl> { const etl = new Etl() etl.use( loadRdf(Source.string(` base <https://triplydb.com/> prefix sdo: <https://schema.org/> <john> a sdo:Person; sdo:children <mary>; sdo:gender sdo:Male.`)), executeRules(Source.string(` base <https://triplydb.com/> prefix rdf: <http://www.w3.org/1999/02/22-rdf-syntax-ns#> prefix sdo: <https://schema.org/> prefix sh: <http://www.w3.org/ns/shacl#> <Person> sh:targetClass sdo:Person; sh:rule [ a sh:TripleRule; sh:condition [ sh:property [ sh:path sdo:children; sh:minCount 1 ] ], [ sh:property [ sh:path sdo:gender; sh:hasValue sdo:Male ] ]; sh:subject sh:this; sh:predicate rdf:type; sh:object <Father> ].`)), logQuads(), ) return etl } When we run this script (command npx etl ), the following linked data is printed: <john> a sdo:Person, <Father>; sdo:children <mary>; sdo:gender sdo:Male. Notice that the fatherhood assertion was correctly added to the internal store, based on the Triple Rule in the data model.","title":"Step C: Write and run the script"},{"location":"triply-etl/enrich/shacl/triple-rules/#step-d-using-files-optional","text":"The script in Step C includes both the instance data and the information model as inline strings, using Source.string() . This is great for creating a self-contained example, but not realistic when the number of rules increases. We therefore show the same script after these inline components have been stored in separate files: The instance data is stored in file static/instances.trig . The information model is stored in file static/model.trig . Now the instance data and information model can be edited in their own files, and the script stays concise: import { logQuads } from '@triplyetl/etl/debug' import { Etl, Source, loadRdf } from '@triplyetl/etl/generic' import { executeRules } from '@triplyetl/etl/shacl' export default async function (): Promise<Etl> { const etl = new Etl() etl.use( loadRdf(Source.file('static/instances.trig')), executeRules(Source.file('static/model.trig')), logQuads(), ) return etl }","title":"Step D: Using files (optional)"},{"location":"triply-etl/enrich/shacl/triple-rules/#see-also","text":"Use SPARQL Rules for rules that are more complex and that cannot be expressed by Triple Rules. Triple Rules are a form of SHACL Rules . SHACL Rules are documented in the SHACL Advanced Features Working Group Note . SHACL Rules are a form of data enrichment. Go to the enrichment overview page for information about other enrichment approaches.","title":"See also"},{"location":"triply-etl/enrich/sparql/construct/","text":"On this page: SPARQL Construct Signature Parameters Example Usage Relation to standards SPARQL Construct \u00b6 SPARQL Construct queries can be used to enrich the data that is in the Internal Store. The following full TriplyETL script loads one triple into the Internal Store, and then uses a SPARQL Construct query to add a second triple: Signature \u00b6 This function has the following signature: construct(query, opts?) Parameters \u00b6 query : is a query string, this can be a SPARQL query string, reference to a query file, or an operation on the Context ( (ctx: Context) => string|string[] ). The aforementioned can query arguments can also be provided in an array of arguments for the query parameter. opts : an optional object containing options for SPARQL Construct toGraph : an optional argument to store the construct query results provided graph, defaults to the ETL's default graph. Example Usage \u00b6 import { logQuads } from '@triplyetl/etl/debug' import { Etl, loadRdf, Source } from '@triplyetl/etl/generic' import { construct } from '@triplyetl/etl/sparql' export default async function (): Promise<Etl> { const etl = new Etl() etl.use( loadRdf(Source.string('<s><p><o>.')), construct('construct { ?o ?p ?s. } where { ?s ?p ?o. }'), logQuads(), ) return etl } This results in the following linked data: <s> <p> <o>. <o> <p> <s>. Relation to standards \u00b6 This function is an implementation of the SPARQL Construct, for more information on the standard see SPARQL Construct .","title":"SPARQL Construct"},{"location":"triply-etl/enrich/sparql/construct/#sparql-construct","text":"SPARQL Construct queries can be used to enrich the data that is in the Internal Store. The following full TriplyETL script loads one triple into the Internal Store, and then uses a SPARQL Construct query to add a second triple:","title":"SPARQL Construct"},{"location":"triply-etl/enrich/sparql/construct/#signature","text":"This function has the following signature: construct(query, opts?)","title":"Signature"},{"location":"triply-etl/enrich/sparql/construct/#parameters","text":"query : is a query string, this can be a SPARQL query string, reference to a query file, or an operation on the Context ( (ctx: Context) => string|string[] ). The aforementioned can query arguments can also be provided in an array of arguments for the query parameter. opts : an optional object containing options for SPARQL Construct toGraph : an optional argument to store the construct query results provided graph, defaults to the ETL's default graph.","title":"Parameters"},{"location":"triply-etl/enrich/sparql/construct/#example-usage","text":"import { logQuads } from '@triplyetl/etl/debug' import { Etl, loadRdf, Source } from '@triplyetl/etl/generic' import { construct } from '@triplyetl/etl/sparql' export default async function (): Promise<Etl> { const etl = new Etl() etl.use( loadRdf(Source.string('<s><p><o>.')), construct('construct { ?o ?p ?s. } where { ?s ?p ?o. }'), logQuads(), ) return etl } This results in the following linked data: <s> <p> <o>. <o> <p> <s>.","title":"Example Usage"},{"location":"triply-etl/enrich/sparql/construct/#relation-to-standards","text":"This function is an implementation of the SPARQL Construct, for more information on the standard see SPARQL Construct .","title":"Relation to standards"},{"location":"triply-etl/enrich/sparql/update/","text":"On this page: SPARQL Update Insert Data Using prefix declarations Delete Data Delete Insert Where SPARQL Update \u00b6 SPARQL is a powerful query language that can be used to modify and enrich linked data in the Internal Store. With SPARQL, you can generate new linked data based on existing linked data, thereby enhancing the contents of the store. The function for using SPARQL Update can be imported as follows: import { update } from '@triplyetl/etl/sparql' Insert Data \u00b6 Insert Data can be used to add linked data to the Internal Store. The following example adds one triple: import { logQuads } from '@triplyetl/etl/debug' import { Etl } from '@triplyetl/etl/generic' import { update } from '@triplyetl/etl/sparql' export default async function (): Promise<Etl> { const etl = new Etl() etl.use( update(` base <https://triplydb.com/> insert data { <john> <knows> <mary>. }`), logQuads(), ) return etl } Debug function logQuads() prints the content of the internal store to standard output: base <https://triplydb.com/> <john> <knows> <mary>. Using prefix declarations \u00b6 Notice that the SPARQL Update function takes a plain string. Any typos you make in this string will only result in errors at runtime, when the query string is interpreted and executed. One of the more difficult things to get right in a SPARQL string are the prefix declarations. We can use the prefix object to insert the correct IRI prefixes. The following example asserts three triples, and uses the prefix object to insert the IRI prefix for Schema.org: import { logQuads } from '@triplyetl/etl/debug' import { Etl } from '@triplyetl/etl/generic' import { update } from '@triplyetl/etl/sparql' import { sdo } from '@triplyetl/vocabularies' export default async function (): Promise<Etl> { const etl = new Etl() etl.use( update(` base <https://triplydb.com/> prefix sdo: <${sdo.$namespace}> insert data { <john> a sdo:Person; sdo:children <mary>; sdo:gender sdo:Male. }`), logQuads(), ) return etl } This prints the following linked data to standard output: base <https://triplydb.com/> prefix sdo: <https://schema.org/> <john> a sdo:Person; sdo:children <mary>; sdo:gender sdo:Male. Delete Data \u00b6 While there are not many uses cases for removing data from the internal store, this is an operation that is supported by the SPARQL Update standard. The following function call removes the parent/child relationship assertion that was added to the internal store earlier: update(` prefix sdo: <${sdo.$namespace}> delete data { <john> sdo:children <mary>. }`), You can use the debug function logQuads() before and after this function call, to see the effects on the internal store. Delete Insert Where \u00b6 SPARQL Update can be used to conditionally add and/or remove linked data to/from the internal store. It uses the following keywords for this: where is the condition that must be met inside the internal store. Conditions can be specified in a generic way by using SPARQL variables. The bindings for these variables are shared with the other two components. delete is the pattern that is removed from the internal store. This requires that the where condition is satisfied in the internal store. Any bindings for variables that are shared between the where condition and the delete pattern are instantiated before deletion is performed. Deletion is performed before insertion. insert is the pattern that is added to the internal store. This requires that the where condition is satisfied in the internal store. Any bindings for variables that are shared between the where condition and the insert pattern are instantiated before insertion is performed. Insertion is performed after deletion. We can use this powerful combination of a where condition and a delete and insert follow-up to implement rules. For example, we may want to formalize the following rule: Persons with at least one child and the male gender, are fathers. At the same time, we may be restricted in the information we are allowed to publish in our linked dataset: After fatherhood has been determined, any specific information about parent/child relationships must be removed from the internal store. The rule can be formalized as follows: import { logQuads } from '@triplyetl/etl/debug' import { Etl } from '@triplyetl/etl/generic' import { update } from '@triplyetl/etl/sparql' import { sdo } from '@triplyetl/vocabularies' const baseIri = 'https://triplydb.com/' export default async function (): Promise<Etl> { const etl = new Etl() etl.use( update(` base <${baseIri}> prefix sdo: <${sdo.$namespace}> insert data { <john> a sdo:Person; sdo:children <mary>; sdo:gender sdo:Male. }`), update(` base <${baseIri}> prefix sdo: <${sdo.$namespace}> delete { $person sdo:children ?child. } insert { $person a <Father>. } where { $person a sdo:Person; sdo:children ?child; sdo:gender sdo:Male. }`), logQuads(), ) return etl }","title":"SPARQL Update"},{"location":"triply-etl/enrich/sparql/update/#sparql-update","text":"SPARQL is a powerful query language that can be used to modify and enrich linked data in the Internal Store. With SPARQL, you can generate new linked data based on existing linked data, thereby enhancing the contents of the store. The function for using SPARQL Update can be imported as follows: import { update } from '@triplyetl/etl/sparql'","title":"SPARQL Update"},{"location":"triply-etl/enrich/sparql/update/#insert-data","text":"Insert Data can be used to add linked data to the Internal Store. The following example adds one triple: import { logQuads } from '@triplyetl/etl/debug' import { Etl } from '@triplyetl/etl/generic' import { update } from '@triplyetl/etl/sparql' export default async function (): Promise<Etl> { const etl = new Etl() etl.use( update(` base <https://triplydb.com/> insert data { <john> <knows> <mary>. }`), logQuads(), ) return etl } Debug function logQuads() prints the content of the internal store to standard output: base <https://triplydb.com/> <john> <knows> <mary>.","title":"Insert Data"},{"location":"triply-etl/enrich/sparql/update/#using-prefix-declarations","text":"Notice that the SPARQL Update function takes a plain string. Any typos you make in this string will only result in errors at runtime, when the query string is interpreted and executed. One of the more difficult things to get right in a SPARQL string are the prefix declarations. We can use the prefix object to insert the correct IRI prefixes. The following example asserts three triples, and uses the prefix object to insert the IRI prefix for Schema.org: import { logQuads } from '@triplyetl/etl/debug' import { Etl } from '@triplyetl/etl/generic' import { update } from '@triplyetl/etl/sparql' import { sdo } from '@triplyetl/vocabularies' export default async function (): Promise<Etl> { const etl = new Etl() etl.use( update(` base <https://triplydb.com/> prefix sdo: <${sdo.$namespace}> insert data { <john> a sdo:Person; sdo:children <mary>; sdo:gender sdo:Male. }`), logQuads(), ) return etl } This prints the following linked data to standard output: base <https://triplydb.com/> prefix sdo: <https://schema.org/> <john> a sdo:Person; sdo:children <mary>; sdo:gender sdo:Male.","title":"Using prefix declarations"},{"location":"triply-etl/enrich/sparql/update/#delete-data","text":"While there are not many uses cases for removing data from the internal store, this is an operation that is supported by the SPARQL Update standard. The following function call removes the parent/child relationship assertion that was added to the internal store earlier: update(` prefix sdo: <${sdo.$namespace}> delete data { <john> sdo:children <mary>. }`), You can use the debug function logQuads() before and after this function call, to see the effects on the internal store.","title":"Delete Data"},{"location":"triply-etl/enrich/sparql/update/#delete-insert-where","text":"SPARQL Update can be used to conditionally add and/or remove linked data to/from the internal store. It uses the following keywords for this: where is the condition that must be met inside the internal store. Conditions can be specified in a generic way by using SPARQL variables. The bindings for these variables are shared with the other two components. delete is the pattern that is removed from the internal store. This requires that the where condition is satisfied in the internal store. Any bindings for variables that are shared between the where condition and the delete pattern are instantiated before deletion is performed. Deletion is performed before insertion. insert is the pattern that is added to the internal store. This requires that the where condition is satisfied in the internal store. Any bindings for variables that are shared between the where condition and the insert pattern are instantiated before insertion is performed. Insertion is performed after deletion. We can use this powerful combination of a where condition and a delete and insert follow-up to implement rules. For example, we may want to formalize the following rule: Persons with at least one child and the male gender, are fathers. At the same time, we may be restricted in the information we are allowed to publish in our linked dataset: After fatherhood has been determined, any specific information about parent/child relationships must be removed from the internal store. The rule can be formalized as follows: import { logQuads } from '@triplyetl/etl/debug' import { Etl } from '@triplyetl/etl/generic' import { update } from '@triplyetl/etl/sparql' import { sdo } from '@triplyetl/vocabularies' const baseIri = 'https://triplydb.com/' export default async function (): Promise<Etl> { const etl = new Etl() etl.use( update(` base <${baseIri}> prefix sdo: <${sdo.$namespace}> insert data { <john> a sdo:Person; sdo:children <mary>; sdo:gender sdo:Male. }`), update(` base <${baseIri}> prefix sdo: <${sdo.$namespace}> delete { $person sdo:children ?child. } insert { $person a <Father>. } where { $person a sdo:Person; sdo:children ?child; sdo:gender sdo:Male. }`), logQuads(), ) return etl }","title":"Delete Insert Where"},{"location":"triply-etl/extract/","text":"On this page: Extract Next steps Extract \u00b6 The TriplyETL Extract step is the first step in any TriplyETL pipeline. It is indicated by the red arrow in the following diagram: graph LR sources -- 1. Extract --> record record -- 2. Transform --> record record -- 3. Assert --> ld ld -- 4. Enrich --> ld ld -- 5. Validate --> ld ld -- 6. Publish --> destinations linkStyle 0 stroke:red,stroke-width:3px; destinations[(\"D. Destinations\\n(TriplyDB)\")] ld[C. Internal Store] record[B. Record] sources[A. Data Sources] In the Extract step, one or more extractors are used to create a stream of records from a data source . The basic structure of every record in is the same: it does not matter which extractor or which source is used. The following extractors are currently supported: CSV or Comma-Separated Values JSON or JavaScript Object Notation OAI-PMH or Open Archives Initiative Protocol for Metadata Harvesting Postgres for PostgreSQL Query & Postgres API Options RDF for Resource Description Format Shapefile for ESRI Shapefiles TSV for Tab-Separated Values XLSX for Microsoft Excel XML for XML Markup Language Next steps \u00b6 The Extract step results in a stream of records that can be processed in the following steps: Step 2. Transform : cleans, combines, and extends data in the[record. Step 3. Assert : uses data from the record to make linked data assertions in the internal store.","title":"Overview"},{"location":"triply-etl/extract/#extract","text":"The TriplyETL Extract step is the first step in any TriplyETL pipeline. It is indicated by the red arrow in the following diagram: graph LR sources -- 1. Extract --> record record -- 2. Transform --> record record -- 3. Assert --> ld ld -- 4. Enrich --> ld ld -- 5. Validate --> ld ld -- 6. Publish --> destinations linkStyle 0 stroke:red,stroke-width:3px; destinations[(\"D. Destinations\\n(TriplyDB)\")] ld[C. Internal Store] record[B. Record] sources[A. Data Sources] In the Extract step, one or more extractors are used to create a stream of records from a data source . The basic structure of every record in is the same: it does not matter which extractor or which source is used. The following extractors are currently supported: CSV or Comma-Separated Values JSON or JavaScript Object Notation OAI-PMH or Open Archives Initiative Protocol for Metadata Harvesting Postgres for PostgreSQL Query & Postgres API Options RDF for Resource Description Format Shapefile for ESRI Shapefiles TSV for Tab-Separated Values XLSX for Microsoft Excel XML for XML Markup Language","title":"Extract"},{"location":"triply-etl/extract/#next-steps","text":"The Extract step results in a stream of records that can be processed in the following steps: Step 2. Transform : cleans, combines, and extends data in the[record. Step 3. Assert : uses data from the record to make linked data assertions in the internal store.","title":"Next steps"},{"location":"triply-etl/extract/csv/","text":"On this page: CSV extractor Basic usage Standards-compliance Encoding configuration Separator configuration CSV with tab separators is not TSV Record representation CSV extractor \u00b6 CSV or Comma Separated Values (file name extension .csv ) is a popular format for storing tabular source data. TriplyETL has a dedicated fromCsv() extractor for this data format. Basic usage \u00b6 The CSV extractor is imported in the following way: import { fromCsv, Source } from '@triplyetl/etl/generic' The following code snippet extracts records from a local CSV file: fromCsv(Source.file('data.csv')), The following code snippet extracts records from an online CSV file, that is hosted at the specified URL: fromCsv(Source.url('https://somewhere.com/data.csv')), The following code snippet extracts records from a TriplyDB Asset . The asset is store in the data with name 'some-data' , under an account with name 'some-account' . The name of the asset is 'example.csv' : fromCsv( Source.TriplyDb.asset( 'some-account', 'some-dataset', { name: 'example.csv' } ) ), Standards-compliance \u00b6 The fromCsv() extractor implements the official CSV standard: IETF RFC 4180 . Some CSV files do not follow the standard precisely. In order to process such CSV files, the default behavior of the extractor can be changed through an optional options parameter. See the CSV Parse for Node.js documentation for all the available options. Encoding configuration \u00b6 According to the official CSV standard, CSV sources are allowed to use any encoding. Since the CSV format does not allow the used encoding to be specified in the format itself, a non-standard encoding must always be configured manually. By default, TriplyETL assumes that CSV sources use the UTF-8 encoding. If another encoding is used, this must be explicitly specified by using the optional options parameter. The following snippet configures that the CSV source uses the ISO Latin-1 encoding: fromCsv( Source.TriplyDb.asset('my-dataset', { name: 'example.csv' }), { encoding: 'latin1' } ), The following encodings are currently supported: Value Encoding Standard Alternative values 'ascii' US-ASCII ANSI 'latin1' Latin-1 ISO-8859-1 binary 'utf8' UTF-8 Unicode 'utf16le' UTF-16 Little Endian Unicode 'ucs2' , 'ucs-2' , 'utf16-le' Read the CSV Parse for Node.js documentation for more information. Separator configuration \u00b6 Some CSV files only deviate in their use of a different separator character. For example, some CSV files use the semi-colon ( ; ) or the at-sign ( @ ) for this. The following snippet extracts records for a CSV file that uses the semi-colon ( ; ) as the separator character: fromCsv(Source.file('example.csv'), { separator: ';' }), CSV with tab separators is not TSV \u00b6 Notice that the popular Tab-Separate Values (TSV) format is not the same as the standardized CSV format with a tab separator character. If you want to process standards-conforming TSV source data, use the fromTsv() extractor instead. Record representation \u00b6 TriplyETL treats every row in a CSV source as one record . The columns are emitted as keys and the cells are emitted as values. All values are of type string . Empty cells (i.e. those containing the empty string) are treated as denoting a null value and are therefore excluded from the record. Any trailing whitespace that appears in headers or cells is removed from the keys and values in the record. For example, the following table: ID Name Age 1 Doe, John 32 2 D., Jane can be expressed by the following CSV snippet: ID,Name,Age 1,\"Doe, John\",32 2,\"D., Jane \", which is emitted as the following two TriplyETL records: { \"ID\": \"1\", \"Name\": \"Doe, John\", \"Age\": \"32\" } { \"ID\": \"2\", \"Name\": \"D., Jane\" } Notice the following details: - All values have type string , including \"ID\" and \"Age\" . The value for field \"Age\" should probably be considered numeric, but the CSV format cannot express this. A TriplyETL transformation can be used to cast string values to numeric values. - The trailing space in \"D., Jane \" is omitted from the second record, since training whitespace is removed from all keys and values. - The \"Age\" key is missing from the second record, since the corresponding CSV cell contains the empty string, which is considered to denote an empty value.","title":"CSV"},{"location":"triply-etl/extract/csv/#csv-extractor","text":"CSV or Comma Separated Values (file name extension .csv ) is a popular format for storing tabular source data. TriplyETL has a dedicated fromCsv() extractor for this data format.","title":"CSV extractor"},{"location":"triply-etl/extract/csv/#basic-usage","text":"The CSV extractor is imported in the following way: import { fromCsv, Source } from '@triplyetl/etl/generic' The following code snippet extracts records from a local CSV file: fromCsv(Source.file('data.csv')), The following code snippet extracts records from an online CSV file, that is hosted at the specified URL: fromCsv(Source.url('https://somewhere.com/data.csv')), The following code snippet extracts records from a TriplyDB Asset . The asset is store in the data with name 'some-data' , under an account with name 'some-account' . The name of the asset is 'example.csv' : fromCsv( Source.TriplyDb.asset( 'some-account', 'some-dataset', { name: 'example.csv' } ) ),","title":"Basic usage"},{"location":"triply-etl/extract/csv/#standards-compliance","text":"The fromCsv() extractor implements the official CSV standard: IETF RFC 4180 . Some CSV files do not follow the standard precisely. In order to process such CSV files, the default behavior of the extractor can be changed through an optional options parameter. See the CSV Parse for Node.js documentation for all the available options.","title":"Standards-compliance"},{"location":"triply-etl/extract/csv/#encoding-configuration","text":"According to the official CSV standard, CSV sources are allowed to use any encoding. Since the CSV format does not allow the used encoding to be specified in the format itself, a non-standard encoding must always be configured manually. By default, TriplyETL assumes that CSV sources use the UTF-8 encoding. If another encoding is used, this must be explicitly specified by using the optional options parameter. The following snippet configures that the CSV source uses the ISO Latin-1 encoding: fromCsv( Source.TriplyDb.asset('my-dataset', { name: 'example.csv' }), { encoding: 'latin1' } ), The following encodings are currently supported: Value Encoding Standard Alternative values 'ascii' US-ASCII ANSI 'latin1' Latin-1 ISO-8859-1 binary 'utf8' UTF-8 Unicode 'utf16le' UTF-16 Little Endian Unicode 'ucs2' , 'ucs-2' , 'utf16-le' Read the CSV Parse for Node.js documentation for more information.","title":"Encoding configuration"},{"location":"triply-etl/extract/csv/#separator-configuration","text":"Some CSV files only deviate in their use of a different separator character. For example, some CSV files use the semi-colon ( ; ) or the at-sign ( @ ) for this. The following snippet extracts records for a CSV file that uses the semi-colon ( ; ) as the separator character: fromCsv(Source.file('example.csv'), { separator: ';' }),","title":"Separator configuration"},{"location":"triply-etl/extract/csv/#csv-with-tab-separators-is-not-tsv","text":"Notice that the popular Tab-Separate Values (TSV) format is not the same as the standardized CSV format with a tab separator character. If you want to process standards-conforming TSV source data, use the fromTsv() extractor instead.","title":"CSV with tab separators is not TSV"},{"location":"triply-etl/extract/csv/#record-representation","text":"TriplyETL treats every row in a CSV source as one record . The columns are emitted as keys and the cells are emitted as values. All values are of type string . Empty cells (i.e. those containing the empty string) are treated as denoting a null value and are therefore excluded from the record. Any trailing whitespace that appears in headers or cells is removed from the keys and values in the record. For example, the following table: ID Name Age 1 Doe, John 32 2 D., Jane can be expressed by the following CSV snippet: ID,Name,Age 1,\"Doe, John\",32 2,\"D., Jane \", which is emitted as the following two TriplyETL records: { \"ID\": \"1\", \"Name\": \"Doe, John\", \"Age\": \"32\" } { \"ID\": \"2\", \"Name\": \"D., Jane\" } Notice the following details: - All values have type string , including \"ID\" and \"Age\" . The value for field \"Age\" should probably be considered numeric, but the CSV format cannot express this. A TriplyETL transformation can be used to cast string values to numeric values. - The trailing space in \"D., Jane \" is omitted from the second record, since training whitespace is removed from all keys and values. - The \"Age\" key is missing from the second record, since the corresponding CSV cell contains the empty string, which is considered to denote an empty value.","title":"Record representation"},{"location":"triply-etl/extract/json/","text":"On this page: JSON extractor Basic usage Path selectors Nested keys Dealing with dots in keys Index-based list access JSON extractor \u00b6 JSON or JavaScript Object Notation (file name extension .json ) is a popular open standard for interchanging tree-shaped data. TriplyETL has a dedicated fromJson() extractor for this format. Basic usage \u00b6 The JSON extractor is imported in the following way: import { fromJson, Source } from '@triplyetl/etl/generic' The following code snippet extracts records from a JSON source that is stored as a TriplyDB asset : fromJson( Source.TriplyDb.asset( 'some-account', 'some-dataset', { name: 'example.json.gz' } ) ), The following example uses an in-line specified JSON source: fromJson([{ a: 'a', b: 'b', c: 'c' }]), TriplyETL supports the IETF RFC 8259 standard for JSON. Path selectors \u00b6 If the JSON data source is large, it may be necessary to stream through subtrees. The subtrees that should used, can be specified through the selectors option. For example, the following snippet streams to each person record individually: fromJson( {data: {persons: [{name: 'John'}, {name: 'Mary'}]}}, {selectors: 'data.persons'} ), Notice that the dot is used to specify paths, i.e. sequences of keys. It is also possible to specify multiple selectors by using an array of strings. Nested keys \u00b6 Since JSON is a tree-shaped format, it is able to store values in a nested structure. This requires a sequence or 'path' of keys to be specified. We use the following example data: { \"metadata\": { \"title\": { \"name\": \"Data about countries.\" } }, \"data\": { \"countries\": [ { \"country.id\": \"nl\", \"name\": \"The Netherlands\" }, { \"country.id\": \"de\", \"name\": \"Germany\" } ] } } Paths are specified as dot-separated sequences of keys, starting at the top-level and ending at the required value. For the JSON example above, TriplyETL can access the \"name\" key inside the \"title\" key, which itself is nested inside the \"metadata\" key. This path is expressed in [1]. Notice that the path expressed in [1] is different from the path expressed in [2], which also accesses the \"name\" key, but nested inside the \"countries\" and then \"data\" keys. (The use of the [0] index is explained in the next section.) [1] metadata.title.name [2] data.countries[0].name Path expressions can be used as string keys in many places in TriplyETL. For example, we can assert the title of a dataset in the following way: triple('_dataset', dct.title, 'metadata.title.name'), This asserts the following linked data: dataset:my-dataset dct:title 'Data about countries.'. Dealing with dots in keys \u00b6 In the previous section we saw that dots are used to separate keys in paths. However, sometimes a dot can occur as a regular character inside a key. In such cases, we need to apply additional escaping of the key name to avoid naming conflicts. The example data from the previous section contains the following key: \"country.id\" Notice that the dot is here part of the key name. We can refer to these keys as follows: triple('_country', dct.id, 'data.countries[0].[\"country.id\"]'), Notice the use of additional escaping: [\"...\"] Index-based list access \u00b6 Tree-shaped data formats often allow multiple values to be specified in an ordered list. Examples of this are arrays in JSON and XML elements with the same tag that are directly nested under the same parent element. TriplyETL is able to access specific elements from lists based on their index or position. Following the standard practice in Computer Science, TriplyETL refers to the first element in the list as having index 0. The second element has index 1, etc. For the above example record, we can assert the name of the first country as follows: triple( iri(prefix.id, 'data.countries[0].[\"country.id\"]'), rdfs.label, 'data.countries[0].name' ), This results in the following linked data: id:nl rdfs:label 'The Netherlands'. We can also assert the name of the second country. Notice that only the index is different ( 1 instead of 0 ): triple( iri(prefix.id, 'data.countries[1].[\"country.id\"]'), rdfs.label, 'data.countries[1].name' ), This results in the following linked data: id:de rdfs:label 'Germany'.","title":"JSON"},{"location":"triply-etl/extract/json/#json-extractor","text":"JSON or JavaScript Object Notation (file name extension .json ) is a popular open standard for interchanging tree-shaped data. TriplyETL has a dedicated fromJson() extractor for this format.","title":"JSON extractor"},{"location":"triply-etl/extract/json/#basic-usage","text":"The JSON extractor is imported in the following way: import { fromJson, Source } from '@triplyetl/etl/generic' The following code snippet extracts records from a JSON source that is stored as a TriplyDB asset : fromJson( Source.TriplyDb.asset( 'some-account', 'some-dataset', { name: 'example.json.gz' } ) ), The following example uses an in-line specified JSON source: fromJson([{ a: 'a', b: 'b', c: 'c' }]), TriplyETL supports the IETF RFC 8259 standard for JSON.","title":"Basic usage"},{"location":"triply-etl/extract/json/#path-selectors","text":"If the JSON data source is large, it may be necessary to stream through subtrees. The subtrees that should used, can be specified through the selectors option. For example, the following snippet streams to each person record individually: fromJson( {data: {persons: [{name: 'John'}, {name: 'Mary'}]}}, {selectors: 'data.persons'} ), Notice that the dot is used to specify paths, i.e. sequences of keys. It is also possible to specify multiple selectors by using an array of strings.","title":"Path selectors"},{"location":"triply-etl/extract/json/#nested-keys","text":"Since JSON is a tree-shaped format, it is able to store values in a nested structure. This requires a sequence or 'path' of keys to be specified. We use the following example data: { \"metadata\": { \"title\": { \"name\": \"Data about countries.\" } }, \"data\": { \"countries\": [ { \"country.id\": \"nl\", \"name\": \"The Netherlands\" }, { \"country.id\": \"de\", \"name\": \"Germany\" } ] } } Paths are specified as dot-separated sequences of keys, starting at the top-level and ending at the required value. For the JSON example above, TriplyETL can access the \"name\" key inside the \"title\" key, which itself is nested inside the \"metadata\" key. This path is expressed in [1]. Notice that the path expressed in [1] is different from the path expressed in [2], which also accesses the \"name\" key, but nested inside the \"countries\" and then \"data\" keys. (The use of the [0] index is explained in the next section.) [1] metadata.title.name [2] data.countries[0].name Path expressions can be used as string keys in many places in TriplyETL. For example, we can assert the title of a dataset in the following way: triple('_dataset', dct.title, 'metadata.title.name'), This asserts the following linked data: dataset:my-dataset dct:title 'Data about countries.'.","title":"Nested keys"},{"location":"triply-etl/extract/json/#dealing-with-dots-in-keys","text":"In the previous section we saw that dots are used to separate keys in paths. However, sometimes a dot can occur as a regular character inside a key. In such cases, we need to apply additional escaping of the key name to avoid naming conflicts. The example data from the previous section contains the following key: \"country.id\" Notice that the dot is here part of the key name. We can refer to these keys as follows: triple('_country', dct.id, 'data.countries[0].[\"country.id\"]'), Notice the use of additional escaping: [\"...\"]","title":"Dealing with dots in keys"},{"location":"triply-etl/extract/json/#index-based-list-access","text":"Tree-shaped data formats often allow multiple values to be specified in an ordered list. Examples of this are arrays in JSON and XML elements with the same tag that are directly nested under the same parent element. TriplyETL is able to access specific elements from lists based on their index or position. Following the standard practice in Computer Science, TriplyETL refers to the first element in the list as having index 0. The second element has index 1, etc. For the above example record, we can assert the name of the first country as follows: triple( iri(prefix.id, 'data.countries[0].[\"country.id\"]'), rdfs.label, 'data.countries[0].name' ), This results in the following linked data: id:nl rdfs:label 'The Netherlands'. We can also assert the name of the second country. Notice that only the index is different ( 1 instead of 0 ): triple( iri(prefix.id, 'data.countries[1].[\"country.id\"]'), rdfs.label, 'data.countries[1].name' ), This results in the following linked data: id:de rdfs:label 'Germany'.","title":"Index-based list access"},{"location":"triply-etl/extract/oai-pmh/","text":"On this page: OAI-PMH extractor Basic usage Standards-compliance Verb 'ListIdentifiers' Verb 'ListRecords' OAI-PMH extractor \u00b6 In the GLAM (Galleries, Libraries, Archives, Museums) domain, the Open Archives Initiative (OAI), Protocol for Metadata Harvesting (PMH) is a popular protocol and format for publishing data collections. TriplyETL has a dedicated fromOai() extractor to tap into these data collections. The fromOai() extractor ensures a continuous stream of data records. Under the hood, the extractor uses resumption tokens to iterate over large collections. Basic usage \u00b6 The OAI-PMH extractor is imported in the following way: import { fromOai, Source } from '@triplyetl/etl/generic' An OAI-PMH endpoint can be configured by specifying its URL (parameter url ). Since one OAI-PMH endpoint typically publishes multiple datasets, it is also common to specify the set parameter. The following code snippet connects to an example dataset that is published in an OAI-PMH endpoint: fromOai({ set: 'some-dataset', url: 'https://somewhere.com/webapioai/oai.ashx' }), Standards-compliance \u00b6 TriplyETL supports the official OAI-PMH standard. The OAI-PMH standard defines 6 'verbs'. These are different sub-APIs that together compose the OAI-PMH API. The fromOai() extractor currently supports the following two verbs: ListIdentifiers and ListRecords . Verb 'ListIdentifiers' \u00b6 This 'verb' or sub-API streams through the headers of all records. It does not returns the actual (body) content of each record (see ListRecords ). This verb can be used to look for header properties like set membership, datestamp, and deletion status. The following code snippet streams through the headers of a public OAI-PMH endpoint: fromOai({ metadataPrefix: 'marcxml', set: 'iish.evergreen.biblio', url: 'https://api.socialhistoryservices.org/solr/all/oai', verb: 'ListIdentifiers' }), logRecord(), Verb 'ListRecords' \u00b6 This 'verb' or sub-API streams through all records and retrieves them in full. This API is used to harvest records. The following code snippet streams through the records of a public OAI-PMH endpoint: fromOai({ metadataPrefix: 'marcxml', set: 'iish.evergreen.biblio', url: 'https://api.socialhistoryservices.org/solr/all/oai', verb: 'ListRecords' }), logRecord(),","title":"OAI-PMH"},{"location":"triply-etl/extract/oai-pmh/#oai-pmh-extractor","text":"In the GLAM (Galleries, Libraries, Archives, Museums) domain, the Open Archives Initiative (OAI), Protocol for Metadata Harvesting (PMH) is a popular protocol and format for publishing data collections. TriplyETL has a dedicated fromOai() extractor to tap into these data collections. The fromOai() extractor ensures a continuous stream of data records. Under the hood, the extractor uses resumption tokens to iterate over large collections.","title":"OAI-PMH extractor"},{"location":"triply-etl/extract/oai-pmh/#basic-usage","text":"The OAI-PMH extractor is imported in the following way: import { fromOai, Source } from '@triplyetl/etl/generic' An OAI-PMH endpoint can be configured by specifying its URL (parameter url ). Since one OAI-PMH endpoint typically publishes multiple datasets, it is also common to specify the set parameter. The following code snippet connects to an example dataset that is published in an OAI-PMH endpoint: fromOai({ set: 'some-dataset', url: 'https://somewhere.com/webapioai/oai.ashx' }),","title":"Basic usage"},{"location":"triply-etl/extract/oai-pmh/#standards-compliance","text":"TriplyETL supports the official OAI-PMH standard. The OAI-PMH standard defines 6 'verbs'. These are different sub-APIs that together compose the OAI-PMH API. The fromOai() extractor currently supports the following two verbs: ListIdentifiers and ListRecords .","title":"Standards-compliance"},{"location":"triply-etl/extract/oai-pmh/#verb-listidentifiers","text":"This 'verb' or sub-API streams through the headers of all records. It does not returns the actual (body) content of each record (see ListRecords ). This verb can be used to look for header properties like set membership, datestamp, and deletion status. The following code snippet streams through the headers of a public OAI-PMH endpoint: fromOai({ metadataPrefix: 'marcxml', set: 'iish.evergreen.biblio', url: 'https://api.socialhistoryservices.org/solr/all/oai', verb: 'ListIdentifiers' }), logRecord(),","title":"Verb 'ListIdentifiers'"},{"location":"triply-etl/extract/oai-pmh/#verb-listrecords","text":"This 'verb' or sub-API streams through all records and retrieves them in full. This API is used to harvest records. The following code snippet streams through the records of a public OAI-PMH endpoint: fromOai({ metadataPrefix: 'marcxml', set: 'iish.evergreen.biblio', url: 'https://api.socialhistoryservices.org/solr/all/oai', verb: 'ListRecords' }), logRecord(),","title":"Verb 'ListRecords'"},{"location":"triply-etl/extract/postgres/","text":"On this page: Postgres extractor Basic usage Connector configuration Postgres extractor \u00b6 PostgreSQL or Postgres is an open-source relational database system. Postgres supports both SQL (relational) and JSON (non-relational) querying. TriplyETL has a dedicated fromPostgres() extractor to retrieve data from a Postgres database. Basic usage \u00b6 The Postgres extractor is imported in the following way: import { fromPostgres, Source } from '@triplyetl/etl/generic' The following code snippet extracts records form a public database URL: fromPostgres( 'select * from rnc_database', { url: 'postgres://reader:NWDMCE5xdipIjRrp@hh-pgsql-public.ebi.ac.uk:5432/pfmegrnargs' } ), Connector configuration \u00b6 Alternatively, a Postgres dataset can be accessed via connector configuration. The following code snippet accesses the same public database, but uses connector configuration to do so: fromPostgres( 'select * from rnc_database', { host: 'hh-pgsql-public.ebi.ac.uk', port: 5432, database: 'pfmegrnargs', user: 'reader', password: 'NWDMCE5xdipIjRrp', } ),","title":"Postgres"},{"location":"triply-etl/extract/postgres/#postgres-extractor","text":"PostgreSQL or Postgres is an open-source relational database system. Postgres supports both SQL (relational) and JSON (non-relational) querying. TriplyETL has a dedicated fromPostgres() extractor to retrieve data from a Postgres database.","title":"Postgres extractor"},{"location":"triply-etl/extract/postgres/#basic-usage","text":"The Postgres extractor is imported in the following way: import { fromPostgres, Source } from '@triplyetl/etl/generic' The following code snippet extracts records form a public database URL: fromPostgres( 'select * from rnc_database', { url: 'postgres://reader:NWDMCE5xdipIjRrp@hh-pgsql-public.ebi.ac.uk:5432/pfmegrnargs' } ),","title":"Basic usage"},{"location":"triply-etl/extract/postgres/#connector-configuration","text":"Alternatively, a Postgres dataset can be accessed via connector configuration. The following code snippet accesses the same public database, but uses connector configuration to do so: fromPostgres( 'select * from rnc_database', { host: 'hh-pgsql-public.ebi.ac.uk', port: 5432, database: 'pfmegrnargs', user: 'reader', password: 'NWDMCE5xdipIjRrp', } ),","title":"Connector configuration"},{"location":"triply-etl/extract/rdf/","text":"On this page: RDF loader Basic usage Loading RDF from an HTML page RDF loader \u00b6 RDF or Resource Description Framework (file name extensions .nq , .nt , .jsonld , .rdf , .trig , .turtle ) is the standardized format for linked data. We do not need to extract records from an RDF data source, but can instead load its contents directly into the internal store . Basic usage \u00b6 The RDF loader is imported in the following way: import { loadRdf, Source } from '@triplyetl/etl/generic' The following code snippet loads RDF from the specified TriplyDB Dataset into the internal store: loadRdf(Source.TriplyDb.rdf('my-account', 'my-dataset')), The following code snippet loads RDF from a SPARQL Construct query that is stored as a TriplyDB Query : loadRdf(Source.TriplyDb.query('Triply', 'network-query')), Loading RDF from an HTML page \u00b6 With loadRdf() extractor, it is also possible to extract data from web pages / HTML, which contain Schema in JSON-LD. This is possible because most websites contain linked data annotations that use Schema.org . Such linked data is enclosed in a tag: <script type='application/ld+json'> ... </script> Schema markup is how Google can serve up rich results (also called rich snippets and rich cards). The schema is included in HTML in the following way: The Script Type : What format your structured data will take (JSON-LD) The Context : Where the language you\u2019re using comes from (schema.org) The Type : What kind of thing is the search engine looking at (Article) The Property : What kind of quality will you be describing when it comes to this type (url) The Value : What you\u2019re actually telling the search engines about this property (the URL of the article) Example taken from Wikipedia: The Wikipedia page of the first programmer in history (https://en.wikipedia.org/wiki/Ada_Lovelace) contains the following linked data: { \"@context\": \"https://schema.org\", \"@type\": \"Article\", \"name\": \"Ada Lovelace\", \"url\": \"https://en.wikipedia.org/wiki/Ada_Lovelace\", \"sameAs\": \"http://www.wikidata.org/entity/Q7259\", \"mainEntity\": \"http://www.wikidata.org/entity/Q7259\", \"author\": { \"@type\": \"Organization\", \"name\": \"Contributors to Wikimedia projects\" }, \"publisher\": { \"@type\": \"Organization\", \"name\": \"Wikimedia Foundation, Inc.\", \"logo\": { \"@type\": \"ImageObject\", \"url\": \"https://www.wikimedia.org/static/images/wmf-hor-googpub.png\" } }, \"datePublished\": \"2001-05-20T14:57:05Z\", \"dateModified\": \"2023-03-17T21:28:23Z\", \"image\": \"https://upload.wikimedia.org/wikipedia/commons/0/0b/Ada_Byron_daguerreotype_by_Antoine_Claudet_1843_or_1850.jpg\", \"headline\": \"1815-1852 British mathematician, considered the first computer programmer\" } This data can be loaded with the following code snippet: loadRdf( Source.url('https://en.wikipedia.org/wiki/Ada_Lovelace'), { contentType: 'text/html' } ),","title":"RDF"},{"location":"triply-etl/extract/rdf/#rdf-loader","text":"RDF or Resource Description Framework (file name extensions .nq , .nt , .jsonld , .rdf , .trig , .turtle ) is the standardized format for linked data. We do not need to extract records from an RDF data source, but can instead load its contents directly into the internal store .","title":"RDF loader"},{"location":"triply-etl/extract/rdf/#basic-usage","text":"The RDF loader is imported in the following way: import { loadRdf, Source } from '@triplyetl/etl/generic' The following code snippet loads RDF from the specified TriplyDB Dataset into the internal store: loadRdf(Source.TriplyDb.rdf('my-account', 'my-dataset')), The following code snippet loads RDF from a SPARQL Construct query that is stored as a TriplyDB Query : loadRdf(Source.TriplyDb.query('Triply', 'network-query')),","title":"Basic usage"},{"location":"triply-etl/extract/rdf/#loading-rdf-from-an-html-page","text":"With loadRdf() extractor, it is also possible to extract data from web pages / HTML, which contain Schema in JSON-LD. This is possible because most websites contain linked data annotations that use Schema.org . Such linked data is enclosed in a tag: <script type='application/ld+json'> ... </script> Schema markup is how Google can serve up rich results (also called rich snippets and rich cards). The schema is included in HTML in the following way: The Script Type : What format your structured data will take (JSON-LD) The Context : Where the language you\u2019re using comes from (schema.org) The Type : What kind of thing is the search engine looking at (Article) The Property : What kind of quality will you be describing when it comes to this type (url) The Value : What you\u2019re actually telling the search engines about this property (the URL of the article) Example taken from Wikipedia: The Wikipedia page of the first programmer in history (https://en.wikipedia.org/wiki/Ada_Lovelace) contains the following linked data: { \"@context\": \"https://schema.org\", \"@type\": \"Article\", \"name\": \"Ada Lovelace\", \"url\": \"https://en.wikipedia.org/wiki/Ada_Lovelace\", \"sameAs\": \"http://www.wikidata.org/entity/Q7259\", \"mainEntity\": \"http://www.wikidata.org/entity/Q7259\", \"author\": { \"@type\": \"Organization\", \"name\": \"Contributors to Wikimedia projects\" }, \"publisher\": { \"@type\": \"Organization\", \"name\": \"Wikimedia Foundation, Inc.\", \"logo\": { \"@type\": \"ImageObject\", \"url\": \"https://www.wikimedia.org/static/images/wmf-hor-googpub.png\" } }, \"datePublished\": \"2001-05-20T14:57:05Z\", \"dateModified\": \"2023-03-17T21:28:23Z\", \"image\": \"https://upload.wikimedia.org/wikipedia/commons/0/0b/Ada_Byron_daguerreotype_by_Antoine_Claudet_1843_or_1850.jpg\", \"headline\": \"1815-1852 British mathematician, considered the first computer programmer\" } This data can be loaded with the following code snippet: loadRdf( Source.url('https://en.wikipedia.org/wiki/Ada_Lovelace'), { contentType: 'text/html' } ),","title":"Loading RDF from an HTML page"},{"location":"triply-etl/extract/shapefile/","text":"On this page: Shapefile extractor Basic usage Record representation Shapefile extractor \u00b6 The ESRI Shapefile format was developed by Esri (Environmental Systems Research Institute) for interoperability between Geographic Information Systems (GIS). An ESRI Shapefile is a ZIP with six files in it (file name extension .shapefile.zip ). Currently only one of the file in a Shapefile ZIP file is supported: the .shp file. Basic usage \u00b6 The Shapefile extractor is imported in the following way: import { fromShapefile, Source } from '@triplyetl/etl/generic' The following code snippet extracts records from a local Shapefile: fromShapefile(Source.file('example.shp')) The following code snippet extracts records from a Shapefile that is stored as a TriplyDB Asset: fromShapefile( Source.TriplyDb.asset( 'some-account', 'some-dataset', { name: 'example.shp' } ) ), Record representation \u00b6 The following example record is obtained from a file called nl_1km.shp that is published by the European Environment Agency : { '$recordId': 1, '$environment': 'Development', '$fileName': 'nl_1km.shp', type: 'Feature', properties: { CELLCODE: '1kmE3793N3217', EOFORIGIN: 3793000, NOFORIGIN: 3217000 }, geometry: { type: 'Polygon', coordinates: [ [ [ 3793000, 3217000 ], [ 3793000, 3218000 ], [ 3794000, 3218000 ], [ 3794000, 3217000 ], [ 3793000, 3217000 ] ] ] } }","title":"Shapefile"},{"location":"triply-etl/extract/shapefile/#shapefile-extractor","text":"The ESRI Shapefile format was developed by Esri (Environmental Systems Research Institute) for interoperability between Geographic Information Systems (GIS). An ESRI Shapefile is a ZIP with six files in it (file name extension .shapefile.zip ). Currently only one of the file in a Shapefile ZIP file is supported: the .shp file.","title":"Shapefile extractor"},{"location":"triply-etl/extract/shapefile/#basic-usage","text":"The Shapefile extractor is imported in the following way: import { fromShapefile, Source } from '@triplyetl/etl/generic' The following code snippet extracts records from a local Shapefile: fromShapefile(Source.file('example.shp')) The following code snippet extracts records from a Shapefile that is stored as a TriplyDB Asset: fromShapefile( Source.TriplyDb.asset( 'some-account', 'some-dataset', { name: 'example.shp' } ) ),","title":"Basic usage"},{"location":"triply-etl/extract/shapefile/#record-representation","text":"The following example record is obtained from a file called nl_1km.shp that is published by the European Environment Agency : { '$recordId': 1, '$environment': 'Development', '$fileName': 'nl_1km.shp', type: 'Feature', properties: { CELLCODE: '1kmE3793N3217', EOFORIGIN: 3793000, NOFORIGIN: 3217000 }, geometry: { type: 'Polygon', coordinates: [ [ [ 3793000, 3217000 ], [ 3793000, 3218000 ], [ 3794000, 3218000 ], [ 3794000, 3217000 ], [ 3793000, 3217000 ] ] ] } }","title":"Record representation"},{"location":"triply-etl/extract/tsv/","text":"On this page: CSV extractor Basic usage Extractor for TSV (Tab-Separated Values) Record representation CSV extractor \u00b6 CSV or Comma Separated Values (file name extension .csv ) is a popular format for storing tabular source data. TriplyETL has a dedicated fromCsv() extractor for this data format. Basic usage \u00b6 The TSV extractor is imported in the following way: import { fromTsv, Source } from '@triplyetl/etl/generic' Extractor for TSV (Tab-Separated Values) \u00b6 TSV or Tab-Separated Values (file name extension .tsv ) is a popular format for tabular source data. TriplyETL has a fromTsv() extractor to support this format. The following code snippet extracts records for TSV file that is stored as a TriplyDB Asset : fromTsv( Source.TriplyDb.asset( 'some-account', 'some-dataset', { name: 'example.tsv.gz' } ) ), TriplyETL supports the IANA standard definition of the TSV format. Record representation \u00b6 TriplyETL treats every row in a TSV source as one record. The columns are emitted as keys and the cells are emitted as values. All values are of type string . Cells that contain the empty string are treated as denoting an empty value and are excluded from the record. Any trailing whitespace that appears in headers or cells is removed from the keys or values in the record. For example, the following table: ID Name Age 1 Doe, John 32 2 D., Jane can be expressed by the following TSV snippet: ID Name Age 1 Doe, John 32 2 D., Jane which is emitted as the following two TriplyETL records: { \"ID\": \"1\", \"Name\": \"Doe, John\", \"Age\": \"32\" } { \"ID\": \"2\", \"Name\": \"D., Jane\" } Notice the following details: All values have type string , including \"ID\" and \"Age\" . The value for field \"Age\" should probably be considered numeric, but the TSV format cannot express this. A TriplyETL transformation can be used to cast string values to numeric values. The trailing space in \"D., Jane \" is omitted from the second record, since training whitespace is removed from all keys and values. The \"Age\" key is missing from the second record, since the corresponding TSV cell contains the empty string, which is considered to denote an empty value.","title":"TSV"},{"location":"triply-etl/extract/tsv/#csv-extractor","text":"CSV or Comma Separated Values (file name extension .csv ) is a popular format for storing tabular source data. TriplyETL has a dedicated fromCsv() extractor for this data format.","title":"CSV extractor"},{"location":"triply-etl/extract/tsv/#basic-usage","text":"The TSV extractor is imported in the following way: import { fromTsv, Source } from '@triplyetl/etl/generic'","title":"Basic usage"},{"location":"triply-etl/extract/tsv/#extractor-for-tsv-tab-separated-values","text":"TSV or Tab-Separated Values (file name extension .tsv ) is a popular format for tabular source data. TriplyETL has a fromTsv() extractor to support this format. The following code snippet extracts records for TSV file that is stored as a TriplyDB Asset : fromTsv( Source.TriplyDb.asset( 'some-account', 'some-dataset', { name: 'example.tsv.gz' } ) ), TriplyETL supports the IANA standard definition of the TSV format.","title":"Extractor for TSV (Tab-Separated Values)"},{"location":"triply-etl/extract/tsv/#record-representation","text":"TriplyETL treats every row in a TSV source as one record. The columns are emitted as keys and the cells are emitted as values. All values are of type string . Cells that contain the empty string are treated as denoting an empty value and are excluded from the record. Any trailing whitespace that appears in headers or cells is removed from the keys or values in the record. For example, the following table: ID Name Age 1 Doe, John 32 2 D., Jane can be expressed by the following TSV snippet: ID Name Age 1 Doe, John 32 2 D., Jane which is emitted as the following two TriplyETL records: { \"ID\": \"1\", \"Name\": \"Doe, John\", \"Age\": \"32\" } { \"ID\": \"2\", \"Name\": \"D., Jane\" } Notice the following details: All values have type string , including \"ID\" and \"Age\" . The value for field \"Age\" should probably be considered numeric, but the TSV format cannot express this. A TriplyETL transformation can be used to cast string values to numeric values. The trailing space in \"D., Jane \" is omitted from the second record, since training whitespace is removed from all keys and values. The \"Age\" key is missing from the second record, since the corresponding TSV cell contains the empty string, which is considered to denote an empty value.","title":"Record representation"},{"location":"triply-etl/extract/xlsx/","text":"On this page: XLSX extractor (Microsoft Excel) Basic usage Multiple sheets Record representation Special key '$sheetName' XLSX extractor (Microsoft Excel) \u00b6 XLSX or Office Open XML Workbook for Microsoft Excel (file name extension .xlsx ) is a popular format for storing tabular source data. This is the standard file format for Microsoft Excel. TriplyETL has a dedicated fromXlsx() extractor for such sources. Basic usage \u00b6 The XLSX extractor is imported in the following way: import { fromXlsx, Source } from '@triplyetl/etl/generic' The following code snippet shows how a TriplyDB assets is used to process records from an XLSX source: fromXlsx( Source.TriplyDb.asset( 'some-account', 'some-dataset', { name: 'example.xlsx' } ) ), The fromXlsx() extractor emits one record per row in the source file. Multiple sheets \u00b6 It is common for XLSX files to have multiple sheets. By default the fromXlsx() extractor enumerates all rows from all sheets as records. If only some sheets should be used, this can be specified as a configuration option. The following code snippet only emits records/rows from the 'people' and 'projects' sheets in the XLSX source file 'example.xlsx' . Rows from other sheets in the same XLSX file are not emitted: fromXlsx(Source.file('example.xlsx'), { sheetNames: ['people', 'projects'] }), Record representation \u00b6 TriplyETL treats every row in XLSX sheet as one record . The columns are emitted as keys and the cells are emitted as values. Unlike other tabular formats like CSV and TSV , values in XLSX can have different types. For example, the following table: ID Name Age 1 Doe, John 32 2 D., Jane can be emitted as the following two TriplyETL records: { \"$recordId\": 1, \"$environment\": \"Development\", \"$sheetName\": \"Sheet1\", \"$fileName\": \"static/Untitled 1.xlsx\", \"ID\": \"1\", \"Name\": \"Doe, John\", \"Age\": 32 } { \"$recordId\": 2, \"$environment\": \"Development\", \"$sheetName\": \"Sheet1\", \"$fileName\": \"static/Untitled 1.xlsx\", \"ID\": \"2\", \"Name\": \"D., Jane\", } Notice the following: - The value for the \"Age\" key is a number. - The special keys $recordId , $environment , and $fileName are documented in the section on Special Keys . - The special key $sheetName is unique to the fromXslx() extractor and is documented in the next subsection. Special key '$sheetName' \u00b6 For every record emitted by the fromXlsx() extractor. the $sheetName special key contains the name of the Excel sheet from which that record originates. The presence of the sheet name allows the TriplyETL configuration to be adjusted for different sheet. For example, an Excel spreadsheet may contain a 'companies' sheet and a 'persons' sheet. The name of the sheet may be used to determine which class should be asserted. The following snippet uses transformation translateAll() to map sheet names to class IRIs: fromXlsx(Source.file('example.xlsx')), translateAll({ content: '$sheetName', table: { 'companies': sdo.Organization, 'persons': sdo.Person, }, key: '_class', }), triple(iri(prefix.id, '$recordId'), a, '_class'),","title":"XLSX"},{"location":"triply-etl/extract/xlsx/#xlsx-extractor-microsoft-excel","text":"XLSX or Office Open XML Workbook for Microsoft Excel (file name extension .xlsx ) is a popular format for storing tabular source data. This is the standard file format for Microsoft Excel. TriplyETL has a dedicated fromXlsx() extractor for such sources.","title":"XLSX extractor (Microsoft Excel)"},{"location":"triply-etl/extract/xlsx/#basic-usage","text":"The XLSX extractor is imported in the following way: import { fromXlsx, Source } from '@triplyetl/etl/generic' The following code snippet shows how a TriplyDB assets is used to process records from an XLSX source: fromXlsx( Source.TriplyDb.asset( 'some-account', 'some-dataset', { name: 'example.xlsx' } ) ), The fromXlsx() extractor emits one record per row in the source file.","title":"Basic usage"},{"location":"triply-etl/extract/xlsx/#multiple-sheets","text":"It is common for XLSX files to have multiple sheets. By default the fromXlsx() extractor enumerates all rows from all sheets as records. If only some sheets should be used, this can be specified as a configuration option. The following code snippet only emits records/rows from the 'people' and 'projects' sheets in the XLSX source file 'example.xlsx' . Rows from other sheets in the same XLSX file are not emitted: fromXlsx(Source.file('example.xlsx'), { sheetNames: ['people', 'projects'] }),","title":"Multiple sheets"},{"location":"triply-etl/extract/xlsx/#record-representation","text":"TriplyETL treats every row in XLSX sheet as one record . The columns are emitted as keys and the cells are emitted as values. Unlike other tabular formats like CSV and TSV , values in XLSX can have different types. For example, the following table: ID Name Age 1 Doe, John 32 2 D., Jane can be emitted as the following two TriplyETL records: { \"$recordId\": 1, \"$environment\": \"Development\", \"$sheetName\": \"Sheet1\", \"$fileName\": \"static/Untitled 1.xlsx\", \"ID\": \"1\", \"Name\": \"Doe, John\", \"Age\": 32 } { \"$recordId\": 2, \"$environment\": \"Development\", \"$sheetName\": \"Sheet1\", \"$fileName\": \"static/Untitled 1.xlsx\", \"ID\": \"2\", \"Name\": \"D., Jane\", } Notice the following: - The value for the \"Age\" key is a number. - The special keys $recordId , $environment , and $fileName are documented in the section on Special Keys . - The special key $sheetName is unique to the fromXslx() extractor and is documented in the next subsection.","title":"Record representation"},{"location":"triply-etl/extract/xlsx/#special-key-sheetname","text":"For every record emitted by the fromXlsx() extractor. the $sheetName special key contains the name of the Excel sheet from which that record originates. The presence of the sheet name allows the TriplyETL configuration to be adjusted for different sheet. For example, an Excel spreadsheet may contain a 'companies' sheet and a 'persons' sheet. The name of the sheet may be used to determine which class should be asserted. The following snippet uses transformation translateAll() to map sheet names to class IRIs: fromXlsx(Source.file('example.xlsx')), translateAll({ content: '$sheetName', table: { 'companies': sdo.Organization, 'persons': sdo.Person, }, key: '_class', }), triple(iri(prefix.id, '$recordId'), a, '_class'),","title":"Special key '$sheetName'"},{"location":"triply-etl/extract/xml/","text":"On this page: XML extractor Basic usage Path selectors Nested keys Dealing with dots in keys Index-based list access XML extractor \u00b6 XML or Extensible Markup Language (file name extension .xml ) is a popular open format for tree-shaped source data. TriplyETL has a dedicated fromXml() extractor for this data format. Basic usage \u00b6 The XML extractor is imported in the following way: import { fromXml, Source } from '@triplyetl/etl/generic' The following code snippet extracts records from an XML file that is stored as a TriplyDB Asset : fromXml( Source.TriplyDb.asset('my-dataset', {name: 'my-data.xml'}), {selectors: 'first-element'} ), Notice that the fromXml() extractor requires a selectors option. This specifies the subtrees in the XML that should be treated as individual records. In the above snippet the records are the subtrees that occur between the <first-element> opening tag and the </first-element> closing tag. Path selectors \u00b6 If a deeper path must be specified, sequential tags in the path must be separated by a dot: fromXml( Source.TriplyDb.asset('my-dataset', {name: 'my-data.xml'}), { selectors: 'first-element.second-element.third-element' } ), It is common for large XML sources to contain different kinds of records. Different kinds of records often occur under different paths. It is therefore possible to specify multiple paths, all of which will be used for extract records from the XML source. The following code snippet extracts records for three different paths in the same XML source: fromXml( Source.TriplyDb.asset('my-dataset', {name: 'my-data.xml'}), { selectors: [ 'first-element.second-element.third-element', 'first-element.second-element.alt-element', 'first-element.second-element.other-element', ] } ), TriplyETL supports the W3C XML standard. Nested keys \u00b6 Since XML can store tree-shaped data, it can have nested keys (paths) and indexed arrays. <?xml version=\"1.0\"?> <root> <metadata> <title> <name>Data about countries.</name> </title> </metadata> <data> <country \"country.id\"=\"nl\"> <name>The Netherlands</name> </country> <country \"country.id\"=\"de\"> <name>Germany</name> </country> </data> </root> Paths are specified as dot-separated sequences of keys, starting at the top-level and ending at the required value. For the XML example above, TriplyETL can access the textual content inside the \"name\" key, which itself is nested inside the \"title\" , \"metadata\" , and \"root\" keys. This path is expressed in [1]. Notice that the path expressed in [1] is different from the path expressed in [2], which accesses the textual content of the \"name\" key, but nested inside the \"country\" , \"data\" , and \"root\" keys. (The use of the [0] index is explained in the next section.) [1] root.metadata.title.name.$text [2] root.data.country[0].name.$text Path expressions can be used as string keys in many places in TriplyETL. For example, we can assert the title of a dataset in the following way: etl.use( triple( prefix.dataset('my-dataset'), dct.title, literal('root.metadata.title.name.$text', 'en') ), ) This results in the following assertion: dataset:my-dataset dct:title 'Data about countries.'@en. Dealing with dots in keys \u00b6 In the previous section we saw that dots are used to separate keys in paths. However, sometimes a dot can occur as a regular character inside a key. In such cases, we need to apply additional escaping of the key name to avoid naming conflicts. The example data from the previous section contains XML attribute [1], which is represented by key [2] in the TriplyETL record. [1] country.id [2] [\"@country.id\"] Notice that the dot in [2] is part of the key name. The escape notation [\"...\"] ensures that the dot is not misinterpreted as denoting a sequence of keys. Overall, \u2018a.b\u2019 notation allow going into nested object and accessing values within the nest while \u2018[\u201ca.b\u201d]\u2019 takes value a.b key as a name, therefore does not go into the nest. The following extensive example shows how complex sequences of keys with dots in them can be used: { \"a\": { \"$text\": \"1\" }, \"b\": { \"c\": { \"$text\": \"2\" } }, \"b.c\": { \"$text\": \"3\" }, \"d.d\": { \"e\": { \"$text\": \"4\" }, \"f\": { \"$text\": \"5\" } }, \"g.g\": [ { \"h.h\": { \"$text\": \"6\" } }, { \"h.h\": { \"$text\": \"7\" } } ] } Key Value 'a.$text' 1 'b.c.$text' 2 '[\"b.c\"].$text' 3 '[\"d.d\"].e.$text' 4 '[\"d.d\"].f'.$text' 5 '[\"g.g\"][0][\"h.h\"].$text' 6 '[\"g.g\"][1][\"h.h\"].$text' 7 Index-based list access \u00b6 Tree-shaped data formats often allow multiple values to be specified in an ordered list. Examples of this are arrays in JSON and XML elements with the same tag that are directly nested under the same parent element. TriplyETL is able to access specific elements from lists based on their index or position. Following the standard practice in Computer Science, TriplyETL refers to the first element in the list as having index 0. The second element has index 1, etc. For the above example record, we can assert the name of the first country as follows: triple( iri(prefix.country, 'root.data.country[0].[\"@country.id\"]'), rdfs.label, literal('root.data.countries[0].name.$text', 'en') ), This results in the following assertion: country:nl rdfs:label 'The Netherlands'@en. We can also assert the name of the second country. Notice that only the index is different (\u20181\u2019 instead of \u20180\u2019): triple( iri(prefix.country, 'root.data.countries[1].[\"@country.id\"]'), ... This results in the following assertion: country:de rdfs:label 'Germany'@en.","title":"XML"},{"location":"triply-etl/extract/xml/#xml-extractor","text":"XML or Extensible Markup Language (file name extension .xml ) is a popular open format for tree-shaped source data. TriplyETL has a dedicated fromXml() extractor for this data format.","title":"XML extractor"},{"location":"triply-etl/extract/xml/#basic-usage","text":"The XML extractor is imported in the following way: import { fromXml, Source } from '@triplyetl/etl/generic' The following code snippet extracts records from an XML file that is stored as a TriplyDB Asset : fromXml( Source.TriplyDb.asset('my-dataset', {name: 'my-data.xml'}), {selectors: 'first-element'} ), Notice that the fromXml() extractor requires a selectors option. This specifies the subtrees in the XML that should be treated as individual records. In the above snippet the records are the subtrees that occur between the <first-element> opening tag and the </first-element> closing tag.","title":"Basic usage"},{"location":"triply-etl/extract/xml/#path-selectors","text":"If a deeper path must be specified, sequential tags in the path must be separated by a dot: fromXml( Source.TriplyDb.asset('my-dataset', {name: 'my-data.xml'}), { selectors: 'first-element.second-element.third-element' } ), It is common for large XML sources to contain different kinds of records. Different kinds of records often occur under different paths. It is therefore possible to specify multiple paths, all of which will be used for extract records from the XML source. The following code snippet extracts records for three different paths in the same XML source: fromXml( Source.TriplyDb.asset('my-dataset', {name: 'my-data.xml'}), { selectors: [ 'first-element.second-element.third-element', 'first-element.second-element.alt-element', 'first-element.second-element.other-element', ] } ), TriplyETL supports the W3C XML standard.","title":"Path selectors"},{"location":"triply-etl/extract/xml/#nested-keys","text":"Since XML can store tree-shaped data, it can have nested keys (paths) and indexed arrays. <?xml version=\"1.0\"?> <root> <metadata> <title> <name>Data about countries.</name> </title> </metadata> <data> <country \"country.id\"=\"nl\"> <name>The Netherlands</name> </country> <country \"country.id\"=\"de\"> <name>Germany</name> </country> </data> </root> Paths are specified as dot-separated sequences of keys, starting at the top-level and ending at the required value. For the XML example above, TriplyETL can access the textual content inside the \"name\" key, which itself is nested inside the \"title\" , \"metadata\" , and \"root\" keys. This path is expressed in [1]. Notice that the path expressed in [1] is different from the path expressed in [2], which accesses the textual content of the \"name\" key, but nested inside the \"country\" , \"data\" , and \"root\" keys. (The use of the [0] index is explained in the next section.) [1] root.metadata.title.name.$text [2] root.data.country[0].name.$text Path expressions can be used as string keys in many places in TriplyETL. For example, we can assert the title of a dataset in the following way: etl.use( triple( prefix.dataset('my-dataset'), dct.title, literal('root.metadata.title.name.$text', 'en') ), ) This results in the following assertion: dataset:my-dataset dct:title 'Data about countries.'@en.","title":"Nested keys"},{"location":"triply-etl/extract/xml/#dealing-with-dots-in-keys","text":"In the previous section we saw that dots are used to separate keys in paths. However, sometimes a dot can occur as a regular character inside a key. In such cases, we need to apply additional escaping of the key name to avoid naming conflicts. The example data from the previous section contains XML attribute [1], which is represented by key [2] in the TriplyETL record. [1] country.id [2] [\"@country.id\"] Notice that the dot in [2] is part of the key name. The escape notation [\"...\"] ensures that the dot is not misinterpreted as denoting a sequence of keys. Overall, \u2018a.b\u2019 notation allow going into nested object and accessing values within the nest while \u2018[\u201ca.b\u201d]\u2019 takes value a.b key as a name, therefore does not go into the nest. The following extensive example shows how complex sequences of keys with dots in them can be used: { \"a\": { \"$text\": \"1\" }, \"b\": { \"c\": { \"$text\": \"2\" } }, \"b.c\": { \"$text\": \"3\" }, \"d.d\": { \"e\": { \"$text\": \"4\" }, \"f\": { \"$text\": \"5\" } }, \"g.g\": [ { \"h.h\": { \"$text\": \"6\" } }, { \"h.h\": { \"$text\": \"7\" } } ] } Key Value 'a.$text' 1 'b.c.$text' 2 '[\"b.c\"].$text' 3 '[\"d.d\"].e.$text' 4 '[\"d.d\"].f'.$text' 5 '[\"g.g\"][0][\"h.h\"].$text' 6 '[\"g.g\"][1][\"h.h\"].$text' 7","title":"Dealing with dots in keys"},{"location":"triply-etl/extract/xml/#index-based-list-access","text":"Tree-shaped data formats often allow multiple values to be specified in an ordered list. Examples of this are arrays in JSON and XML elements with the same tag that are directly nested under the same parent element. TriplyETL is able to access specific elements from lists based on their index or position. Following the standard practice in Computer Science, TriplyETL refers to the first element in the list as having index 0. The second element has index 1, etc. For the above example record, we can assert the name of the first country as follows: triple( iri(prefix.country, 'root.data.country[0].[\"@country.id\"]'), rdfs.label, literal('root.data.countries[0].name.$text', 'en') ), This results in the following assertion: country:nl rdfs:label 'The Netherlands'@en. We can also assert the name of the second country. Notice that only the index is different (\u20181\u2019 instead of \u20180\u2019): triple( iri(prefix.country, 'root.data.countries[1].[\"@country.id\"]'), ... This results in the following assertion: country:de rdfs:label 'Germany'@en.","title":"Index-based list access"},{"location":"triply-etl/generic/changelog/","text":"On this page: Changelog TriplyETL 4.10.0 [Added] [Fixed] TriplyETL 4.8.12 TriplyETL 4.4.7 TriplyETL 4.4.6 [Fixed] Minor clean-ups TriplyETL 4.4.5 [Fixed] TriplyDB-JS instantiation TriplyETL 4.4.0 [Added] linked data event stream middleware (LDES) TriplyETL 4.2.0 [Added] retry mechanism for OAI TriplyETL 4.1.16 [Fixed] SHACL version update and fixing now() in SHACL rules engine TriplyETL 4.1.15 [Fixed] large default timeout for OAI requests [Added] support for parsing multiple RDF/XML docs from OAI TriplyETL 4.1.14 [Fixed] SHACL dependency version TriplyETL 4.1.13 [Fixed] hanging ETL and preparations for a simplified eslint / prettier setup TriplyETL 4.1.12 [Fixed] SHACL executeRules() and added Speedy as a peer dependency TriplyETL 4.1.11 [Added] multiple hashing algorithms to hashedIri() TriplyETL 4.1.10 [Fixed] performance issue with validate() TriplyETL 4.1.9 [Fixed] issue with fromOai() in combination with metadataYieldsRdf TriplyETL 4.1.8 [Added] preventServiceDowntime option to avoid Service downtime in toTriplyDb() function TriplyETL 4.1.7 [Enhanced] Using NamedNodes and/or Literal as content for addHashedIri() Bug fixes TriplyETL 4.1.6 [Fixed] SHACL validate() with SPARQL target returned incorrect results TriplyETL 4.1.2 through 4.1.5 [Enhanced] Improved the timeouts handling for fromOai() extractor TriplyETL 4.1.1 [Changed] executeRules() supports only two arguments [Enhanced] Increased stack size [Added] CLI flag --keep-tmp-dir to save temporary data directory TriplyETL 4.0.0 [Changed] IRI-related middlewares no longer use skolem URLs [Changed] fromShapeFile() is now called fromShapefile() [Removed] Function addRandomIri() [Added] New variables added to ETL TriplyETL 3.1.0 && 3.1.1 [Deprecated] Deprecated fromShapeFile() for fromShapefile() [Deprecated] Deprecated addRandomIri() function. [Enhanced] Improved SHACL report. [Enhanced] Improved objects() function [Enhanced] RML middleware [Enhanced] Static vocabularies [Enhanced] NPM packages [Fixed] Base IRI when using loadRdf() [Fixed] String encoding for IRIs [Fixed] New datatype added to addPoint() middleware TriplyETL 3.0.20 [Enhanced] Improved copySource() function [Enhanced] Prefix uploading TriplyETL 3.0.15 through 3.0.18 [Enhanced] RDF compression before upload [Enhanced] Skolem IRI prefix use Bug fixes TriplyETL 3.0.14 [Added] Support for RML [Build] Environments beyond the DTAP defaults Bug fixes TriplyETL 3.0.7 through 3.0.9 [Added] CLI flag to skip version check [Added] Support for JPath expressions [Added] Authentication for the OAI-PMH extractor [Added] XSLT support for the OAI-PMH extractor Bug fixes TriplyETL 3.0.6 Bug fixes TriplyETL 3.0.5 Bug fixes TriplyETL 3.0.4 [Added] Dataset metadata specification [CLI] Reverse logic for creating error traces Bug fixes TriplyETL 3.0.3 [Changed] Support for the NDE Dataset Register TriplyETL 3.0.2 [Added] Static statement assertion Bug fixes TriplyETL 3.0.1 [Enhanced] Source string validation [Enhanced] Synchronize specific services [Fixed] Bug fixes TriplyETL 3.0.0 [Added] Support for XSLT [Added] Support for the SPARQL Select and SPARQL Ask queries [Enhanced] Simplified usage of 'nestedPairs()' [Changed] Automatic prefix handling in TriplyDB using 'toRdf()' [Changed] New approach to prefix handling in TriplyETL [Changed] New package '@triplyetl/vocabularies' [Changed] RDF serialization parsing with 'loadRdf()' [Changed] Extended log and terminal output for ETL debugging [Changed] 'toRdf()' for account-based token access [Changed] Relocation middleware: 'resetStore()' and 'randomKey()' [Changed] Record selection with '--offset' and '--limit' [Changed] Removal of 'mapQuads()' [Changed] Warning for old Node.JS versions [Changed] SHACL Validation Engine [Changed] Trace for large records [Changed] Transition to in-memory engine Speedy [Enhanced] Improvements to ETL logs [Enhanced] Prevent using multiple extractors [Enhanced] Better error reporting for CSV, TSV, and XML sources. [Enhanced] Default CRS for 'wkt.addPoint()' [Enhanced] Handle conflicting TriplyDB instance specifications [Enhanced] More information for failing HTTP calls Bug fixes TriplyETL 2.0.7 through 2.0.19 Bug fixes TriplyETL 2.0.6 [Added] Support for the PREMIS vocabulary [Added] New debug function logMemory() [Added] Support for the 'ListIdentifiers' verb in the OAI-PMH extractor TriplyETL 2.0.5 [Changed] New default engine for SPARQL Construct [Added] New CLI tool for comparing graphs Bug fixes TriplyETL 2.0.4 [Enhanced] Better output for graph comparison TriplyETL 2.0.3 Bug fixes TriplyETL 2.0.2 Bug fixes TriplyETL 2.0.1 [Added] Timeout flag for TriplyETL Runner TriplyETL 2.0.0 [Changed] Modules infrastructure moves from CommonJS to ESM [Changed] Debug functions move to a new module [Enhanced] Better error messages when things go wrong Bug fixes TriplyETL 1.0.x Changelog \u00b6 You can use this changelog to perform a safe update from an older version of TriplyETL to a newer one. See the documentation for Upgrading TriplyETL repositories for the advised approach, and how the changelog factors into that. TriplyETL 4.10.0 \u00b6 Release date: 2025-04-01 [Added] \u00b6 Add uniqueBy option to list Support node 22 [Fixed] \u00b6 DELETE is not working in the MWE of the documentation SHACL validation broken all together SPARQL targets bleeding to next record TriplyETL 4.8.12 \u00b6 Release date: 2024-06-24 Cleaning up types and upgradeing TriplyDB-js to 8.2.1 TriplyETL 4.4.7 \u00b6 Release date: 2024-07-04 Added logInterval property to fromOai. This is useful in situations where TriplyETL cannot render a progress bar (because the OAI endpoint does not return a total-results value). In such cases you can use this property to render a status update every x number of records. Improved logging information for errors TriplyETL 4.4.6 \u00b6 Release date: 2024-06-24 [Fixed] Minor clean-ups \u00b6 Replacing rdf-js for @rdfjs/types. Adding a resolution for what-wg and rimraf so that there are no deprecation warnings for those modules. Removing some old yarn specific commands, such as pinst. TriplyETL 4.4.5 \u00b6 Release date: 2024-06-12 [Fixed] TriplyDB-JS instantiation \u00b6 Ensure TriplyDB-JS is instantiated properly when proxy settings are passed to TriplyETL TriplyETL 4.4.0 \u00b6 Release date: 2024-06-04 [Added] linked data event stream middleware (LDES) \u00b6 TriplyETL 4.2.0 \u00b6 Release date: 2024-05-10 [Added] retry mechanism for OAI \u00b6 Added retry-mechanism to from-oai. By default, all OAI requests now retry 3 times TriplyETL 4.1.16 \u00b6 Release date: 2024-05-09 [Fixed] SHACL version update and fixing now() in SHACL rules engine \u00b6 TriplyETL 4.1.15 \u00b6 Release date: 2024-05-08 [Fixed] large default timeout for OAI requests \u00b6 Add (very) large default timeout for oai requests, to avoid possibly hanging when server does not respond [Added] support for parsing multiple RDF/XML docs from OAI \u00b6 TriplyETL 4.1.14 \u00b6 Release date: 2024-04-18 [Fixed] SHACL dependency version \u00b6 Update SHACL dependency, with some performance improvements TriplyETL 4.1.13 \u00b6 Release date: 2024-04-18 [Fixed] hanging ETL and preparations for a simplified eslint / prettier setup \u00b6 TriplyETL 4.1.12 \u00b6 Release date: 2024-04-16 [Fixed] SHACL executeRules() and added Speedy as a peer dependency \u00b6 Includes fix regarding SPARQL Functions used in executeRules() , the used Speedy SPARQL engine is now a peer dependency, which will use the same version as the one used as in the @triplyetl/etl package TriplyETL 4.1.11 \u00b6 Release date: 2024-03-27 [Added] multiple hashing algorithms to hashedIri() \u00b6 hashedIri() now supports SHA1, SHA256, SHA384 & SHA512 hashtypes, next to the existing (and still default) MD5 hashtype. See issue #390 . Improvements in the SHACL performance and usage. TriplyETL 4.1.10 \u00b6 Release date: 2024-03-12 [Fixed] performance issue with validate() \u00b6 Because of a previous fix, the validate middleware become slower. This fix makes it performant again. TriplyETL 4.1.9 \u00b6 Release date: 2024-03-12 [Fixed] issue with fromOai() in combination with metadataYieldsRdf \u00b6 Fixed issue where fromOai() middleware reported an error when using metadataYieldsRdf and OAI response contained exactly 1 record. TriplyETL 4.1.8 \u00b6 Release date: 2024-03-10 [Added] preventServiceDowntime option to avoid Service downtime in toTriplyDb() function \u00b6 You can now update services on TriplyDB without experiencing any downtime. Once data uploading is complete, each service will be recreated using a temporary name and the same configuration as the outdated service. Once the temporary service is up and running, the outdated one will be removed. toTriplyDb({dataset: 'my-dataset', opts: { synchronizeServices: ['my-elastic-service', 'my-jena-service'], preventServiceDowntime: true } }) The execution of the above snippet will result in the following console output: Warning Service my-elastic-service of type elasticSearch with status running is out of sync. Info Creating temporary elasticSearch service triplyetl-temp-1710169198327 for my-elastic-service. Warning Service my-jena-service of type jena with status running is out of sync. Info Creating temporary jena service triplyetl-temp-1710169198339 for my-jena-service. Info Swapping service my-jena-service with triplyetl-temp-1710169198339 Info Service my-jena-service updated in 1 minute, Info Swapping service my-elastic-service with triplyetl-temp-1710169198327 Info Service my-elastic-service updated in 2 minutes, 7 seconds TriplyETL 4.1.7 \u00b6 Release date: 2024-03-09 [Enhanced] Using NamedNodes and/or Literal as content for addHashedIri() \u00b6 The addHashedIri() function now considers whether a NamedNode and/or Literal object is utilized to generate a hash. In such cases, the internal JSON representation is no longer employed. Instead, we utilize the value property for a NamedNode or the combination of the value, language, and datatype value properties for a Literal. This enhancement aims to produce more consistent hashed IRIs over time. Bug fixes \u00b6 Using skipRest() in ifElse() and switch() middlewares have caused unexpected ETL execution. TriplyETL 4.1.6 \u00b6 Release date: 2024-03-07 [Fixed] SHACL validate() with SPARQL target returned incorrect results \u00b6 Each shape undergoes conversion to a SHACL Validator object only once during ETL to avoid reloading shapes from disk or string for every record. This approach isn't feasible when SPARQL target nodes alter the model, as it would result in adding those targets for each record. TriplyETL 4.1.2 through 4.1.5 \u00b6 Release date: 2024-03-01 [Enhanced] Improved the timeouts handling for fromOai() extractor \u00b6 This enhancement resolves timeout errors that occurred with requests taking an extended period to respond. By utilizing a custom Fetch Agent from undici package, we've eliminated internal timeouts. TriplyETL 4.1.1 \u00b6 Release date: 2024-02-18 [Changed] executeRules() supports only two arguments \u00b6 To set a maximum number of iterations of the execution of the SHACL rules the maxIterations or errorOnMaxIterations needs to be specified in the executeRules() function. [Enhanced] Increased stack size \u00b6 Maximum call stack size has increased to load the large datasets without trowing an errors. [Added] CLI flag --keep-tmp-dir to save temporary data directory \u00b6 Introduced the cli flag --keep-tmp-dir in order to store all temporary files disregarding the completion status. This allows the user to debug ETL's by studying the intermediate files the ETL has created. TriplyETL 4.0.0 \u00b6 Release date: 2024-01-29 [Changed] IRI-related middlewares no longer use skolem URLs \u00b6 The following middlewares: addHashedIri() , addIri() , addRandomIri() , would no longer allow users to create URLs that have pathnames start with \"/.well-known/genid/\", since they would be consideres skolemised URLs. [Changed] fromShapeFile() is now called fromShapefile() \u00b6 The format is called ESRI Shapefile, hence our extractor function's name had to be changed from fromShapeFile() to fromShapefile() . [Removed] Function addRandomIri() \u00b6 Since function addRandomIri() does not add anything beyond addSkolemIri() , the function has been removed from the TriplyETL library. Random IRIs should be skolem IRIs that can be readily replaced by blank nodes. [Added] New variables added to ETL \u00b6 New flag has been introduced when constructing an ETL: /** * Timeout ETL after set duration in milliseconds */ timeout: number; /** * If set to TRUE, the ETL will do a hard exit, preventing uploads to TDB on timeouts */ exitOnTimeout: boolean; which can be set as following: const etl = new Etl({timeout: 1000, exitOnTimeout: true}) This will cause a hard exit when a timeout occurs and nothing will be executed after this timeout. TriplyETL 3.1.0 && 3.1.1 \u00b6 Release date: 2024-01-15 && 2024-01-17 [Deprecated] Deprecated fromShapeFile() for fromShapefile() \u00b6 [Deprecated] Deprecated addRandomIri() function. \u00b6 Function addRandomIri() does not add anything beyond addSkolemIri() . Random IRIs should be skolem IRIs that can be readily replaced by blank nodes. [Enhanced] Improved SHACL report. \u00b6 When a SHACL shape is used to validate data does by itself not conform to the SHACL-SHACL shape, the report of that non-conforming shape is now printed. [Enhanced] Improved objects() function \u00b6 The objects() middleware now requires a minimum of 2 objects, deviating from its previous behavior, which was limited to functionality similar to the triple() function. [Enhanced] RML middleware \u00b6 RML map() middleware now allows a string Source and a string primitive as input. [Enhanced] Static vocabularies \u00b6 With the latest update, TriplyETL vocabularies are now represented as Vocabulary objects, replacing the previous usage of objects with the type IRI . This change may necessitate adjustments to existing ETLs that utilize static vocabularies, such as aat . In this case, the vocabulary would need to be updated to aat.toIri() to ensure compatibility with the correct type. [Enhanced] NPM packages \u00b6 All NPM packages are up to date with their latest version. [Fixed] Base IRI when using loadRdf() \u00b6 There were some inconsistency between the expected base IRI. For example, the following snippet: import { logQuads } from '@triplyetl/etl/debug' import { Etl, loadRdf, Source } from '@triplyetl/etl/generic' export default async function (): Promise<Etl> { const etl = new Etl() etl.use( loadRdf(Source.string('<s><p><o>.')), logQuads(), ) return etl } would result in: <https://triplydb.com/graph/default> { <https://triplydb.com/graph/s> <https://triplydb.com/graph/p> <https://triplydb.com/graph/o> } rather than: <https://triplydb.com/graph/default> { <https://triplydb.com/s> <https://triplydb.com/p> <https://triplydb.com/o> } This issue has been fixed. [Fixed] String encoding for IRIs \u00b6 It is now possible to check whether a value of a key used to create an IRI contains valid characters. A previous warning incorrectly flagged a space (' ') as an invalid character in the IRI, but that has been taken care of that. Now, when you run the script, you won't encounter the misleading warning, providing a more accurate and hassle-free execution. In this case, [1] is resulting in [2] instead of invalid [3]: [1] a b [2] http://ex.com/a%20b [3] http://ex.com/ a b As well as [4] being encoded as [5]: [4] a&b [5] a&amp;b Or [6] can be legitimately encoded in CSV using [7]: [6] a,b [7] \"a,b\" [Fixed] New datatype added to addPoint() middleware \u00b6 Datatype wktLiteral has been added to the addPoint() middleware. TriplyETL 3.0.20 \u00b6 Release date: 2024-01-04 [Enhanced] Improved copySource() function \u00b6 Function etl.copySource() accepts the same destination format as toTriplyDB(), so that the same destination does not need to be specified twice. [Enhanced] Prefix uploading \u00b6 Prefixes are no longer uploaded by default, only explicit prefixes that are defined when constructing an ETL with new Etl({ prefixes }) . TriplyETL 3.0.15 through 3.0.18 \u00b6 Release date: 2023-12-07 through 2023-12-28 [Enhanced] RDF compression before upload \u00b6 It is now possible to enable compression of RDF data before being uploaded to TriplyDB. See the toRdf() function for more information. [Enhanced] Skolem IRI prefix use \u00b6 TriplyETL now emits an error when a Skolem IRI prefix is used with addHashedIri() . Bug fixes \u00b6 This release provides bug fixes to XSLT support (see XSLT Transformations and XSLT Assertions ). TriplyETL 3.0.14 \u00b6 Release date: 2023-12-04 [Added] Support for RML \u00b6 This release introduces support for the RML transformation and assertion language. RML is an ETL configuration language that has gained traction in the linked data community over the last couple of years. See the following pages for more information: RML Transformations RML Assertions [Build] Environments beyond the DTAP defaults \u00b6 It is now possible to extend the standard environments offered by TriplyETL. Bug fixes \u00b6 This release fixes a URL/request-related error in the fromOai extractor. TriplyETL 3.0.7 through 3.0.9 \u00b6 Release date: 2023-11-29 [Added] CLI flag to skip version check \u00b6 Introduced the cli flag --skip-version-check because some users can not use remote connections because of security policies. [Added] Support for JPath expressions \u00b6 toJson() middleware now uses path selectors just as fromXml() , but also JPath expressions. [Added] Authentication for the OAI-PMH extractor \u00b6 fromOai() now accepts a Request object as the value for the url option, allowing more fine graded use of the HTTP request (including authentication information). [Added] XSLT support for the OAI-PMH extractor \u00b6 fromOai() now accepts an optional parameter stylesheet . The argument must be an object with 1 required key source pointing to the source of the stylesheet and 1 optional argument yieldsRdf . When provided the XSL Stylesheet source is processed using the OAI response. The result of the transformation must still be a valid OAI response. Bug fixes \u00b6 Fixes issue where keys that are used for internal administration were shown in logged records, after using the ifElse() and switch() control structures. TriplyETL 3.0.6 \u00b6 Release date: 2023-11-14 Bug fixes \u00b6 This release fixes issues with upstream packages that contained breaking changes. TriplyETL 3.0.5 \u00b6 Release date: 2023-11-09 Bug fixes \u00b6 The progress bar would sometimes terminate at 99% or 101%, instead of the expected 100%. TriplyETL 3.0.4 \u00b6 Release date: 2023-11-07 [Added] Dataset metadata specification \u00b6 It is now possible to use metadata when creating datasets. If a new dataset is created then the metadata is always used, for existing datasets you can choose to ignore, merge or replace existing metadata. This feature is to prevent ETLs accidentally overwriting metadata might have changed in the UI/Dashboard of their TriplyDB instance. The following options are available for this new feature in the opts.existingMetadata parameter: ignore : no metadata will be changed even if no metadata is present for this dataset (this is the default value) merge : only properties that have no value will be overwritten bij the provided metadata replace : all existing metadata will be replaced, even if the provided metadata contains empty keys [CLI] Reverse logic for creating error traces \u00b6 Before this release, running an ETL would always create an error trace file. It was possible to disable this behavior with CLI flag --skip-error-trace . Starting in this release, the error trace file is no longer created by default, and a newly added CLI flag --create-error-trace must now be specified in order ot create the error trace file. Bug fixes \u00b6 The following bugs were fixed: The number of synchronized services was not always reported correctly in CLI output. A package we depend on introduced a breaking change causing a function not to be there anymore. TriplyETL 3.0.3 \u00b6 Release date: 2023-11-01 [Changed] Support for the NDE Dataset Register \u00b6 The code to submit datasets to the NDE Dataset Register has been moved to TriplyDB-JS. The way to publish a dataset now is to add an option to the toTriplyDb() function: { submitToNDEDatasetRegister: true } . Example: toTriplyDb({dataset: 'nde', opts: {submitToNDEDatasetRegister: true}}) TriplyETL 3.0.2 \u00b6 Release date: 2023-10-23 [Added] Static statement assertion \u00b6 export default async function(): Promise<Etl> { const etl = new Etl({baseIri: Iri('https://example.com/')}) await etl.staticAssertions( pairs( iri(etl.standardGraphs.default), [a, dcat.Dataset], [skos.prefLabel, literal(str(\"Family Doe\"), lang.en)], [dct.created, literal(str(new Date().toISOString()), xsd.dateTime)], ), ); await etl.staticAssertions( pairs(iri(etl.standardGraphs.default), [skos.prefLabel, literal(str(\"Familie Doe\"), lang.nl)]), ); etl.use( fromJson([{ name: \"John Doe\" }, { name: \"Jane Doe\" }]), triple(iri(etl.standardPrefixes.id, \"$recordId\"), sdo.name, \"name\"), logQuads(), ); return etl } You can now assert so called \"static triples\": triples that are not related to the source extractors but should only be asserted once per ETL. Bug fixes \u00b6 There was an error in the ifElse() control structure, that caused ETLs to not use the fallback 'else' block in some situations. TriplyETL 3.0.1 \u00b6 Release date: 2023-10-19 [Enhanced] Source string validation \u00b6 The addLiteral() function can now validate string data that occurs in the Record. Such validation can be used in addition to validation in the Internal Store (graph comparison and SHACL validation). Look at the documentation of addLiteral() for more information. [Enhanced] Synchronize specific services \u00b6 When publishing linked data to TriplyDB, it is now possible to synchronize one specific service. This is specifically useful in case an Acceptance and a Production service are used, and only the former should be synchronized. See the documentation for publishing to remote data destinations for more information. [Fixed] Bug fixes \u00b6 The following bugs have been fixed: The progress bar would sometimes go over 100%. the error report file ( etl.err ) would sometimes contain sentinel keys like $sentinel-${MD5-HASH} . These sentinel keys are used for internal bookkeeping in TriplyETL, and are no longer part of the Record. Some XSLT transformations failed on Windows, because of incorrect redirecting of error messages. TriplyETL 3.0.0 \u00b6 Release date: 2023-10-12 [Added] Support for XSLT \u00b6 Support was added for the Extensible Stylesheet Language Transformations (XSLT) configuration language. This can be used in the following two functions: The fromXml() extractor function, for XML sources that transformed to regular XML. The loadRdf() function, for XML sources that are transformed into RDF/XML. In both cases, this functionality is used by configuring the stylesheet parameter with an XSLT Stylesheet (e.g. using Source.file() ). Example code that uses fromXml() : fromXml(Source.file(XMLFile), { selectors: \"rdf:RDF.sdo:Person\", stylesheet: Source.file(XSLTStylesheet), }), Example code that uses loadRdf() : loadRdf(Source.file(XMLFile), { contentType: \"application/rdf+xml\", stylesheet: Source.file(XSLTStylesheet), }), [Added] Support for the SPARQL Select and SPARQL Ask queries \u00b6 The extractors fromCsv() , fromJson() , fromTsv() and fromXml() now support SPARQL Select queries. The extractors fromJson() and fromXml() also support SPARQL Ask queries. The example below hows how to use a SPARQL ask query in the fromJson() extractor: fromJson(Source.TriplyDb.query('account', 'query-name', { triplyDb: { url: 'https://api.triplydb.com' } })) [Enhanced] Simplified usage of 'nestedPairs()' \u00b6 The nestedPairs() middleware can be used without providing the subject node that connects the pairs to the object/predicate. This will automatically create a Skolem IRI for the subject: nestedPairs(S, P, [a, sdo.Person]) For example: fromJson([{ id: '1', height: 15 }]), addSkolemIri({ prefix: prefix.skolem, key: '_height', }), nestedPairs(iri(prefix.product, 'id'), sdo.height, [qudt.unit, unit.CentiM], [rdf.value, 'height'], ), Will result in the following linked data assertions: product:1 sdo:height [ qudt:unit unit:CentiM; rdf:value 15 ]. [Changed] Automatic prefix handling in TriplyDB using 'toRdf()' \u00b6 Manually specified and standard prefixes are automatically added to TriplyDB when toRdf() is used. The middleware uploadPrefixes() is removed. [Changed] New approach to prefix handling in TriplyETL \u00b6 Prefixes are no longer defined as function that concatenates a value to an Iri. The Iri is a new type of Object in TriplyETL, that has a concat() method which allows you to add a value to the first part of an Iri. For example: const baseIri = Iri('https://example.com/') const prefixId = baseIri.concat('id/') const johnDoe = prefixId.concat('john-doe') [Changed] New package '@triplyetl/vocabularies' \u00b6 The vocabularies and languages are no longer part of @triplyetl/etl package. A new module has been released: @triplyetl/vocabularies : Individual vocabularies like rdf and schema.org can be imported in the following way: import { a, rdf, sdo } from '@triplyetl/vocabularies' To import all vocabularies, use: import * as vocab from \"@triplyetl/vocabularies\" Some vocabularies are too large to include, but they can still be used like this: import { aat } from '@triplyetl/vocabularies' const moustache = aat.concat('300379271') or import { aat } from '@triplyetl/vocabularies' addIri({prefix: aat, content: str('300379271'), key: 'moustache'}) To use the RATT lang tools : Import languages : import { languages } from '@triplyetl/vocabularies' Import languages and region : import { region, language } from '@triplyetl/vocabularies' const nl_BE = language.nl.addRegion(region.BE) [Changed] RDF serialization parsing with 'loadRdf()' \u00b6 The loadRdf() function is able to parse known RDF serializations ( Turtle , TriG , N-Triples , N-Quads ) provided as a string without specifying mimetype. const data = Source.string('...') loadRdf(data) [Changed] Extended log and terminal output for ETL debugging \u00b6 The output of the logfile and terminal output is changed. It contains more information to help users debugging ETL's. The format of time representation is now H:i:s.u where: H : 24-hour format of an hour with leading zeros (00 through 23) i : Minutes with leading zeros (00 to 59) s : Seconds with leading zeros (00 through 59) u : Microseconds (example: 654321) [Changed] 'toRdf()' for account-based token access \u00b6 The toRdf() middleware now accepts \"me\" as account name based on the token. Below are some examples of this being used. toTriplyDb({account: \"me\", dataset: \"myDataset\"}) loadRdf(Source.TriplyDb.rdf(\"me\", datasetName)) Destination.TriplyDb.rdf(\"me\", datasetName) [Changed] Relocation middleware: 'resetStore()' and 'randomKey()' \u00b6 The resetStore() middleware is now moved from ratt to the generic namespace . The randomKey() middleware moved from generic to ratt . [Changed] Record selection with '--offset' and '--limit' \u00b6 You can now use --offset and --limit instead of --from-record-id and --head , e.g. LIMIT=1 OFFSET=8 npx etl . The old arguments can still be used for backwards compatibility. [Changed] Removal of 'mapQuads()' \u00b6 The mapQuads() function was removed. [Changed] Warning for old Node.JS versions \u00b6 If the users Node.JS version is older that the recommended version (currently >=18.0.0) a warning is shown. [Changed] SHACL Validation Engine \u00b6 A SHACL Validation Engine improved performance. [Changed] Trace for large records \u00b6 A new flag now bypasses generating the trace for very large records: ---skip-error-trace . Thus, no trace file is created. [Changed] Transition to in-memory engine Speedy \u00b6 Comunica is no longer part of TriplyETL, the in-memory engine is now Triply's Speedy. [Enhanced] Improvements to ETL logs \u00b6 The logging format was improved by including the following information: the TriplyETL version the Node.js version the DTAP mode the start date and time the end date and time [Enhanced] Prevent using multiple extractors \u00b6 TriplyETL only supports one extractor per ETL configuration object. In the past, it was possible to use multiple extractors, which would result in faulty behavior during ETL runs. Starting in this release, TriplyETL will emit an error when multiple extractors are used. [Enhanced] Better error reporting for CSV, TSV, and XML sources. \u00b6 In previous releases, the extractor functions fromCsv() , fromTsv() , and fromXml() would not emit the file name in case an error occurred. This was specifically problematic when a large number of data source files were used. Starting in this release, the file name is included in error message. [Enhanced] Default CRS for 'wkt.addPoint()' \u00b6 In previous releases, the Coordinate Reference System (CRS) was a required attribute for transformation function wkt.addPoint() . Starting in this release, the CRS argument has become optional. When not specified, the default CRS http://www.opengis.net/def/crs/OGC/1.3/CRS84 is used. [Enhanced] Handle conflicting TriplyDB instance specifications \u00b6 In previous releases, it was possible to introduce an ambiguity in specify the TriplyDB instance to publish data to. This was possible by (1) specifying a TriplyDB API Token in the environment (e.g. though an .env file), and (2) by configuring the triplyDb option in the loadRdf() function. Starting in this release, TriplyETL will emit an error if the TriplyDB instance in the API Token differs from the TriplyDB instance configured in the triplyDb option. [Enhanced] More information for failing HTTP calls \u00b6 In previous releases, when a failing HTTP call resulted in an error message, only the body of that HTTP call would be included in the error message. Starting in this release, the HTTP status code of the failing HTTP call is included in the error message as well. Bug fixes \u00b6 This release fixes several out-of-memory bugs in the SHACL validation function . TriplyETL 2.0.7 through 2.0.19 \u00b6 Release dates: 2023-06-17 through 2023-09-29 Bug fixes \u00b6 The following bugs were fixed: Processing an Excel sheet with fromXml() would sometimes consume too much memory. Several installation issues on Windows have been resolved. The async-saxophone library for XML processing was adjusted to support the current LTS version of Node.js (v18). TriplyETL 2.0.6 \u00b6 Release date: 2023-06-07 [Added] Support for the PREMIS vocabulary \u00b6 Support was added for the PREMIS 3.0.0 vocabulary. This vocabulary is published by the Library of Congress and can be used to publish metadata about the preservation of digital objects. See the PREMIS documentation for more information. The vocabulary can be imported from the 'vocab' module: import { premis } from '@triplyetl/vocabularies' See the documentation on external vocabulary declarations for more information. [Added] New debug function logMemory() \u00b6 A new debug function logMemory() is added. This function prints an overview of the current memory usage of TriplyETL. This allows users to detect fluctuations in memory consumption inside their pipelines. [Added] Support for the 'ListIdentifiers' verb in the OAI-PMH extractor \u00b6 The fromOai() extractor already supported the ListRecords verb. This release adds support for the ListIdentifiers verb as well. This new verb allows users to stream through the headers of all records in an OAI-PMH collection, without requiring the full record (i.e. body) to be retrieved. TriplyETL 2.0.5 \u00b6 Release date: 2023-05-25 [Changed] New default engine for SPARQL Construct \u00b6 The default engine for evaluating SPARQL Construct queries (function construct() ) has changed from Comunica to Speedy. Speedy is a new SPARQL engine that is developed by Triply. Comunica is an open source engine that is developed by the open source community. Since SPARQL is a standardized query language, this change should not cause a difference in behavior for your ETL pipelines. In the unexpected case where an ETL pipeline is negatively affected by this change, the old situation can be restored by explicitly configuring the Comunica engine: import { construct } from '@triplyetl/etl/sparql' construct(Source.TriplyDb.query('my-query'), { sparqlEngine: 'comunica' }), The benefit of switching to the Speedy engine is that this engine is expected to be faster for most queries. Overall, this change will therefore result in speed improvements for your TriplyETL pipelines. [Added] New CLI tool for comparing graphs \u00b6 The new CLI tool compare allows graph comparison to be performed from the command-line. This uses the same algorithm that is used by the compareGraphs() validation function. Bug fixes \u00b6 This release fixes the following bugs: fromXlsx() did not remove trailing whitespace in cell values. When a SHACL result was printed, an incorrect message about a faulty SHACL model was shown. Some RDF processors did not handle empty RDF inputs correctly. TriplyETL 2.0.4 \u00b6 Release date: 2023-05-11 [Enhanced] Better output for graph comparison \u00b6 Before this release, when two graphs were not isomorph and their difference consisted of a mapping from blank nodes onto blank nodes exclusively, an empty difference message was communicated. From this release onwards, the difference message is non-empty, and specifically indicates the difference between the non-isomorphic graphs in terms of the mismatching blank nodes. Look at this example from the graph comparison documentation, which emits such a difference message. TriplyETL 2.0.3 \u00b6 Release date: 2023-05-10 Bug fixes \u00b6 This release includes the following bug fixes: Error location information is not shown in TriplyETL Runner. Issue when a URL data source ( Source.url() ) includes an HTTP body. TriplyETL 2.0.2 \u00b6 Release date: 2023-05-09 Bug fixes \u00b6 This release fixes bugs related to the recent switch from CommonJS to ESM: Dynamic import bug on Windows. Error reporting issues due to ESM imports. TriplyETL 2.0.1 \u00b6 Release date: 2023-05-03 [Added] Timeout flag for TriplyETL Runner \u00b6 The TriplyETL Runner is the CLI tool that is used to run ETL pipelines. Starting with this version, you can specify a --timeout flag when using the TriplyETL Runner. When the indicated timeout is reached before the pipeline finishes, the TriplyETL Runner will gracefully terminate the ETL by acting as if there are no more incoming records. See the TriplyETL Runner documentation page for more information. TriplyETL 2.0.0 \u00b6 Release date: 2023-05-01 [Changed] Modules infrastructure moves from CommonJS to ESM \u00b6 Before this release, TriplyETL used CommonJS modules to modularize its functionality into different components. Starting in this release, ECMAScript Modules (ESM) are used to modularize TriplyETL functionality into different modules. ESM is a more modern approach for modularizing ECMAScript (JavaScript, TypeScript, and Node.js) code. While CommonJS imports are evaluated at runtime, ESM imports are evaluated at compile time. TriplyETL users benefit from this change, since error messages related to module imports will be detected much earlier in the development process. All documentation examples were update to use ESM syntax for module imports, for example: import { logRecord } from '@triplyetl/etl/debug' [Changed] Debug functions move to a new module \u00b6 Before this release, debug functions like logRecord() and startTrace() were part of the RATT module. Since debug functions can be used in combination with any ETL configuration approach, they were moved to a new module. The debug functions are imported from their new module in the following way: import { logRecord, traceEnd, traceStart } from '@triplyetl/etl/debug' [Enhanced] Better error messages when things go wrong \u00b6 This release introduces a new approach for communicating errors back to the user. When TriplyETL functionality detects an error condition, a unified 'trace middleware' is now used to retrieve information from the environment in which the error occurred. This information is then printed to the error output stream for communication with the user. Bug fixes \u00b6 The following bug fixes are included in this release: Incorrect behavior of the _switch() control function . The fromOai() extractor now communicates clearer when the accessed OAI-PMH endpoint encounters any issues. When a key with a NULL value was accessed, the name of that key is now included in the error message. This makes it easier for users to find the NULL value in their source data. TriplyETL 1.0.x \u00b6 TriplyETL 1.0.0 was released on 2023-03-20.","title":"Changelog"},{"location":"triply-etl/generic/changelog/#changelog","text":"You can use this changelog to perform a safe update from an older version of TriplyETL to a newer one. See the documentation for Upgrading TriplyETL repositories for the advised approach, and how the changelog factors into that.","title":"Changelog"},{"location":"triply-etl/generic/changelog/#triplyetl-4100","text":"Release date: 2025-04-01","title":"TriplyETL 4.10.0"},{"location":"triply-etl/generic/changelog/#added","text":"Add uniqueBy option to list Support node 22","title":"[Added]"},{"location":"triply-etl/generic/changelog/#fixed","text":"DELETE is not working in the MWE of the documentation SHACL validation broken all together SPARQL targets bleeding to next record","title":"[Fixed]"},{"location":"triply-etl/generic/changelog/#triplyetl-4812","text":"Release date: 2024-06-24 Cleaning up types and upgradeing TriplyDB-js to 8.2.1","title":"TriplyETL 4.8.12"},{"location":"triply-etl/generic/changelog/#triplyetl-447","text":"Release date: 2024-07-04 Added logInterval property to fromOai. This is useful in situations where TriplyETL cannot render a progress bar (because the OAI endpoint does not return a total-results value). In such cases you can use this property to render a status update every x number of records. Improved logging information for errors","title":"TriplyETL 4.4.7"},{"location":"triply-etl/generic/changelog/#triplyetl-446","text":"Release date: 2024-06-24","title":"TriplyETL 4.4.6"},{"location":"triply-etl/generic/changelog/#fixed-minor-clean-ups","text":"Replacing rdf-js for @rdfjs/types. Adding a resolution for what-wg and rimraf so that there are no deprecation warnings for those modules. Removing some old yarn specific commands, such as pinst.","title":"[Fixed] Minor clean-ups"},{"location":"triply-etl/generic/changelog/#triplyetl-445","text":"Release date: 2024-06-12","title":"TriplyETL 4.4.5"},{"location":"triply-etl/generic/changelog/#fixed-triplydb-js-instantiation","text":"Ensure TriplyDB-JS is instantiated properly when proxy settings are passed to TriplyETL","title":"[Fixed] TriplyDB-JS instantiation"},{"location":"triply-etl/generic/changelog/#triplyetl-440","text":"Release date: 2024-06-04","title":"TriplyETL 4.4.0"},{"location":"triply-etl/generic/changelog/#added-linked-data-event-stream-middleware-ldes","text":"","title":"[Added] linked data event stream middleware (LDES)"},{"location":"triply-etl/generic/changelog/#triplyetl-420","text":"Release date: 2024-05-10","title":"TriplyETL 4.2.0"},{"location":"triply-etl/generic/changelog/#added-retry-mechanism-for-oai","text":"Added retry-mechanism to from-oai. By default, all OAI requests now retry 3 times","title":"[Added] retry mechanism for OAI"},{"location":"triply-etl/generic/changelog/#triplyetl-4116","text":"Release date: 2024-05-09","title":"TriplyETL 4.1.16"},{"location":"triply-etl/generic/changelog/#fixed-shacl-version-update-and-fixing-now-in-shacl-rules-engine","text":"","title":"[Fixed] SHACL version update and fixing now() in SHACL rules engine"},{"location":"triply-etl/generic/changelog/#triplyetl-4115","text":"Release date: 2024-05-08","title":"TriplyETL 4.1.15"},{"location":"triply-etl/generic/changelog/#fixed-large-default-timeout-for-oai-requests","text":"Add (very) large default timeout for oai requests, to avoid possibly hanging when server does not respond","title":"[Fixed] large default timeout for OAI requests"},{"location":"triply-etl/generic/changelog/#added-support-for-parsing-multiple-rdfxml-docs-from-oai","text":"","title":"[Added] support for parsing multiple RDF/XML docs from OAI"},{"location":"triply-etl/generic/changelog/#triplyetl-4114","text":"Release date: 2024-04-18","title":"TriplyETL 4.1.14"},{"location":"triply-etl/generic/changelog/#fixed-shacl-dependency-version","text":"Update SHACL dependency, with some performance improvements","title":"[Fixed] SHACL dependency version"},{"location":"triply-etl/generic/changelog/#triplyetl-4113","text":"Release date: 2024-04-18","title":"TriplyETL 4.1.13"},{"location":"triply-etl/generic/changelog/#fixed-hanging-etl-and-preparations-for-a-simplified-eslint-prettier-setup","text":"","title":"[Fixed] hanging ETL and preparations for a simplified eslint / prettier setup"},{"location":"triply-etl/generic/changelog/#triplyetl-4112","text":"Release date: 2024-04-16","title":"TriplyETL 4.1.12"},{"location":"triply-etl/generic/changelog/#fixed-shacl-executerules-and-added-speedy-as-a-peer-dependency","text":"Includes fix regarding SPARQL Functions used in executeRules() , the used Speedy SPARQL engine is now a peer dependency, which will use the same version as the one used as in the @triplyetl/etl package","title":"[Fixed] SHACL executeRules() and added Speedy as a peer dependency"},{"location":"triply-etl/generic/changelog/#triplyetl-4111","text":"Release date: 2024-03-27","title":"TriplyETL 4.1.11"},{"location":"triply-etl/generic/changelog/#added-multiple-hashing-algorithms-to-hashediri","text":"hashedIri() now supports SHA1, SHA256, SHA384 & SHA512 hashtypes, next to the existing (and still default) MD5 hashtype. See issue #390 . Improvements in the SHACL performance and usage.","title":"[Added] multiple hashing algorithms to hashedIri()"},{"location":"triply-etl/generic/changelog/#triplyetl-4110","text":"Release date: 2024-03-12","title":"TriplyETL 4.1.10"},{"location":"triply-etl/generic/changelog/#fixed-performance-issue-with-validate","text":"Because of a previous fix, the validate middleware become slower. This fix makes it performant again.","title":"[Fixed] performance issue with validate()"},{"location":"triply-etl/generic/changelog/#triplyetl-419","text":"Release date: 2024-03-12","title":"TriplyETL 4.1.9"},{"location":"triply-etl/generic/changelog/#fixed-issue-with-fromoai-in-combination-with-metadatayieldsrdf","text":"Fixed issue where fromOai() middleware reported an error when using metadataYieldsRdf and OAI response contained exactly 1 record.","title":"[Fixed] issue with fromOai() in combination with metadataYieldsRdf"},{"location":"triply-etl/generic/changelog/#triplyetl-418","text":"Release date: 2024-03-10","title":"TriplyETL 4.1.8"},{"location":"triply-etl/generic/changelog/#added-preventservicedowntime-option-to-avoid-service-downtime-in-totriplydb-function","text":"You can now update services on TriplyDB without experiencing any downtime. Once data uploading is complete, each service will be recreated using a temporary name and the same configuration as the outdated service. Once the temporary service is up and running, the outdated one will be removed. toTriplyDb({dataset: 'my-dataset', opts: { synchronizeServices: ['my-elastic-service', 'my-jena-service'], preventServiceDowntime: true } }) The execution of the above snippet will result in the following console output: Warning Service my-elastic-service of type elasticSearch with status running is out of sync. Info Creating temporary elasticSearch service triplyetl-temp-1710169198327 for my-elastic-service. Warning Service my-jena-service of type jena with status running is out of sync. Info Creating temporary jena service triplyetl-temp-1710169198339 for my-jena-service. Info Swapping service my-jena-service with triplyetl-temp-1710169198339 Info Service my-jena-service updated in 1 minute, Info Swapping service my-elastic-service with triplyetl-temp-1710169198327 Info Service my-elastic-service updated in 2 minutes, 7 seconds","title":"[Added] preventServiceDowntime option to avoid Service downtime in toTriplyDb() function"},{"location":"triply-etl/generic/changelog/#triplyetl-417","text":"Release date: 2024-03-09","title":"TriplyETL 4.1.7"},{"location":"triply-etl/generic/changelog/#enhanced-using-namednodes-andor-literal-as-content-for-addhashediri","text":"The addHashedIri() function now considers whether a NamedNode and/or Literal object is utilized to generate a hash. In such cases, the internal JSON representation is no longer employed. Instead, we utilize the value property for a NamedNode or the combination of the value, language, and datatype value properties for a Literal. This enhancement aims to produce more consistent hashed IRIs over time.","title":"[Enhanced] Using NamedNodes and/or Literal as content for addHashedIri()"},{"location":"triply-etl/generic/changelog/#bug-fixes","text":"Using skipRest() in ifElse() and switch() middlewares have caused unexpected ETL execution.","title":"Bug fixes"},{"location":"triply-etl/generic/changelog/#triplyetl-416","text":"Release date: 2024-03-07","title":"TriplyETL 4.1.6"},{"location":"triply-etl/generic/changelog/#fixed-shacl-validate-with-sparql-target-returned-incorrect-results","text":"Each shape undergoes conversion to a SHACL Validator object only once during ETL to avoid reloading shapes from disk or string for every record. This approach isn't feasible when SPARQL target nodes alter the model, as it would result in adding those targets for each record.","title":"[Fixed] SHACL validate() with SPARQL target returned incorrect results"},{"location":"triply-etl/generic/changelog/#triplyetl-412-through-415","text":"Release date: 2024-03-01","title":"TriplyETL 4.1.2 through 4.1.5"},{"location":"triply-etl/generic/changelog/#enhanced-improved-the-timeouts-handling-for-fromoai-extractor","text":"This enhancement resolves timeout errors that occurred with requests taking an extended period to respond. By utilizing a custom Fetch Agent from undici package, we've eliminated internal timeouts.","title":"[Enhanced] Improved the timeouts handling for fromOai() extractor"},{"location":"triply-etl/generic/changelog/#triplyetl-411","text":"Release date: 2024-02-18","title":"TriplyETL 4.1.1"},{"location":"triply-etl/generic/changelog/#changed-executerules-supports-only-two-arguments","text":"To set a maximum number of iterations of the execution of the SHACL rules the maxIterations or errorOnMaxIterations needs to be specified in the executeRules() function.","title":"[Changed] executeRules() supports only two arguments"},{"location":"triply-etl/generic/changelog/#enhanced-increased-stack-size","text":"Maximum call stack size has increased to load the large datasets without trowing an errors.","title":"[Enhanced] Increased stack size"},{"location":"triply-etl/generic/changelog/#added-cli-flag-keep-tmp-dir-to-save-temporary-data-directory","text":"Introduced the cli flag --keep-tmp-dir in order to store all temporary files disregarding the completion status. This allows the user to debug ETL's by studying the intermediate files the ETL has created.","title":"[Added] CLI flag --keep-tmp-dir to save temporary data directory"},{"location":"triply-etl/generic/changelog/#triplyetl-400","text":"Release date: 2024-01-29","title":"TriplyETL 4.0.0"},{"location":"triply-etl/generic/changelog/#changed-iri-related-middlewares-no-longer-use-skolem-urls","text":"The following middlewares: addHashedIri() , addIri() , addRandomIri() , would no longer allow users to create URLs that have pathnames start with \"/.well-known/genid/\", since they would be consideres skolemised URLs.","title":"[Changed] IRI-related middlewares no longer use skolem URLs"},{"location":"triply-etl/generic/changelog/#changed-fromshapefile-is-now-called-fromshapefile","text":"The format is called ESRI Shapefile, hence our extractor function's name had to be changed from fromShapeFile() to fromShapefile() .","title":"[Changed] fromShapeFile() is now called fromShapefile()"},{"location":"triply-etl/generic/changelog/#removed-function-addrandomiri","text":"Since function addRandomIri() does not add anything beyond addSkolemIri() , the function has been removed from the TriplyETL library. Random IRIs should be skolem IRIs that can be readily replaced by blank nodes.","title":"[Removed] Function addRandomIri()"},{"location":"triply-etl/generic/changelog/#added-new-variables-added-to-etl","text":"New flag has been introduced when constructing an ETL: /** * Timeout ETL after set duration in milliseconds */ timeout: number; /** * If set to TRUE, the ETL will do a hard exit, preventing uploads to TDB on timeouts */ exitOnTimeout: boolean; which can be set as following: const etl = new Etl({timeout: 1000, exitOnTimeout: true}) This will cause a hard exit when a timeout occurs and nothing will be executed after this timeout.","title":"[Added] New variables added to ETL"},{"location":"triply-etl/generic/changelog/#triplyetl-310-311","text":"Release date: 2024-01-15 && 2024-01-17","title":"TriplyETL 3.1.0 &amp;&amp; 3.1.1"},{"location":"triply-etl/generic/changelog/#deprecated-deprecated-fromshapefile-for-fromshapefile","text":"","title":"[Deprecated] Deprecated fromShapeFile() for fromShapefile()"},{"location":"triply-etl/generic/changelog/#deprecated-deprecated-addrandomiri-function","text":"Function addRandomIri() does not add anything beyond addSkolemIri() . Random IRIs should be skolem IRIs that can be readily replaced by blank nodes.","title":"[Deprecated] Deprecated addRandomIri() function."},{"location":"triply-etl/generic/changelog/#enhanced-improved-shacl-report","text":"When a SHACL shape is used to validate data does by itself not conform to the SHACL-SHACL shape, the report of that non-conforming shape is now printed.","title":"[Enhanced] Improved SHACL report."},{"location":"triply-etl/generic/changelog/#enhanced-improved-objects-function","text":"The objects() middleware now requires a minimum of 2 objects, deviating from its previous behavior, which was limited to functionality similar to the triple() function.","title":"[Enhanced] Improved objects() function"},{"location":"triply-etl/generic/changelog/#enhanced-rml-middleware","text":"RML map() middleware now allows a string Source and a string primitive as input.","title":"[Enhanced] RML middleware"},{"location":"triply-etl/generic/changelog/#enhanced-static-vocabularies","text":"With the latest update, TriplyETL vocabularies are now represented as Vocabulary objects, replacing the previous usage of objects with the type IRI . This change may necessitate adjustments to existing ETLs that utilize static vocabularies, such as aat . In this case, the vocabulary would need to be updated to aat.toIri() to ensure compatibility with the correct type.","title":"[Enhanced] Static vocabularies"},{"location":"triply-etl/generic/changelog/#enhanced-npm-packages","text":"All NPM packages are up to date with their latest version.","title":"[Enhanced] NPM packages"},{"location":"triply-etl/generic/changelog/#fixed-base-iri-when-using-loadrdf","text":"There were some inconsistency between the expected base IRI. For example, the following snippet: import { logQuads } from '@triplyetl/etl/debug' import { Etl, loadRdf, Source } from '@triplyetl/etl/generic' export default async function (): Promise<Etl> { const etl = new Etl() etl.use( loadRdf(Source.string('<s><p><o>.')), logQuads(), ) return etl } would result in: <https://triplydb.com/graph/default> { <https://triplydb.com/graph/s> <https://triplydb.com/graph/p> <https://triplydb.com/graph/o> } rather than: <https://triplydb.com/graph/default> { <https://triplydb.com/s> <https://triplydb.com/p> <https://triplydb.com/o> } This issue has been fixed.","title":"[Fixed] Base IRI when using loadRdf()"},{"location":"triply-etl/generic/changelog/#fixed-string-encoding-for-iris","text":"It is now possible to check whether a value of a key used to create an IRI contains valid characters. A previous warning incorrectly flagged a space (' ') as an invalid character in the IRI, but that has been taken care of that. Now, when you run the script, you won't encounter the misleading warning, providing a more accurate and hassle-free execution. In this case, [1] is resulting in [2] instead of invalid [3]: [1] a b [2] http://ex.com/a%20b [3] http://ex.com/ a b As well as [4] being encoded as [5]: [4] a&b [5] a&amp;b Or [6] can be legitimately encoded in CSV using [7]: [6] a,b [7] \"a,b\"","title":"[Fixed] String encoding for IRIs"},{"location":"triply-etl/generic/changelog/#fixed-new-datatype-added-to-addpoint-middleware","text":"Datatype wktLiteral has been added to the addPoint() middleware.","title":"[Fixed] New datatype added to addPoint() middleware"},{"location":"triply-etl/generic/changelog/#triplyetl-3020","text":"Release date: 2024-01-04","title":"TriplyETL 3.0.20"},{"location":"triply-etl/generic/changelog/#enhanced-improved-copysource-function","text":"Function etl.copySource() accepts the same destination format as toTriplyDB(), so that the same destination does not need to be specified twice.","title":"[Enhanced] Improved copySource() function"},{"location":"triply-etl/generic/changelog/#enhanced-prefix-uploading","text":"Prefixes are no longer uploaded by default, only explicit prefixes that are defined when constructing an ETL with new Etl({ prefixes }) .","title":"[Enhanced] Prefix uploading"},{"location":"triply-etl/generic/changelog/#triplyetl-3015-through-3018","text":"Release date: 2023-12-07 through 2023-12-28","title":"TriplyETL 3.0.15 through 3.0.18"},{"location":"triply-etl/generic/changelog/#enhanced-rdf-compression-before-upload","text":"It is now possible to enable compression of RDF data before being uploaded to TriplyDB. See the toRdf() function for more information.","title":"[Enhanced] RDF compression before upload"},{"location":"triply-etl/generic/changelog/#enhanced-skolem-iri-prefix-use","text":"TriplyETL now emits an error when a Skolem IRI prefix is used with addHashedIri() .","title":"[Enhanced] Skolem IRI prefix use"},{"location":"triply-etl/generic/changelog/#bug-fixes_1","text":"This release provides bug fixes to XSLT support (see XSLT Transformations and XSLT Assertions ).","title":"Bug fixes"},{"location":"triply-etl/generic/changelog/#triplyetl-3014","text":"Release date: 2023-12-04","title":"TriplyETL 3.0.14"},{"location":"triply-etl/generic/changelog/#added-support-for-rml","text":"This release introduces support for the RML transformation and assertion language. RML is an ETL configuration language that has gained traction in the linked data community over the last couple of years. See the following pages for more information: RML Transformations RML Assertions","title":"[Added] Support for RML"},{"location":"triply-etl/generic/changelog/#build-environments-beyond-the-dtap-defaults","text":"It is now possible to extend the standard environments offered by TriplyETL.","title":"[Build] Environments beyond the DTAP defaults"},{"location":"triply-etl/generic/changelog/#bug-fixes_2","text":"This release fixes a URL/request-related error in the fromOai extractor.","title":"Bug fixes"},{"location":"triply-etl/generic/changelog/#triplyetl-307-through-309","text":"Release date: 2023-11-29","title":"TriplyETL 3.0.7 through 3.0.9"},{"location":"triply-etl/generic/changelog/#added-cli-flag-to-skip-version-check","text":"Introduced the cli flag --skip-version-check because some users can not use remote connections because of security policies.","title":"[Added] CLI flag to skip version check"},{"location":"triply-etl/generic/changelog/#added-support-for-jpath-expressions","text":"toJson() middleware now uses path selectors just as fromXml() , but also JPath expressions.","title":"[Added] Support for JPath expressions"},{"location":"triply-etl/generic/changelog/#added-authentication-for-the-oai-pmh-extractor","text":"fromOai() now accepts a Request object as the value for the url option, allowing more fine graded use of the HTTP request (including authentication information).","title":"[Added] Authentication for the OAI-PMH extractor"},{"location":"triply-etl/generic/changelog/#added-xslt-support-for-the-oai-pmh-extractor","text":"fromOai() now accepts an optional parameter stylesheet . The argument must be an object with 1 required key source pointing to the source of the stylesheet and 1 optional argument yieldsRdf . When provided the XSL Stylesheet source is processed using the OAI response. The result of the transformation must still be a valid OAI response.","title":"[Added] XSLT support for the OAI-PMH extractor"},{"location":"triply-etl/generic/changelog/#bug-fixes_3","text":"Fixes issue where keys that are used for internal administration were shown in logged records, after using the ifElse() and switch() control structures.","title":"Bug fixes"},{"location":"triply-etl/generic/changelog/#triplyetl-306","text":"Release date: 2023-11-14","title":"TriplyETL 3.0.6"},{"location":"triply-etl/generic/changelog/#bug-fixes_4","text":"This release fixes issues with upstream packages that contained breaking changes.","title":"Bug fixes"},{"location":"triply-etl/generic/changelog/#triplyetl-305","text":"Release date: 2023-11-09","title":"TriplyETL 3.0.5"},{"location":"triply-etl/generic/changelog/#bug-fixes_5","text":"The progress bar would sometimes terminate at 99% or 101%, instead of the expected 100%.","title":"Bug fixes"},{"location":"triply-etl/generic/changelog/#triplyetl-304","text":"Release date: 2023-11-07","title":"TriplyETL 3.0.4"},{"location":"triply-etl/generic/changelog/#added-dataset-metadata-specification","text":"It is now possible to use metadata when creating datasets. If a new dataset is created then the metadata is always used, for existing datasets you can choose to ignore, merge or replace existing metadata. This feature is to prevent ETLs accidentally overwriting metadata might have changed in the UI/Dashboard of their TriplyDB instance. The following options are available for this new feature in the opts.existingMetadata parameter: ignore : no metadata will be changed even if no metadata is present for this dataset (this is the default value) merge : only properties that have no value will be overwritten bij the provided metadata replace : all existing metadata will be replaced, even if the provided metadata contains empty keys","title":"[Added] Dataset metadata specification"},{"location":"triply-etl/generic/changelog/#cli-reverse-logic-for-creating-error-traces","text":"Before this release, running an ETL would always create an error trace file. It was possible to disable this behavior with CLI flag --skip-error-trace . Starting in this release, the error trace file is no longer created by default, and a newly added CLI flag --create-error-trace must now be specified in order ot create the error trace file.","title":"[CLI] Reverse logic for creating error traces"},{"location":"triply-etl/generic/changelog/#bug-fixes_6","text":"The following bugs were fixed: The number of synchronized services was not always reported correctly in CLI output. A package we depend on introduced a breaking change causing a function not to be there anymore.","title":"Bug fixes"},{"location":"triply-etl/generic/changelog/#triplyetl-303","text":"Release date: 2023-11-01","title":"TriplyETL 3.0.3"},{"location":"triply-etl/generic/changelog/#changed-support-for-the-nde-dataset-register","text":"The code to submit datasets to the NDE Dataset Register has been moved to TriplyDB-JS. The way to publish a dataset now is to add an option to the toTriplyDb() function: { submitToNDEDatasetRegister: true } . Example: toTriplyDb({dataset: 'nde', opts: {submitToNDEDatasetRegister: true}})","title":"[Changed] Support for the NDE Dataset Register"},{"location":"triply-etl/generic/changelog/#triplyetl-302","text":"Release date: 2023-10-23","title":"TriplyETL 3.0.2"},{"location":"triply-etl/generic/changelog/#added-static-statement-assertion","text":"export default async function(): Promise<Etl> { const etl = new Etl({baseIri: Iri('https://example.com/')}) await etl.staticAssertions( pairs( iri(etl.standardGraphs.default), [a, dcat.Dataset], [skos.prefLabel, literal(str(\"Family Doe\"), lang.en)], [dct.created, literal(str(new Date().toISOString()), xsd.dateTime)], ), ); await etl.staticAssertions( pairs(iri(etl.standardGraphs.default), [skos.prefLabel, literal(str(\"Familie Doe\"), lang.nl)]), ); etl.use( fromJson([{ name: \"John Doe\" }, { name: \"Jane Doe\" }]), triple(iri(etl.standardPrefixes.id, \"$recordId\"), sdo.name, \"name\"), logQuads(), ); return etl } You can now assert so called \"static triples\": triples that are not related to the source extractors but should only be asserted once per ETL.","title":"[Added] Static statement assertion"},{"location":"triply-etl/generic/changelog/#bug-fixes_7","text":"There was an error in the ifElse() control structure, that caused ETLs to not use the fallback 'else' block in some situations.","title":"Bug fixes"},{"location":"triply-etl/generic/changelog/#triplyetl-301","text":"Release date: 2023-10-19","title":"TriplyETL 3.0.1"},{"location":"triply-etl/generic/changelog/#enhanced-source-string-validation","text":"The addLiteral() function can now validate string data that occurs in the Record. Such validation can be used in addition to validation in the Internal Store (graph comparison and SHACL validation). Look at the documentation of addLiteral() for more information.","title":"[Enhanced] Source string validation"},{"location":"triply-etl/generic/changelog/#enhanced-synchronize-specific-services","text":"When publishing linked data to TriplyDB, it is now possible to synchronize one specific service. This is specifically useful in case an Acceptance and a Production service are used, and only the former should be synchronized. See the documentation for publishing to remote data destinations for more information.","title":"[Enhanced] Synchronize specific services"},{"location":"triply-etl/generic/changelog/#fixed-bug-fixes","text":"The following bugs have been fixed: The progress bar would sometimes go over 100%. the error report file ( etl.err ) would sometimes contain sentinel keys like $sentinel-${MD5-HASH} . These sentinel keys are used for internal bookkeeping in TriplyETL, and are no longer part of the Record. Some XSLT transformations failed on Windows, because of incorrect redirecting of error messages.","title":"[Fixed] Bug fixes"},{"location":"triply-etl/generic/changelog/#triplyetl-300","text":"Release date: 2023-10-12","title":"TriplyETL 3.0.0"},{"location":"triply-etl/generic/changelog/#added-support-for-xslt","text":"Support was added for the Extensible Stylesheet Language Transformations (XSLT) configuration language. This can be used in the following two functions: The fromXml() extractor function, for XML sources that transformed to regular XML. The loadRdf() function, for XML sources that are transformed into RDF/XML. In both cases, this functionality is used by configuring the stylesheet parameter with an XSLT Stylesheet (e.g. using Source.file() ). Example code that uses fromXml() : fromXml(Source.file(XMLFile), { selectors: \"rdf:RDF.sdo:Person\", stylesheet: Source.file(XSLTStylesheet), }), Example code that uses loadRdf() : loadRdf(Source.file(XMLFile), { contentType: \"application/rdf+xml\", stylesheet: Source.file(XSLTStylesheet), }),","title":"[Added] Support for XSLT"},{"location":"triply-etl/generic/changelog/#added-support-for-the-sparql-select-and-sparql-ask-queries","text":"The extractors fromCsv() , fromJson() , fromTsv() and fromXml() now support SPARQL Select queries. The extractors fromJson() and fromXml() also support SPARQL Ask queries. The example below hows how to use a SPARQL ask query in the fromJson() extractor: fromJson(Source.TriplyDb.query('account', 'query-name', { triplyDb: { url: 'https://api.triplydb.com' } }))","title":"[Added] Support for the SPARQL Select and SPARQL Ask queries"},{"location":"triply-etl/generic/changelog/#enhanced-simplified-usage-of-nestedpairs","text":"The nestedPairs() middleware can be used without providing the subject node that connects the pairs to the object/predicate. This will automatically create a Skolem IRI for the subject: nestedPairs(S, P, [a, sdo.Person]) For example: fromJson([{ id: '1', height: 15 }]), addSkolemIri({ prefix: prefix.skolem, key: '_height', }), nestedPairs(iri(prefix.product, 'id'), sdo.height, [qudt.unit, unit.CentiM], [rdf.value, 'height'], ), Will result in the following linked data assertions: product:1 sdo:height [ qudt:unit unit:CentiM; rdf:value 15 ].","title":"[Enhanced] Simplified usage of 'nestedPairs()'"},{"location":"triply-etl/generic/changelog/#changed-automatic-prefix-handling-in-triplydb-using-tordf","text":"Manually specified and standard prefixes are automatically added to TriplyDB when toRdf() is used. The middleware uploadPrefixes() is removed.","title":"[Changed] Automatic prefix handling in TriplyDB using 'toRdf()'"},{"location":"triply-etl/generic/changelog/#changed-new-approach-to-prefix-handling-in-triplyetl","text":"Prefixes are no longer defined as function that concatenates a value to an Iri. The Iri is a new type of Object in TriplyETL, that has a concat() method which allows you to add a value to the first part of an Iri. For example: const baseIri = Iri('https://example.com/') const prefixId = baseIri.concat('id/') const johnDoe = prefixId.concat('john-doe')","title":"[Changed] New approach to prefix handling in TriplyETL"},{"location":"triply-etl/generic/changelog/#changed-new-package-triplyetlvocabularies","text":"The vocabularies and languages are no longer part of @triplyetl/etl package. A new module has been released: @triplyetl/vocabularies : Individual vocabularies like rdf and schema.org can be imported in the following way: import { a, rdf, sdo } from '@triplyetl/vocabularies' To import all vocabularies, use: import * as vocab from \"@triplyetl/vocabularies\" Some vocabularies are too large to include, but they can still be used like this: import { aat } from '@triplyetl/vocabularies' const moustache = aat.concat('300379271') or import { aat } from '@triplyetl/vocabularies' addIri({prefix: aat, content: str('300379271'), key: 'moustache'}) To use the RATT lang tools : Import languages : import { languages } from '@triplyetl/vocabularies' Import languages and region : import { region, language } from '@triplyetl/vocabularies' const nl_BE = language.nl.addRegion(region.BE)","title":"[Changed] New package '@triplyetl/vocabularies'"},{"location":"triply-etl/generic/changelog/#changed-rdf-serialization-parsing-with-loadrdf","text":"The loadRdf() function is able to parse known RDF serializations ( Turtle , TriG , N-Triples , N-Quads ) provided as a string without specifying mimetype. const data = Source.string('...') loadRdf(data)","title":"[Changed] RDF serialization parsing with 'loadRdf()'"},{"location":"triply-etl/generic/changelog/#changed-extended-log-and-terminal-output-for-etl-debugging","text":"The output of the logfile and terminal output is changed. It contains more information to help users debugging ETL's. The format of time representation is now H:i:s.u where: H : 24-hour format of an hour with leading zeros (00 through 23) i : Minutes with leading zeros (00 to 59) s : Seconds with leading zeros (00 through 59) u : Microseconds (example: 654321)","title":"[Changed] Extended log and terminal output for ETL debugging"},{"location":"triply-etl/generic/changelog/#changed-tordf-for-account-based-token-access","text":"The toRdf() middleware now accepts \"me\" as account name based on the token. Below are some examples of this being used. toTriplyDb({account: \"me\", dataset: \"myDataset\"}) loadRdf(Source.TriplyDb.rdf(\"me\", datasetName)) Destination.TriplyDb.rdf(\"me\", datasetName)","title":"[Changed] 'toRdf()' for account-based token access"},{"location":"triply-etl/generic/changelog/#changed-relocation-middleware-resetstore-and-randomkey","text":"The resetStore() middleware is now moved from ratt to the generic namespace . The randomKey() middleware moved from generic to ratt .","title":"[Changed] Relocation middleware: 'resetStore()' and 'randomKey()'"},{"location":"triply-etl/generic/changelog/#changed-record-selection-with-offset-and-limit","text":"You can now use --offset and --limit instead of --from-record-id and --head , e.g. LIMIT=1 OFFSET=8 npx etl . The old arguments can still be used for backwards compatibility.","title":"[Changed] Record selection with '--offset' and '--limit'"},{"location":"triply-etl/generic/changelog/#changed-removal-of-mapquads","text":"The mapQuads() function was removed.","title":"[Changed] Removal of 'mapQuads()'"},{"location":"triply-etl/generic/changelog/#changed-warning-for-old-nodejs-versions","text":"If the users Node.JS version is older that the recommended version (currently >=18.0.0) a warning is shown.","title":"[Changed] Warning for old Node.JS versions"},{"location":"triply-etl/generic/changelog/#changed-shacl-validation-engine","text":"A SHACL Validation Engine improved performance.","title":"[Changed] SHACL Validation Engine"},{"location":"triply-etl/generic/changelog/#changed-trace-for-large-records","text":"A new flag now bypasses generating the trace for very large records: ---skip-error-trace . Thus, no trace file is created.","title":"[Changed] Trace for large records"},{"location":"triply-etl/generic/changelog/#changed-transition-to-in-memory-engine-speedy","text":"Comunica is no longer part of TriplyETL, the in-memory engine is now Triply's Speedy.","title":"[Changed] Transition to in-memory engine Speedy"},{"location":"triply-etl/generic/changelog/#enhanced-improvements-to-etl-logs","text":"The logging format was improved by including the following information: the TriplyETL version the Node.js version the DTAP mode the start date and time the end date and time","title":"[Enhanced] Improvements to ETL logs"},{"location":"triply-etl/generic/changelog/#enhanced-prevent-using-multiple-extractors","text":"TriplyETL only supports one extractor per ETL configuration object. In the past, it was possible to use multiple extractors, which would result in faulty behavior during ETL runs. Starting in this release, TriplyETL will emit an error when multiple extractors are used.","title":"[Enhanced] Prevent using multiple extractors"},{"location":"triply-etl/generic/changelog/#enhanced-better-error-reporting-for-csv-tsv-and-xml-sources","text":"In previous releases, the extractor functions fromCsv() , fromTsv() , and fromXml() would not emit the file name in case an error occurred. This was specifically problematic when a large number of data source files were used. Starting in this release, the file name is included in error message.","title":"[Enhanced] Better error reporting for CSV, TSV, and XML sources."},{"location":"triply-etl/generic/changelog/#enhanced-default-crs-for-wktaddpoint","text":"In previous releases, the Coordinate Reference System (CRS) was a required attribute for transformation function wkt.addPoint() . Starting in this release, the CRS argument has become optional. When not specified, the default CRS http://www.opengis.net/def/crs/OGC/1.3/CRS84 is used.","title":"[Enhanced] Default CRS for 'wkt.addPoint()'"},{"location":"triply-etl/generic/changelog/#enhanced-handle-conflicting-triplydb-instance-specifications","text":"In previous releases, it was possible to introduce an ambiguity in specify the TriplyDB instance to publish data to. This was possible by (1) specifying a TriplyDB API Token in the environment (e.g. though an .env file), and (2) by configuring the triplyDb option in the loadRdf() function. Starting in this release, TriplyETL will emit an error if the TriplyDB instance in the API Token differs from the TriplyDB instance configured in the triplyDb option.","title":"[Enhanced] Handle conflicting TriplyDB instance specifications"},{"location":"triply-etl/generic/changelog/#enhanced-more-information-for-failing-http-calls","text":"In previous releases, when a failing HTTP call resulted in an error message, only the body of that HTTP call would be included in the error message. Starting in this release, the HTTP status code of the failing HTTP call is included in the error message as well.","title":"[Enhanced] More information for failing HTTP calls"},{"location":"triply-etl/generic/changelog/#bug-fixes_8","text":"This release fixes several out-of-memory bugs in the SHACL validation function .","title":"Bug fixes"},{"location":"triply-etl/generic/changelog/#triplyetl-207-through-2019","text":"Release dates: 2023-06-17 through 2023-09-29","title":"TriplyETL 2.0.7 through 2.0.19"},{"location":"triply-etl/generic/changelog/#bug-fixes_9","text":"The following bugs were fixed: Processing an Excel sheet with fromXml() would sometimes consume too much memory. Several installation issues on Windows have been resolved. The async-saxophone library for XML processing was adjusted to support the current LTS version of Node.js (v18).","title":"Bug fixes"},{"location":"triply-etl/generic/changelog/#triplyetl-206","text":"Release date: 2023-06-07","title":"TriplyETL 2.0.6"},{"location":"triply-etl/generic/changelog/#added-support-for-the-premis-vocabulary","text":"Support was added for the PREMIS 3.0.0 vocabulary. This vocabulary is published by the Library of Congress and can be used to publish metadata about the preservation of digital objects. See the PREMIS documentation for more information. The vocabulary can be imported from the 'vocab' module: import { premis } from '@triplyetl/vocabularies' See the documentation on external vocabulary declarations for more information.","title":"[Added] Support for the PREMIS vocabulary"},{"location":"triply-etl/generic/changelog/#added-new-debug-function-logmemory","text":"A new debug function logMemory() is added. This function prints an overview of the current memory usage of TriplyETL. This allows users to detect fluctuations in memory consumption inside their pipelines.","title":"[Added] New debug function logMemory()"},{"location":"triply-etl/generic/changelog/#added-support-for-the-listidentifiers-verb-in-the-oai-pmh-extractor","text":"The fromOai() extractor already supported the ListRecords verb. This release adds support for the ListIdentifiers verb as well. This new verb allows users to stream through the headers of all records in an OAI-PMH collection, without requiring the full record (i.e. body) to be retrieved.","title":"[Added] Support for the 'ListIdentifiers' verb in the OAI-PMH extractor"},{"location":"triply-etl/generic/changelog/#triplyetl-205","text":"Release date: 2023-05-25","title":"TriplyETL 2.0.5"},{"location":"triply-etl/generic/changelog/#changed-new-default-engine-for-sparql-construct","text":"The default engine for evaluating SPARQL Construct queries (function construct() ) has changed from Comunica to Speedy. Speedy is a new SPARQL engine that is developed by Triply. Comunica is an open source engine that is developed by the open source community. Since SPARQL is a standardized query language, this change should not cause a difference in behavior for your ETL pipelines. In the unexpected case where an ETL pipeline is negatively affected by this change, the old situation can be restored by explicitly configuring the Comunica engine: import { construct } from '@triplyetl/etl/sparql' construct(Source.TriplyDb.query('my-query'), { sparqlEngine: 'comunica' }), The benefit of switching to the Speedy engine is that this engine is expected to be faster for most queries. Overall, this change will therefore result in speed improvements for your TriplyETL pipelines.","title":"[Changed] New default engine for SPARQL Construct"},{"location":"triply-etl/generic/changelog/#added-new-cli-tool-for-comparing-graphs","text":"The new CLI tool compare allows graph comparison to be performed from the command-line. This uses the same algorithm that is used by the compareGraphs() validation function.","title":"[Added] New CLI tool for comparing graphs"},{"location":"triply-etl/generic/changelog/#bug-fixes_10","text":"This release fixes the following bugs: fromXlsx() did not remove trailing whitespace in cell values. When a SHACL result was printed, an incorrect message about a faulty SHACL model was shown. Some RDF processors did not handle empty RDF inputs correctly.","title":"Bug fixes"},{"location":"triply-etl/generic/changelog/#triplyetl-204","text":"Release date: 2023-05-11","title":"TriplyETL 2.0.4"},{"location":"triply-etl/generic/changelog/#enhanced-better-output-for-graph-comparison","text":"Before this release, when two graphs were not isomorph and their difference consisted of a mapping from blank nodes onto blank nodes exclusively, an empty difference message was communicated. From this release onwards, the difference message is non-empty, and specifically indicates the difference between the non-isomorphic graphs in terms of the mismatching blank nodes. Look at this example from the graph comparison documentation, which emits such a difference message.","title":"[Enhanced] Better output for graph comparison"},{"location":"triply-etl/generic/changelog/#triplyetl-203","text":"Release date: 2023-05-10","title":"TriplyETL 2.0.3"},{"location":"triply-etl/generic/changelog/#bug-fixes_11","text":"This release includes the following bug fixes: Error location information is not shown in TriplyETL Runner. Issue when a URL data source ( Source.url() ) includes an HTTP body.","title":"Bug fixes"},{"location":"triply-etl/generic/changelog/#triplyetl-202","text":"Release date: 2023-05-09","title":"TriplyETL 2.0.2"},{"location":"triply-etl/generic/changelog/#bug-fixes_12","text":"This release fixes bugs related to the recent switch from CommonJS to ESM: Dynamic import bug on Windows. Error reporting issues due to ESM imports.","title":"Bug fixes"},{"location":"triply-etl/generic/changelog/#triplyetl-201","text":"Release date: 2023-05-03","title":"TriplyETL 2.0.1"},{"location":"triply-etl/generic/changelog/#added-timeout-flag-for-triplyetl-runner","text":"The TriplyETL Runner is the CLI tool that is used to run ETL pipelines. Starting with this version, you can specify a --timeout flag when using the TriplyETL Runner. When the indicated timeout is reached before the pipeline finishes, the TriplyETL Runner will gracefully terminate the ETL by acting as if there are no more incoming records. See the TriplyETL Runner documentation page for more information.","title":"[Added] Timeout flag for TriplyETL Runner"},{"location":"triply-etl/generic/changelog/#triplyetl-200","text":"Release date: 2023-05-01","title":"TriplyETL 2.0.0"},{"location":"triply-etl/generic/changelog/#changed-modules-infrastructure-moves-from-commonjs-to-esm","text":"Before this release, TriplyETL used CommonJS modules to modularize its functionality into different components. Starting in this release, ECMAScript Modules (ESM) are used to modularize TriplyETL functionality into different modules. ESM is a more modern approach for modularizing ECMAScript (JavaScript, TypeScript, and Node.js) code. While CommonJS imports are evaluated at runtime, ESM imports are evaluated at compile time. TriplyETL users benefit from this change, since error messages related to module imports will be detected much earlier in the development process. All documentation examples were update to use ESM syntax for module imports, for example: import { logRecord } from '@triplyetl/etl/debug'","title":"[Changed] Modules infrastructure moves from CommonJS to ESM"},{"location":"triply-etl/generic/changelog/#changed-debug-functions-move-to-a-new-module","text":"Before this release, debug functions like logRecord() and startTrace() were part of the RATT module. Since debug functions can be used in combination with any ETL configuration approach, they were moved to a new module. The debug functions are imported from their new module in the following way: import { logRecord, traceEnd, traceStart } from '@triplyetl/etl/debug'","title":"[Changed] Debug functions move to a new module"},{"location":"triply-etl/generic/changelog/#enhanced-better-error-messages-when-things-go-wrong","text":"This release introduces a new approach for communicating errors back to the user. When TriplyETL functionality detects an error condition, a unified 'trace middleware' is now used to retrieve information from the environment in which the error occurred. This information is then printed to the error output stream for communication with the user.","title":"[Enhanced] Better error messages when things go wrong"},{"location":"triply-etl/generic/changelog/#bug-fixes_13","text":"The following bug fixes are included in this release: Incorrect behavior of the _switch() control function . The fromOai() extractor now communicates clearer when the accessed OAI-PMH endpoint encounters any issues. When a key with a NULL value was accessed, the name of that key is now included in the error message. This makes it easier for users to find the NULL value in their source data.","title":"Bug fixes"},{"location":"triply-etl/generic/changelog/#triplyetl-10x","text":"TriplyETL 1.0.0 was released on 2023-03-20.","title":"TriplyETL 1.0.x"},{"location":"triply-etl/generic/cli/","text":"On this page: Command Line Interface (CLI) Installing dependencies Transpiling to JavaScript TriplyETL Runner Output summary Limit the number of records Specify a range of records Process a specific record Set a timeout Verbose mode Secure verbose mode TriplyETL Tools Compare Create TriplyDB API Token Print TriplyDB API Token Validate Command Line Interface (CLI) \u00b6 TriplyETL allows you to manually perform various tasks in a terminal application (a Command-Line Interface or CLI). Installing dependencies must be repeated when dependencies were changed. Transpiling to JavaScript must be repeated when one or more TypeScript files are changed. TriplyETL Runner allows you to manually run local TriplyETL projects in your terminal. TriplyETL Tools explains how you can perform common ETL tasks. Installing dependencies \u00b6 When you work on an existing TriplyETL project, you sometimes pull in changes made by your team members. Such changes are typically obtained by running the following Git command: git pull This command prints a list of files that were changed by your team members. If this list includes changes to the file package.json , this means that one or more dependencies were changed. In order to effectuate these changes in your local copy of the TriplyETL project, you must run the following command: npm i Transpiling to JavaScript \u00b6 When you make changes to one or more TypeScript files, the corresponding JavaScript files will have become outdated. If you now use the TriplyETL Runner , it will use one or more outdated JavaScript files, and will not take into account your most recent changes to the TypeScript files. In order to keep your JavaScript files up-to-date relative to your TypeScript files, you must run the following command after making changes to TypeScript files: npm run build If you edit your TypeScript files repeatedly, having to run this extra command may get tedious. In such cases, you can run the following command to automatically perform the transpile step in the background: npm run dev Notice that this prevents you from using the terminal application for new commands. It is typical to open a new terminal application window, and run the npx etl command from there. TriplyETL Runner \u00b6 The TriplyETL Runner allows you to run a local TriplyETL project in your terminal application. We assume that you have a local TriplyETL project in which you can successfully run the npx etl command. Follow the Getting Started instructions for TriplyETL Runner if this is not yet the case. Run the following command to run the ETL pipeline: npx etl This command implicitly uses the file lib/main.js , which is the transpiled JavaScript file that corresponds to the TypeScript file src/main.ts . The following command has the same behavior, but makes explicit which file is used: npx etl lib/main.js Some TriplyETL projects have multiple top-level scripts. In such cases, it is possible to run each of these scripts individually as follows: npx etl lib/some-script.js Output summary \u00b6 TriplyETL Runner will start processing data. Depending on the size of the data source, the Runner may take more or less time to finish. When the Runner finishes successfully, it will print the following summary: \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u2502 Etl: #Error 0 | #Warning 0 | #Info 0 \u2502 \u2502 #Statements 2 \u2502 \u2502 #Records 2 \u2502 \u2502 Started at 2023-06-18 10:05:20 \u2502 \u2502 Runtime 0 sec \u2502 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 This summary includes the following information: \"#Error\" shows the number of errors encountered. With default settings, this number is at most 1, since the Runner will immediately stop after an error occurs. \"#Warning\" shows the number of warnings encountered. With default settings, this includes warnings emitted by the SHACL Validator . \"#Info\" shows the number of informational messages. With default settings, this includes informational messages emitted by the SHACL Validator . \"#Statements\" shows the number of triples or quads that was generated. This number is equal to or higher than the number of statements that is uploaded to the triple store. The reason for this is that TriplyETL processes records in parallel. If the same statement is generated for two records, the number of statements with be incremented by 2, but only 1 unique statement will be uploaded to the triple store. \"#Records\" shows the number of records that was processed. \"Started at\" shows the date and time at which the Runner started. \"Runtime\" shows the wall time duration of the run. Limit the number of records \u00b6 When developing a pipeline, it is almost never necessary to process all records from the source data. Instead, it is common to run the ETL for a small number of example record, which results in quick feedback. The --head flag indicates the maximum number of records that is processed by the Runner: npx etl --head 1 npx etl --head 10 These commands run the ETL for the first record (if one is available) and for the first 10 records (if these are available). Specify a range of records \u00b6 When developing a pipeline over a large source data collection, it is often standard practice to use the first 10 or 100 records most of the time. The benefit of this approach is that the feedback loop between making changes and receiving feedback is short. A downside of this approach is that the ETL may be overly optimized towards these first few records. For example, if a value is missing in the first 1.000 records, then transformations that are necessary for when the value is present will not be developed initially. An alternative is to run the entire ETL, but that takes a long time. To avoid the downsides of using --head , TriplyETL also supports the --from-record-id flag. This flag specifies the number of records that are skipped. This allows us to specify an arbitrary consecutive range of records. For example, the following processes the 1.001-st until and including the 1.010-th record: npx etl --from-record-id 1000 --head 10 Process a specific record \u00b6 When the --head flag is set to 1, the --from-record-id flag specifies the index of a single specific record that is processed. This is useful when a record is known to be problematic, for instance during debugging. The following command runs TriplyETL for the 27th record: npx etl --from-record-id 26 --head 1 Set a timeout \u00b6 For large ETL pipelines, it is sometimes useful to specify a maximum duration for which the TriplyETL Runner is allowed to run. In such cases, the --timeout flag can be used. The --timeout option accepts human-readable duration strings, such as '1h 30m 5s', '1hr', '1 hour', or '3hrs'. When the indicated timeout is reached before the pipeline finishes, the TriplyETL Runner will gracefully terminate the ETL by acting as if there are no more incoming records. As a result, the Runner will upload all linked data (graphs) that was produced up to that point, and it will write a performance log. For TriplyETLs that run in a CI/CD environment, the timeout must be set lower than the CI/CD timeout, in order for the Runner to be able to perform the termination step. Verbose mode \u00b6 When TriplyETL is run normally, the following information is displayed: The number of added triples. The runtime of the script. An error message, if any occurred. It is possible to also show the following additional information by specifying the --verbose flag: In case of an error, the first 20 values from the last processed record. In case of an error, the full stack trace. The following example shows how the --verbose flag can be used: npx etl --verbose Secure verbose mode \u00b6 Verbose mode may perform a reset of your current terminal session. If this happens you lose visible access to the commands that were run prior to the last TriplyETL invocation. This destructive behavior of verbose mode can be disabled by setting the following environment variable: export CI=true This fixes the reset issue, but also makes the output less colorful. TriplyETL Tools \u00b6 TriplyETL Tools is a collection of small tools that can be used to run isolated tasks from your terminal application. TriplyETL Tools can be used when you are inside a TriplyETL project. If you do not have an ETL project yet, use the TriplyETL Generator first to create one. The following command prints an overview of the supported tools: npx tools The following tools are supported: Tool Description compare Compare the contents of two RDF files create-token Create a new TriplyDB API Token print-token Print the currently set TriplyDB API Token, if any validate Validate a data file against a SHACL shapes file For each tool, the following command prints more information on how to use it: npx tools {name} --help Compare \u00b6 The compare tool checks whether two RDF files encode the same linked data: - If the two files contain the same data, the command succeeds and does not print any output. - If the two files do not contain the same data, the command exits with an error code, and the difference between the two files is printed. The compare tools is invoked over the two RDF files one.ttl and two.ttl as follows: npx tools compare one.ttl two.ttl This tool can be used to compare two RDF files that contain multiple graphs, for example: npx tools compare one.trig two.trig This tool uses the graph isomorphism property as defined in the RDF 1.1 standard: link Create TriplyDB API Token \u00b6 This tool creates a new TriplyDB API Token from the command-line. This command can be used as follows: npx tools create-token The command will ask a couple of questions in order to create the TriplyDB API Token: The hostname of the TriplyDB instance The name of the token Your TriplyDB account e-mail Your TriplyDB account password The command exists in case a TriplyDB API Token is already configured. Print TriplyDB API Token \u00b6 This tool prints the currently configured TriplyDB API Token, if any. This command can be used as follows: npx tools print-token This command is useful when there are issues with configuring a TriplyDB API Token. Validate \u00b6 This tool validates the content of one data file against the SHACL shapes in another file. The resulting SHACL validation report is printed to standard output. The command can be used as follows: $ npx tools validate -d data.trig -s model.trig See this section to learn more about the SHACL validation report.","title":"Command Line Interface (CLI)"},{"location":"triply-etl/generic/cli/#command-line-interface-cli","text":"TriplyETL allows you to manually perform various tasks in a terminal application (a Command-Line Interface or CLI). Installing dependencies must be repeated when dependencies were changed. Transpiling to JavaScript must be repeated when one or more TypeScript files are changed. TriplyETL Runner allows you to manually run local TriplyETL projects in your terminal. TriplyETL Tools explains how you can perform common ETL tasks.","title":"Command Line Interface (CLI)"},{"location":"triply-etl/generic/cli/#installing-dependencies","text":"When you work on an existing TriplyETL project, you sometimes pull in changes made by your team members. Such changes are typically obtained by running the following Git command: git pull This command prints a list of files that were changed by your team members. If this list includes changes to the file package.json , this means that one or more dependencies were changed. In order to effectuate these changes in your local copy of the TriplyETL project, you must run the following command: npm i","title":"Installing dependencies"},{"location":"triply-etl/generic/cli/#transpiling-to-javascript","text":"When you make changes to one or more TypeScript files, the corresponding JavaScript files will have become outdated. If you now use the TriplyETL Runner , it will use one or more outdated JavaScript files, and will not take into account your most recent changes to the TypeScript files. In order to keep your JavaScript files up-to-date relative to your TypeScript files, you must run the following command after making changes to TypeScript files: npm run build If you edit your TypeScript files repeatedly, having to run this extra command may get tedious. In such cases, you can run the following command to automatically perform the transpile step in the background: npm run dev Notice that this prevents you from using the terminal application for new commands. It is typical to open a new terminal application window, and run the npx etl command from there.","title":"Transpiling to JavaScript"},{"location":"triply-etl/generic/cli/#triplyetl-runner","text":"The TriplyETL Runner allows you to run a local TriplyETL project in your terminal application. We assume that you have a local TriplyETL project in which you can successfully run the npx etl command. Follow the Getting Started instructions for TriplyETL Runner if this is not yet the case. Run the following command to run the ETL pipeline: npx etl This command implicitly uses the file lib/main.js , which is the transpiled JavaScript file that corresponds to the TypeScript file src/main.ts . The following command has the same behavior, but makes explicit which file is used: npx etl lib/main.js Some TriplyETL projects have multiple top-level scripts. In such cases, it is possible to run each of these scripts individually as follows: npx etl lib/some-script.js","title":"TriplyETL Runner"},{"location":"triply-etl/generic/cli/#output-summary","text":"TriplyETL Runner will start processing data. Depending on the size of the data source, the Runner may take more or less time to finish. When the Runner finishes successfully, it will print the following summary: \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u2502 Etl: #Error 0 | #Warning 0 | #Info 0 \u2502 \u2502 #Statements 2 \u2502 \u2502 #Records 2 \u2502 \u2502 Started at 2023-06-18 10:05:20 \u2502 \u2502 Runtime 0 sec \u2502 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 This summary includes the following information: \"#Error\" shows the number of errors encountered. With default settings, this number is at most 1, since the Runner will immediately stop after an error occurs. \"#Warning\" shows the number of warnings encountered. With default settings, this includes warnings emitted by the SHACL Validator . \"#Info\" shows the number of informational messages. With default settings, this includes informational messages emitted by the SHACL Validator . \"#Statements\" shows the number of triples or quads that was generated. This number is equal to or higher than the number of statements that is uploaded to the triple store. The reason for this is that TriplyETL processes records in parallel. If the same statement is generated for two records, the number of statements with be incremented by 2, but only 1 unique statement will be uploaded to the triple store. \"#Records\" shows the number of records that was processed. \"Started at\" shows the date and time at which the Runner started. \"Runtime\" shows the wall time duration of the run.","title":"Output summary"},{"location":"triply-etl/generic/cli/#limit-the-number-of-records","text":"When developing a pipeline, it is almost never necessary to process all records from the source data. Instead, it is common to run the ETL for a small number of example record, which results in quick feedback. The --head flag indicates the maximum number of records that is processed by the Runner: npx etl --head 1 npx etl --head 10 These commands run the ETL for the first record (if one is available) and for the first 10 records (if these are available).","title":"Limit the number of records"},{"location":"triply-etl/generic/cli/#specify-a-range-of-records","text":"When developing a pipeline over a large source data collection, it is often standard practice to use the first 10 or 100 records most of the time. The benefit of this approach is that the feedback loop between making changes and receiving feedback is short. A downside of this approach is that the ETL may be overly optimized towards these first few records. For example, if a value is missing in the first 1.000 records, then transformations that are necessary for when the value is present will not be developed initially. An alternative is to run the entire ETL, but that takes a long time. To avoid the downsides of using --head , TriplyETL also supports the --from-record-id flag. This flag specifies the number of records that are skipped. This allows us to specify an arbitrary consecutive range of records. For example, the following processes the 1.001-st until and including the 1.010-th record: npx etl --from-record-id 1000 --head 10","title":"Specify a range of records"},{"location":"triply-etl/generic/cli/#process-a-specific-record","text":"When the --head flag is set to 1, the --from-record-id flag specifies the index of a single specific record that is processed. This is useful when a record is known to be problematic, for instance during debugging. The following command runs TriplyETL for the 27th record: npx etl --from-record-id 26 --head 1","title":"Process a specific record"},{"location":"triply-etl/generic/cli/#set-a-timeout","text":"For large ETL pipelines, it is sometimes useful to specify a maximum duration for which the TriplyETL Runner is allowed to run. In such cases, the --timeout flag can be used. The --timeout option accepts human-readable duration strings, such as '1h 30m 5s', '1hr', '1 hour', or '3hrs'. When the indicated timeout is reached before the pipeline finishes, the TriplyETL Runner will gracefully terminate the ETL by acting as if there are no more incoming records. As a result, the Runner will upload all linked data (graphs) that was produced up to that point, and it will write a performance log. For TriplyETLs that run in a CI/CD environment, the timeout must be set lower than the CI/CD timeout, in order for the Runner to be able to perform the termination step.","title":"Set a timeout"},{"location":"triply-etl/generic/cli/#verbose-mode","text":"When TriplyETL is run normally, the following information is displayed: The number of added triples. The runtime of the script. An error message, if any occurred. It is possible to also show the following additional information by specifying the --verbose flag: In case of an error, the first 20 values from the last processed record. In case of an error, the full stack trace. The following example shows how the --verbose flag can be used: npx etl --verbose","title":"Verbose mode"},{"location":"triply-etl/generic/cli/#secure-verbose-mode","text":"Verbose mode may perform a reset of your current terminal session. If this happens you lose visible access to the commands that were run prior to the last TriplyETL invocation. This destructive behavior of verbose mode can be disabled by setting the following environment variable: export CI=true This fixes the reset issue, but also makes the output less colorful.","title":"Secure verbose mode"},{"location":"triply-etl/generic/cli/#triplyetl-tools","text":"TriplyETL Tools is a collection of small tools that can be used to run isolated tasks from your terminal application. TriplyETL Tools can be used when you are inside a TriplyETL project. If you do not have an ETL project yet, use the TriplyETL Generator first to create one. The following command prints an overview of the supported tools: npx tools The following tools are supported: Tool Description compare Compare the contents of two RDF files create-token Create a new TriplyDB API Token print-token Print the currently set TriplyDB API Token, if any validate Validate a data file against a SHACL shapes file For each tool, the following command prints more information on how to use it: npx tools {name} --help","title":"TriplyETL Tools"},{"location":"triply-etl/generic/cli/#compare","text":"The compare tool checks whether two RDF files encode the same linked data: - If the two files contain the same data, the command succeeds and does not print any output. - If the two files do not contain the same data, the command exits with an error code, and the difference between the two files is printed. The compare tools is invoked over the two RDF files one.ttl and two.ttl as follows: npx tools compare one.ttl two.ttl This tool can be used to compare two RDF files that contain multiple graphs, for example: npx tools compare one.trig two.trig This tool uses the graph isomorphism property as defined in the RDF 1.1 standard: link","title":"Compare"},{"location":"triply-etl/generic/cli/#create-triplydb-api-token","text":"This tool creates a new TriplyDB API Token from the command-line. This command can be used as follows: npx tools create-token The command will ask a couple of questions in order to create the TriplyDB API Token: The hostname of the TriplyDB instance The name of the token Your TriplyDB account e-mail Your TriplyDB account password The command exists in case a TriplyDB API Token is already configured.","title":"Create TriplyDB API Token"},{"location":"triply-etl/generic/cli/#print-triplydb-api-token","text":"This tool prints the currently configured TriplyDB API Token, if any. This command can be used as follows: npx tools print-token This command is useful when there are issues with configuring a TriplyDB API Token.","title":"Print TriplyDB API Token"},{"location":"triply-etl/generic/cli/#validate","text":"This tool validates the content of one data file against the SHACL shapes in another file. The resulting SHACL validation report is printed to standard output. The command can be used as follows: $ npx tools validate -d data.trig -s model.trig See this section to learn more about the SHACL validation report.","title":"Validate"},{"location":"triply-etl/generic/control-structures/","text":"On this page: Control Structures Process data conditionally (when()) Missing values The empty string NULL values (when() and whenNotEqual()) Iterating over lists of objects (forEach()) Index key ($index) Parent key ($parent) Root key ($root) Iterating over lists of primitives Specify multiple conditions (ifElse()) Parameters Example 1 Example 2 Switch between different cases (_switch()) Parameters Example 1 Example 2 Skipping remaining functions (skipRest()) Parameters Control Structures \u00b6 This page documents how you can use control structures in your ETL configuration. Process data conditionally ( when() ) \u00b6 Source data often contains optional values. These are values that appear in some, but not all records. Source data often contains 'special' values that denote the absence of a value. Common examples are values such as 'NULL' or the empty string ( '' ) or 'clear' outliers such as 9999 for a missing year. We call such values \u2018null values\u2019. The when() function allows part of a TriplyETL configuration to run when certain conditions are met. The first parameter is used to determine whether or not the remaining parameters should be called: when('{condition}', '{statement-1}', '{statement-2}', '{statement-3}', // etc ), Notice that it is often useful to specify multiple statements under the same condition: The first statement transforms an optional value, and the second statement uses the transformed optional value to make a triple assertion. The first statement asserts one triple based on the optional value, and the second statement asserts a second triple based on the same optional value. Missing values \u00b6 If a value is sometimes completely missing from a source data record, the when() conditional function can be used. The following code snippet assets a triple if and only if a value for the 'zipcode' key is present in the Record: when(context => context.isNotEmpty('zipcode'), triple(iri(prefix.id, 'id'), def.zipcode, 'zipcode'), ), Since checking for the presence or absence of a single record is very common, the above can also be written as follows: when('zipcode', triple(iri(prefix.id, 'id'), def.zipcode, 'zipcode'), ), The empty string \u00b6 In many source data formats, the empty string is used to signify a missing value, this particular string is treated in a special way by when() . A key whose value is the empty string is treated in the same way as a key that is altogether absent. The following code snippet will not print the record to standard output, because the 'zipcode' key is considered empty: fromJson([{ zipcode: '' }]), when('zipcode', logRecord(), ), Notice that it is almost never useful to store the empty string in linked data. So the treatment of empty strings as NULL values is the correct default behavior. NULL values ( when() and whenNotEqual() ) \u00b6 If a key contains specific values that are indended to represent NULL values, then these must be specifically identified the first when() parameter. The following code snippet identifies the value 9999 for the 'created' key as denoting a NULL values. This means that the year 9999 is used in the source system whenever the actual year of creation was unknown. when(context => context.getNumber('created') != 9999, triple(iri(prefix.id, 'id'), dct.created, literal('created', xsd.gYear)), ), Since checking the value of one specific key is very common, the above can be written as follows, using the more specific whenNotEqual function: whenNotEqual('created', 9999, triple(iri(prefix.id, 'id'), dct.created, literal('created', xsd.gYear)), ), Notice that the use of whenNotEqual() makes the configuration easier to read. The same shorthand notation works when there are multiple NULL values in the source data. The following code snippet only asserts a triple if the year of creation is neither 9999 nor -1. Notice that the array can contain any number of potential NULL values: whenNotEqual('created', [-1, 9999], triple(iri(prefix.id, 'id'), dct.created, literal('created', xsd.gYear)), ), Iterating over lists of objects ( forEach() ) \u00b6 In the previous section, we saw that we were able to assert the name of the first country and the name of the second country. But what do we do if we want to assert the name for every country in the world? And what do we do if some countries have a name in 2 languages, but other countries have a name in 1 or 3 languages? What we need is a simple way to express that we want to make an assertion for every element in a list. TriplyETL provides the forEach() function for this purpose. The following code snippet asserts the name for each country in the example data: forEach('data.countries', triple(iri(prefix.id, 'id'), rdfs.label, 'name'), ), Notice the following details: - forEach() uses the path expression 'data.countries' to identify the list. - Inside the forEach() function, each element in the list is made available separately. - This allows the 'id' and 'name' keys to be identified directly. The above code snippet makes one assertion for every element in the \"countries\" list: country:nl rdfs:label 'The Netherlands'. country:de rdfs:label 'Germany'. Notice that forEach() only works for lists whose elements are objects*. See Iterating over lists of primitives for dealing with lists that do not contain objects. The elements that forEach() iterates over are themselves (sub)records. This implies that all functions that work for full records also work for the (sub)records inside forEach() . The (sub)records inside an forEach() function are smaller. This allows the regular keys of the iterated-over elements to be accessed directly. In addition to these regular keys, (sub)records inside forEach() also contain additional keys that simplify common operations. The following subsections explain the following special keys: Index key ( $index ) Parent key ( $parent ) Root key ( $root ) Index key ( $index ) \u00b6 Each (sub)record that is made available in forEach() contains the $index key. The value of this key is the index of the element in the list. This is the same index that is used to access specific elements in an list, as explained in the section on accessing lists by index . The index key is often useful for assigning a unique subject IRI to every element. Suppose we have the following source data. We do not want to use the values of the \"name\" key for our subject IRI, because these names contain spaces and possibly other problematic characters that make the IRI more difficult to read and use. { \"countries\": [ { \"name\": \"The Netherlands\" }, { \"name\": \"Germany\" }, { \"name\": \"Italy\" } ] } The following code snippet uses the $index key that is made available inside forEach in order to create a unique subject IRI for each country: forEach('countries', triple(iri(prefix.id, '$index'), rdfs.label, 'name'), ), This results in the following assertions: country:0 rdfs:label 'The Netherlands'. country:1 rdfs:label 'Germany'. country:2 rdfs:label 'Italy'. Parent key ( $parent ) \u00b6 When forEach() iterates through a list of elements, it makes the enclosingparent* record available under key $parent . The parent record is the record that directly contains the first key that appears in the path that was specified in forEach() . For example, the parent record in the following call is the record that directly contains the \"data\" key: forEach('data.countries', // etc ), The $parent key can be observed when logRecord` is used to print the iterated-over elements to the terminal: forEach('data.countries', logRecord(), ), For our example source data, this emits the following 2 records: { \"id\": \"en\", \"name\": \"The Netherlands\", \"$index\": 0, \"$parent\": { \"data\": { \"labels\": [ { \"id\": \"en\", \"name\": \"The Netherlands\", }, { \"id\": \"de\" \"name\": \"Germany\", } ] } }, \"$root\": \"__circular__\" } and: { \"id\": \"de\", \"name\": \"Germany\", \"$index\": 1, \"$parent\": { \"data\": { \"labels\": [ { \"id\": \"en\", \"name\": \"The Netherlands\", }, { \"id\": \"de\" \"name\": \"Germany\", } ] } }, \"$root\": \"__circular__\" } The $root key is explained in the next section . Root key ( $root ) \u00b6 Sometimes it may be necessary to access a part of the original record that is outside of the scope of the forEach() call. Every (sub)record inside a forEach() call contains the \"$root\" key. The value of the root key provides a link to the full record. Because the $root key is part of the linked-to record, it is not possible to print the value of the root key. (This would result in infinite output.) For this reason, the value of the $root key is printed as the special value \"__circular__\" . For the above examples, the parent record and root record are the same, but this is not always the case. Specifically, the parent record and root record are different when forEach() calls are nested. The following data contains an inner list (key \"labels\" ) inside an outer list ( \"countries\" ): { \"data\": { \"countries\": [ { \"id\": \"NL\", \"labels\": [ { \"name\": \"The Netherlands\", \"locale\": \"en-us\" }, { \"name\": \"Nederland\", \"locale\": \"nl-nl\" } ] }, { \"id\": \"EN\", \"labels\": [ { \"name\": \"England\", \"locale\": \"en-gb\" } ] } ] } } The following nested forEach() call shows the difference between the \"$parent\" key and the $root key. The $parent key allows the individual country objects to be accessed, while the \"$root\" key allows the entire tree to be accessed: forEach('data.countries', forEach('labels', logRecord(), ), ), The following record is printed first (3 records are printed in total). Notice that the value of the outer $parent and \"$root\" keys are now different: - The $parent key allows access to the first country. - The $root key allows access to the full record (describing multiple countries). { \"name\": \"The Netherlands\", \"locale\": \"en-us\", \"$index\": 0, \"$parent\": { \"id\": \"NL\", \"labels\": [ { \"name\": \"The Netherlands\", \"locale\": \"en-us\" }, { \"name\": \"Nederland\", \"locale\": \"nl-nl\" } ], \"$index\": 0, \"$parent\": { \"data\": { \"countries\": [ { \"id\": \"NL\", \"labels\": \"__circular__\" }, { \"id\": \"EN\", \"labels\": [ { \"name\": \"England\", \"locale\": \"en-gb\" } ] } ] } }, \"$root\": \"__circular__\" }, \"$root\": \"__circular__\" } Iterating over lists of primitives \u00b6 Notice that forEach() can only iterate over Records. Sometimes, it is necessary to iterate over primitive values, for example strings. If the iteration directly results in RDF assertions, iris() and literals() can be used. But in other cases, we must map the primitive values to objects. These objects can then be processed as regular Records. Here is an example: fromJson({value: 'a b c'}), split({content: 'value', separator: ' ', key: 'valueStrings'}), // Since we cannot transform 'valueStrings' directly, we first create // objects that contain those strings ('valueObjects'). custom.addFrom.value({ content: 'valueStrings', change: values => (values as string[]).map(value => ({'valueObject': value})), type: 'unknown', key: 'valueObjects' }), // Here we can apply any regular transformation that works with Records. Specify multiple conditions ( ifElse() ) \u00b6 The ifElse() function in TriplyETL allows us to specify multiple conditions based on which other functions are run. Every condition is specified with an if key. In case the condition is true, the functions specified in the then key are run. If none of the if conditions are true, the functions specified in an else key, if present, are run. Parameters \u00b6 The first parameter must be an { if: ..., then: ... } object. The non-first parameters are either additional { if: ..., then: ... } objects or a final { else: ... } object. Each if key specifies a condition that is either true or false. Conditions are either a key name or a function that takes the Etl Context and returns a Boolean value. Specifying a key name is identical to specifying the following function: ctx => ctx.getString('KEY') The then and else keys take either one function, or an array of zero or more functions. Example 1 \u00b6 The following code snippet uses different conditions to determine the age category that a person belongs to: fromJson([ { id: 'johndoe', age: 12 }, { id: 'janedoe', age: 32 }, // ... ]), addIri({ prefix: prefix.person, content: 'id', key: '_person', }), ifElse({ if: ctx => ctx.getNumber('age') < 12, then: triple('_person', a, def.Child), }, { if: ctx => { const age = ctx.getNumber('age') return age >= 12 && age < 20 }, then: triple('_person', a, def.Teenager), }, { if: ctx => { const age = ctx.getNumber('age') return age >= 20 && age < 65 }, then: triple('_person', a, def.Adult), }, { else: triple('_person', a, def.Senior), }), Example 2 \u00b6 The following snippet either asserts data about persons or data about organizations, and uses an ifElse to make the conditional determination on which assertion to make: fromJson([ { first: 'John', last: 'Doe' }, { name: 'Triply' }, ]), ifElse({ if: 'name', then: pairs(iri(prefix.id, 'name'), [a, sdo.Organization], [sdo.name, 'name'], ), }, { else: [ concat({ content: ['first', 'last'], separator: '-', key: 'name', }), pairs(iri(prefix.id, 'name'), [a, sdo.Person], [sdo.givenName, 'first'], [sdo.familyName, 'last'], ), ], }), Switch between different cases ( _switch() ) \u00b6 The function _switch() allows us to switch between different cases, based on the value of a specified key. The signature is as follows; _switch(key, [value_1, functions_1], ..., [value_n, functions_n], default_functions, ) Parameters \u00b6 key The key parameter whose value is compared against the specified values. Each case consists of a list of two elements: value_i is the value that is checked for equivalence with the value stored in key . functions_i is the function or list of functions that is executed when the value in key is equivalent to value_i . default_functions is the function or list of functions that is executed when key matches neither of the cases. Notice that we must write _switch() because switch is a reserved keyword in ECMAScript. An error is emitted if the value for key does not match any of the cases and no default case is specified. Example 1 \u00b6 When an ETL uses multiple data sources, we can use a _switch() to run a dedicated sub-ETL for each data source. Suppose we have two tabular data sources: file.episodes and file.people . We can use the following _switch() statement to run different sub-ETLs: _switch(key.fileName, [file.episodes, etl_episodes], [file.people, etl_people], ), Example 2 \u00b6 When ETLs transform different kinds of entities, it can be useful to run a sub-ETL based on the type of entity. For example, if the current Etl Record represents a person, we want to assert their age. But if the current Etl Record represents a location, we want to assert its latitude and longitude: const etl_location = [ triple('iri', sdo.latitude, literal('lat', xsd.double)), triple('iri', sdo.longitude, literal('long', xsd.double)), ] const etl_person = [ triple('iri', sdo.age, literal('age', xsd.nonNegativeInteger)), ] etl.run( _switch('type', ['location', etl_location], ['person', etl_person], ), ) Skipping remaining functions ( skipRest() ) \u00b6 The skipRest() function allows us to stop the execution of any subsequent functions declared within the same code block where the skipRest() function is located. When you provide a key argument, skipRest() will skip over any following functions if the specified key is found in the record. Whenever there is no key argument specified, any functions after skipRest() will not be executed. Parameters \u00b6 key The optional key parameter value is compared against the keys in the record, if present in the record the remaining functions will not be executed. Example 1 : The following code snippet will stop executing any function after skipRest() , because no key is specified: fromJson([ { id: '123', first: 'John', last: 'Doe' }, { id: '456', first: 'Jane', last: 'Smith' }, ]), addIri({ content: 'id', key: '_id', prefix: prefix.person, }), triple('_id', foaf.lastName, 'last'), skipRest(), triple('_id', foaf.firstName, 'first') Since skipRest() is declared before triple('_id', foaf.firstName, 'first') , the following assertion is not made: triple('_id', foaf.firstName, 'first') Example 2 : whenForEach with a specified key for skipRest() : fromJson( { Person: [ { firstName: 'John', lastName: 'Doe' }, { firstName: 'Tom', last: 'Smith' }, { firstName: 'Lisa', lastName: 'Kennedy' } ] } ), whenForEach(\"Person\", skipRest('last'), addIri({ content: 'firstName', key: '_firstName', prefix: prefix.person }), triple('_firstName', foaf.firstName, 'firstName'), triple('_firstName', foaf.lastName, 'lastName')), As a result, only the following triples will be asserted: <https://example.com/person/John> <http://xmlns.com/foaf/0.1/firstName> \"John\"; <http://xmlns.com/foaf/0.1/lastName> \"Doe\". <https://example.com/person/Lisa> <http://xmlns.com/foaf/0.1/firstName> \"Lisa\"; <http://xmlns.com/foaf/0.1/lastName> \"Kennedy\" Note that the record for \"Tom Smith\" was skipped, and no triples were asserted! This because the key 'last' was present in that record, and due to the usage of skipRest('last') , all functions after skipRest() will not be executed.","title":"Control Structures"},{"location":"triply-etl/generic/control-structures/#control-structures","text":"This page documents how you can use control structures in your ETL configuration.","title":"Control Structures"},{"location":"triply-etl/generic/control-structures/#process-data-conditionally-when","text":"Source data often contains optional values. These are values that appear in some, but not all records. Source data often contains 'special' values that denote the absence of a value. Common examples are values such as 'NULL' or the empty string ( '' ) or 'clear' outliers such as 9999 for a missing year. We call such values \u2018null values\u2019. The when() function allows part of a TriplyETL configuration to run when certain conditions are met. The first parameter is used to determine whether or not the remaining parameters should be called: when('{condition}', '{statement-1}', '{statement-2}', '{statement-3}', // etc ), Notice that it is often useful to specify multiple statements under the same condition: The first statement transforms an optional value, and the second statement uses the transformed optional value to make a triple assertion. The first statement asserts one triple based on the optional value, and the second statement asserts a second triple based on the same optional value.","title":"Process data conditionally (when())"},{"location":"triply-etl/generic/control-structures/#missing-values","text":"If a value is sometimes completely missing from a source data record, the when() conditional function can be used. The following code snippet assets a triple if and only if a value for the 'zipcode' key is present in the Record: when(context => context.isNotEmpty('zipcode'), triple(iri(prefix.id, 'id'), def.zipcode, 'zipcode'), ), Since checking for the presence or absence of a single record is very common, the above can also be written as follows: when('zipcode', triple(iri(prefix.id, 'id'), def.zipcode, 'zipcode'), ),","title":"Missing values"},{"location":"triply-etl/generic/control-structures/#the-empty-string","text":"In many source data formats, the empty string is used to signify a missing value, this particular string is treated in a special way by when() . A key whose value is the empty string is treated in the same way as a key that is altogether absent. The following code snippet will not print the record to standard output, because the 'zipcode' key is considered empty: fromJson([{ zipcode: '' }]), when('zipcode', logRecord(), ), Notice that it is almost never useful to store the empty string in linked data. So the treatment of empty strings as NULL values is the correct default behavior.","title":"The empty string"},{"location":"triply-etl/generic/control-structures/#null-values-when-and-whennotequal","text":"If a key contains specific values that are indended to represent NULL values, then these must be specifically identified the first when() parameter. The following code snippet identifies the value 9999 for the 'created' key as denoting a NULL values. This means that the year 9999 is used in the source system whenever the actual year of creation was unknown. when(context => context.getNumber('created') != 9999, triple(iri(prefix.id, 'id'), dct.created, literal('created', xsd.gYear)), ), Since checking the value of one specific key is very common, the above can be written as follows, using the more specific whenNotEqual function: whenNotEqual('created', 9999, triple(iri(prefix.id, 'id'), dct.created, literal('created', xsd.gYear)), ), Notice that the use of whenNotEqual() makes the configuration easier to read. The same shorthand notation works when there are multiple NULL values in the source data. The following code snippet only asserts a triple if the year of creation is neither 9999 nor -1. Notice that the array can contain any number of potential NULL values: whenNotEqual('created', [-1, 9999], triple(iri(prefix.id, 'id'), dct.created, literal('created', xsd.gYear)), ),","title":"NULL values (when() and whenNotEqual())"},{"location":"triply-etl/generic/control-structures/#iterating-over-lists-of-objects-foreach","text":"In the previous section, we saw that we were able to assert the name of the first country and the name of the second country. But what do we do if we want to assert the name for every country in the world? And what do we do if some countries have a name in 2 languages, but other countries have a name in 1 or 3 languages? What we need is a simple way to express that we want to make an assertion for every element in a list. TriplyETL provides the forEach() function for this purpose. The following code snippet asserts the name for each country in the example data: forEach('data.countries', triple(iri(prefix.id, 'id'), rdfs.label, 'name'), ), Notice the following details: - forEach() uses the path expression 'data.countries' to identify the list. - Inside the forEach() function, each element in the list is made available separately. - This allows the 'id' and 'name' keys to be identified directly. The above code snippet makes one assertion for every element in the \"countries\" list: country:nl rdfs:label 'The Netherlands'. country:de rdfs:label 'Germany'. Notice that forEach() only works for lists whose elements are objects*. See Iterating over lists of primitives for dealing with lists that do not contain objects. The elements that forEach() iterates over are themselves (sub)records. This implies that all functions that work for full records also work for the (sub)records inside forEach() . The (sub)records inside an forEach() function are smaller. This allows the regular keys of the iterated-over elements to be accessed directly. In addition to these regular keys, (sub)records inside forEach() also contain additional keys that simplify common operations. The following subsections explain the following special keys: Index key ( $index ) Parent key ( $parent ) Root key ( $root )","title":"Iterating over lists of objects (forEach())"},{"location":"triply-etl/generic/control-structures/#index-key-index","text":"Each (sub)record that is made available in forEach() contains the $index key. The value of this key is the index of the element in the list. This is the same index that is used to access specific elements in an list, as explained in the section on accessing lists by index . The index key is often useful for assigning a unique subject IRI to every element. Suppose we have the following source data. We do not want to use the values of the \"name\" key for our subject IRI, because these names contain spaces and possibly other problematic characters that make the IRI more difficult to read and use. { \"countries\": [ { \"name\": \"The Netherlands\" }, { \"name\": \"Germany\" }, { \"name\": \"Italy\" } ] } The following code snippet uses the $index key that is made available inside forEach in order to create a unique subject IRI for each country: forEach('countries', triple(iri(prefix.id, '$index'), rdfs.label, 'name'), ), This results in the following assertions: country:0 rdfs:label 'The Netherlands'. country:1 rdfs:label 'Germany'. country:2 rdfs:label 'Italy'.","title":"Index key ($index)"},{"location":"triply-etl/generic/control-structures/#parent-key-parent","text":"When forEach() iterates through a list of elements, it makes the enclosingparent* record available under key $parent . The parent record is the record that directly contains the first key that appears in the path that was specified in forEach() . For example, the parent record in the following call is the record that directly contains the \"data\" key: forEach('data.countries', // etc ), The $parent key can be observed when logRecord` is used to print the iterated-over elements to the terminal: forEach('data.countries', logRecord(), ), For our example source data, this emits the following 2 records: { \"id\": \"en\", \"name\": \"The Netherlands\", \"$index\": 0, \"$parent\": { \"data\": { \"labels\": [ { \"id\": \"en\", \"name\": \"The Netherlands\", }, { \"id\": \"de\" \"name\": \"Germany\", } ] } }, \"$root\": \"__circular__\" } and: { \"id\": \"de\", \"name\": \"Germany\", \"$index\": 1, \"$parent\": { \"data\": { \"labels\": [ { \"id\": \"en\", \"name\": \"The Netherlands\", }, { \"id\": \"de\" \"name\": \"Germany\", } ] } }, \"$root\": \"__circular__\" } The $root key is explained in the next section .","title":"Parent key ($parent)"},{"location":"triply-etl/generic/control-structures/#root-key-root","text":"Sometimes it may be necessary to access a part of the original record that is outside of the scope of the forEach() call. Every (sub)record inside a forEach() call contains the \"$root\" key. The value of the root key provides a link to the full record. Because the $root key is part of the linked-to record, it is not possible to print the value of the root key. (This would result in infinite output.) For this reason, the value of the $root key is printed as the special value \"__circular__\" . For the above examples, the parent record and root record are the same, but this is not always the case. Specifically, the parent record and root record are different when forEach() calls are nested. The following data contains an inner list (key \"labels\" ) inside an outer list ( \"countries\" ): { \"data\": { \"countries\": [ { \"id\": \"NL\", \"labels\": [ { \"name\": \"The Netherlands\", \"locale\": \"en-us\" }, { \"name\": \"Nederland\", \"locale\": \"nl-nl\" } ] }, { \"id\": \"EN\", \"labels\": [ { \"name\": \"England\", \"locale\": \"en-gb\" } ] } ] } } The following nested forEach() call shows the difference between the \"$parent\" key and the $root key. The $parent key allows the individual country objects to be accessed, while the \"$root\" key allows the entire tree to be accessed: forEach('data.countries', forEach('labels', logRecord(), ), ), The following record is printed first (3 records are printed in total). Notice that the value of the outer $parent and \"$root\" keys are now different: - The $parent key allows access to the first country. - The $root key allows access to the full record (describing multiple countries). { \"name\": \"The Netherlands\", \"locale\": \"en-us\", \"$index\": 0, \"$parent\": { \"id\": \"NL\", \"labels\": [ { \"name\": \"The Netherlands\", \"locale\": \"en-us\" }, { \"name\": \"Nederland\", \"locale\": \"nl-nl\" } ], \"$index\": 0, \"$parent\": { \"data\": { \"countries\": [ { \"id\": \"NL\", \"labels\": \"__circular__\" }, { \"id\": \"EN\", \"labels\": [ { \"name\": \"England\", \"locale\": \"en-gb\" } ] } ] } }, \"$root\": \"__circular__\" }, \"$root\": \"__circular__\" }","title":"Root key ($root)"},{"location":"triply-etl/generic/control-structures/#iterating-over-lists-of-primitives","text":"Notice that forEach() can only iterate over Records. Sometimes, it is necessary to iterate over primitive values, for example strings. If the iteration directly results in RDF assertions, iris() and literals() can be used. But in other cases, we must map the primitive values to objects. These objects can then be processed as regular Records. Here is an example: fromJson({value: 'a b c'}), split({content: 'value', separator: ' ', key: 'valueStrings'}), // Since we cannot transform 'valueStrings' directly, we first create // objects that contain those strings ('valueObjects'). custom.addFrom.value({ content: 'valueStrings', change: values => (values as string[]).map(value => ({'valueObject': value})), type: 'unknown', key: 'valueObjects' }), // Here we can apply any regular transformation that works with Records.","title":"Iterating over lists of primitives"},{"location":"triply-etl/generic/control-structures/#specify-multiple-conditions-ifelse","text":"The ifElse() function in TriplyETL allows us to specify multiple conditions based on which other functions are run. Every condition is specified with an if key. In case the condition is true, the functions specified in the then key are run. If none of the if conditions are true, the functions specified in an else key, if present, are run.","title":"Specify multiple conditions (ifElse())"},{"location":"triply-etl/generic/control-structures/#parameters","text":"The first parameter must be an { if: ..., then: ... } object. The non-first parameters are either additional { if: ..., then: ... } objects or a final { else: ... } object. Each if key specifies a condition that is either true or false. Conditions are either a key name or a function that takes the Etl Context and returns a Boolean value. Specifying a key name is identical to specifying the following function: ctx => ctx.getString('KEY') The then and else keys take either one function, or an array of zero or more functions.","title":"Parameters"},{"location":"triply-etl/generic/control-structures/#example-1","text":"The following code snippet uses different conditions to determine the age category that a person belongs to: fromJson([ { id: 'johndoe', age: 12 }, { id: 'janedoe', age: 32 }, // ... ]), addIri({ prefix: prefix.person, content: 'id', key: '_person', }), ifElse({ if: ctx => ctx.getNumber('age') < 12, then: triple('_person', a, def.Child), }, { if: ctx => { const age = ctx.getNumber('age') return age >= 12 && age < 20 }, then: triple('_person', a, def.Teenager), }, { if: ctx => { const age = ctx.getNumber('age') return age >= 20 && age < 65 }, then: triple('_person', a, def.Adult), }, { else: triple('_person', a, def.Senior), }),","title":"Example 1"},{"location":"triply-etl/generic/control-structures/#example-2","text":"The following snippet either asserts data about persons or data about organizations, and uses an ifElse to make the conditional determination on which assertion to make: fromJson([ { first: 'John', last: 'Doe' }, { name: 'Triply' }, ]), ifElse({ if: 'name', then: pairs(iri(prefix.id, 'name'), [a, sdo.Organization], [sdo.name, 'name'], ), }, { else: [ concat({ content: ['first', 'last'], separator: '-', key: 'name', }), pairs(iri(prefix.id, 'name'), [a, sdo.Person], [sdo.givenName, 'first'], [sdo.familyName, 'last'], ), ], }),","title":"Example 2"},{"location":"triply-etl/generic/control-structures/#switch-between-different-cases-_switch","text":"The function _switch() allows us to switch between different cases, based on the value of a specified key. The signature is as follows; _switch(key, [value_1, functions_1], ..., [value_n, functions_n], default_functions, )","title":"Switch between different cases (_switch())"},{"location":"triply-etl/generic/control-structures/#parameters_1","text":"key The key parameter whose value is compared against the specified values. Each case consists of a list of two elements: value_i is the value that is checked for equivalence with the value stored in key . functions_i is the function or list of functions that is executed when the value in key is equivalent to value_i . default_functions is the function or list of functions that is executed when key matches neither of the cases. Notice that we must write _switch() because switch is a reserved keyword in ECMAScript. An error is emitted if the value for key does not match any of the cases and no default case is specified.","title":"Parameters"},{"location":"triply-etl/generic/control-structures/#example-1_1","text":"When an ETL uses multiple data sources, we can use a _switch() to run a dedicated sub-ETL for each data source. Suppose we have two tabular data sources: file.episodes and file.people . We can use the following _switch() statement to run different sub-ETLs: _switch(key.fileName, [file.episodes, etl_episodes], [file.people, etl_people], ),","title":"Example 1"},{"location":"triply-etl/generic/control-structures/#example-2_1","text":"When ETLs transform different kinds of entities, it can be useful to run a sub-ETL based on the type of entity. For example, if the current Etl Record represents a person, we want to assert their age. But if the current Etl Record represents a location, we want to assert its latitude and longitude: const etl_location = [ triple('iri', sdo.latitude, literal('lat', xsd.double)), triple('iri', sdo.longitude, literal('long', xsd.double)), ] const etl_person = [ triple('iri', sdo.age, literal('age', xsd.nonNegativeInteger)), ] etl.run( _switch('type', ['location', etl_location], ['person', etl_person], ), )","title":"Example 2"},{"location":"triply-etl/generic/control-structures/#skipping-remaining-functions-skiprest","text":"The skipRest() function allows us to stop the execution of any subsequent functions declared within the same code block where the skipRest() function is located. When you provide a key argument, skipRest() will skip over any following functions if the specified key is found in the record. Whenever there is no key argument specified, any functions after skipRest() will not be executed.","title":"Skipping remaining functions (skipRest())"},{"location":"triply-etl/generic/control-structures/#parameters_2","text":"key The optional key parameter value is compared against the keys in the record, if present in the record the remaining functions will not be executed. Example 1 : The following code snippet will stop executing any function after skipRest() , because no key is specified: fromJson([ { id: '123', first: 'John', last: 'Doe' }, { id: '456', first: 'Jane', last: 'Smith' }, ]), addIri({ content: 'id', key: '_id', prefix: prefix.person, }), triple('_id', foaf.lastName, 'last'), skipRest(), triple('_id', foaf.firstName, 'first') Since skipRest() is declared before triple('_id', foaf.firstName, 'first') , the following assertion is not made: triple('_id', foaf.firstName, 'first') Example 2 : whenForEach with a specified key for skipRest() : fromJson( { Person: [ { firstName: 'John', lastName: 'Doe' }, { firstName: 'Tom', last: 'Smith' }, { firstName: 'Lisa', lastName: 'Kennedy' } ] } ), whenForEach(\"Person\", skipRest('last'), addIri({ content: 'firstName', key: '_firstName', prefix: prefix.person }), triple('_firstName', foaf.firstName, 'firstName'), triple('_firstName', foaf.lastName, 'lastName')), As a result, only the following triples will be asserted: <https://example.com/person/John> <http://xmlns.com/foaf/0.1/firstName> \"John\"; <http://xmlns.com/foaf/0.1/lastName> \"Doe\". <https://example.com/person/Lisa> <http://xmlns.com/foaf/0.1/firstName> \"Lisa\"; <http://xmlns.com/foaf/0.1/lastName> \"Kennedy\" Note that the record for \"Tom Smith\" was skipped, and no triples were asserted! This because the key 'last' was present in that record, and due to the usage of skipRest('last') , all functions after skipRest() will not be executed.","title":"Parameters"},{"location":"triply-etl/generic/debug/","text":"On this page: Debug Overview Function logMemory() Function logQuads() Function logQuery() Function logRecord() Use when writing a new ETL Observe the effects of transformations Log a specific key Functions traceStart() and traceEnd() Debug \u00b6 TriplyETL includes functions that can be used during debugging. These debug function allow you to inspect in a detailed way how data flows through your pipeline. This allows you to find problems more quickly, and allows you to determine whether data is handled correctly by your TriplyETL configuration. Overview \u00b6 The following debug function are available: Function Description logMemory() Prints the current memory consumption. logQuads() Prints the contents of the internal store to standard output. logQuery() Prints a query string to standard output. logRecord() Prints the record in its current state to standard output. traceEnd() Ends a trace of the record and internal store. traceStart() Starts a trace of the record and internal store. These functions can be imported from the debug module: import { logMemory, logQuads, logQuery, logRecord, traceEnd, traceStart } from '@triplyetl/etl/debug' Function logMemory() \u00b6 This function prints information about the current memory consumption. It includes the following fields: Field name Meaning Use case CallCount The number of times that a specific use of logMemory() can been invoked. Find a location in your ETL script that is visited many times, e.g. because it occurs inside a (nested) loop. RecordId The numeric identifier of the record that is currently processed. Find a specific record that causes memory consumption to increase. Heap used The number of megabytes that are currently used on the heap. Find places in your ETL where an unexpected amount of memory is used. Heap total The number of megabytes that are currently allocated on the heap. Find places in your ETL where memory reallocation occurs. The following code snippet prints the memory consumption of TriplyETL for each record (first call), and for each member of key 'a' (second call): fromJson([{ a: [{ b: 1 }, { b: 2 }] }, { a: [] }, { a: [] }]), logMemory(), forEach('a', logMemory()), This prints the following messages to standard output: Info CallCount: 1 | RecordId: 1 | Heap (MB) used: 92 / total: 122 Info CallCount: 1 | RecordId: 1 | Heap (MB) used: 92 / total: 122 Info CallCount: 2 | RecordId: 1 | Heap (MB) used: 92 / total: 122 Info CallCount: 2 | RecordId: 2 | Heap (MB) used: 92 / total: 122 Info CallCount: 3 | RecordId: 2 | Heap (MB) used: 92 / total: 122 Function logQuads() \u00b6 This function prints the current contents of the internal store to standard output. The following snippet asserts one triple into the default graph of the internal store, and then prints the contents of the internal store: fromJson([{}]), triple(rdfs.Class, a, rdfs.Class), logQuads(), This results in the following output: @prefix rdf: <http://www.w3.org/1999/02/22-rdf-syntax-ns#>. @prefix rdfs: <http://www.w3.org/2000/01/rdf-schema#>. @prefix sdo: <https://schema.org/>. <https://triplydb.com/graph/default> { rdfs:Class a rdfs:Class } Function logQuery() \u00b6 This function prints a query string to standard output. This is specifically useful when the query string is stored in an external system, e.g. a SPARQL query string that is stored on a TriplyDB server: logQuery(Source.TriplyDb.query('my-account', 'my-query')), Depending on the query string that is stored in 'my-query' , this could result in the following output: select * { ?s ?p ?o. } limit 10 Function logRecord() \u00b6 This function prints the current state of the record to standard output. The record is a generic representation of the data that is extracted from one of the data sources (see the Record documentation page for more information). The following snippet prints the inline JSON record to standard output: fromJson([{ a: 1 }]), logRecord(), This emits the following: { \"a\": 1, \"$recordId\": 1, \"$environment\": \"Development\" } Use when writing a new ETL \u00b6 When writing a new ETL, logRecord() is often used as the first function to invoke immediately after extracting the record. For example: fromJson(Source.url('https://example.com/some/api/call')), logRecord(), Since this prints a full overview of what is available in the data source, this forms a good starting point for writing the rest of the ETL configurations. Observe the effects of transformations \u00b6 Another common use case for logRecord() is to observe the record at different moments in time. This is specifically useful to observe the effects of transformation functions , since these are the functions that modify the record. The following snippet logs the record directly before and directly after the transformation function split() is called. fromJson([{ a: '1, 2, 3' }]), logRecord(), split({ content: 'a', separator: ',', key: 'b' }), logRecord(), This makes it easy to observe the result of applying the transformation function: Running lib/main.js { \"a\": \"1, 2, 3\", \"$recordId\": 1, \"$environment\": \"Development\" } { \"a\": \"1, 2, 3\", \"$recordId\": 1, \"$environment\": \"Development\", \"b\": [ \"1\", \"2\", \"3\" ] } Log a specific key \u00b6 Since records can be quite long, in some cases it may be easier to print only a specific key. The following code snippet only prints the key that was added by the transformation function: fromJson([{ a: '1, 2, 3' }]), split({ content: 'a', separator: ',', key: 'b' }), logRecord({ key: 'b' }), This results in the following output: [ \"1\", \"2\", \"3\" ] Functions traceStart() and traceEnd() \u00b6 Sometimes you are interested to find one specific record based on a certain value of a key and/or to see the changes in this record made by specific middlewares. For these purposes, trace middleware can be used. Below, there is an example of how this middleware can be used: fromJson([ { a: 1, b: 1 }, // first dummy record { a: 2, b: 2 }, // second dummy record ]), change({key:'a', type:'number', change: (val) => val +100}), // change the 'a' key traceStart(), change({key:'b', type:'number', change: (val) => val +100}), // change the 'b' key traceEnd(), The result would be: \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u2502 Record trace information \u2502 \u2502 { \u2502 \u2502 \"a\": 101, \u2502 \u2502 \"b\": 1 \u2502 \u2502 \"b\": 101 \u2502 \u2502 } \u2502 \u2502 \u2502 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u2502 Quads trace information (unchanged) \u2502 \u2502 empty \u2502 \u2502 \u2502 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 To rerun the traced middlewares for this record use the following command: > npx etl lib/{script-name} --trace .trace-1650542307095 In your terminal the line with \"b\": 1 will be red colored, showing the previous state of this key-value and the line with \"b\": 101 will be green colored, showing the new state. Also you can rerun the trace information for this specific record by running: npx etl lib/{script-name} --trace .trace-1650542307095","title":"Debug"},{"location":"triply-etl/generic/debug/#debug","text":"TriplyETL includes functions that can be used during debugging. These debug function allow you to inspect in a detailed way how data flows through your pipeline. This allows you to find problems more quickly, and allows you to determine whether data is handled correctly by your TriplyETL configuration.","title":"Debug"},{"location":"triply-etl/generic/debug/#overview","text":"The following debug function are available: Function Description logMemory() Prints the current memory consumption. logQuads() Prints the contents of the internal store to standard output. logQuery() Prints a query string to standard output. logRecord() Prints the record in its current state to standard output. traceEnd() Ends a trace of the record and internal store. traceStart() Starts a trace of the record and internal store. These functions can be imported from the debug module: import { logMemory, logQuads, logQuery, logRecord, traceEnd, traceStart } from '@triplyetl/etl/debug'","title":"Overview"},{"location":"triply-etl/generic/debug/#function-logmemory","text":"This function prints information about the current memory consumption. It includes the following fields: Field name Meaning Use case CallCount The number of times that a specific use of logMemory() can been invoked. Find a location in your ETL script that is visited many times, e.g. because it occurs inside a (nested) loop. RecordId The numeric identifier of the record that is currently processed. Find a specific record that causes memory consumption to increase. Heap used The number of megabytes that are currently used on the heap. Find places in your ETL where an unexpected amount of memory is used. Heap total The number of megabytes that are currently allocated on the heap. Find places in your ETL where memory reallocation occurs. The following code snippet prints the memory consumption of TriplyETL for each record (first call), and for each member of key 'a' (second call): fromJson([{ a: [{ b: 1 }, { b: 2 }] }, { a: [] }, { a: [] }]), logMemory(), forEach('a', logMemory()), This prints the following messages to standard output: Info CallCount: 1 | RecordId: 1 | Heap (MB) used: 92 / total: 122 Info CallCount: 1 | RecordId: 1 | Heap (MB) used: 92 / total: 122 Info CallCount: 2 | RecordId: 1 | Heap (MB) used: 92 / total: 122 Info CallCount: 2 | RecordId: 2 | Heap (MB) used: 92 / total: 122 Info CallCount: 3 | RecordId: 2 | Heap (MB) used: 92 / total: 122","title":"Function logMemory()"},{"location":"triply-etl/generic/debug/#function-logquads","text":"This function prints the current contents of the internal store to standard output. The following snippet asserts one triple into the default graph of the internal store, and then prints the contents of the internal store: fromJson([{}]), triple(rdfs.Class, a, rdfs.Class), logQuads(), This results in the following output: @prefix rdf: <http://www.w3.org/1999/02/22-rdf-syntax-ns#>. @prefix rdfs: <http://www.w3.org/2000/01/rdf-schema#>. @prefix sdo: <https://schema.org/>. <https://triplydb.com/graph/default> { rdfs:Class a rdfs:Class }","title":"Function logQuads()"},{"location":"triply-etl/generic/debug/#function-logquery","text":"This function prints a query string to standard output. This is specifically useful when the query string is stored in an external system, e.g. a SPARQL query string that is stored on a TriplyDB server: logQuery(Source.TriplyDb.query('my-account', 'my-query')), Depending on the query string that is stored in 'my-query' , this could result in the following output: select * { ?s ?p ?o. } limit 10","title":"Function logQuery()"},{"location":"triply-etl/generic/debug/#function-logrecord","text":"This function prints the current state of the record to standard output. The record is a generic representation of the data that is extracted from one of the data sources (see the Record documentation page for more information). The following snippet prints the inline JSON record to standard output: fromJson([{ a: 1 }]), logRecord(), This emits the following: { \"a\": 1, \"$recordId\": 1, \"$environment\": \"Development\" }","title":"Function logRecord()"},{"location":"triply-etl/generic/debug/#use-when-writing-a-new-etl","text":"When writing a new ETL, logRecord() is often used as the first function to invoke immediately after extracting the record. For example: fromJson(Source.url('https://example.com/some/api/call')), logRecord(), Since this prints a full overview of what is available in the data source, this forms a good starting point for writing the rest of the ETL configurations.","title":"Use when writing a new ETL"},{"location":"triply-etl/generic/debug/#observe-the-effects-of-transformations","text":"Another common use case for logRecord() is to observe the record at different moments in time. This is specifically useful to observe the effects of transformation functions , since these are the functions that modify the record. The following snippet logs the record directly before and directly after the transformation function split() is called. fromJson([{ a: '1, 2, 3' }]), logRecord(), split({ content: 'a', separator: ',', key: 'b' }), logRecord(), This makes it easy to observe the result of applying the transformation function: Running lib/main.js { \"a\": \"1, 2, 3\", \"$recordId\": 1, \"$environment\": \"Development\" } { \"a\": \"1, 2, 3\", \"$recordId\": 1, \"$environment\": \"Development\", \"b\": [ \"1\", \"2\", \"3\" ] }","title":"Observe the effects of transformations"},{"location":"triply-etl/generic/debug/#log-a-specific-key","text":"Since records can be quite long, in some cases it may be easier to print only a specific key. The following code snippet only prints the key that was added by the transformation function: fromJson([{ a: '1, 2, 3' }]), split({ content: 'a', separator: ',', key: 'b' }), logRecord({ key: 'b' }), This results in the following output: [ \"1\", \"2\", \"3\" ]","title":"Log a specific key"},{"location":"triply-etl/generic/debug/#functions-tracestart-and-traceend","text":"Sometimes you are interested to find one specific record based on a certain value of a key and/or to see the changes in this record made by specific middlewares. For these purposes, trace middleware can be used. Below, there is an example of how this middleware can be used: fromJson([ { a: 1, b: 1 }, // first dummy record { a: 2, b: 2 }, // second dummy record ]), change({key:'a', type:'number', change: (val) => val +100}), // change the 'a' key traceStart(), change({key:'b', type:'number', change: (val) => val +100}), // change the 'b' key traceEnd(), The result would be: \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u2502 Record trace information \u2502 \u2502 { \u2502 \u2502 \"a\": 101, \u2502 \u2502 \"b\": 1 \u2502 \u2502 \"b\": 101 \u2502 \u2502 } \u2502 \u2502 \u2502 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u2502 Quads trace information (unchanged) \u2502 \u2502 empty \u2502 \u2502 \u2502 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 To rerun the traced middlewares for this record use the following command: > npx etl lib/{script-name} --trace .trace-1650542307095 In your terminal the line with \"b\": 1 will be red colored, showing the previous state of this key-value and the line with \"b\": 101 will be green colored, showing the new state. Also you can rerun the trace information for this specific record by running: npx etl lib/{script-name} --trace .trace-1650542307095","title":"Functions traceStart() and traceEnd()"},{"location":"triply-etl/generic/declarations/","text":"On this page: Declarations Introduction What is a declaration? Why use declarations? How to import declaration functionality? Base IRI declaration Prefix declarations Individual prefix declarations Example Prefix declaration tree / IRI strategy Example Term declarations Concept term declarations Vocabulary term declarations Shape term declarations Individual term declarations Graph name declarations External vocabularies Example: Using the PREMIS external vocabulary Language tag declarations Shorthand declarations The standard shorthand Example User-defined shorthands Example Geospatial declarations Declarations \u00b6 This page documents the declaration functionalities that are supported by TriplyETL. Introduction \u00b6 This section explains what declarations are, and what are the benefits of using them. What is a declaration? \u00b6 A declarations introduces a constant that can be (re)used throughout the rest of the ETL configuration. This is best shown through an example. The following code snippet asserts that John Doe is a person. It uses the following components that probably occur multiple times in the same ETL configuration: The namespace for this dataset is <https://triplydb.com/my-account/my-dataset/> . The IRI for each person in the dataset starts with <https://triplydb.com/my-account/my-dataset/id/person/> . The IRI <http://www.w3.org/1999/02/22-rdf-syntax-ns#type> is used by the external RDF vocabulary to relate instances to their class. The IRI <http://xmlns.com/foaf/0.1/Person> is used by the external FOAF vocabulary to denote the set of all persons. triple( iri('https://triplydb.com/my-account/my-dataset/id/person/john-doe'), iri('http://www.w3.org/1999/02/22-rdf-syntax-ns#type'), iri('http://xmlns.com/foaf/0.1/Person') ), By using declarations, we can use constants to abbreviate the components that occur many times. This results in the following, identical assertion: triple(iri(prefix.person, str('john-doe')), a, foaf.Person), Why use declarations? \u00b6 The use of declarations has the following benefits: Readability : Shorter expressions are easier to read. See the example from the previous subsection for an example. Modularity : By putting all declarations in one place, it is easy to include them into multiple places in the ETL configuration. Maintenance : A configuration change that is made in a declaration immediately becomes available to all the locations in which it is used. This makes it easy to update things like namespaces, that would be cumbersome and error-prone to change in each location of use. Editor support : Declarations support auto-complete functionality in text editors. When documentation is included, this is shown alongside the declarations in text editors. How to import declaration functionality? \u00b6 Declaration objects are found in the following modules: import { Iri } from '@triplyetl/etl/generic' import { sdo, owl, sh, skos } from '@triplyetl/vocabularies' Base IRI declaration \u00b6 Within one dataset, it is common for the majority of IRIs to share the same IRI prefix or 'namespace'. It is convenient to declare this shared IRI prefix or namespace once, and use it throughout the rest of the ETL. This most broadly shared IRI prefix is called the base IRI . In TriplyETL, the base IRI is declared in the following way: const baseIri = Iri('https://triplydb.com/triply/iris/') Notice that the base IRI that is declared above, is also the location where the dataset that uses this base IRI can be found: https://triplydb.com/triply/iris/ . This is the dereferencing principle of linked data: IRIs are used for both naming \u00e1nd locating data. Prefix declarations \u00b6 Linked data uses IRIs for uniquely identifying most data items. Since IRIs can be long and complex, it is common to declare shorter aliases that can be used to abbreviate them. Such aliases are introduced in prefix declarations . Individual prefix declarations \u00b6 A new prefix declaration is created by using the concat() member function on an existing IRI object. The concat() function specifies the string that is added to the existing IRI. The added string must meet the syntactic criteria for the path segment component in IRI syntax (see RFC 3987 for the official syntax). It is common practice to end the added string with a forward slash, which ends a path segment in IRI syntax. Prefix declarations are often based off of the base IRI , since that is the prefix IRI that is shared by most IRIs in a dataset. Example \u00b6 The following code snippet declares a base IRI, and then adds the following two prefix declarations: Alias prefix_id abbreviates IRI <https://triplydb.com/my-account/my-dataset/id/> , which is used by all IRIs that denote instances. Alias prefix_model abbreviates IRI <https://triplydb.com/my-account/my-dataset/model/> , which is used by all IRIs that are used in the data model. import { Iri } from '@triplyetl/etl/generic' const baseIri = Iri('https://triplydb.com/my-account/my-dataset/') const prefix_id = baseIri.concat('id/') const prefix_model = baseIri.concat('model/') Prefix declaration tree / IRI strategy \u00b6 It is common to declare the base IRI declaration and all prefix declarations in one single spot. This consolidates the full IRI strategy for a dataset in one place. This is easy for documentation purposes, since all project members can see the full set of IRI prefixes in one place. And this supports optimal reuse of these declarations throughout the ETL configuration. IRI prefixes form a tree : The root of the tree is the base IRI. The internal nodes of the tree are prefix declarations that are extended by some other prefix declaration. The external nodes or leaves of the tree are prefix declarations that are not extended by another prefix declaration. It is common to declare the leaves of the IRI prefix tree in an object, since such an object can be conveniently used to make term assertions throughout the ETL. Example \u00b6 The following code snippet gives an example of such an IRI prefix tree, where: The base IRI is <https://triplydb.com/my-account/my-dataset/> . The internal nodes are the base IRI, prefix_id , and prefix_model . The leaves are the three prefix declarations that appear in the prefix object. import { Iri } from '@triplyetl/etl/generic' const baseIri = Iri('https://triplydb.com/my-account/my-dataset/') const prefix_id = baseIri.concat('id/') const prefix_model = baseIri.concat('model/') const prefix = { city: prefix_id.concat('city/'), def: prefix_model.concat('def/'), person: prefix_id.concat('person/'), } With the above declarations in place, the following IRI term assertions can be made (see the iri() function for more information): iri(prefix.city, 'name') iri(prefix.city, str('Amsterdam')), iri(prefix.def, str('livesIn')), iri(prefix.person, 'first name') iri(prefix.person, str('John')), Static terms can also be expressed with the concat() member function: prefix.city.concat('Amsterdam') prefix.def.concat('livesIn') prefix.person.concat('John') The following statement assertion can be made (see the triple() function for more information). Notice that it is possible to mix (dynamic and static) iri() term assertions with IRIs created with concat() : triple( iri(prefix.person, 'first name'), iri(prefix.def, str('livesIn')), prefix.city.concat('Amsterdam') ), The statement assertion results in the following linked data: <https://triplydb.com/my-account/my-dataset/id/person/John> <https://triplydb.com/my-account/my-dataset/model/def/livesIn> <https://triplydb.com/my-account/my-dataset/id/city/Amsterdam> . Term declarations \u00b6 When a term is used in multiple places in the ETL configuration, it is often better to declare it first and (re)use it later. This ensures that changes to the term are applied in every location of use. We will use the following prefix declaration tree in our examples: import { Iri } from '@triplyetl/etl/generic' const baseIri = Iri('https://triplydb.com/my-account/my-dataset/') const prefix_id = baseIri.concat('id/') const prefix_model = baseIri.concat('model/') const prefix = { city: prefix_id.concat('city/'), con: prefix_model.concat('con/'), def: prefix_model.concat('def/'), graph: prefix_id.concat('graph/')), person: prefix_id.concat('person/'), shp: prefix_model.concat('shp/'), } Concept term declarations \u00b6 Concepts are expressed in linked data with SKOS. Concepts are often (re)used in multiple places, and they often form a fixed collection. This makes terms that denote concepts eligible for a term declaration object. The following code snippet declares the terms that denote concepts: const concept = { animal: prefix.con.concat('animal'), mammal: prefix.con.concat('mammal'), } This object can be used through the ETL configuration. For example in the following statement assertion: triple(concept.mammal, skos.broader, concept.animal), Vocabulary term declarations \u00b6 Classes and properties are expressed in linked data with RDFS/OWL. Classes and properties are often (re)used in multiple places, and they often form a fixed vocabulary. This makes terms that denote classes or properties eligible for a term declaration object. The following code snippet declares the terms that denote classes and properties: const def = { City: prefix.def.concat('City'), Person: prefix.def.concat('Person'), livesIn: prefix.def.concat('livesIn'), } Vocabulary term declarations can be used in statement assertions, for example: triple(iri(prefix.city, 'name'), a, def.City), pairs(iri(prefix.person, 'first name'), [a, def.Person], [def.livesIn, iri(prefix.city, 'name')], ), This results in the following linked data: city:Amsterdam a def:City. person:John a def:Person; def:livesIn city:Amsterdam. Or diagrammatically: graph LR john -- a --> Person john -- def:livesIn --> amsterdam Person[def:Person]:::model amsterdam[city:Amsterdam]:::data john[person:John]:::data classDef data fill:yellow classDef model fill:lightblue Shape term declarations \u00b6 Shapes are expressed in linked data with SHACL. shapes are often (re)used in multiple places, and they often form a fixed vocabulary. This makes terms that denote shapes eligible for a term declaration object. The following code snippet declares the terms that denote shapes: const shp = { City: prefix.shp.concat('City'), Person: prefix.shp.concat('Person'), Person_livesIn: prefix.shp.concat('livesIn'), } This object can be used through the ETL configuration. For example in the following statement assertions: pairs(shp.Person, [a, sh.NodeShape], [sh.property, shp.Person_livesIn], [sh.targetClass, def.Person], ), pairs(shp.Person_livesIn, [a, sh.PropertyShape], [sh.class, def.City], [sh.path, def:livesIn], ), This results in the following linked data: shp:Person a sh:NodeShape; sh:property shp:Person_livesIn; sh:targetClass def:Person. shp:Person_livesIn a sh:PropertyShape; sh:class def:City; sh:path def:livesIn. Or diagrammatically: graph LR shp_Person -- a --> sh:NodeShape shp_Person -- sh:property --> shp_Person_livesIn shp_Person -- sh:targetClass --> def_Person shp_Person_livesIn -- a --> sh:PropertyShape shp_Person_livesIn -- sh:class --> def_City shp_Person_livesIn -- sh:path --> def_livesIn def_City[def:City]:::model def_Person[def:Person]:::model def_livesIn[def:livesIn]:::model shp_Person[shp:Person]:::shape shp_Person_livesIn[shp:Person_livesIn]:::shape classDef model fill:lightblue classDef shape fill:orange Individual term declarations \u00b6 Individuals are instances of classes. For example, John Doe is an individual of class def:Person ; Amsterdam is an individual of class def:City . If terms that denote individuals are used multiple times in an ETL configuration, term declarations may be introduced for them. The following code snippet declares the terms that denote individual persons: const person = { jane: prefix.person('Jane'), john: prefix.person('John'), mary: prefix.person('Mary'), } Instance term declarations can be used in statement assertions, for example: triple(person.john, foaf.knows, person.mary), This results in the following linked data: person:john foaf:knows person:mary. Graph name declarations \u00b6 Linked data statements belong to graphs. Graphs are denoted by graph names. For example, a graph name may denote a graph that contains metadata statements, while another graph name may denote a graph that contains instance data. If graph names are used multiple times in an ETL configuration, term declarations may be introduced for them. The following code snippet declares three graph names: const graph = { metadata: prefix.graph.concat('metadata'), model: prefix.graph.concat('model'), instances: prefix.graph.concat('instances'), } The declared graph names can now be used in statement assertions: triples(graph.metadata, ['_dataset', a, dcat.Dataset], ['_dataset', rdfs.label, str('My Dataset')], ), External vocabularies \u00b6 In linked data, it is common to reuse existing vocabularies. Popular vocabularies can be imported from the TriplyETL vocabulary library. See the table of currently supported vocabularies for a full overview. The following example imports three vocabularies (FOAF, OWL, PREMIS): import { foaf, owl, premis } from '@triplyetl/vocabularies' This allows you to make the following statement assertion: triple(foaf.Person, a, owl.Class), Notice that the notation in TriplyETL comes very close to the notation in the standardized linked data syntax for Turtle, TriG, and SPARQL. For the example above: foaf:Person a owl:Class. Example: Using the PREMIS external vocabulary \u00b6 The following code snippet uses the external PREMIS vocabulary. This vocabulary is published by the Library of Congress and is used to publish metadata about the preservation of digital objects. The following code snippet asserts that a PREMIS file is stored in a PREMIS storage location: pairs(iri(prefix.file, 'File ID'), [a, premis.File], [premis.storedAt, iri(prefix.location, 'Location ID')], ), triple(iri(prefix.location, 'Location ID'), a, premis.StorageLocation), Language tag declarations \u00b6 Linked data includes support for language-tagged strings . These are literals that specify a string value and a code that denotes the natural language in which that string value should be interpreted. The natural language tags follow a syntax that is standardized in RFC 5646 , and must occur in the Language Subtag Registry that is maintained by IANA. TriplyETL includes declarations for these natural language tags. They can be imported as follows: import { language } from '@triplyetl/vocabularies' Language tag declaration can be used in literal() term assertions: literal(str('Nederland'), language.nl) literal(str('Netherlangs'), language.en) Language tag declarations can also be used in addLiteral() transformations: addLiteral({ content: 'label', languageTag: lang.fr, key: '_languageTaggedString', }), Shorthand declarations \u00b6 Shorthands are convenient names that stand for commonly used IRIs. There is one standard shorthand ( a ), and TriplyETL allows other shorthands to be declared as needed. The standard shorthand \u00b6 The standardized linked data syntax for Turtle, TriG, and SPARQL allow the shorthand a to be used to stand for the rdf:type property. TriplyETL supports this standard shorthand, which can be imported from the vocabulary library: import { a } from '@triplyetl/vocabularies' In the standardized linked data syntax for Turtle, TriG and SPARQL, this shorthand can only be used in the predicate position. This restriction is not enforced in TriplyETL, where the a shorthand can be used in the subject, predicate, object, and even graph position. Example \u00b6 The following code snippet makes a true statement assertion, while using the stands shorthand twice: triple(a, a, rdf.Property), This results in the following linked data: rdf:type a rdf:Property. User-defined shorthands \u00b6 TriplyETL allows the introduction of arbitrary, user-defined shorthands. User-defined shorthands can make linked data assertions in the ETL configuration more readable for users from certain domains. For example, \"is a\" is a commonly used phrase in many modeling languages to denote the subsumption relation. Example \u00b6 The following code snippet declares is_a as a user-defined shorthand for the rdfs:subClassOf property (which is the subsumption relation in linked data): import { foaf, rdfs } from '@triplyetl/vocabularies' const is_a = rdfs.subClassOf This declaration is used in the following statement assertion: triple(foaf.Person, is_a, foaf.Agent), This results in the following linked data: foaf:Person rdfs:subClassOf foaf:Agent. Geospatial declarations \u00b6 TriplyETL includes declarations for geospatial coordinate reference systems. These are identified by EPSG codes, and can imported as follows: import { epsg } from '@triplyetl/vocabularies' EPSG codes can be used in geospatial transformation functions like geojsonToWkt() : geojsonToWkt({ content: 'geojson', crs: epsg[28992], key: '_wkt', }),","title":"Declarations"},{"location":"triply-etl/generic/declarations/#declarations","text":"This page documents the declaration functionalities that are supported by TriplyETL.","title":"Declarations"},{"location":"triply-etl/generic/declarations/#introduction","text":"This section explains what declarations are, and what are the benefits of using them.","title":"Introduction"},{"location":"triply-etl/generic/declarations/#what-is-a-declaration","text":"A declarations introduces a constant that can be (re)used throughout the rest of the ETL configuration. This is best shown through an example. The following code snippet asserts that John Doe is a person. It uses the following components that probably occur multiple times in the same ETL configuration: The namespace for this dataset is <https://triplydb.com/my-account/my-dataset/> . The IRI for each person in the dataset starts with <https://triplydb.com/my-account/my-dataset/id/person/> . The IRI <http://www.w3.org/1999/02/22-rdf-syntax-ns#type> is used by the external RDF vocabulary to relate instances to their class. The IRI <http://xmlns.com/foaf/0.1/Person> is used by the external FOAF vocabulary to denote the set of all persons. triple( iri('https://triplydb.com/my-account/my-dataset/id/person/john-doe'), iri('http://www.w3.org/1999/02/22-rdf-syntax-ns#type'), iri('http://xmlns.com/foaf/0.1/Person') ), By using declarations, we can use constants to abbreviate the components that occur many times. This results in the following, identical assertion: triple(iri(prefix.person, str('john-doe')), a, foaf.Person),","title":"What is a declaration?"},{"location":"triply-etl/generic/declarations/#why-use-declarations","text":"The use of declarations has the following benefits: Readability : Shorter expressions are easier to read. See the example from the previous subsection for an example. Modularity : By putting all declarations in one place, it is easy to include them into multiple places in the ETL configuration. Maintenance : A configuration change that is made in a declaration immediately becomes available to all the locations in which it is used. This makes it easy to update things like namespaces, that would be cumbersome and error-prone to change in each location of use. Editor support : Declarations support auto-complete functionality in text editors. When documentation is included, this is shown alongside the declarations in text editors.","title":"Why use declarations?"},{"location":"triply-etl/generic/declarations/#how-to-import-declaration-functionality","text":"Declaration objects are found in the following modules: import { Iri } from '@triplyetl/etl/generic' import { sdo, owl, sh, skos } from '@triplyetl/vocabularies'","title":"How to import declaration functionality?"},{"location":"triply-etl/generic/declarations/#base-iri-declaration","text":"Within one dataset, it is common for the majority of IRIs to share the same IRI prefix or 'namespace'. It is convenient to declare this shared IRI prefix or namespace once, and use it throughout the rest of the ETL. This most broadly shared IRI prefix is called the base IRI . In TriplyETL, the base IRI is declared in the following way: const baseIri = Iri('https://triplydb.com/triply/iris/') Notice that the base IRI that is declared above, is also the location where the dataset that uses this base IRI can be found: https://triplydb.com/triply/iris/ . This is the dereferencing principle of linked data: IRIs are used for both naming \u00e1nd locating data.","title":"Base IRI declaration"},{"location":"triply-etl/generic/declarations/#prefix-declarations","text":"Linked data uses IRIs for uniquely identifying most data items. Since IRIs can be long and complex, it is common to declare shorter aliases that can be used to abbreviate them. Such aliases are introduced in prefix declarations .","title":"Prefix declarations"},{"location":"triply-etl/generic/declarations/#individual-prefix-declarations","text":"A new prefix declaration is created by using the concat() member function on an existing IRI object. The concat() function specifies the string that is added to the existing IRI. The added string must meet the syntactic criteria for the path segment component in IRI syntax (see RFC 3987 for the official syntax). It is common practice to end the added string with a forward slash, which ends a path segment in IRI syntax. Prefix declarations are often based off of the base IRI , since that is the prefix IRI that is shared by most IRIs in a dataset.","title":"Individual prefix declarations"},{"location":"triply-etl/generic/declarations/#example","text":"The following code snippet declares a base IRI, and then adds the following two prefix declarations: Alias prefix_id abbreviates IRI <https://triplydb.com/my-account/my-dataset/id/> , which is used by all IRIs that denote instances. Alias prefix_model abbreviates IRI <https://triplydb.com/my-account/my-dataset/model/> , which is used by all IRIs that are used in the data model. import { Iri } from '@triplyetl/etl/generic' const baseIri = Iri('https://triplydb.com/my-account/my-dataset/') const prefix_id = baseIri.concat('id/') const prefix_model = baseIri.concat('model/')","title":"Example"},{"location":"triply-etl/generic/declarations/#prefix-declaration-tree-iri-strategy","text":"It is common to declare the base IRI declaration and all prefix declarations in one single spot. This consolidates the full IRI strategy for a dataset in one place. This is easy for documentation purposes, since all project members can see the full set of IRI prefixes in one place. And this supports optimal reuse of these declarations throughout the ETL configuration. IRI prefixes form a tree : The root of the tree is the base IRI. The internal nodes of the tree are prefix declarations that are extended by some other prefix declaration. The external nodes or leaves of the tree are prefix declarations that are not extended by another prefix declaration. It is common to declare the leaves of the IRI prefix tree in an object, since such an object can be conveniently used to make term assertions throughout the ETL.","title":"Prefix declaration tree / IRI strategy"},{"location":"triply-etl/generic/declarations/#example_1","text":"The following code snippet gives an example of such an IRI prefix tree, where: The base IRI is <https://triplydb.com/my-account/my-dataset/> . The internal nodes are the base IRI, prefix_id , and prefix_model . The leaves are the three prefix declarations that appear in the prefix object. import { Iri } from '@triplyetl/etl/generic' const baseIri = Iri('https://triplydb.com/my-account/my-dataset/') const prefix_id = baseIri.concat('id/') const prefix_model = baseIri.concat('model/') const prefix = { city: prefix_id.concat('city/'), def: prefix_model.concat('def/'), person: prefix_id.concat('person/'), } With the above declarations in place, the following IRI term assertions can be made (see the iri() function for more information): iri(prefix.city, 'name') iri(prefix.city, str('Amsterdam')), iri(prefix.def, str('livesIn')), iri(prefix.person, 'first name') iri(prefix.person, str('John')), Static terms can also be expressed with the concat() member function: prefix.city.concat('Amsterdam') prefix.def.concat('livesIn') prefix.person.concat('John') The following statement assertion can be made (see the triple() function for more information). Notice that it is possible to mix (dynamic and static) iri() term assertions with IRIs created with concat() : triple( iri(prefix.person, 'first name'), iri(prefix.def, str('livesIn')), prefix.city.concat('Amsterdam') ), The statement assertion results in the following linked data: <https://triplydb.com/my-account/my-dataset/id/person/John> <https://triplydb.com/my-account/my-dataset/model/def/livesIn> <https://triplydb.com/my-account/my-dataset/id/city/Amsterdam> .","title":"Example"},{"location":"triply-etl/generic/declarations/#term-declarations","text":"When a term is used in multiple places in the ETL configuration, it is often better to declare it first and (re)use it later. This ensures that changes to the term are applied in every location of use. We will use the following prefix declaration tree in our examples: import { Iri } from '@triplyetl/etl/generic' const baseIri = Iri('https://triplydb.com/my-account/my-dataset/') const prefix_id = baseIri.concat('id/') const prefix_model = baseIri.concat('model/') const prefix = { city: prefix_id.concat('city/'), con: prefix_model.concat('con/'), def: prefix_model.concat('def/'), graph: prefix_id.concat('graph/')), person: prefix_id.concat('person/'), shp: prefix_model.concat('shp/'), }","title":"Term declarations"},{"location":"triply-etl/generic/declarations/#concept-term-declarations","text":"Concepts are expressed in linked data with SKOS. Concepts are often (re)used in multiple places, and they often form a fixed collection. This makes terms that denote concepts eligible for a term declaration object. The following code snippet declares the terms that denote concepts: const concept = { animal: prefix.con.concat('animal'), mammal: prefix.con.concat('mammal'), } This object can be used through the ETL configuration. For example in the following statement assertion: triple(concept.mammal, skos.broader, concept.animal),","title":"Concept term declarations"},{"location":"triply-etl/generic/declarations/#vocabulary-term-declarations","text":"Classes and properties are expressed in linked data with RDFS/OWL. Classes and properties are often (re)used in multiple places, and they often form a fixed vocabulary. This makes terms that denote classes or properties eligible for a term declaration object. The following code snippet declares the terms that denote classes and properties: const def = { City: prefix.def.concat('City'), Person: prefix.def.concat('Person'), livesIn: prefix.def.concat('livesIn'), } Vocabulary term declarations can be used in statement assertions, for example: triple(iri(prefix.city, 'name'), a, def.City), pairs(iri(prefix.person, 'first name'), [a, def.Person], [def.livesIn, iri(prefix.city, 'name')], ), This results in the following linked data: city:Amsterdam a def:City. person:John a def:Person; def:livesIn city:Amsterdam. Or diagrammatically: graph LR john -- a --> Person john -- def:livesIn --> amsterdam Person[def:Person]:::model amsterdam[city:Amsterdam]:::data john[person:John]:::data classDef data fill:yellow classDef model fill:lightblue","title":"Vocabulary term declarations"},{"location":"triply-etl/generic/declarations/#shape-term-declarations","text":"Shapes are expressed in linked data with SHACL. shapes are often (re)used in multiple places, and they often form a fixed vocabulary. This makes terms that denote shapes eligible for a term declaration object. The following code snippet declares the terms that denote shapes: const shp = { City: prefix.shp.concat('City'), Person: prefix.shp.concat('Person'), Person_livesIn: prefix.shp.concat('livesIn'), } This object can be used through the ETL configuration. For example in the following statement assertions: pairs(shp.Person, [a, sh.NodeShape], [sh.property, shp.Person_livesIn], [sh.targetClass, def.Person], ), pairs(shp.Person_livesIn, [a, sh.PropertyShape], [sh.class, def.City], [sh.path, def:livesIn], ), This results in the following linked data: shp:Person a sh:NodeShape; sh:property shp:Person_livesIn; sh:targetClass def:Person. shp:Person_livesIn a sh:PropertyShape; sh:class def:City; sh:path def:livesIn. Or diagrammatically: graph LR shp_Person -- a --> sh:NodeShape shp_Person -- sh:property --> shp_Person_livesIn shp_Person -- sh:targetClass --> def_Person shp_Person_livesIn -- a --> sh:PropertyShape shp_Person_livesIn -- sh:class --> def_City shp_Person_livesIn -- sh:path --> def_livesIn def_City[def:City]:::model def_Person[def:Person]:::model def_livesIn[def:livesIn]:::model shp_Person[shp:Person]:::shape shp_Person_livesIn[shp:Person_livesIn]:::shape classDef model fill:lightblue classDef shape fill:orange","title":"Shape term declarations"},{"location":"triply-etl/generic/declarations/#individual-term-declarations","text":"Individuals are instances of classes. For example, John Doe is an individual of class def:Person ; Amsterdam is an individual of class def:City . If terms that denote individuals are used multiple times in an ETL configuration, term declarations may be introduced for them. The following code snippet declares the terms that denote individual persons: const person = { jane: prefix.person('Jane'), john: prefix.person('John'), mary: prefix.person('Mary'), } Instance term declarations can be used in statement assertions, for example: triple(person.john, foaf.knows, person.mary), This results in the following linked data: person:john foaf:knows person:mary.","title":"Individual term declarations"},{"location":"triply-etl/generic/declarations/#graph-name-declarations","text":"Linked data statements belong to graphs. Graphs are denoted by graph names. For example, a graph name may denote a graph that contains metadata statements, while another graph name may denote a graph that contains instance data. If graph names are used multiple times in an ETL configuration, term declarations may be introduced for them. The following code snippet declares three graph names: const graph = { metadata: prefix.graph.concat('metadata'), model: prefix.graph.concat('model'), instances: prefix.graph.concat('instances'), } The declared graph names can now be used in statement assertions: triples(graph.metadata, ['_dataset', a, dcat.Dataset], ['_dataset', rdfs.label, str('My Dataset')], ),","title":"Graph name declarations"},{"location":"triply-etl/generic/declarations/#external-vocabularies","text":"In linked data, it is common to reuse existing vocabularies. Popular vocabularies can be imported from the TriplyETL vocabulary library. See the table of currently supported vocabularies for a full overview. The following example imports three vocabularies (FOAF, OWL, PREMIS): import { foaf, owl, premis } from '@triplyetl/vocabularies' This allows you to make the following statement assertion: triple(foaf.Person, a, owl.Class), Notice that the notation in TriplyETL comes very close to the notation in the standardized linked data syntax for Turtle, TriG, and SPARQL. For the example above: foaf:Person a owl:Class.","title":"External vocabularies"},{"location":"triply-etl/generic/declarations/#example-using-the-premis-external-vocabulary","text":"The following code snippet uses the external PREMIS vocabulary. This vocabulary is published by the Library of Congress and is used to publish metadata about the preservation of digital objects. The following code snippet asserts that a PREMIS file is stored in a PREMIS storage location: pairs(iri(prefix.file, 'File ID'), [a, premis.File], [premis.storedAt, iri(prefix.location, 'Location ID')], ), triple(iri(prefix.location, 'Location ID'), a, premis.StorageLocation),","title":"Example: Using the PREMIS external vocabulary"},{"location":"triply-etl/generic/declarations/#language-tag-declarations","text":"Linked data includes support for language-tagged strings . These are literals that specify a string value and a code that denotes the natural language in which that string value should be interpreted. The natural language tags follow a syntax that is standardized in RFC 5646 , and must occur in the Language Subtag Registry that is maintained by IANA. TriplyETL includes declarations for these natural language tags. They can be imported as follows: import { language } from '@triplyetl/vocabularies' Language tag declaration can be used in literal() term assertions: literal(str('Nederland'), language.nl) literal(str('Netherlangs'), language.en) Language tag declarations can also be used in addLiteral() transformations: addLiteral({ content: 'label', languageTag: lang.fr, key: '_languageTaggedString', }),","title":"Language tag declarations"},{"location":"triply-etl/generic/declarations/#shorthand-declarations","text":"Shorthands are convenient names that stand for commonly used IRIs. There is one standard shorthand ( a ), and TriplyETL allows other shorthands to be declared as needed.","title":"Shorthand declarations"},{"location":"triply-etl/generic/declarations/#the-standard-shorthand","text":"The standardized linked data syntax for Turtle, TriG, and SPARQL allow the shorthand a to be used to stand for the rdf:type property. TriplyETL supports this standard shorthand, which can be imported from the vocabulary library: import { a } from '@triplyetl/vocabularies' In the standardized linked data syntax for Turtle, TriG and SPARQL, this shorthand can only be used in the predicate position. This restriction is not enforced in TriplyETL, where the a shorthand can be used in the subject, predicate, object, and even graph position.","title":"The standard shorthand"},{"location":"triply-etl/generic/declarations/#example_2","text":"The following code snippet makes a true statement assertion, while using the stands shorthand twice: triple(a, a, rdf.Property), This results in the following linked data: rdf:type a rdf:Property.","title":"Example"},{"location":"triply-etl/generic/declarations/#user-defined-shorthands","text":"TriplyETL allows the introduction of arbitrary, user-defined shorthands. User-defined shorthands can make linked data assertions in the ETL configuration more readable for users from certain domains. For example, \"is a\" is a commonly used phrase in many modeling languages to denote the subsumption relation.","title":"User-defined shorthands"},{"location":"triply-etl/generic/declarations/#example_3","text":"The following code snippet declares is_a as a user-defined shorthand for the rdfs:subClassOf property (which is the subsumption relation in linked data): import { foaf, rdfs } from '@triplyetl/vocabularies' const is_a = rdfs.subClassOf This declaration is used in the following statement assertion: triple(foaf.Person, is_a, foaf.Agent), This results in the following linked data: foaf:Person rdfs:subClassOf foaf:Agent.","title":"Example"},{"location":"triply-etl/generic/declarations/#geospatial-declarations","text":"TriplyETL includes declarations for geospatial coordinate reference systems. These are identified by EPSG codes, and can imported as follows: import { epsg } from '@triplyetl/vocabularies' EPSG codes can be used in geospatial transformation functions like geojsonToWkt() : geojsonToWkt({ content: 'geojson', crs: epsg[28992], key: '_wkt', }),","title":"Geospatial declarations"},{"location":"triply-etl/generic/getting-started/","text":"On this page: TriplyETL Getting Started Prerequisites Minimum versions Update the prerequisites TriplyETL Generator TriplyETL Runner TriplyETL Library TriplyETL Getting Started \u00b6 This page helps you to get started with TriplyETL. You can get started with TriplyETL in any of the following ways: TriplyETL Generator creates a new ETL project based on your answers to a set of question. TriplyETL Runner runs an existing ETL project. TriplyETL Library can be included as a dependency in your TypeScript project. Prerequisites \u00b6 In order to use TriplyETL, you must first install the following programs on your computer: Install Git Go to this link and follow the instructions for your operating system (Windows, macOS, or Linux). Run the following commands to set your user name and email in Git: git config --global user.email \"ada@triply.cc\" git config --global user.name \"Ada Lovelace\" This information will be used in the Git version history for the TriplyETL project. This allows your team to keep track of who made which change. Install Node.js (simple approach) Go to nodejs.org and click on option \u201c18.x.y LTS (Recommended For Most Users)\u201d. This will download the installer for your operating system. Run the installer on your computer. On Windows, you must also select the number of bits on your computer: 32 or 64. The correct number of bits is 64 for almost all Windows computers. Install Node.js (advanced approach) For more advanced us, you can install the Node Version Manager ( nvm ). This allows you to install multiple versions of Node.js on the same computer. See the following links for more information: On Windows You can following the official instructions from Microsoft for installing NVM on Windows. On macOS or Linux You can follow the instructions for installing NVM on any operating system (including macOS and Linux). Find a terminal application You must use a terminal application in order to run commands from the TriplyETL CLI . Here are some examples of terminal applications on different operating systems: On Windows Most Windows versions come with some version of PowerShell preinstalled. You can also follow these instructions by Microsoft to update to the latest version of PowerShell. On macOS Most macOS version come with a Terminal application preinstalled. On Linux Most Linux versions come with a preinstalled terminal application. For example, on Ubuntu the GNOME Terminal application is preinstalled. Minimum versions \u00b6 TriplyETL requires the following minimum versions for the prerequisites: NPM v10.2.1 Node.js v18 Update the prerequisites \u00b6 Update NPM If you have NPM installed, you can run npm -v to see your current version. If you want to upgrade to a different version (for example 10.2.1), you can run the following command: npm install -g npm@10.2.1 For more information see NPM's \"try latest stable version of npm\" documentation. Update Node.js If you have Node.js installed, you can run node -v to see your current version. If you want to upgrade to a different version you need to use a Node package manager (e.g. nvm , n etc.). For more information visit Installing Node.js via package manager . Update Git If you have Git installed, you can run git -v to see your current version, if you want to upgrade to latest version you can: Linux Run command sudo apt-get update && sudo apt-get install git Windows What you need to do depends on your current Git version: Older than 2.14.1 Uninstall Git from your system and reinstall Git for Windows . Between 2.14.2 and 2.16.1 Run command git update Greater than or equal to 2.16.1 Run command git update-git-for-windows MacOS (with Homebrew) Install Homebrew Run command brew update && brew install git && brew upgrade git TriplyETL Generator \u00b6 The TriplyETL Generator allows you to create new ETL projects in your terminal application. If a TriplyETL project already exists, use the TriplyETL Runner instead. In order to use TriplyETL Generator, you must have: Satisfied the prerequisites . A TriplyETL License Key. Contact info@triply.cc to obtain a License Key for your organization. A user account on a TriplyDB server. Contact info@triply.cc to set up a TriplyDB server for your organization, or create a free account on https://triplydb.com . Run the following command to use the TriplyETL Generator: npx @triply/etl-generator If you use TriplyETL Generator for the first time, this command automatically downloads and installs the latest version on your computer. If you have used TriplyETL Generator in the past, this command automatically updates your installation to the latest version, if one is available. TriplyETL Generator will ask the following questions (the exact sequence of questions depends on the answers given): a. TriplyETL License Key b. Project name c. Target folder d. Dataset name e. TriplyDB API Token f. TriplyDB URL g. TriplyDB email h. TriplyDB password Here is an example of a possible run: ? TriplyETL License Key: [hidden] ? Project name: my-etl ? Target folder: my-etl ? Dataset name: my-etl ? Create a new TriplyDB API Token? Yes ? Your TriplyDB URL: my-org.triply.cc ? Your TriplyDB email: my-account@my-organization.com ? Your TriplyDB password: [hidden] \ud83c\udfc1 Your project my-etl is ready for use in my-etl. Go to the target folder that you have specified: cd my-etl You can now use the TriplyETL Runner to run the ETL: npx etl TriplyETL Runner \u00b6 The TriplyETL Runner allows you to run a local TriplyETL project in your terminal application. In order to use TriplyETL Runner, you must have: Satisfied the prerequisites . A user account on a TriplyDB server. Contact info@triply.cc to set up a TriplyDB server for your organization, or create a free account on https://triplydb.com . Perform the following steps to use the TriplyETL Runner: Create a local copy of an existing ETL project. If you do not have access to an existing TriplyETL project yet, use the TriplyETL Generator to create a new one. If you have access to an existing TriplyETL project, use the following command to make a local copy with Git: git clone ssh://git@git.triply.cc:10072/customers/my-org/my-project.git Once you have created a local copy of an existing ETL project, go into the corresponding directory: cd my-project Install the dependencies: npm i Transpile the TypeScript files into JavaScript: npm run build You can now use the TriplyETL Runner: npx etl At this point, you should see a first TriplyETL process in your terminal application. If this is not the case, please contact support@triply.cc to help you out. Visit the TriplyETL CLI documentation to learn more about how you can use the TriplyETL Runner. Visit the TriplyETL CI/CD documentation to learn more about how you can automate TriplyETL runs. TriplyETL Library \u00b6 If you are a software developer that is building a software application in TypeScript, you can include the TriplyETL Library in your project. In order to use the TriplyETL Library, you must have: Satisfied the prerequisites . A TriplyETL License Key. Contact info@triply.cc to obtain a License Key for your organization. A user account on a TriplyDB server. Contact info@triply.cc to set up a TriplyDB server for your organization, or create a free account on https://triplydb.com . Perform the following steps to use the TriplyETL Library: Open the file .npmrc in your text editor, or create the file if it does not yet exist. Add the following content: @triplydb:registry=https://git.triply.cc/api/v4/packages/npm/ @triplyetl:registry=https://git.triply.cc/api/v4/packages/npm/ //git.triply.cc/api/v4/packages/npm/:_authToken={LICENSE_KEY} Replace {LICENSE_KEY} with your TriplyETL License Key. Contact support@triply.cc if you do not have such a license key yet. Run the following command to add the TriplyETL dependency to your package.json file: npm i @triplyetl/etl Open one of the TypeScript files in your software project. When you add the following line to the top of your file, it should be recognized by your TypeScript editor: import { sdo } from '@triplyetl/vocabularies'","title":"Getting Started"},{"location":"triply-etl/generic/getting-started/#triplyetl-getting-started","text":"This page helps you to get started with TriplyETL. You can get started with TriplyETL in any of the following ways: TriplyETL Generator creates a new ETL project based on your answers to a set of question. TriplyETL Runner runs an existing ETL project. TriplyETL Library can be included as a dependency in your TypeScript project.","title":"TriplyETL Getting Started"},{"location":"triply-etl/generic/getting-started/#prerequisites","text":"In order to use TriplyETL, you must first install the following programs on your computer: Install Git Go to this link and follow the instructions for your operating system (Windows, macOS, or Linux). Run the following commands to set your user name and email in Git: git config --global user.email \"ada@triply.cc\" git config --global user.name \"Ada Lovelace\" This information will be used in the Git version history for the TriplyETL project. This allows your team to keep track of who made which change. Install Node.js (simple approach) Go to nodejs.org and click on option \u201c18.x.y LTS (Recommended For Most Users)\u201d. This will download the installer for your operating system. Run the installer on your computer. On Windows, you must also select the number of bits on your computer: 32 or 64. The correct number of bits is 64 for almost all Windows computers. Install Node.js (advanced approach) For more advanced us, you can install the Node Version Manager ( nvm ). This allows you to install multiple versions of Node.js on the same computer. See the following links for more information: On Windows You can following the official instructions from Microsoft for installing NVM on Windows. On macOS or Linux You can follow the instructions for installing NVM on any operating system (including macOS and Linux). Find a terminal application You must use a terminal application in order to run commands from the TriplyETL CLI . Here are some examples of terminal applications on different operating systems: On Windows Most Windows versions come with some version of PowerShell preinstalled. You can also follow these instructions by Microsoft to update to the latest version of PowerShell. On macOS Most macOS version come with a Terminal application preinstalled. On Linux Most Linux versions come with a preinstalled terminal application. For example, on Ubuntu the GNOME Terminal application is preinstalled.","title":"Prerequisites"},{"location":"triply-etl/generic/getting-started/#minimum-versions","text":"TriplyETL requires the following minimum versions for the prerequisites: NPM v10.2.1 Node.js v18","title":"Minimum versions"},{"location":"triply-etl/generic/getting-started/#update-the-prerequisites","text":"Update NPM If you have NPM installed, you can run npm -v to see your current version. If you want to upgrade to a different version (for example 10.2.1), you can run the following command: npm install -g npm@10.2.1 For more information see NPM's \"try latest stable version of npm\" documentation. Update Node.js If you have Node.js installed, you can run node -v to see your current version. If you want to upgrade to a different version you need to use a Node package manager (e.g. nvm , n etc.). For more information visit Installing Node.js via package manager . Update Git If you have Git installed, you can run git -v to see your current version, if you want to upgrade to latest version you can: Linux Run command sudo apt-get update && sudo apt-get install git Windows What you need to do depends on your current Git version: Older than 2.14.1 Uninstall Git from your system and reinstall Git for Windows . Between 2.14.2 and 2.16.1 Run command git update Greater than or equal to 2.16.1 Run command git update-git-for-windows MacOS (with Homebrew) Install Homebrew Run command brew update && brew install git && brew upgrade git","title":"Update the prerequisites"},{"location":"triply-etl/generic/getting-started/#triplyetl-generator","text":"The TriplyETL Generator allows you to create new ETL projects in your terminal application. If a TriplyETL project already exists, use the TriplyETL Runner instead. In order to use TriplyETL Generator, you must have: Satisfied the prerequisites . A TriplyETL License Key. Contact info@triply.cc to obtain a License Key for your organization. A user account on a TriplyDB server. Contact info@triply.cc to set up a TriplyDB server for your organization, or create a free account on https://triplydb.com . Run the following command to use the TriplyETL Generator: npx @triply/etl-generator If you use TriplyETL Generator for the first time, this command automatically downloads and installs the latest version on your computer. If you have used TriplyETL Generator in the past, this command automatically updates your installation to the latest version, if one is available. TriplyETL Generator will ask the following questions (the exact sequence of questions depends on the answers given): a. TriplyETL License Key b. Project name c. Target folder d. Dataset name e. TriplyDB API Token f. TriplyDB URL g. TriplyDB email h. TriplyDB password Here is an example of a possible run: ? TriplyETL License Key: [hidden] ? Project name: my-etl ? Target folder: my-etl ? Dataset name: my-etl ? Create a new TriplyDB API Token? Yes ? Your TriplyDB URL: my-org.triply.cc ? Your TriplyDB email: my-account@my-organization.com ? Your TriplyDB password: [hidden] \ud83c\udfc1 Your project my-etl is ready for use in my-etl. Go to the target folder that you have specified: cd my-etl You can now use the TriplyETL Runner to run the ETL: npx etl","title":"TriplyETL Generator"},{"location":"triply-etl/generic/getting-started/#triplyetl-runner","text":"The TriplyETL Runner allows you to run a local TriplyETL project in your terminal application. In order to use TriplyETL Runner, you must have: Satisfied the prerequisites . A user account on a TriplyDB server. Contact info@triply.cc to set up a TriplyDB server for your organization, or create a free account on https://triplydb.com . Perform the following steps to use the TriplyETL Runner: Create a local copy of an existing ETL project. If you do not have access to an existing TriplyETL project yet, use the TriplyETL Generator to create a new one. If you have access to an existing TriplyETL project, use the following command to make a local copy with Git: git clone ssh://git@git.triply.cc:10072/customers/my-org/my-project.git Once you have created a local copy of an existing ETL project, go into the corresponding directory: cd my-project Install the dependencies: npm i Transpile the TypeScript files into JavaScript: npm run build You can now use the TriplyETL Runner: npx etl At this point, you should see a first TriplyETL process in your terminal application. If this is not the case, please contact support@triply.cc to help you out. Visit the TriplyETL CLI documentation to learn more about how you can use the TriplyETL Runner. Visit the TriplyETL CI/CD documentation to learn more about how you can automate TriplyETL runs.","title":"TriplyETL Runner"},{"location":"triply-etl/generic/getting-started/#triplyetl-library","text":"If you are a software developer that is building a software application in TypeScript, you can include the TriplyETL Library in your project. In order to use the TriplyETL Library, you must have: Satisfied the prerequisites . A TriplyETL License Key. Contact info@triply.cc to obtain a License Key for your organization. A user account on a TriplyDB server. Contact info@triply.cc to set up a TriplyDB server for your organization, or create a free account on https://triplydb.com . Perform the following steps to use the TriplyETL Library: Open the file .npmrc in your text editor, or create the file if it does not yet exist. Add the following content: @triplydb:registry=https://git.triply.cc/api/v4/packages/npm/ @triplyetl:registry=https://git.triply.cc/api/v4/packages/npm/ //git.triply.cc/api/v4/packages/npm/:_authToken={LICENSE_KEY} Replace {LICENSE_KEY} with your TriplyETL License Key. Contact support@triply.cc if you do not have such a license key yet. Run the following command to add the TriplyETL dependency to your package.json file: npm i @triplyetl/etl Open one of the TypeScript files in your software project. When you add the following line to the top of your file, it should be recognized by your TypeScript editor: import { sdo } from '@triplyetl/vocabularies'","title":"TriplyETL Library"},{"location":"triply-etl/generic/internal-store/","text":"On this page: Internal Store Internal Store \u00b6 The internal store is the storage location for linked data that is created by one or more of the following steps: Step 3 Assert uses data from the record to make linked data assertions in the internal store . Step 4 Enrich improves and extends linked data in the internal store. Step 5 Validate ensures that linked data in the internal store meets the specified quality criteria. Every record that is extracted from a data source has its own internal store (decoupling). The size of the internal store is typically small (because at the record level). This is done on purpose, to ensure that a large number of records can be processed in parallel, without using many hardware resources. Once linked data in the internal store is finalized for one record, the following step can be performed: Step 6 Publish takes the linked data from the internal store, and publishes it to a destination such as TriplyDB .","title":"Internal Store"},{"location":"triply-etl/generic/internal-store/#internal-store","text":"The internal store is the storage location for linked data that is created by one or more of the following steps: Step 3 Assert uses data from the record to make linked data assertions in the internal store . Step 4 Enrich improves and extends linked data in the internal store. Step 5 Validate ensures that linked data in the internal store meets the specified quality criteria. Every record that is extracted from a data source has its own internal store (decoupling). The size of the internal store is typically small (because at the record level). This is done on purpose, to ensure that a large number of records can be processed in parallel, without using many hardware resources. Once linked data in the internal store is finalized for one record, the following step can be performed: Step 6 Publish takes the linked data from the internal store, and publishes it to a destination such as TriplyDB .","title":"Internal Store"},{"location":"triply-etl/generic/maintenance/","text":"On this page: Maintenance Update the TriplyETL dependency Check the current version Check for new versions Assess the impact of updating Perform the update Patch and Minor version update Major version update DTAP configuration Configure CI/CD CI/CD configuration file CI/CD environment variables Understanding Runtime Differences Maintenance \u00b6 Once a TriplyETL repository is configured, it goes into maintenance mode. TriplyETL contains specific functionality to support maintenance. Update the TriplyETL dependency \u00b6 New versions of TriplyETL are released regularly. Moving to a new version is generally a good idea, because it allows new features to be used and will include fixes for known/reported bugs. At the same time, updating to a new version may require you to make some changes to your pipeline. It is important to determine an approach for updating your TriplyETL projects that fits your team and organization. The following sections describe how you can make such a determination. Check the current version \u00b6 The following command prints the TriplyETL version that you are currently using: npm list @triplyetl/etl Check for new versions \u00b6 The following command prints the latest TriplyETL version that is available: npm outdated TriplyETL repositories typically include several developer dependencies as well. These developer dependencies make it easier to write and maintain your ETLs. These developer dependencies are not part of TriplyETL, and must therefore be updated independently of TriplyETL. Assess the impact of updating \u00b6 TriplyETL uses the Semantic Versioning approach: {major}.{minor}.{patch} The impact of updating to a new TriplyETL version can therefore be determined as follows: Patch update Only the {patch} number has increased. This means that one or more bugs have been fixed in a backward compatible manner. You should always be able to perform a patch update without having to make any changes to your configuration. Minor update The {minor} number has increased, but the {major} number is still the same. This means that new functionality was added in a backward compatible manner. You should always be able to perform a minor update without having to make any changes to your configuration. But you may want to check the changelog to see which new functionalities were added. Major update The {major} number has increased. This means that there are incompatible changes. This means that features may have been removed, or existing features may have changed. In such cases, changes to your configuration are almost certainly necessary, and may take some time to implement. Any changes you need to make are described in the changelog . Perform the update \u00b6 Based on the outcome of the previous step, a maintainer of the repository decides which dependencies should be updated to which versions. Since Patch and Minor version updates are always safe to make, we discuss them separately from the more impactful Major version updates. Patch and Minor version update \u00b6 You can update to the latest patch or minor version with the following command: npm up This command may change the contents of the package-lock.json file. These changes must be committed and pushed as part of performing the update. Notice that this command will only perform safe (i.e. patch and/or minor) updates. Major version update \u00b6 You can update to the latest major version with the following command: npm i <package-name>@version This means that the following command is used to update to a specific TriplyETL major version: npm i @triplyetl/etl@3.0.0 This command will change the contents of the package.json file. These changes must be committed and pushed as part of performing the update. DTAP configuration \u00b6 TriplyETL provides out-of-the-box support for the DTAP approach for configuring production systems. DTAP stands for the four environments in which the ETL can run: Development Test Acceptance Production When working on a pipeline it is best to at least run it in the following two modes: Acceptance mode Upload the result of the pipeline to the user account for which the API Token was created. Production mode Upload the result of the pipeline to the organization where the production version of the data is published. Having multiple modes ensures that the production version of a dataset is not accidentally overwritten during development. export function account(): any { switch (Etl.environment) { case 'Development': return undefined case 'Testing': return 'my-org-testing' case 'Acceptance': return 'my-org-acceptance' case 'Production': return 'my-org' } } const etl = new Etl() etl.use( // Your ETL pipeline is configured here. toRdf(Destination.triplyDb.rdf(account(), 'my-dataset')), ) By default, you run the pipeline in Development mode. If you want to run in another mode, you must set the ENV environment variable. You can do this in the .env file of your TriplyETL repository. For example, the following runs the pipeline in Testing mode: ENV=Testing You can also set the ENV variable in the GitLab CI/CD environment. This allows you to automatically run different pipelines, according to the DTAP approach for production systems. Configure CI/CD \u00b6 TriplyETL pipelines can be configured to run automatically in any CI/CD environment. This section explains how you can configure an automated TriplyETL pipeline in GitLab. Notice that the configuration in any other CI/CD environment will be more or less similar to what is explained in this section. CI/CD configuration file \u00b6 The TriplyETL Generator creates a basic configuration file for running TriplyETL in GitLab CI/CD. The configuration file is called .gitlab-ci.yml . The configuration contains a list of stages: stages: - first_stage - second_stage - third_stage These stages will run sequentially. For the above example: the pipeline starts by running the first stage, then runs the second stage, and finally runs the third stage. Within each stage, you can configure one or more TriplyETL scripts. When more then one script is specified for the same stage, these scripts will run in parallel. This allows you to specify any combination of sequential and parallel processes. The following example assumes that the following scripts are present in the TriplyETL repository: - src/ - create_vocabulary.ts - create_dataset_a.ts - create_dataset_b.ts - create_knowledge_graph.ts - .gitlab-ci.yml We want to configure our CI/CD in the following way: Start by creating the vocabulary (script create_vocabulary.ts ). This vocabulary will be used in the validation step of the two scripts that create the two datasets. Once the vocabulary is created, create the two datasets (scripts create_dataset_a.ts and create_dataset_b.ts ). The datasets can be created in parallel, but they both require that vocabulary creation is finalized. Once the two datasets are created, create the knowledge graph (script create_knowledge_graph.ts ), which combines the two datasets and the vocabulary in one dataset. This specific configuration looks as follows: create_vocabulary: stage: first_stage interruptible: true allow_failure: false artifacts: !reference [.etl-template, artifacts] script: - !reference [.etl-template, install] - !reference [.etl-template, run-etl] rules: - !reference [.etl-template, rules] create_dataset_a: stage: second_stage interruptible: true allow_failure: false artifacts: !reference [.etl-template, artifacts] script: - !reference [.etl-template, install] - !reference [.etl-template, run-etl] rules: - !reference [.etl-template, rules] create_dataset_b: stage: second_stage interruptible: true allow_failure: false artifacts: !reference [.etl-template, artifacts] script: - !reference [.etl-template, install] - !reference [.etl-template, run-etl] rules: - !reference [.etl-template, rules] create_knowledge_graph: stage: third_stage interruptible: true allow_failure: false artifacts: !reference [.etl-template, artifacts] script: - !reference [.etl-template, install] - !reference [.etl-template, run-etl] rules: - !reference [.etl-template, rules] CI/CD environment variables \u00b6 In a normal ETL the only variables that should be present AFAIK are: ENV (value: acceptance or production), TRIPLYDB_TOKEN (value: customer's TriplyDB token), PIPELINE_NAME (value: explained below), and optionally TIMEOUT (value: time description e.g. \"1H\") TriplyETL pipelines interpret the following environment variables, that may be specified in the CI/CD environment: ENV When DTAP is used, this environment variable specifies whether the pipeline runs in \"Development\", \"Test\", \"Acceptance\", or in \"Production\". TRIPLYDB_TOKEN The TriplyDB API Token that is used by the automated pipeline, and that allows the pipeline to read from and write to a TriplyDB server. PIPELINE_NAME A descriptive name that is used by GitLab in pipeline overviews. This is specifically useful if you are running multiple pipelines, in which case this descriptive name helps you to distinguish runs. One example of running multiple pipelines is running in DTAP; in which case the descriptive names for the pipelines may be \"Schedule: Acceptance\" and \"Schedule: Production\". TIMEOUT This environment variable can be set to a duration that is shorted than the duration of the pipeline. If a timeout is set and reached, TriplyETL will finish the ETL in an orderly fashion: saving the processed data, saving the performance log files, and saving the generated validation report (if any). This is useful for pipelines that would otherwise be terminated by the CI/CD environment, in which case TriplyETL is terminated immediately, without having the ability to nicely save before exiting. HEAD The maximum number of records that is being processed by the TriplyETL pipeline. This environment variable can be set in test runs that only want to test whether the ETL works for some records, without requiring it to run for all records. For example, in a DTAP Test run this number may be set to 10 to test whether the source can be accessed and the generated data can be uploaded to a TriplyDB server. Understanding Runtime Differences \u00b6 It's important to be aware that runtime differences can occur when comparing TriplyETL pipeline runtimes in different environments, particularly when comparing them to GitLab CI/CD runtimes. There are two main factors that can influence runtime differences: Overhead in CI Jobs : GitLab CI jobs may introduce overhead beyond the actual ETL computation, such as setting up a containerized environment and additional CI-specific steps. A difference of 1 to 5 minutes between GitLab CI and TriplyETL runtimes is normal due to this overhead. Use of copySource() Function : Significant runtime differences exceeding 5 minutes can be attributed to the use of the copySource() function, which operates outside of the ETL application and contributes to the total runtime but not the middleware runtime. If you encounter a runtime difference greater than 5 minutes, and the copySource() function hasn't been used, it is recommended to report the issue to Triply. The issue will be further investigated to identify and address any potential causes. Understanding these factors and taking appropriate action will help you manage your TriplyETL pipelines effectively in a CI/CD environment.","title":"Maintenance"},{"location":"triply-etl/generic/maintenance/#maintenance","text":"Once a TriplyETL repository is configured, it goes into maintenance mode. TriplyETL contains specific functionality to support maintenance.","title":"Maintenance"},{"location":"triply-etl/generic/maintenance/#update-the-triplyetl-dependency","text":"New versions of TriplyETL are released regularly. Moving to a new version is generally a good idea, because it allows new features to be used and will include fixes for known/reported bugs. At the same time, updating to a new version may require you to make some changes to your pipeline. It is important to determine an approach for updating your TriplyETL projects that fits your team and organization. The following sections describe how you can make such a determination.","title":"Update the TriplyETL dependency"},{"location":"triply-etl/generic/maintenance/#check-the-current-version","text":"The following command prints the TriplyETL version that you are currently using: npm list @triplyetl/etl","title":"Check the current version"},{"location":"triply-etl/generic/maintenance/#check-for-new-versions","text":"The following command prints the latest TriplyETL version that is available: npm outdated TriplyETL repositories typically include several developer dependencies as well. These developer dependencies make it easier to write and maintain your ETLs. These developer dependencies are not part of TriplyETL, and must therefore be updated independently of TriplyETL.","title":"Check for new versions"},{"location":"triply-etl/generic/maintenance/#assess-the-impact-of-updating","text":"TriplyETL uses the Semantic Versioning approach: {major}.{minor}.{patch} The impact of updating to a new TriplyETL version can therefore be determined as follows: Patch update Only the {patch} number has increased. This means that one or more bugs have been fixed in a backward compatible manner. You should always be able to perform a patch update without having to make any changes to your configuration. Minor update The {minor} number has increased, but the {major} number is still the same. This means that new functionality was added in a backward compatible manner. You should always be able to perform a minor update without having to make any changes to your configuration. But you may want to check the changelog to see which new functionalities were added. Major update The {major} number has increased. This means that there are incompatible changes. This means that features may have been removed, or existing features may have changed. In such cases, changes to your configuration are almost certainly necessary, and may take some time to implement. Any changes you need to make are described in the changelog .","title":"Assess the impact of updating"},{"location":"triply-etl/generic/maintenance/#perform-the-update","text":"Based on the outcome of the previous step, a maintainer of the repository decides which dependencies should be updated to which versions. Since Patch and Minor version updates are always safe to make, we discuss them separately from the more impactful Major version updates.","title":"Perform the update"},{"location":"triply-etl/generic/maintenance/#patch-and-minor-version-update","text":"You can update to the latest patch or minor version with the following command: npm up This command may change the contents of the package-lock.json file. These changes must be committed and pushed as part of performing the update. Notice that this command will only perform safe (i.e. patch and/or minor) updates.","title":"Patch and Minor version update"},{"location":"triply-etl/generic/maintenance/#major-version-update","text":"You can update to the latest major version with the following command: npm i <package-name>@version This means that the following command is used to update to a specific TriplyETL major version: npm i @triplyetl/etl@3.0.0 This command will change the contents of the package.json file. These changes must be committed and pushed as part of performing the update.","title":"Major version update"},{"location":"triply-etl/generic/maintenance/#dtap-configuration","text":"TriplyETL provides out-of-the-box support for the DTAP approach for configuring production systems. DTAP stands for the four environments in which the ETL can run: Development Test Acceptance Production When working on a pipeline it is best to at least run it in the following two modes: Acceptance mode Upload the result of the pipeline to the user account for which the API Token was created. Production mode Upload the result of the pipeline to the organization where the production version of the data is published. Having multiple modes ensures that the production version of a dataset is not accidentally overwritten during development. export function account(): any { switch (Etl.environment) { case 'Development': return undefined case 'Testing': return 'my-org-testing' case 'Acceptance': return 'my-org-acceptance' case 'Production': return 'my-org' } } const etl = new Etl() etl.use( // Your ETL pipeline is configured here. toRdf(Destination.triplyDb.rdf(account(), 'my-dataset')), ) By default, you run the pipeline in Development mode. If you want to run in another mode, you must set the ENV environment variable. You can do this in the .env file of your TriplyETL repository. For example, the following runs the pipeline in Testing mode: ENV=Testing You can also set the ENV variable in the GitLab CI/CD environment. This allows you to automatically run different pipelines, according to the DTAP approach for production systems.","title":"DTAP configuration"},{"location":"triply-etl/generic/maintenance/#configure-cicd","text":"TriplyETL pipelines can be configured to run automatically in any CI/CD environment. This section explains how you can configure an automated TriplyETL pipeline in GitLab. Notice that the configuration in any other CI/CD environment will be more or less similar to what is explained in this section.","title":"Configure CI/CD"},{"location":"triply-etl/generic/maintenance/#cicd-configuration-file","text":"The TriplyETL Generator creates a basic configuration file for running TriplyETL in GitLab CI/CD. The configuration file is called .gitlab-ci.yml . The configuration contains a list of stages: stages: - first_stage - second_stage - third_stage These stages will run sequentially. For the above example: the pipeline starts by running the first stage, then runs the second stage, and finally runs the third stage. Within each stage, you can configure one or more TriplyETL scripts. When more then one script is specified for the same stage, these scripts will run in parallel. This allows you to specify any combination of sequential and parallel processes. The following example assumes that the following scripts are present in the TriplyETL repository: - src/ - create_vocabulary.ts - create_dataset_a.ts - create_dataset_b.ts - create_knowledge_graph.ts - .gitlab-ci.yml We want to configure our CI/CD in the following way: Start by creating the vocabulary (script create_vocabulary.ts ). This vocabulary will be used in the validation step of the two scripts that create the two datasets. Once the vocabulary is created, create the two datasets (scripts create_dataset_a.ts and create_dataset_b.ts ). The datasets can be created in parallel, but they both require that vocabulary creation is finalized. Once the two datasets are created, create the knowledge graph (script create_knowledge_graph.ts ), which combines the two datasets and the vocabulary in one dataset. This specific configuration looks as follows: create_vocabulary: stage: first_stage interruptible: true allow_failure: false artifacts: !reference [.etl-template, artifacts] script: - !reference [.etl-template, install] - !reference [.etl-template, run-etl] rules: - !reference [.etl-template, rules] create_dataset_a: stage: second_stage interruptible: true allow_failure: false artifacts: !reference [.etl-template, artifacts] script: - !reference [.etl-template, install] - !reference [.etl-template, run-etl] rules: - !reference [.etl-template, rules] create_dataset_b: stage: second_stage interruptible: true allow_failure: false artifacts: !reference [.etl-template, artifacts] script: - !reference [.etl-template, install] - !reference [.etl-template, run-etl] rules: - !reference [.etl-template, rules] create_knowledge_graph: stage: third_stage interruptible: true allow_failure: false artifacts: !reference [.etl-template, artifacts] script: - !reference [.etl-template, install] - !reference [.etl-template, run-etl] rules: - !reference [.etl-template, rules]","title":"CI/CD configuration file"},{"location":"triply-etl/generic/maintenance/#cicd-environment-variables","text":"In a normal ETL the only variables that should be present AFAIK are: ENV (value: acceptance or production), TRIPLYDB_TOKEN (value: customer's TriplyDB token), PIPELINE_NAME (value: explained below), and optionally TIMEOUT (value: time description e.g. \"1H\") TriplyETL pipelines interpret the following environment variables, that may be specified in the CI/CD environment: ENV When DTAP is used, this environment variable specifies whether the pipeline runs in \"Development\", \"Test\", \"Acceptance\", or in \"Production\". TRIPLYDB_TOKEN The TriplyDB API Token that is used by the automated pipeline, and that allows the pipeline to read from and write to a TriplyDB server. PIPELINE_NAME A descriptive name that is used by GitLab in pipeline overviews. This is specifically useful if you are running multiple pipelines, in which case this descriptive name helps you to distinguish runs. One example of running multiple pipelines is running in DTAP; in which case the descriptive names for the pipelines may be \"Schedule: Acceptance\" and \"Schedule: Production\". TIMEOUT This environment variable can be set to a duration that is shorted than the duration of the pipeline. If a timeout is set and reached, TriplyETL will finish the ETL in an orderly fashion: saving the processed data, saving the performance log files, and saving the generated validation report (if any). This is useful for pipelines that would otherwise be terminated by the CI/CD environment, in which case TriplyETL is terminated immediately, without having the ability to nicely save before exiting. HEAD The maximum number of records that is being processed by the TriplyETL pipeline. This environment variable can be set in test runs that only want to test whether the ETL works for some records, without requiring it to run for all records. For example, in a DTAP Test run this number may be set to 10 to test whether the source can be accessed and the generated data can be uploaded to a TriplyDB server.","title":"CI/CD environment variables"},{"location":"triply-etl/generic/maintenance/#understanding-runtime-differences","text":"It's important to be aware that runtime differences can occur when comparing TriplyETL pipeline runtimes in different environments, particularly when comparing them to GitLab CI/CD runtimes. There are two main factors that can influence runtime differences: Overhead in CI Jobs : GitLab CI jobs may introduce overhead beyond the actual ETL computation, such as setting up a containerized environment and additional CI-specific steps. A difference of 1 to 5 minutes between GitLab CI and TriplyETL runtimes is normal due to this overhead. Use of copySource() Function : Significant runtime differences exceeding 5 minutes can be attributed to the use of the copySource() function, which operates outside of the ETL application and contributes to the total runtime but not the middleware runtime. If you encounter a runtime difference greater than 5 minutes, and the copySource() function hasn't been used, it is recommended to report the issue to Triply. The issue will be further investigated to identify and address any potential causes. Understanding these factors and taking appropriate action will help you manage your TriplyETL pipelines effectively in a CI/CD environment.","title":"Understanding Runtime Differences"},{"location":"triply-etl/generic/record/","text":"On this page: Record The generic Record Extractor loadRecords() Special keys Special key $recordId Use case: Unique identifiers Use case: Debugging Special key $environment Special key $sheetName Record \u00b6 When a TriplyETL is connected to one of more data sources, a stream of Records will be generated. Records use a generic representation that is independent of the format used in the data sources. The generic Record \u00b6 We illustrate the representation of the generic Record with the following code snippet. This snippet uses extractor fromJson() to extract data from inline JSON source data: import { Etl, fromJson, logRecord } from '@triplyetl/etl/generic' export default async function (): Promise<Etl> { const etl = new Etl() etl.use( fromJson([ { id: '123', name: 'John' }, { id: '456', name: 'Jane' }, ]), logRecord(), ) return etl } Debug function logRecord() prints the current record to standard output. When this pipeline is run, the two inline records are printed as follows: { \"id\": \"123\", \"name\": \"John\", \"$recordId\": 1, \"$environment\": \"Development\" } { \"id\": \"456\", \"name\": \"Jane\", \"$recordId\": 2, \"$environment\": \"Development\" } Notice that TriplyETL adds two keys to both records: $recordId and $environment (see Special Key for more information). Now suppose that we change the source system. We no longer use inline JSON, but a local XML file. The contents of the XML file are as follows: <?xml version=\"1.0\"?> <persons> <person> <id>123</id> <name>John</name> </person> <person> <id>456</id> <name>Jane</name> </person> </persons> Let us change the TriplyETL script to use extractor fromXml() and the local file source type: import { Etl, fromXml, logRecord } from '@triplyetl/etl/generic' export default async function (): Promise<Etl> { const etl = new Etl() etl.use( fromXml(Source.file('example.xml')), logRecord(), ) return etl } This new script logs the following two records: { \"id\": \"123\", \"name\": \"John\", \"$recordId\": 1, \"$environment\": \"Development\" } { \"id\": \"456\", \"name\": \"Jane\", \"$recordId\": 2, \"$environment\": \"Development\" } Notice that the two records that are logged from an XML source are completely identical to the two records that were previously logged from a JSON source. This is an essential property of TriplyETL: it treats data from any source system in the same way, using the same intermediary Record format. This makes it easy to write pipelines that process data from a large number of different data sources. This also makes replacing a data source in one format with a data source in another format a relatively cheap operation. More often than not, only the source extractor needs to be changed, and all transformations and assertions remain as they were. Extractor loadRecords() \u00b6 The loadRecords() function allows us to run a sub ETL and store its records to the main ETL. It is used when we would like to add additional data from different source to the main ETL. The function expects two arguments and can be run with the following snippet: fromSrc - The Source to load the data from. The list of available extractors can be seen in data sources overview page . key - A new key where the records are stored. loadRecords(fromSrc, 'key'), It is important to call the loadRecords() function after loading record data in the main ETL. The following code snippet extracts records from a json object (main ETL), then extracts records from a json file ( tableMap.json ) stored as an asset and stores them in the key _table in the record of the main ETL: fromJson({ country: 'be' }), loadRecords(fromJson(Source.TriplyDb.asset('test', { name: 'tableMap.json' })), '_table'), The combined record looks as following: { \"country\": \"be\", \"$recordId\": 1, \"$environment\": \"Development\", \"_table\": [ { \"be\": \"http://ex.com/Belgium\", \"nl\": \"http://ex.com/Netherlands\", \"de\": \"http://ex.com/Germany\", \"en\": \"http://ex.com/England\", \"$recordId\": 1, \"$environment\": \"Development\", \"$fileName\": \"tableMap.json\" } ] } Special keys \u00b6 Records in TriplyETL contain several special keys. These special keys start with a dollar sign character ( $ ). The special keys contain values that are inserted during the Extract step. These special keys can be used in the same way as regular keys in your TriplyETL configuration. We now discuss these special keys in detail. Special key $recordId \u00b6 The special key $recordId assigns a unique number to every record that is processed in one single run of a TriplyETL pipeline. If the source data does not change, multiple runs of the TriplyETL pipeline will always generate the same record IDs. However, if source data changes, multiple runs of the TriplyETL pipeline may generate different record IDs for the same record. Use case: Unique identifiers \u00b6 The first main use case of the $recordId key is to create IRIs that are unique within one single run of a TriplyETL pipeline. Suppose the following table is our source data: First name Last name John Doe Jane Doe John Doe We need to create an IRI for every person in this table. Notice that the table contains no unique properties: there are two different persons with the same first and last name. This means that we cannot use the keys \"First name\" and \"Last name\" in our record in order to create our IRIs. Luckily, the source connector adds the $recordId for us: { \"First name\": \"John\", \"Last name\": \"Doe\", \"$recordId\": 1 } { \"First name\": \"Jane\", \"Last name\": \"Doe\", \"$recordId\": 2 } { \"First name\": \"John\", \"Last name\": \"Doe\", \"$recordId\": 3 } This allows us to make the following assertion: pairs(iri(prefix.id, '$recordId'), [a, sdo.Person], [sdo.givenName, 'First name'], [sdo.familyName, 'Last name'], ), Which results in the following linked data: id:1 a sdo:Person; sdo:givenName 'John'; sdo:familyName 'Doe'. id:2 a sdo:Person; sdo:givenName 'Jane'; sdo:familyName 'Doe'. id:3 a sdo:Person; sdo:givenName 'John'; sdo:familyName 'Doe'. Notice that the use of the $recordId results in a correct single run of the TriplyETL pipeline. But if the source data changes, the IRIs may change as well. For example, if the first and second row in the source table are swapped, the IRI that denotes \"Jane Doe\" will change from id:2 to id:1 . Use case: Debugging \u00b6 When you are debugging the configuration of a TriplyETL pipeline, it is sometimes useful to perform a specific actions for a specific record. Assuming the stream of records is stable during the debugging effort, the $recordId key can be used to perform such a debugging action; for example: whenEqual('$recordId', 908, logRecord()), Do note that it is generally better to run the TriplyETL for a specific record using the --from-record-id 908 --head 1 command line flags (see CLI ). Special key $environment \u00b6 The TriplyETL record contains special key $environment . Its value denotes the DTAP environment that the pipeline is currently running in. This is one of the following values: \"Development\", \"Test\", \"Acceptance\", or \"Production\". Special key $sheetName \u00b6 The special key $sheetName only occurs in records that original from data source that use the Microsoft Excel format. In such records, this special key contains the name of the sheet from which the record originates. See the documentation for the Microsoft Excel format for more information about this special key.","title":"Record"},{"location":"triply-etl/generic/record/#record","text":"When a TriplyETL is connected to one of more data sources, a stream of Records will be generated. Records use a generic representation that is independent of the format used in the data sources.","title":"Record"},{"location":"triply-etl/generic/record/#the-generic-record","text":"We illustrate the representation of the generic Record with the following code snippet. This snippet uses extractor fromJson() to extract data from inline JSON source data: import { Etl, fromJson, logRecord } from '@triplyetl/etl/generic' export default async function (): Promise<Etl> { const etl = new Etl() etl.use( fromJson([ { id: '123', name: 'John' }, { id: '456', name: 'Jane' }, ]), logRecord(), ) return etl } Debug function logRecord() prints the current record to standard output. When this pipeline is run, the two inline records are printed as follows: { \"id\": \"123\", \"name\": \"John\", \"$recordId\": 1, \"$environment\": \"Development\" } { \"id\": \"456\", \"name\": \"Jane\", \"$recordId\": 2, \"$environment\": \"Development\" } Notice that TriplyETL adds two keys to both records: $recordId and $environment (see Special Key for more information). Now suppose that we change the source system. We no longer use inline JSON, but a local XML file. The contents of the XML file are as follows: <?xml version=\"1.0\"?> <persons> <person> <id>123</id> <name>John</name> </person> <person> <id>456</id> <name>Jane</name> </person> </persons> Let us change the TriplyETL script to use extractor fromXml() and the local file source type: import { Etl, fromXml, logRecord } from '@triplyetl/etl/generic' export default async function (): Promise<Etl> { const etl = new Etl() etl.use( fromXml(Source.file('example.xml')), logRecord(), ) return etl } This new script logs the following two records: { \"id\": \"123\", \"name\": \"John\", \"$recordId\": 1, \"$environment\": \"Development\" } { \"id\": \"456\", \"name\": \"Jane\", \"$recordId\": 2, \"$environment\": \"Development\" } Notice that the two records that are logged from an XML source are completely identical to the two records that were previously logged from a JSON source. This is an essential property of TriplyETL: it treats data from any source system in the same way, using the same intermediary Record format. This makes it easy to write pipelines that process data from a large number of different data sources. This also makes replacing a data source in one format with a data source in another format a relatively cheap operation. More often than not, only the source extractor needs to be changed, and all transformations and assertions remain as they were.","title":"The generic Record"},{"location":"triply-etl/generic/record/#extractor-loadrecords","text":"The loadRecords() function allows us to run a sub ETL and store its records to the main ETL. It is used when we would like to add additional data from different source to the main ETL. The function expects two arguments and can be run with the following snippet: fromSrc - The Source to load the data from. The list of available extractors can be seen in data sources overview page . key - A new key where the records are stored. loadRecords(fromSrc, 'key'), It is important to call the loadRecords() function after loading record data in the main ETL. The following code snippet extracts records from a json object (main ETL), then extracts records from a json file ( tableMap.json ) stored as an asset and stores them in the key _table in the record of the main ETL: fromJson({ country: 'be' }), loadRecords(fromJson(Source.TriplyDb.asset('test', { name: 'tableMap.json' })), '_table'), The combined record looks as following: { \"country\": \"be\", \"$recordId\": 1, \"$environment\": \"Development\", \"_table\": [ { \"be\": \"http://ex.com/Belgium\", \"nl\": \"http://ex.com/Netherlands\", \"de\": \"http://ex.com/Germany\", \"en\": \"http://ex.com/England\", \"$recordId\": 1, \"$environment\": \"Development\", \"$fileName\": \"tableMap.json\" } ] }","title":"Extractor loadRecords()"},{"location":"triply-etl/generic/record/#special-keys","text":"Records in TriplyETL contain several special keys. These special keys start with a dollar sign character ( $ ). The special keys contain values that are inserted during the Extract step. These special keys can be used in the same way as regular keys in your TriplyETL configuration. We now discuss these special keys in detail.","title":"Special keys"},{"location":"triply-etl/generic/record/#special-key-recordid","text":"The special key $recordId assigns a unique number to every record that is processed in one single run of a TriplyETL pipeline. If the source data does not change, multiple runs of the TriplyETL pipeline will always generate the same record IDs. However, if source data changes, multiple runs of the TriplyETL pipeline may generate different record IDs for the same record.","title":"Special key $recordId"},{"location":"triply-etl/generic/record/#use-case-unique-identifiers","text":"The first main use case of the $recordId key is to create IRIs that are unique within one single run of a TriplyETL pipeline. Suppose the following table is our source data: First name Last name John Doe Jane Doe John Doe We need to create an IRI for every person in this table. Notice that the table contains no unique properties: there are two different persons with the same first and last name. This means that we cannot use the keys \"First name\" and \"Last name\" in our record in order to create our IRIs. Luckily, the source connector adds the $recordId for us: { \"First name\": \"John\", \"Last name\": \"Doe\", \"$recordId\": 1 } { \"First name\": \"Jane\", \"Last name\": \"Doe\", \"$recordId\": 2 } { \"First name\": \"John\", \"Last name\": \"Doe\", \"$recordId\": 3 } This allows us to make the following assertion: pairs(iri(prefix.id, '$recordId'), [a, sdo.Person], [sdo.givenName, 'First name'], [sdo.familyName, 'Last name'], ), Which results in the following linked data: id:1 a sdo:Person; sdo:givenName 'John'; sdo:familyName 'Doe'. id:2 a sdo:Person; sdo:givenName 'Jane'; sdo:familyName 'Doe'. id:3 a sdo:Person; sdo:givenName 'John'; sdo:familyName 'Doe'. Notice that the use of the $recordId results in a correct single run of the TriplyETL pipeline. But if the source data changes, the IRIs may change as well. For example, if the first and second row in the source table are swapped, the IRI that denotes \"Jane Doe\" will change from id:2 to id:1 .","title":"Use case: Unique identifiers"},{"location":"triply-etl/generic/record/#use-case-debugging","text":"When you are debugging the configuration of a TriplyETL pipeline, it is sometimes useful to perform a specific actions for a specific record. Assuming the stream of records is stable during the debugging effort, the $recordId key can be used to perform such a debugging action; for example: whenEqual('$recordId', 908, logRecord()), Do note that it is generally better to run the TriplyETL for a specific record using the --from-record-id 908 --head 1 command line flags (see CLI ).","title":"Use case: Debugging"},{"location":"triply-etl/generic/record/#special-key-environment","text":"The TriplyETL record contains special key $environment . Its value denotes the DTAP environment that the pipeline is currently running in. This is one of the following values: \"Development\", \"Test\", \"Acceptance\", or \"Production\".","title":"Special key $environment"},{"location":"triply-etl/generic/record/#special-key-sheetname","text":"The special key $sheetName only occurs in records that original from data source that use the Microsoft Excel format. In such records, this special key contains the name of the sheet from which the record originates. See the documentation for the Microsoft Excel format for more information about this special key.","title":"Special key $sheetName"},{"location":"triply-etl/generic/skolem-iris/","text":"On this page: Skolem IRIs What are Skolem IRIs? Why does TriplyETL use Skolem IRIs? Skolem IRIs are a lossless approach An illustrative example Skolem IRIs \u00b6 TriplyETL uses Skolem IRIs instead of blank nodes. This approach is consistent with the RDF 1.1 standard. This page details why TriplyETL uses Skolem IRIs, and shows how they are used to stand in for blank nodes in a generic and standards-compliant way. What are Skolem IRIs? \u00b6 Skolem IRIs are IRIs that are used to systematically stand in for blank nodes. Whenever a blank node occurs in linked data, it is allowed to be replaced by a Skolem IRI. Skolem IRIs are guaranteed to universally unique, while blank nodes are only guaranteed to be unique within the context in which they occur. Why does TriplyETL use Skolem IRIs? \u00b6 Before linked data that contains blank nodes can be used, all blank nodes in that linked data must be renamed in order to avoid name collisions. Since Skolem IRIs are universally unique, there is no such requirements when using linked data that contains Skolem IRIs instead of blank nodes. TriplyETL uses Skolem IRIs instead of blank nodes, because this makes the linked data that TriplyETL creates easier to use. This easy-of-use applies to processing inside TriplyETL, but also applies to the use of linked data produced by TriplyETL after publication. Skolem IRIs are a lossless approach \u00b6 Since Skolem IRIs are required to use a specific path prefix (i.e. /.well-known/genid/ ), users of linked data containing Skolem IRIs are able to distinguish them from other IRIs. As such, it is possible to systematically replace Skolem IRIs with blank nodes again, since the translation from and to Skolem IRIs does not lose any information. Notice that while there are no benefits to replacing Skolem IRIs with blank nodes, only downsides, some users may still wish to perform such replacements. An illustrative example \u00b6 We show the downsides of linked data that contains blank nodes, and the benefits of linked data that contains Skolem IRIs that replace blank nodes, with an example. The following two linked data snippets use the same blank node label ('eFsgehcX9k25dv'): prefix ns: <https://example.com/> ns:product ns:height _:eFsgehcX9k25dv. _:eFsgehcX9k25dv ns:unitOfMeasure ns:meter; ns:value 1.1e0. and: prefix ns: <https://example.com/> ns:product ns:width _:eFsgehcX9k25dv. _:eFsgehcX9k25dv ns:unitOfMeasure ns:centimeter; ns:value 1.5e0. Since the blank node label occurs in two different contexts, we are not allowed to naively combine these two snippets. In fact, if we would naive combine them, we would end up with the following incorrect information: prefix ns: <https://example.com/> ns:product ns:height _:eFsgehcX9k25dv. ns:width _:eFsgehcX9k25dv. _:eFsgehcX9k25dv ns:unitOfMeasure ns:centimeter, ns:meter; ns:value 1.1e0, 1.5e0. Notice that in the combined snippet, it is no longer possible to determine what is the height and what is the width of the product. Neither is it possible to determine which unit of measure belongs to which numeric value. In order to avoid this issue with blank nodes, users are required to systematically rename them whenever they make use of the data. For example, in order to combine the two original linked data snippets, their blank node labels must first be renamed: prefix ns: <https://example.com/> ns:product ns:height _:eFsgehcX9k25dv_renamed1; ns:width _:eFsgehcX9k25dv_renamed2. _:eFsgehcX9k25dv_renamed1 ns:unitOfMeasure ns:meter; ns:value 1.1e0. _:eFsgehcX9k25dv_renamed2 ns:unitOfMeasure ns:centimeter; ns:value 1.5e0. This renaming of blank nodes must be performed every time the data is used . This is very cumbersome, and many users of linked data are unable to perform such renaming operations reliably. Now we show the same example, but by first replacing all blank nodes with Skolem IRIs. We start with the following two linked data snippets: prefix ns: <https://example.com/> prefix skolem: <https://example.com/.well-known/genid/> ns:product ns:height skolem:eFsgehcX9k25dv. skolem:eFsgehcX9k25dv ns:unitOfMeasure ns:meter; ns:value 1.1e0. and: prefix ns: <https://example.com/> prefix skolem: <https://example.com/.well-known/genid/> ns:product ns:width skolem:JmsR9ev5QgHZyx. skolem:JmsR9ev5QgHZyx ns:unitOfMeasure ns:centimeter; ns:value 1.5e0. Notice that we are able to use these linked data snippets directly, without having to perform a renaming operation. For example, we can naively combine the two snippets into one: prefix ns: <https://example.com/> prefix skolem: <https://example.com/.well-known/genid/> ns:product ns:height skolem:eFsgehcX9k25dv; ns:width skolem:JmsR9ev5QgHZyx. skolem:eFsgehcX9k25dv ns:unitOfMeasure ns:meter; ns:value 1.1e0. skolem:JmsR9ev5QgHZyx ns:unitOfMeasure ns:centimeter; ns:value 1.5e0.","title":"Skolem iris"},{"location":"triply-etl/generic/skolem-iris/#skolem-iris","text":"TriplyETL uses Skolem IRIs instead of blank nodes. This approach is consistent with the RDF 1.1 standard. This page details why TriplyETL uses Skolem IRIs, and shows how they are used to stand in for blank nodes in a generic and standards-compliant way.","title":"Skolem IRIs"},{"location":"triply-etl/generic/skolem-iris/#what-are-skolem-iris","text":"Skolem IRIs are IRIs that are used to systematically stand in for blank nodes. Whenever a blank node occurs in linked data, it is allowed to be replaced by a Skolem IRI. Skolem IRIs are guaranteed to universally unique, while blank nodes are only guaranteed to be unique within the context in which they occur.","title":"What are Skolem IRIs?"},{"location":"triply-etl/generic/skolem-iris/#why-does-triplyetl-use-skolem-iris","text":"Before linked data that contains blank nodes can be used, all blank nodes in that linked data must be renamed in order to avoid name collisions. Since Skolem IRIs are universally unique, there is no such requirements when using linked data that contains Skolem IRIs instead of blank nodes. TriplyETL uses Skolem IRIs instead of blank nodes, because this makes the linked data that TriplyETL creates easier to use. This easy-of-use applies to processing inside TriplyETL, but also applies to the use of linked data produced by TriplyETL after publication.","title":"Why does TriplyETL use Skolem IRIs?"},{"location":"triply-etl/generic/skolem-iris/#skolem-iris-are-a-lossless-approach","text":"Since Skolem IRIs are required to use a specific path prefix (i.e. /.well-known/genid/ ), users of linked data containing Skolem IRIs are able to distinguish them from other IRIs. As such, it is possible to systematically replace Skolem IRIs with blank nodes again, since the translation from and to Skolem IRIs does not lose any information. Notice that while there are no benefits to replacing Skolem IRIs with blank nodes, only downsides, some users may still wish to perform such replacements.","title":"Skolem IRIs are a lossless approach"},{"location":"triply-etl/generic/skolem-iris/#an-illustrative-example","text":"We show the downsides of linked data that contains blank nodes, and the benefits of linked data that contains Skolem IRIs that replace blank nodes, with an example. The following two linked data snippets use the same blank node label ('eFsgehcX9k25dv'): prefix ns: <https://example.com/> ns:product ns:height _:eFsgehcX9k25dv. _:eFsgehcX9k25dv ns:unitOfMeasure ns:meter; ns:value 1.1e0. and: prefix ns: <https://example.com/> ns:product ns:width _:eFsgehcX9k25dv. _:eFsgehcX9k25dv ns:unitOfMeasure ns:centimeter; ns:value 1.5e0. Since the blank node label occurs in two different contexts, we are not allowed to naively combine these two snippets. In fact, if we would naive combine them, we would end up with the following incorrect information: prefix ns: <https://example.com/> ns:product ns:height _:eFsgehcX9k25dv. ns:width _:eFsgehcX9k25dv. _:eFsgehcX9k25dv ns:unitOfMeasure ns:centimeter, ns:meter; ns:value 1.1e0, 1.5e0. Notice that in the combined snippet, it is no longer possible to determine what is the height and what is the width of the product. Neither is it possible to determine which unit of measure belongs to which numeric value. In order to avoid this issue with blank nodes, users are required to systematically rename them whenever they make use of the data. For example, in order to combine the two original linked data snippets, their blank node labels must first be renamed: prefix ns: <https://example.com/> ns:product ns:height _:eFsgehcX9k25dv_renamed1; ns:width _:eFsgehcX9k25dv_renamed2. _:eFsgehcX9k25dv_renamed1 ns:unitOfMeasure ns:meter; ns:value 1.1e0. _:eFsgehcX9k25dv_renamed2 ns:unitOfMeasure ns:centimeter; ns:value 1.5e0. This renaming of blank nodes must be performed every time the data is used . This is very cumbersome, and many users of linked data are unable to perform such renaming operations reliably. Now we show the same example, but by first replacing all blank nodes with Skolem IRIs. We start with the following two linked data snippets: prefix ns: <https://example.com/> prefix skolem: <https://example.com/.well-known/genid/> ns:product ns:height skolem:eFsgehcX9k25dv. skolem:eFsgehcX9k25dv ns:unitOfMeasure ns:meter; ns:value 1.1e0. and: prefix ns: <https://example.com/> prefix skolem: <https://example.com/.well-known/genid/> ns:product ns:width skolem:JmsR9ev5QgHZyx. skolem:JmsR9ev5QgHZyx ns:unitOfMeasure ns:centimeter; ns:value 1.5e0. Notice that we are able to use these linked data snippets directly, without having to perform a renaming operation. For example, we can naively combine the two snippets into one: prefix ns: <https://example.com/> prefix skolem: <https://example.com/.well-known/genid/> ns:product ns:height skolem:eFsgehcX9k25dv; ns:width skolem:JmsR9ev5QgHZyx. skolem:eFsgehcX9k25dv ns:unitOfMeasure ns:meter; ns:value 1.1e0. skolem:JmsR9ev5QgHZyx ns:unitOfMeasure ns:centimeter; ns:value 1.5e0.","title":"An illustrative example"},{"location":"triply-etl/generic/vocabularies/","text":"On this page: Supported vocabularies Supported vocabularies \u00b6 TriplyETL includes out-of-the-box support for a large number of external vocabularies, enumerated in the table below. If you miss an external vocabulary in that table, then let us know via: support@triply.cc With the latest update, TriplyETL vocabularies are now represented as Vocabulary objects, replacing the previous usage of objects with the type IRI . This change may necessitate adjustments to existing ETLs that utilize static vocabularies, such as aat . In this case, the vocabulary would need to be updated to aat.toIri() to ensure compatibility with the correct type. See the external vocabularies section for more information on how to use external vocabularies in ETL configuration. The following table lists the currently supported vocabularies: Name Version Use cases Description Argument Model Ontology (AMO) 1.0 Fake news detection, argumentation structure An ontology for describing argumentation according to Toulmin's argumentation model. Bibliographic Ontology Specification (BIBO) no version info Libraries, citation graphs, bibliography The Bibliographic Ontology Specification provides main concepts and properties for describing citations and bibliographic references (i.e. quotes, books, articles, etc) on the Semantic Web. Building Topology Ontology (BOT) 0.3.2 Buildings The Building Topology Ontology (BOT) is a minimal ontology for describing the core topological concepts of a building. Brick: A uniform metadata schema for buildings no version info Buildings Brick is an open-source effort to standardize semantic descriptions of the physical, logical and virtual assets in buildings and the relationships between them. Cultural Heritage Ontology (CEO) 1.41 Cultural heritage The CEO is the complete semantic representation of the logical data models CHO and KENNIS from the data layer of the RCE. Conceptual Reference Model (CRM) 7.1.2 Cultural heritage The CIDOC Conceptual Reference Model (CRM) provides definitions and a formal structure for describing the implicit and explicit concepts and relationships used in cultural heritage documentation. Conceptual Reference Model (CRM) - Digital no version info Digitization products An ontology and RDF Schema to encode metadata about the steps and methods of production (\u201cprovenance\u201d) of digitization products and synthetic digital representations such as 2D, 3D or even animated Models created by various technologies. Its distinct features compared to competitive models is the complete inclusion of the initial physical measurement processes and their parameters. Conceptual Reference Model (CRM) - PC no version info Cultural heritage CIDOC CRM v7.1.2 module for the implementation of properties of properties in RDFs. DBpedia Ontology 1.0.0 DBpedia Ontology for DBpedia Data Catalog Vocabulary (DCAT) 2.0.0 Data catalogs, datasets DCAT is an RDF vocabulary designed to facilitate interoperability between data catalogs published on the Web. Dublin Core Type Vocabulary 2012-06-14 Classes The DCMI Type Vocabulary was created in 2001. It defines classes for basic types of thing that can be described using DCMI metadata terms. Dublin Core Terms 1.1.0 Metadata terms This document is an up-to-date specification of all metadata terms maintained by the Dublin Core Metadata Initiative, including properties, vocabulary encoding schemes, syntax encoding schemes, and classes. Document Elements Ontology (DEO) 2015-07-03 Rhetorical elements within documents DEO, The Discourse Elements Ontology, is an ontology written in OWL 2 DL that provides a structured vocabulary for rhetorical elements within documents (e.g. Introduction, Discussion, Acknowledgements, Reference List, Figures, Appendix), enabling these to be described in RDF. It uses some of the rhetorical block elements from the SALT Rhetorical Ontology and the Ontology of Rhetorical Blocks. Document Components Ontology (DoCo) 1.3.0 Document components The Document Components Ontology (DoCO) in an ontology that provides a structured vocabulary written of document components, both structural (e.g., block, inline, paragraph, section, chapter) and rhetorical (e.g., introduction, discussion, acknowledgements, reference list, figure, appendix). ERA Vocabulary 2022-02-02 Railway infrastructure Vocabulary defined by the European Union Agency for Railways to describe the concepts and relationships related to the European railway infrastructure and the vehicles authorized to operate over it. FRBR-aligned Bibliographic Ontology (FaBiO) no version info Publishing, bibliography, textual publications An ontology for recording and publishing on the Semantic Web descriptions of entities that are published or potentially publishable, and that contain or are referred to by bibliographic references, or entities used to define such bibliographic references. Friend of a Friend (FOAF) 0.1.0 People, information FOAF is a project devoted to linking people and information using the Web. Regardless of whether information is in people's heads, in physical or digital documents, or in the form of factual data, it can be linked. FOAF integrates three kinds of network: social networks of human collaboration, friendship and association; representational networks that describe a simplified view of a cartoon universe in factual terms, and information networks that use Web-based linking to share independently published descriptions of this inter-connected world. FOAF does not compete with socially-oriented Web sites; rather it provides an approach in which different sites can tell different parts of the larger story, and by which users can retain some control over their information in a non-proprietary format. Functional Requirements for Bibliographic Records (FRBR) 2005-08-10 Bibliography This vocabulary is an expression in RDF of the concepts and relations described in the IFLA report on the Functional Requirements for Bibliographic Records (FRBR). GeoSPARQL 1.0 Geospatial data The OGC GeoSPARQL standard supports representing and querying geospatial data on the Semantic Web. GeoSPARQL defines a vocabulary for representing geospatial data in RDF, and it defines an extension to the SPARQL query language for processing geospatial data. In addition, GeoSPARQL is designed to accommodate systems based on qualitative spatial reasoning and systems based on quantitative spatial computations. Geography Markup Language (GML) Encoding Standard 3.2.1 Geography XML grammar for expressing geographical features. GML serves as a modeling language for geographic systems as well as an open interchange format for geographic transactions on the Internet. Getty Vocabulary Program (GVP) 3.3.0 Classes, properties and values in GVP LOD The GVP Ontology defines classes, properties and values ( skos:Concept s) used in GVP LOD. Linked Art no version info Cultural heritage Linked Art describes cultural heritage resources, with a focus on artworks and museum-oriented activities. It defines common patterns and terms to ensure that the resulting data can be easily used and is based on real-world data and use cases. Metagegevens voor duurzaam toegankelijke overheidsinformatie (MDTO) 1.0 Government information MDTO (Metadata for sustainably accessible government information) is a standard for recording and exchanging unambiguous metadata to enable the sustainable accessibility of government information. Organization ontology 0.8.0 Organizational structures Vocabulary for describing organizational structures, specializable to a broad variety of types of organization. Web Ontology Language (OWL) 2.0.0 Things, groups of things, and relations between things Language (OWL) is a Semantic Web language designed to represent rich and complex knowledge about things, groups of things, and relations between things. Person Name Vocabulary (PNV) 1.1 Persons' names The Person Name Vocabulary (PNV) is an RDF vocabulary and data model for persons' names. It is applicable to many datasets in which persons are described, as it accommodates different levels of data granularity. It furthermore allows for easy alignment of name elements, including idiosyncratic ones, such as family name prefixes and patronymics, with standard vocabularies such as Schema.org, FOAF, DBpedia and Wikidata, thus guaranteeing optimal data interoperability. PREMIS 3 Ontology 3.0.0 Digital objects Ontology for PREMIS 3, the international standard for metadata to support the preservation of digital objects and ensure their long-term usability. PROV Ontology (PROV-O) no version info Provenance information The PROV Ontology (PROV-O) expresses the PROV Data Model using the OWL2 Web Ontology Language (OWL2). It provides a set of classes, properties, and restrictions that can be used to represent and interchange provenance information generated in different systems and under different contexts. It can also be specialized to create new classes and properties to model provenance information for different applications and domains. Data Cube Vocabulary 0.2 Statistical data, multi-dimensional data sets There are many situations where it would be useful to be able to publish multi-dimensional data, such as statistics, on the web in such a way that it can be linked to related data sets and concepts. The Data Cube vocabulary provides a means to do this using the W3C RDF (Resource Description Framework) standard. The model underpinning the Data Cube vocabulary is compatible with the cube model that underlies SDMX (Statistical Data and Metadata eXchange), an ISO standard for exchanging and sharing statistical data and metadata among organizations. The Data Cube vocabulary is a core foundation which supports extension vocabularies to enable publication of other aspects of statistical data flows or other multi-dimensional data sets. Quantities, Units, Dimensions and Types (QUDT) 2.1.2 Physical quantities, units of measure, dimensions The QUDT, or \u201cQuantity, Unit, Dimension and Type\u201d schema defines the base classes properties, and restrictions used for modeling physical quantities, units of measure, and their dimensions in various measurement systems. RDA element sets: Agent properties 1.0.0 RDA Agent The Agent properties element set consists of properties representing attributes and relationships of the RDA Agent , Collective Agent , Person , Family , and Corporate Body entities. RDA element sets: Classes 1.0.0 Classes representing the RDA entities The Classes element set consists of classes representing the RDA entities, including RDA Entity, Work, Expression, Manifestation, Item, Agent, Collective Agent, Person, Family, Corporate Body, Nomen, Place, and Timespan. RDA Content Type 1.0.0 Content A categorization reflecting the fundamental form of communication in which the content is expressed and the human sense through which it is intended to be perceived. RDA Carrier Type 1.0.0 Carrier A categorization reflecting the format of the storage medium and housing of a carrier in combination with the type of intermediation device required to view, play, run, etc., the content of a resource. RDA Element Sets: Expression Properties 1.0.0 RDA Expression properties The Expression properties element set consists of properties representing attributes and relationships of the RDA Expression entity. RDA element sets: Item properties 5.0.12 RDA Item The Item properties element set consists of properties representing attributes and relationships of the RDA Item entity. RDA Element Sets: Manifestation Properties 1.0.0 RDA Manifestation The Manifestation properties element set consists of properties representing attributes and relationships of the RDA Manifestation entity. RDA Media Type 1.0.0 Media type A categorization reflecting the general type of intermediation device required to view, play, run, etc., the content of a resource. RDA element sets: Nomen properties 1.0.0 RDA Nomen The Nomen properties element set consists of properties representing attributes and relationships of the RDA Nomen entity. RDA element sets: Place properties 1.0.0 RDA Place The Place properties element set consists of properties representing attributes and relationships of the RDA Place entity. RDA element sets: Timespan properties 1.0.0 RDA Timespan The Expression properties element set consists of properties representing attributes and relationships of the RDA Timespan entity. RDA element sets: Unconstrained properties 1.0.0 Properties of all RDA entities The Unconstrained properties element set consists of properties representing the elements of all of the RDA entities. Each property in the element set has semantics which are independent of the LRM model and has no specified domain or range. RDA element sets: Work properties 1.0.0 RDA Work The Work properties element set consists of properties representing attributes and relationships of the RDA Work entity. RDA element sets: Entity properties 1.0.0 RDA Entity The RDA Entity properties element set consists of properties representing elements of the RDA Entity entity. Resource Description Framework (RDF) 1.1.0 RDF This is the RDF Schema for the RDF vocabulary terms in the RDF Namespace, defined in RDF Concepts. RDF Schema 1.1.0 Data-modelling vocabulary for RDF data RDF Schema provides a data-modelling vocabulary for RDF data. RDF Schema is an extension of the basic RDF vocabulary. MARC Code List for Relators Scheme 2017-09-07 Relator terms Relator terms and their associated codes designate the relationship between a name and a bibliographic resource. The relator codes are three-character lowercase alphabetic strings that serve as identifiers. Either the term or the code may be used as controlled values. Records in Contexts Ontology (ICA RiC-O) 0.2 Archives RiC-O (Records in Contexts-Ontology) is an OWL ontology for describing archival record resources. As the second part of Records in Contexts standard, it is a formal representation of Records in Contexts Conceptual Model (RiC-CM). Reconstructions and Observations in Archival Resources (ROAR) 0.1 Archives Ontology to describe person, location etc. observations in archival resources. One or multiple observations can be bundled into a reconstruction that combines complementary (or sometimes conflicting) information from the observation(s) so that a single entity is reconstructed out of several entity observations from one or multiple sources. Schema.org 22.0 Collection of shared vocabularies The Schema.org vocabulary, including the core vocabulary and all domain-specific layers. Shapes Constraint Language (SHACL) 1.0.0 Validation of RDF graphs SHACL Shapes Constraint Language is a language for validating RDF graphs against a set of conditions. These conditions are provided as shapes and other constructs expressed in the form of an RDF graph. RDF graphs that are used in this manner are called \u201cshapes graphs\u201d in SHACL and the RDF graphs that are validated against a shapes graph are called \u201cdata graphs\u201d. As SHACL shape graphs are used to validate that data graphs satisfy a set of conditions they can also be viewed as a description of the data graphs that do satisfy these conditions. Such descriptions may be used for a variety of purposes beside validation, including user interface building, code generation and data integration. Simple Knowledge Organization System (SKOS) 1.2.0 Knowledge organization systems The Simple Knowledge Organization System (SKOS) is a common data model for sharing and linking knowledge organization systems via the Semantic Web. Simple Knowledge Organization System eXtension for Labels 1.4.0 Labels SKOS-XL defines an extension for the Simple Knowledge Organization System, providing additional support for describing and linking lexical entities. SPARQL Service Description 1.1 SPARQL SPARQL Service Description Time Ontology 1.0.0 Temporal properties OWL-Time is an OWL-2 DL ontology of temporal concepts, for describing the temporal properties of resources in the world or described in Web pages. The ontology provides a vocabulary for expressing facts about topological (ordering) relations among instants and intervals, together with information about durations, and about temporal position including date-time information. Time positions and durations may be expressed using either the conventional (Gregorian) calendar and clock, or using another temporal reference system such as Unix-time, geologic time, or different calendars. All Units Ontology 2.1.2 Units of measure Standard units of measure for all units. Vocabulary for Annotating Vocabulary Descriptions (VANN) 1.0.0 Annotation A vocabulary for annotating descriptions of vocabularies with examples and usage notes. Vocabulary of Interlinked Datasets (VoID) 1.0.0 Metadata about RDF datasets VoID is an RDF Schema vocabulary for expressing metadata about RDF datasets. It is intended as a bridge between the publishers and users of RDF data, with applications ranging from data discovery to cataloging and archiving of datasets. WGS84 Geo Positioning 1.22.0 Latitude, longitude and altitude A vocabulary for representing latitude, longitude and altitude information in the WGS84 geodetic reference datum. WGS stands for the World Geodetic Survey.","title":"Vocabularies"},{"location":"triply-etl/generic/vocabularies/#supported-vocabularies","text":"TriplyETL includes out-of-the-box support for a large number of external vocabularies, enumerated in the table below. If you miss an external vocabulary in that table, then let us know via: support@triply.cc With the latest update, TriplyETL vocabularies are now represented as Vocabulary objects, replacing the previous usage of objects with the type IRI . This change may necessitate adjustments to existing ETLs that utilize static vocabularies, such as aat . In this case, the vocabulary would need to be updated to aat.toIri() to ensure compatibility with the correct type. See the external vocabularies section for more information on how to use external vocabularies in ETL configuration. The following table lists the currently supported vocabularies: Name Version Use cases Description Argument Model Ontology (AMO) 1.0 Fake news detection, argumentation structure An ontology for describing argumentation according to Toulmin's argumentation model. Bibliographic Ontology Specification (BIBO) no version info Libraries, citation graphs, bibliography The Bibliographic Ontology Specification provides main concepts and properties for describing citations and bibliographic references (i.e. quotes, books, articles, etc) on the Semantic Web. Building Topology Ontology (BOT) 0.3.2 Buildings The Building Topology Ontology (BOT) is a minimal ontology for describing the core topological concepts of a building. Brick: A uniform metadata schema for buildings no version info Buildings Brick is an open-source effort to standardize semantic descriptions of the physical, logical and virtual assets in buildings and the relationships between them. Cultural Heritage Ontology (CEO) 1.41 Cultural heritage The CEO is the complete semantic representation of the logical data models CHO and KENNIS from the data layer of the RCE. Conceptual Reference Model (CRM) 7.1.2 Cultural heritage The CIDOC Conceptual Reference Model (CRM) provides definitions and a formal structure for describing the implicit and explicit concepts and relationships used in cultural heritage documentation. Conceptual Reference Model (CRM) - Digital no version info Digitization products An ontology and RDF Schema to encode metadata about the steps and methods of production (\u201cprovenance\u201d) of digitization products and synthetic digital representations such as 2D, 3D or even animated Models created by various technologies. Its distinct features compared to competitive models is the complete inclusion of the initial physical measurement processes and their parameters. Conceptual Reference Model (CRM) - PC no version info Cultural heritage CIDOC CRM v7.1.2 module for the implementation of properties of properties in RDFs. DBpedia Ontology 1.0.0 DBpedia Ontology for DBpedia Data Catalog Vocabulary (DCAT) 2.0.0 Data catalogs, datasets DCAT is an RDF vocabulary designed to facilitate interoperability between data catalogs published on the Web. Dublin Core Type Vocabulary 2012-06-14 Classes The DCMI Type Vocabulary was created in 2001. It defines classes for basic types of thing that can be described using DCMI metadata terms. Dublin Core Terms 1.1.0 Metadata terms This document is an up-to-date specification of all metadata terms maintained by the Dublin Core Metadata Initiative, including properties, vocabulary encoding schemes, syntax encoding schemes, and classes. Document Elements Ontology (DEO) 2015-07-03 Rhetorical elements within documents DEO, The Discourse Elements Ontology, is an ontology written in OWL 2 DL that provides a structured vocabulary for rhetorical elements within documents (e.g. Introduction, Discussion, Acknowledgements, Reference List, Figures, Appendix), enabling these to be described in RDF. It uses some of the rhetorical block elements from the SALT Rhetorical Ontology and the Ontology of Rhetorical Blocks. Document Components Ontology (DoCo) 1.3.0 Document components The Document Components Ontology (DoCO) in an ontology that provides a structured vocabulary written of document components, both structural (e.g., block, inline, paragraph, section, chapter) and rhetorical (e.g., introduction, discussion, acknowledgements, reference list, figure, appendix). ERA Vocabulary 2022-02-02 Railway infrastructure Vocabulary defined by the European Union Agency for Railways to describe the concepts and relationships related to the European railway infrastructure and the vehicles authorized to operate over it. FRBR-aligned Bibliographic Ontology (FaBiO) no version info Publishing, bibliography, textual publications An ontology for recording and publishing on the Semantic Web descriptions of entities that are published or potentially publishable, and that contain or are referred to by bibliographic references, or entities used to define such bibliographic references. Friend of a Friend (FOAF) 0.1.0 People, information FOAF is a project devoted to linking people and information using the Web. Regardless of whether information is in people's heads, in physical or digital documents, or in the form of factual data, it can be linked. FOAF integrates three kinds of network: social networks of human collaboration, friendship and association; representational networks that describe a simplified view of a cartoon universe in factual terms, and information networks that use Web-based linking to share independently published descriptions of this inter-connected world. FOAF does not compete with socially-oriented Web sites; rather it provides an approach in which different sites can tell different parts of the larger story, and by which users can retain some control over their information in a non-proprietary format. Functional Requirements for Bibliographic Records (FRBR) 2005-08-10 Bibliography This vocabulary is an expression in RDF of the concepts and relations described in the IFLA report on the Functional Requirements for Bibliographic Records (FRBR). GeoSPARQL 1.0 Geospatial data The OGC GeoSPARQL standard supports representing and querying geospatial data on the Semantic Web. GeoSPARQL defines a vocabulary for representing geospatial data in RDF, and it defines an extension to the SPARQL query language for processing geospatial data. In addition, GeoSPARQL is designed to accommodate systems based on qualitative spatial reasoning and systems based on quantitative spatial computations. Geography Markup Language (GML) Encoding Standard 3.2.1 Geography XML grammar for expressing geographical features. GML serves as a modeling language for geographic systems as well as an open interchange format for geographic transactions on the Internet. Getty Vocabulary Program (GVP) 3.3.0 Classes, properties and values in GVP LOD The GVP Ontology defines classes, properties and values ( skos:Concept s) used in GVP LOD. Linked Art no version info Cultural heritage Linked Art describes cultural heritage resources, with a focus on artworks and museum-oriented activities. It defines common patterns and terms to ensure that the resulting data can be easily used and is based on real-world data and use cases. Metagegevens voor duurzaam toegankelijke overheidsinformatie (MDTO) 1.0 Government information MDTO (Metadata for sustainably accessible government information) is a standard for recording and exchanging unambiguous metadata to enable the sustainable accessibility of government information. Organization ontology 0.8.0 Organizational structures Vocabulary for describing organizational structures, specializable to a broad variety of types of organization. Web Ontology Language (OWL) 2.0.0 Things, groups of things, and relations between things Language (OWL) is a Semantic Web language designed to represent rich and complex knowledge about things, groups of things, and relations between things. Person Name Vocabulary (PNV) 1.1 Persons' names The Person Name Vocabulary (PNV) is an RDF vocabulary and data model for persons' names. It is applicable to many datasets in which persons are described, as it accommodates different levels of data granularity. It furthermore allows for easy alignment of name elements, including idiosyncratic ones, such as family name prefixes and patronymics, with standard vocabularies such as Schema.org, FOAF, DBpedia and Wikidata, thus guaranteeing optimal data interoperability. PREMIS 3 Ontology 3.0.0 Digital objects Ontology for PREMIS 3, the international standard for metadata to support the preservation of digital objects and ensure their long-term usability. PROV Ontology (PROV-O) no version info Provenance information The PROV Ontology (PROV-O) expresses the PROV Data Model using the OWL2 Web Ontology Language (OWL2). It provides a set of classes, properties, and restrictions that can be used to represent and interchange provenance information generated in different systems and under different contexts. It can also be specialized to create new classes and properties to model provenance information for different applications and domains. Data Cube Vocabulary 0.2 Statistical data, multi-dimensional data sets There are many situations where it would be useful to be able to publish multi-dimensional data, such as statistics, on the web in such a way that it can be linked to related data sets and concepts. The Data Cube vocabulary provides a means to do this using the W3C RDF (Resource Description Framework) standard. The model underpinning the Data Cube vocabulary is compatible with the cube model that underlies SDMX (Statistical Data and Metadata eXchange), an ISO standard for exchanging and sharing statistical data and metadata among organizations. The Data Cube vocabulary is a core foundation which supports extension vocabularies to enable publication of other aspects of statistical data flows or other multi-dimensional data sets. Quantities, Units, Dimensions and Types (QUDT) 2.1.2 Physical quantities, units of measure, dimensions The QUDT, or \u201cQuantity, Unit, Dimension and Type\u201d schema defines the base classes properties, and restrictions used for modeling physical quantities, units of measure, and their dimensions in various measurement systems. RDA element sets: Agent properties 1.0.0 RDA Agent The Agent properties element set consists of properties representing attributes and relationships of the RDA Agent , Collective Agent , Person , Family , and Corporate Body entities. RDA element sets: Classes 1.0.0 Classes representing the RDA entities The Classes element set consists of classes representing the RDA entities, including RDA Entity, Work, Expression, Manifestation, Item, Agent, Collective Agent, Person, Family, Corporate Body, Nomen, Place, and Timespan. RDA Content Type 1.0.0 Content A categorization reflecting the fundamental form of communication in which the content is expressed and the human sense through which it is intended to be perceived. RDA Carrier Type 1.0.0 Carrier A categorization reflecting the format of the storage medium and housing of a carrier in combination with the type of intermediation device required to view, play, run, etc., the content of a resource. RDA Element Sets: Expression Properties 1.0.0 RDA Expression properties The Expression properties element set consists of properties representing attributes and relationships of the RDA Expression entity. RDA element sets: Item properties 5.0.12 RDA Item The Item properties element set consists of properties representing attributes and relationships of the RDA Item entity. RDA Element Sets: Manifestation Properties 1.0.0 RDA Manifestation The Manifestation properties element set consists of properties representing attributes and relationships of the RDA Manifestation entity. RDA Media Type 1.0.0 Media type A categorization reflecting the general type of intermediation device required to view, play, run, etc., the content of a resource. RDA element sets: Nomen properties 1.0.0 RDA Nomen The Nomen properties element set consists of properties representing attributes and relationships of the RDA Nomen entity. RDA element sets: Place properties 1.0.0 RDA Place The Place properties element set consists of properties representing attributes and relationships of the RDA Place entity. RDA element sets: Timespan properties 1.0.0 RDA Timespan The Expression properties element set consists of properties representing attributes and relationships of the RDA Timespan entity. RDA element sets: Unconstrained properties 1.0.0 Properties of all RDA entities The Unconstrained properties element set consists of properties representing the elements of all of the RDA entities. Each property in the element set has semantics which are independent of the LRM model and has no specified domain or range. RDA element sets: Work properties 1.0.0 RDA Work The Work properties element set consists of properties representing attributes and relationships of the RDA Work entity. RDA element sets: Entity properties 1.0.0 RDA Entity The RDA Entity properties element set consists of properties representing elements of the RDA Entity entity. Resource Description Framework (RDF) 1.1.0 RDF This is the RDF Schema for the RDF vocabulary terms in the RDF Namespace, defined in RDF Concepts. RDF Schema 1.1.0 Data-modelling vocabulary for RDF data RDF Schema provides a data-modelling vocabulary for RDF data. RDF Schema is an extension of the basic RDF vocabulary. MARC Code List for Relators Scheme 2017-09-07 Relator terms Relator terms and their associated codes designate the relationship between a name and a bibliographic resource. The relator codes are three-character lowercase alphabetic strings that serve as identifiers. Either the term or the code may be used as controlled values. Records in Contexts Ontology (ICA RiC-O) 0.2 Archives RiC-O (Records in Contexts-Ontology) is an OWL ontology for describing archival record resources. As the second part of Records in Contexts standard, it is a formal representation of Records in Contexts Conceptual Model (RiC-CM). Reconstructions and Observations in Archival Resources (ROAR) 0.1 Archives Ontology to describe person, location etc. observations in archival resources. One or multiple observations can be bundled into a reconstruction that combines complementary (or sometimes conflicting) information from the observation(s) so that a single entity is reconstructed out of several entity observations from one or multiple sources. Schema.org 22.0 Collection of shared vocabularies The Schema.org vocabulary, including the core vocabulary and all domain-specific layers. Shapes Constraint Language (SHACL) 1.0.0 Validation of RDF graphs SHACL Shapes Constraint Language is a language for validating RDF graphs against a set of conditions. These conditions are provided as shapes and other constructs expressed in the form of an RDF graph. RDF graphs that are used in this manner are called \u201cshapes graphs\u201d in SHACL and the RDF graphs that are validated against a shapes graph are called \u201cdata graphs\u201d. As SHACL shape graphs are used to validate that data graphs satisfy a set of conditions they can also be viewed as a description of the data graphs that do satisfy these conditions. Such descriptions may be used for a variety of purposes beside validation, including user interface building, code generation and data integration. Simple Knowledge Organization System (SKOS) 1.2.0 Knowledge organization systems The Simple Knowledge Organization System (SKOS) is a common data model for sharing and linking knowledge organization systems via the Semantic Web. Simple Knowledge Organization System eXtension for Labels 1.4.0 Labels SKOS-XL defines an extension for the Simple Knowledge Organization System, providing additional support for describing and linking lexical entities. SPARQL Service Description 1.1 SPARQL SPARQL Service Description Time Ontology 1.0.0 Temporal properties OWL-Time is an OWL-2 DL ontology of temporal concepts, for describing the temporal properties of resources in the world or described in Web pages. The ontology provides a vocabulary for expressing facts about topological (ordering) relations among instants and intervals, together with information about durations, and about temporal position including date-time information. Time positions and durations may be expressed using either the conventional (Gregorian) calendar and clock, or using another temporal reference system such as Unix-time, geologic time, or different calendars. All Units Ontology 2.1.2 Units of measure Standard units of measure for all units. Vocabulary for Annotating Vocabulary Descriptions (VANN) 1.0.0 Annotation A vocabulary for annotating descriptions of vocabularies with examples and usage notes. Vocabulary of Interlinked Datasets (VoID) 1.0.0 Metadata about RDF datasets VoID is an RDF Schema vocabulary for expressing metadata about RDF datasets. It is intended as a bridge between the publishers and users of RDF data, with applications ranging from data discovery to cataloging and archiving of datasets. WGS84 Geo Positioning 1.22.0 Latitude, longitude and altitude A vocabulary for representing latitude, longitude and altitude information in the WGS84 geodetic reference datum. WGS stands for the World Geodetic Survey.","title":"Supported vocabularies"},{"location":"triply-etl/publish/","text":"On this page: Publish Destinations Remote data destinations Publishing datasets to the NDE Dataset Register Local data destinations Static and Dynamic destinations Configuring multiple TriplyDB instances Direct copying from source to destination Using TriplyDB.js in TriplyETL Upload prefix declarations Publish \u00b6 The Publish step makes the linked data that is produced by the TriplyETL pipeline available in a Triple Store for use by others. graph LR sources -- 1. Extract --> record record -- 2. Transform --> record record -- 3. Assert --> ld ld -- 4. Enrich --> ld ld -- 5. Validate --> ld ld -- 6. Publish --> destinations linkStyle 5 stroke:red,stroke-width:3px; destinations[(\"D. Destinations\\n(TriplyDB)\")] ld[C. Internal Store] record[B. Record] sources[A. Data Sources] Destinations \u00b6 Linked data that is produced by a TriplyETL pipeline can be published to one or more destinations. Remote data destinations \u00b6 Destinations are usually online locations in TriplyDB where the output of your pipeline will be published. If no account name is given, pipeline output is uploaded under the user account tied to the currently used API Token. To upload the output to TriplyDB you can use the toTriplyDb() function, as the snippet below shows. toTriplyDb({dataset: 'my-dataset'}) toTriplyDb({account: 'my-account', dataset: 'my-dataset'}) toTriplyDb({account: 'my-account', dataset: 'my-dataset', opts:{ overwrite: true }}) In the previous versions of TriplyETL, this was done with the toRdf() function as shown below: toRdf(Destination.TriplyDb.rdf('my-account', 'my-dataset', {triplyDb: etl.triplyDb})) It is still possible to upload to TriplyDB using toRdf() , but the new toTriplyDb() function represents a simplified version of this. The following options can be specified to configure the destination behavior: mergeGraphs Whether the results of the new graph should be added to the old graph without overwriting it. The default value is false . overwrite Whether the graphs that are being uploaded by TriplyETL should replace any existing graphs with the same name in the dataset. Graphs appearing in the dataset with a different name than those uploaded by TriplyETL are kept. The default value is false . synchronizeServices Whether or more active services should be automatically synchronized once new data is uploaded. The default value is false . This value can be changed to `true`, to automatically synchronize all services. Alternatively, this value can be set to a string that names a specific services, to automatically synchronize only that specific service. triplyDb A configuration object describing a TriplyDB instance that is different from the one associated with the current API Token. (See the section on configuring multiple TriplyDB instance for more information.) truncateGraphs Whether to delete all graphs in the dataset before uploading any graphs from TriplyETL. Notice that this will also remove graphs that will not be re-uploaded by TriplyETL. The default value is false . Example: The following code snippet publishes linked data to a TriplyDB dataset called 'my-dataset' and synchronizes only the 'acceptance' service for that dataset: toRdf(Destination.TriplyDb.rdf('my-dataset', {synchronizeServices: 'acceptance'})), Publishing datasets to the NDE Dataset Register \u00b6 If you wat to publish a dataset to the NDE Dataset Register, you can do it by adding the {submitToNDEDatasetRegister: true} option to toTriplyDB() middleware. toTriplyDb({dataset: 'nde', opts: {submitToNDEDatasetRegister: true}}) Local data destinations \u00b6 TriplyETL supports publishing RDF output into a local file. This is not often used, because files lack many of the features that TriplyDB destinations support, such as: The ability to browse the data. The ability to query the data. The ability to configure metadata. The ability to configure prefix declarations. Still, there may be cases in which a local file destination is useful, for example when you do not have an active Internet connection: toRdf(Destination.file('my-file.trig')), Static and Dynamic destinations \u00b6 Destinations can be defined as static objects meaning that you can define destination beforehand. But it might be the case that you want to have multiple destinations for different records. In this case, you would need a dynamic destination, which should change based on certain information inside your source data. You can set static and dynamic destinations: const etl = new Etl({ sources: { someSource: Source.file('source.trig'), }, destinations: { someStaticDestination: Destination.file('static.ttl'), someDynamicDestination: context => Destination.file(context.getString('destination')), }, }) Configuring multiple TriplyDB instances \u00b6 It is possible to use multiple TriplyDB instances in one TriplyETL pipeline. The following example illustrates how the data model is used from the production instance of TriplyDB. const etl = new Etl({ sources: { data_model: Source.TriplyDb.rdf( 'my-account', 'my-dataset', { triplyDb: { token: process.env['PRODUCTION_INSTANCE_TOKEN'], url: 'https://api.production.example.com' } } ), instance_data: Source.TriplyDb.rdf( 'my-account', 'my-dataset', { triplyDb: { token: process.env['ACCEPTANCE_INSTANCE_TOKEN'], url: 'https://api.acceptance.example.com' } } ), }, }) Direct copying from source to destination \u00b6 TriplyETL supports copying sources directly to destination locations. This function is useful when you already have linked data that is used as a source, but is also needed at the destination. An example would be the information model. This would be available as a source, and with the copy function it can be uploaded to TriplyDB via TriplyETL. The following example shows the copy function: await etl.copySource( Source.file(`${source_location}`), Destination.TriplyDb.rdf(`${destination_name}`) ) The function destination expects that source data is linked data. Copying a source that is not linked data can result in errors. Please note that the copySource function is not considered part of the middleware layer but is a specialized function used for direct source-to-destination copying. As a result, it won't be counted in the middleware runtime. Using TriplyDB.js in TriplyETL \u00b6 All operations that can be performed in a TriplyDB instance can be automated with classes and methods in the TriplyDB.js library. This library is also used by TriplyETL in the background to implement many of the TriplyETL functionalities. Sometimes it is useful to use classes and methods in TriplyDB.js directly. This is done in the following way: // Create the ETL context. const etl = new Etl() // Use the context to access the TriplyDB.js connection. console.log((await etl.triplyDb.getInfo()).name) The above example prints the name of the TriplyDB instance. But any other TriplyDB.js operations can be performed. For example, the user of the current API Token can change their avatar image in TriplyDB: const user = await etl.triplyDb.getUser() await user.setAvatar('my-avatar.png') Upload prefix declarations \u00b6 At the end of a TriplyETL script, it is common to upload the prefix declarations that are configured for that pipeline. This is often done directly before or after graphs are uploaded (function toTriplyDb() ): import { toTriplyDb, uploadPrefixes } from '@triplyetl/etl/generic' const prefix = { // Your prefix declarations. } export default async function(): Promise<Etl> { const etl = new Etl({ prefixes: prefix }) etl.run( // You ETL pipeline. toTriplyDb({ account: 'my-account', dataset: 'my-dataset' }), uploadPrefixes({ account: 'my-account', dataset: 'my-dataset' }), ) return etl }","title":"Publish"},{"location":"triply-etl/publish/#publish","text":"The Publish step makes the linked data that is produced by the TriplyETL pipeline available in a Triple Store for use by others. graph LR sources -- 1. Extract --> record record -- 2. Transform --> record record -- 3. Assert --> ld ld -- 4. Enrich --> ld ld -- 5. Validate --> ld ld -- 6. Publish --> destinations linkStyle 5 stroke:red,stroke-width:3px; destinations[(\"D. Destinations\\n(TriplyDB)\")] ld[C. Internal Store] record[B. Record] sources[A. Data Sources]","title":"Publish"},{"location":"triply-etl/publish/#destinations","text":"Linked data that is produced by a TriplyETL pipeline can be published to one or more destinations.","title":"Destinations"},{"location":"triply-etl/publish/#remote-data-destinations","text":"Destinations are usually online locations in TriplyDB where the output of your pipeline will be published. If no account name is given, pipeline output is uploaded under the user account tied to the currently used API Token. To upload the output to TriplyDB you can use the toTriplyDb() function, as the snippet below shows. toTriplyDb({dataset: 'my-dataset'}) toTriplyDb({account: 'my-account', dataset: 'my-dataset'}) toTriplyDb({account: 'my-account', dataset: 'my-dataset', opts:{ overwrite: true }}) In the previous versions of TriplyETL, this was done with the toRdf() function as shown below: toRdf(Destination.TriplyDb.rdf('my-account', 'my-dataset', {triplyDb: etl.triplyDb})) It is still possible to upload to TriplyDB using toRdf() , but the new toTriplyDb() function represents a simplified version of this. The following options can be specified to configure the destination behavior: mergeGraphs Whether the results of the new graph should be added to the old graph without overwriting it. The default value is false . overwrite Whether the graphs that are being uploaded by TriplyETL should replace any existing graphs with the same name in the dataset. Graphs appearing in the dataset with a different name than those uploaded by TriplyETL are kept. The default value is false . synchronizeServices Whether or more active services should be automatically synchronized once new data is uploaded. The default value is false . This value can be changed to `true`, to automatically synchronize all services. Alternatively, this value can be set to a string that names a specific services, to automatically synchronize only that specific service. triplyDb A configuration object describing a TriplyDB instance that is different from the one associated with the current API Token. (See the section on configuring multiple TriplyDB instance for more information.) truncateGraphs Whether to delete all graphs in the dataset before uploading any graphs from TriplyETL. Notice that this will also remove graphs that will not be re-uploaded by TriplyETL. The default value is false . Example: The following code snippet publishes linked data to a TriplyDB dataset called 'my-dataset' and synchronizes only the 'acceptance' service for that dataset: toRdf(Destination.TriplyDb.rdf('my-dataset', {synchronizeServices: 'acceptance'})),","title":"Remote data destinations"},{"location":"triply-etl/publish/#publishing-datasets-to-the-nde-dataset-register","text":"If you wat to publish a dataset to the NDE Dataset Register, you can do it by adding the {submitToNDEDatasetRegister: true} option to toTriplyDB() middleware. toTriplyDb({dataset: 'nde', opts: {submitToNDEDatasetRegister: true}})","title":"Publishing datasets to the NDE Dataset Register"},{"location":"triply-etl/publish/#local-data-destinations","text":"TriplyETL supports publishing RDF output into a local file. This is not often used, because files lack many of the features that TriplyDB destinations support, such as: The ability to browse the data. The ability to query the data. The ability to configure metadata. The ability to configure prefix declarations. Still, there may be cases in which a local file destination is useful, for example when you do not have an active Internet connection: toRdf(Destination.file('my-file.trig')),","title":"Local data destinations"},{"location":"triply-etl/publish/#static-and-dynamic-destinations","text":"Destinations can be defined as static objects meaning that you can define destination beforehand. But it might be the case that you want to have multiple destinations for different records. In this case, you would need a dynamic destination, which should change based on certain information inside your source data. You can set static and dynamic destinations: const etl = new Etl({ sources: { someSource: Source.file('source.trig'), }, destinations: { someStaticDestination: Destination.file('static.ttl'), someDynamicDestination: context => Destination.file(context.getString('destination')), }, })","title":"Static and Dynamic destinations"},{"location":"triply-etl/publish/#configuring-multiple-triplydb-instances","text":"It is possible to use multiple TriplyDB instances in one TriplyETL pipeline. The following example illustrates how the data model is used from the production instance of TriplyDB. const etl = new Etl({ sources: { data_model: Source.TriplyDb.rdf( 'my-account', 'my-dataset', { triplyDb: { token: process.env['PRODUCTION_INSTANCE_TOKEN'], url: 'https://api.production.example.com' } } ), instance_data: Source.TriplyDb.rdf( 'my-account', 'my-dataset', { triplyDb: { token: process.env['ACCEPTANCE_INSTANCE_TOKEN'], url: 'https://api.acceptance.example.com' } } ), }, })","title":"Configuring multiple TriplyDB instances"},{"location":"triply-etl/publish/#direct-copying-from-source-to-destination","text":"TriplyETL supports copying sources directly to destination locations. This function is useful when you already have linked data that is used as a source, but is also needed at the destination. An example would be the information model. This would be available as a source, and with the copy function it can be uploaded to TriplyDB via TriplyETL. The following example shows the copy function: await etl.copySource( Source.file(`${source_location}`), Destination.TriplyDb.rdf(`${destination_name}`) ) The function destination expects that source data is linked data. Copying a source that is not linked data can result in errors. Please note that the copySource function is not considered part of the middleware layer but is a specialized function used for direct source-to-destination copying. As a result, it won't be counted in the middleware runtime.","title":"Direct copying from source to destination"},{"location":"triply-etl/publish/#using-triplydbjs-in-triplyetl","text":"All operations that can be performed in a TriplyDB instance can be automated with classes and methods in the TriplyDB.js library. This library is also used by TriplyETL in the background to implement many of the TriplyETL functionalities. Sometimes it is useful to use classes and methods in TriplyDB.js directly. This is done in the following way: // Create the ETL context. const etl = new Etl() // Use the context to access the TriplyDB.js connection. console.log((await etl.triplyDb.getInfo()).name) The above example prints the name of the TriplyDB instance. But any other TriplyDB.js operations can be performed. For example, the user of the current API Token can change their avatar image in TriplyDB: const user = await etl.triplyDb.getUser() await user.setAvatar('my-avatar.png')","title":"Using TriplyDB.js in TriplyETL"},{"location":"triply-etl/publish/#upload-prefix-declarations","text":"At the end of a TriplyETL script, it is common to upload the prefix declarations that are configured for that pipeline. This is often done directly before or after graphs are uploaded (function toTriplyDb() ): import { toTriplyDb, uploadPrefixes } from '@triplyetl/etl/generic' const prefix = { // Your prefix declarations. } export default async function(): Promise<Etl> { const etl = new Etl({ prefixes: prefix }) etl.run( // You ETL pipeline. toTriplyDb({ account: 'my-account', dataset: 'my-dataset' }), uploadPrefixes({ account: 'my-account', dataset: 'my-dataset' }), ) return etl }","title":"Upload prefix declarations"},{"location":"triply-etl/sources/","text":"On this page: Sources Sources \u00b6 TriplyETL Sources are locations that hold data that can be extracted with one or more TriplyETL extractors . graph LR sources -- 1. Extract --> record record -- 2. Transform --> record record -- 3. Assert --> ld ld -- 4. Enrich --> ld ld -- 5. Validate --> ld ld -- 6. Publish --> destinations style sources fill:#f9f,stroke:#333,stroke-width:4px destinations[(\"D. Destinations\\n(TriplyDB)\")] ld[C. Internal Store] record[B. Record] sources[A. Data Sources] The following kinds of sources are supported: APIs Inline JSON Inline strings Local files Online files TriplyDB Assets TriplyDB Datasets TriplyDB Queries","title":"Overview"},{"location":"triply-etl/sources/#sources","text":"TriplyETL Sources are locations that hold data that can be extracted with one or more TriplyETL extractors . graph LR sources -- 1. Extract --> record record -- 2. Transform --> record record -- 3. Assert --> ld ld -- 4. Enrich --> ld ld -- 5. Validate --> ld ld -- 6. Publish --> destinations style sources fill:#f9f,stroke:#333,stroke-width:4px destinations[(\"D. Destinations\\n(TriplyDB)\")] ld[C. Internal Store] record[B. Record] sources[A. Data Sources] The following kinds of sources are supported: APIs Inline JSON Inline strings Local files Online files TriplyDB Assets TriplyDB Datasets TriplyDB Queries","title":"Sources"},{"location":"triply-etl/sources/apis/","text":"On this page: APIs Raw SPARQL endpoints Use in production systems APIs \u00b6 The URL source type can also be used to extract records from online endpoints and APIs. The following code snippet extracts records from a TriplyDB RESTful API: fromJson(Source.url('https://api.triplydb.com/datasets')), Raw SPARQL endpoints \u00b6 SPARQL endpoints are online APIs. The following code snippet issues a raw SPARQL query against a public SPARQL endpoint. Since we specified CSV as the result set format (Media Type text/csv ), the result set can be accessed as any other CSV source: fromCsv( Source.url( 'https://dbpedia.org/sparql', { request: { headers: { accept: 'text/csv', 'content-type': 'application/query-string', }, body: 'select * { ?s ?p ?o. } limit 1', method: 'POST', }, } ) ) Use in production systems \u00b6 Raw SPARQL endpoints lack several features that are essential for use in production systems: - secure access control - pagination - reliable retrieval of large result sets - API variables - versioning These features are all supported by TriplyDB queries . It is therefore simpler and safer to use TriplyDB queries. Still, when used outside of production systems, raw SPARQL endpoints can still be used as regular web APIs.","title":"APIs"},{"location":"triply-etl/sources/apis/#apis","text":"The URL source type can also be used to extract records from online endpoints and APIs. The following code snippet extracts records from a TriplyDB RESTful API: fromJson(Source.url('https://api.triplydb.com/datasets')),","title":"APIs"},{"location":"triply-etl/sources/apis/#raw-sparql-endpoints","text":"SPARQL endpoints are online APIs. The following code snippet issues a raw SPARQL query against a public SPARQL endpoint. Since we specified CSV as the result set format (Media Type text/csv ), the result set can be accessed as any other CSV source: fromCsv( Source.url( 'https://dbpedia.org/sparql', { request: { headers: { accept: 'text/csv', 'content-type': 'application/query-string', }, body: 'select * { ?s ?p ?o. } limit 1', method: 'POST', }, } ) )","title":"Raw SPARQL endpoints"},{"location":"triply-etl/sources/apis/#use-in-production-systems","text":"Raw SPARQL endpoints lack several features that are essential for use in production systems: - secure access control - pagination - reliable retrieval of large result sets - API variables - versioning These features are all supported by TriplyDB queries . It is therefore simpler and safer to use TriplyDB queries. Still, when used outside of production systems, raw SPARQL endpoints can still be used as regular web APIs.","title":"Use in production systems"},{"location":"triply-etl/sources/inline-json/","text":"On this page: Inline JSON Inline JSON \u00b6 Because TriplyETL configurations are implemented in TypeScript, it is possible to specify JSON data inline with TypeScript Objects. JSON is the only data format that be specified in such a native inline way in TriplyETL. The following code snippet specifies two records using inline TypeScript objects: fromJson([ { id: '123', name: 'John' }, { id: '456', name: 'Jane' }, ]), This results in the following two records: { \"id\": \"123\", \"name\": \"John\" } { \"id\": \"456\", \"name\": \"Jane\" } In documentation, we often use such inline JSON sources since that makes code snippets self-contained, without having to rely on external sources such as files. In production systems this native inline source type is almost never used.","title":"Inline JSON"},{"location":"triply-etl/sources/inline-json/#inline-json","text":"Because TriplyETL configurations are implemented in TypeScript, it is possible to specify JSON data inline with TypeScript Objects. JSON is the only data format that be specified in such a native inline way in TriplyETL. The following code snippet specifies two records using inline TypeScript objects: fromJson([ { id: '123', name: 'John' }, { id: '456', name: 'Jane' }, ]), This results in the following two records: { \"id\": \"123\", \"name\": \"John\" } { \"id\": \"456\", \"name\": \"Jane\" } In documentation, we often use such inline JSON sources since that makes code snippets self-contained, without having to rely on external sources such as files. In production systems this native inline source type is almost never used.","title":"Inline JSON"},{"location":"triply-etl/sources/inline-strings/","text":"On this page: Inline strings Inline strings \u00b6 Data in the JSON or RDF formats can be specified with inline strings. The following code snippet loads triples into the Internal Store: loadRdf( Source.string(` prefix person: <https://example.com/id/person/> prefix sdo: <https://schema.org/> person:1 a sdo:Person; sdo:name 'J. Doe'.`), { contentType: 'text/turtle' } ), This loads the following triples: graph LR person:1 -- a --> sdo:Person person:1 -- sdo:name --> J.Doe Notice that we must specify the RDF serialization format that we use. This is necessary because loadRdf() supports a large number of formats, some of which are difficult to autodetect. The following formats are supported: Format contentType value HTML 'text/html' JSON-LD 'application/ld+json' JSON 'application/json' N-Quads 'application/n-quads' N-Triples 'application/n-triples' N3 'text/n3' RDF/XML 'application/rdf+xml' SVG 'image/svg+xml' TriG 'application/trig' Turtle 'text/turtle' XHTML 'application/xhtml+xml' XML 'application/xml' The following example makes RDF source data available to the SHACL validate() function: import { Source } from '@triplyetl/etl/generic' import { validate } from '@triplyetl/etl/shacl' validate(Source.string(` prefix sh: <http://www.w3.org/ns/shacl#> prefix shp: <https://example.com/model/shp/> prefix sdo: <https://schema.org/> shp:Person a sh:NodeShape; sh:property shp:Person_name; sh:targetClass sdo:Person. shp:Person_name a sh:PropertyShape; sh:datatype xsd:string; sh:minLength 1; sh:path sdo:name.`)) This makes the following linked data SHACL specification available: graph LR shp:Person -- a --> sh:NodeShape shp:Person -- sh:property --> shp:Person_name shp:Person -- sh:targetClass --> sdo:Person shp:Person_name -- a --> sh:PropertyShape shp:Person_name -- sh:datatype --> xsd:string shp:Person_name -- sh:minLength --> 1 shp:Person_name -- sh:path --> sdo:name Notice that validate() does not require us to set the content-type, since it only supports N-Quads, N-Triples, TriG and Turtle (and these formats can be detected automatically). The following example makes a string source available to the fromJson() source extractor: fromJson(Source.string(` [ { id: '123', name: 'John' }, { id: '456', name: 'Jane' } ]`)), Notice that the inline JSON source is often a more intuitive specification format for the fromJson() source extractor than its corresponding string source. While inline JSON and string sources are mostly used for small examples, local files are somewhat more widely used.","title":"Inline Strings"},{"location":"triply-etl/sources/inline-strings/#inline-strings","text":"Data in the JSON or RDF formats can be specified with inline strings. The following code snippet loads triples into the Internal Store: loadRdf( Source.string(` prefix person: <https://example.com/id/person/> prefix sdo: <https://schema.org/> person:1 a sdo:Person; sdo:name 'J. Doe'.`), { contentType: 'text/turtle' } ), This loads the following triples: graph LR person:1 -- a --> sdo:Person person:1 -- sdo:name --> J.Doe Notice that we must specify the RDF serialization format that we use. This is necessary because loadRdf() supports a large number of formats, some of which are difficult to autodetect. The following formats are supported: Format contentType value HTML 'text/html' JSON-LD 'application/ld+json' JSON 'application/json' N-Quads 'application/n-quads' N-Triples 'application/n-triples' N3 'text/n3' RDF/XML 'application/rdf+xml' SVG 'image/svg+xml' TriG 'application/trig' Turtle 'text/turtle' XHTML 'application/xhtml+xml' XML 'application/xml' The following example makes RDF source data available to the SHACL validate() function: import { Source } from '@triplyetl/etl/generic' import { validate } from '@triplyetl/etl/shacl' validate(Source.string(` prefix sh: <http://www.w3.org/ns/shacl#> prefix shp: <https://example.com/model/shp/> prefix sdo: <https://schema.org/> shp:Person a sh:NodeShape; sh:property shp:Person_name; sh:targetClass sdo:Person. shp:Person_name a sh:PropertyShape; sh:datatype xsd:string; sh:minLength 1; sh:path sdo:name.`)) This makes the following linked data SHACL specification available: graph LR shp:Person -- a --> sh:NodeShape shp:Person -- sh:property --> shp:Person_name shp:Person -- sh:targetClass --> sdo:Person shp:Person_name -- a --> sh:PropertyShape shp:Person_name -- sh:datatype --> xsd:string shp:Person_name -- sh:minLength --> 1 shp:Person_name -- sh:path --> sdo:name Notice that validate() does not require us to set the content-type, since it only supports N-Quads, N-Triples, TriG and Turtle (and these formats can be detected automatically). The following example makes a string source available to the fromJson() source extractor: fromJson(Source.string(` [ { id: '123', name: 'John' }, { id: '456', name: 'Jane' } ]`)), Notice that the inline JSON source is often a more intuitive specification format for the fromJson() source extractor than its corresponding string source. While inline JSON and string sources are mostly used for small examples, local files are somewhat more widely used.","title":"Inline strings"},{"location":"triply-etl/sources/local-files/","text":"On this page: Local files Basic usage Multiple local files Production systems Local files \u00b6 Local files are files that are on the same computer that the TriplyETL pipeline runs on. TriplyETL supports local files with the Source.file() function. Basic usage \u00b6 Local files are declared by using the Source object, which is imported in the following way: import { Source } from '@triplyetl/etl/generic' The following code snippet uses a local JSON file. The local file is used by the fromJson() extractor : fromJson(Source.file('./static/example.json')), Multiple local files \u00b6 It is possible to specify one or more local files, by using array notation. The following code snippet extracts records from a large number of local JSON files: fromJson(Source.file([ './static/data-001.json', './static/data-002.json', ..., './static/data-999.json', ])), Production systems \u00b6 Local files are not typically used in production systems. The reason for this is that it is difficult to guarantee that all project partners have exactly the same local files on their respective computers. The risk of using outdated files, and the overhead of securely sharing files with multiple team members, are often sufficient reason to use TriplyDB Assets instead.","title":"Local Files"},{"location":"triply-etl/sources/local-files/#local-files","text":"Local files are files that are on the same computer that the TriplyETL pipeline runs on. TriplyETL supports local files with the Source.file() function.","title":"Local files"},{"location":"triply-etl/sources/local-files/#basic-usage","text":"Local files are declared by using the Source object, which is imported in the following way: import { Source } from '@triplyetl/etl/generic' The following code snippet uses a local JSON file. The local file is used by the fromJson() extractor : fromJson(Source.file('./static/example.json')),","title":"Basic usage"},{"location":"triply-etl/sources/local-files/#multiple-local-files","text":"It is possible to specify one or more local files, by using array notation. The following code snippet extracts records from a large number of local JSON files: fromJson(Source.file([ './static/data-001.json', './static/data-002.json', ..., './static/data-999.json', ])),","title":"Multiple local files"},{"location":"triply-etl/sources/local-files/#production-systems","text":"Local files are not typically used in production systems. The reason for this is that it is difficult to guarantee that all project partners have exactly the same local files on their respective computers. The risk of using outdated files, and the overhead of securely sharing files with multiple team members, are often sufficient reason to use TriplyDB Assets instead.","title":"Production systems"},{"location":"triply-etl/sources/online-files/","text":"On this page: Online files Basic usage Authorization Other HTTP options Use in production systems Online files \u00b6 Online files are files that are publishing on some (public or private) server. TriplyETL supports online files with the Source.url() function. Basic usage \u00b6 Online files are declared by using the Source object, which is imported in the following way: import { Source } from '@triplyetl/etl/generic' The following code snippet uses a public online file. The files is used by the fromJson() extractor: fromJson(Source.url('https://somewhere.com/example.json')), Authorization \u00b6 It is possible to access online files that are not publicly available. In such cases, the HTTP Authorization header must be specified. The following code snippet uses the options object of Source.url() to specify the authorization header that is necessary to access the specified online file: fromJson( Source.url( 'https://somewhere.com/example.json', { request: { headers: { Authorization: `Basic ${username}:${password}` } } } ) ), Other HTTP options \u00b6 TriplyETL uses the node-fetch library to implement Source.url() . This means that all options supported by that library are also supported for online files. For example, the following code snippet specifies the media type that is requested from an online location. Specifically, it requests the Turtle representation of the Amsterdam resource from DBpedia: loadRdf( Source.url( 'https://dbpedia.org/Amsterdam', { request: { headers: { Accept: 'text/turtle' } } } ) ), Use in production systems \u00b6 Online files are typically not used in production pipelines, because the availability of many Internet resources is outside of the control of the project team. Internet resources that are not maintained by team members may be subject to content-wise changes, which may affect the production pipeline. If the project team controls the Internet resources, then risks are smaller. But at that point it is even better to upload the online files as TriplyDB asset for additional benefits such as access controls.","title":"Online Files"},{"location":"triply-etl/sources/online-files/#online-files","text":"Online files are files that are publishing on some (public or private) server. TriplyETL supports online files with the Source.url() function.","title":"Online files"},{"location":"triply-etl/sources/online-files/#basic-usage","text":"Online files are declared by using the Source object, which is imported in the following way: import { Source } from '@triplyetl/etl/generic' The following code snippet uses a public online file. The files is used by the fromJson() extractor: fromJson(Source.url('https://somewhere.com/example.json')),","title":"Basic usage"},{"location":"triply-etl/sources/online-files/#authorization","text":"It is possible to access online files that are not publicly available. In such cases, the HTTP Authorization header must be specified. The following code snippet uses the options object of Source.url() to specify the authorization header that is necessary to access the specified online file: fromJson( Source.url( 'https://somewhere.com/example.json', { request: { headers: { Authorization: `Basic ${username}:${password}` } } } ) ),","title":"Authorization"},{"location":"triply-etl/sources/online-files/#other-http-options","text":"TriplyETL uses the node-fetch library to implement Source.url() . This means that all options supported by that library are also supported for online files. For example, the following code snippet specifies the media type that is requested from an online location. Specifically, it requests the Turtle representation of the Amsterdam resource from DBpedia: loadRdf( Source.url( 'https://dbpedia.org/Amsterdam', { request: { headers: { Accept: 'text/turtle' } } } ) ),","title":"Other HTTP options"},{"location":"triply-etl/sources/online-files/#use-in-production-systems","text":"Online files are typically not used in production pipelines, because the availability of many Internet resources is outside of the control of the project team. Internet resources that are not maintained by team members may be subject to content-wise changes, which may affect the production pipeline. If the project team controls the Internet resources, then risks are smaller. But at that point it is even better to upload the online files as TriplyDB asset for additional benefits such as access controls.","title":"Use in production systems"},{"location":"triply-etl/sources/rml/","text":"On this page: RML Sources RML Sources \u00b6","title":"RML Sources"},{"location":"triply-etl/sources/rml/#rml-sources","text":"","title":"RML Sources"},{"location":"triply-etl/sources/triplydb-assets/","text":"On this page: TriplyDB Assets Filtering Versioning Access TriplyDB instance Compression TriplyDB Assets \u00b6 Assets are a core feature of TriplyDB. Assets allow arbitrary files to be stored in the context of a linked dataset. A typical use case for assets is to upload (new versions of) source files. The TriplyETL pipeline can pick the latest versions of these source files and publish the resulting linked data in the the same dataset. The following code snippet uses a JSON source that is stored in a TriplyDB asset: fromJson( Source.TriplyDb.asset( 'some-account', 'some-dataset', { name: 'example.json' } ) ), As with other TriplyDB sources, the account name is optional. When omitted, the user account that is associated with the current API Token is used: loadRdf( Source.TriplyDb.rdf('my-dataset', { name: 'example.json' }) ), As with other source type, multiple assets can be specified: fromCsv([ Source.TriplyDb.asset('my-dataset', { name: 'table1.csv' }), Source.TriplyDb.asset('my-dataset', { name: 'table2.csv' }), ]), Filtering \u00b6 If the asset name is omitted, all assets are returned. This is often unpractical, since only some assets must be processed. For example, if a dataset has PDF and JSON assets, only the latter should be processed by the fromJson() source extractor. For such use cases the filter option can be used instead of the name option. The filter option takes a TypeScript function that maps assets names onto Boolean values (true or false). Only the assets for which the function returns truth are included. The following snippet processes all and only assets whose name ends in .json : fromJson( Source.TriplyDb.asset( 'my-dataset', { filter: name => name.endsWith('json') } ) ), Versioning \u00b6 It is possible to upload new versions of an existing TriplyDB asset. When no specific version is specified, a TriplyETL pipeline will use the latest version automatically. In order to use a specific version, the version option can be set to a version number. The following snippet uses a specific version of an asset: fromJson( Source.TriplyDb.asset( 'some-account', 'some-dataset', { name: 'example.json', assetVersion: 2 } ) ), Access \u00b6 Since TriplyDB assets are part of a TriplyDB dataset: - they are accessible under the same access level as the rest of the dataset, and - they are accessible with the same API Token that allows linked data to be published in that dataset. Notice that this makes it easier and safer to deal with source data that is not public. When private data is retrieved from online files or APIs , authorization information must be configured at the HTTP level. This is possible but cumbersome. And, depending on the authentication approach, it is required to create a new API Token and securely configure that in addition to the TriplyDB API Token. Notice that access also is more transparent when TriplyDB assets are used. All and only collaborators that have access to the TriplyDB dataset also have access to the source data. It is clear for all collaborators which source files should be used, and which versions are available. This is more transparent than having to share (multiple versions of) source files over email or by other indirect means. TriplyDB instance \u00b6 By default, assets are loaded from the TriplyDB instance that is associated with the currently used API Token. In some situations it is useful to connect to a linked dataset from a different TriplyDB instance. This can be configured with the triplyDb option. The following snippet loads the OWL vocabulary from TriplyDB.com. Notice that the URL of the API must be specified; this is different from the URL of the web-based GUI. loadRdf( Source.TriplyDb.rdf( 'w3c', 'owl', { triplyDb: { url: 'https://triplydb.com' } } ) ), If an asset is part of a non-public dataset, specifying the URL is insufficient. In such cases an API Token from this other TriplyDB instance must be created and configured using the token option in combination with the url option. Compression \u00b6 Source data is often text-based. This means that such source data can often be compressed to minimize storage space and/or Internet bandwidth. TriplyETL provides automatic support for the GNU zip (file name extension *.gz ) compression format. The following snippet uses a TriplyDB assets that was compressed with GNU zip (file extension *.gz ): fromCsv(Source.TriplyDb.asset('my-dataset', { name: 'example.csv.gz' })),","title":"TriplyDB Assets"},{"location":"triply-etl/sources/triplydb-assets/#triplydb-assets","text":"Assets are a core feature of TriplyDB. Assets allow arbitrary files to be stored in the context of a linked dataset. A typical use case for assets is to upload (new versions of) source files. The TriplyETL pipeline can pick the latest versions of these source files and publish the resulting linked data in the the same dataset. The following code snippet uses a JSON source that is stored in a TriplyDB asset: fromJson( Source.TriplyDb.asset( 'some-account', 'some-dataset', { name: 'example.json' } ) ), As with other TriplyDB sources, the account name is optional. When omitted, the user account that is associated with the current API Token is used: loadRdf( Source.TriplyDb.rdf('my-dataset', { name: 'example.json' }) ), As with other source type, multiple assets can be specified: fromCsv([ Source.TriplyDb.asset('my-dataset', { name: 'table1.csv' }), Source.TriplyDb.asset('my-dataset', { name: 'table2.csv' }), ]),","title":"TriplyDB Assets"},{"location":"triply-etl/sources/triplydb-assets/#filtering","text":"If the asset name is omitted, all assets are returned. This is often unpractical, since only some assets must be processed. For example, if a dataset has PDF and JSON assets, only the latter should be processed by the fromJson() source extractor. For such use cases the filter option can be used instead of the name option. The filter option takes a TypeScript function that maps assets names onto Boolean values (true or false). Only the assets for which the function returns truth are included. The following snippet processes all and only assets whose name ends in .json : fromJson( Source.TriplyDb.asset( 'my-dataset', { filter: name => name.endsWith('json') } ) ),","title":"Filtering"},{"location":"triply-etl/sources/triplydb-assets/#versioning","text":"It is possible to upload new versions of an existing TriplyDB asset. When no specific version is specified, a TriplyETL pipeline will use the latest version automatically. In order to use a specific version, the version option can be set to a version number. The following snippet uses a specific version of an asset: fromJson( Source.TriplyDb.asset( 'some-account', 'some-dataset', { name: 'example.json', assetVersion: 2 } ) ),","title":"Versioning"},{"location":"triply-etl/sources/triplydb-assets/#access","text":"Since TriplyDB assets are part of a TriplyDB dataset: - they are accessible under the same access level as the rest of the dataset, and - they are accessible with the same API Token that allows linked data to be published in that dataset. Notice that this makes it easier and safer to deal with source data that is not public. When private data is retrieved from online files or APIs , authorization information must be configured at the HTTP level. This is possible but cumbersome. And, depending on the authentication approach, it is required to create a new API Token and securely configure that in addition to the TriplyDB API Token. Notice that access also is more transparent when TriplyDB assets are used. All and only collaborators that have access to the TriplyDB dataset also have access to the source data. It is clear for all collaborators which source files should be used, and which versions are available. This is more transparent than having to share (multiple versions of) source files over email or by other indirect means.","title":"Access"},{"location":"triply-etl/sources/triplydb-assets/#triplydb-instance","text":"By default, assets are loaded from the TriplyDB instance that is associated with the currently used API Token. In some situations it is useful to connect to a linked dataset from a different TriplyDB instance. This can be configured with the triplyDb option. The following snippet loads the OWL vocabulary from TriplyDB.com. Notice that the URL of the API must be specified; this is different from the URL of the web-based GUI. loadRdf( Source.TriplyDb.rdf( 'w3c', 'owl', { triplyDb: { url: 'https://triplydb.com' } } ) ), If an asset is part of a non-public dataset, specifying the URL is insufficient. In such cases an API Token from this other TriplyDB instance must be created and configured using the token option in combination with the url option.","title":"TriplyDB instance"},{"location":"triply-etl/sources/triplydb-assets/#compression","text":"Source data is often text-based. This means that such source data can often be compressed to minimize storage space and/or Internet bandwidth. TriplyETL provides automatic support for the GNU zip (file name extension *.gz ) compression format. The following snippet uses a TriplyDB assets that was compressed with GNU zip (file extension *.gz ): fromCsv(Source.TriplyDb.asset('my-dataset', { name: 'example.csv.gz' })),","title":"Compression"},{"location":"triply-etl/sources/triplydb-datasets/","text":"TriplyDB datasets \u00b6 Datasets in TriplyDB store linked data in one or more graphs. Such datasets can be loaded as a TriplyETL source. The following snippet loads a dataset from TriplyDB into the internal RDF store of TriplyETL: loadRdf(Source.TriplyDb.rdf('my-account', 'my-dataset')), As with other TriplyDB sources, the account name is optional. When omitted, a dataset from the user account that is associated with the current API Token is used: loadRdf(Source.TriplyDb.rdf('my-dataset')), Graphs option \u00b6 By default, all graphs from a linked dataset are loaded. It is possible to specify a only those graphs that should be loaded. The following snippet only loads the data model, but not the instance data: loadRdf( Source.TriplyDb.rdf( 'my-account', 'my-dataset', { graphs: ['https://example.com/id/graph/model'] } ) ), TriplyDB instance \u00b6 The triplyDb option can be used to specify that a linked dataset from a different TriplyDB instance should be used. This option works in the same way as for TriplyDB assets: link","title":"TriplyDB Datasets"},{"location":"triply-etl/sources/triplydb-datasets/#triplydb-datasets","text":"Datasets in TriplyDB store linked data in one or more graphs. Such datasets can be loaded as a TriplyETL source. The following snippet loads a dataset from TriplyDB into the internal RDF store of TriplyETL: loadRdf(Source.TriplyDb.rdf('my-account', 'my-dataset')), As with other TriplyDB sources, the account name is optional. When omitted, a dataset from the user account that is associated with the current API Token is used: loadRdf(Source.TriplyDb.rdf('my-dataset')),","title":"TriplyDB datasets"},{"location":"triply-etl/sources/triplydb-datasets/#graphs-option","text":"By default, all graphs from a linked dataset are loaded. It is possible to specify a only those graphs that should be loaded. The following snippet only loads the data model, but not the instance data: loadRdf( Source.TriplyDb.rdf( 'my-account', 'my-dataset', { graphs: ['https://example.com/id/graph/model'] } ) ),","title":"Graphs option"},{"location":"triply-etl/sources/triplydb-datasets/#triplydb-instance","text":"The triplyDb option can be used to specify that a linked dataset from a different TriplyDB instance should be used. This option works in the same way as for TriplyDB assets: link","title":"TriplyDB instance"},{"location":"triply-etl/sources/triplydb-queries/","text":"On this page: TriplyDB Queries SPARQL Ask queries SPARQL Construct and Describe queries SPARQL Select queries Versioning API variables Pagination Result graph TriplyDB instance TriplyDB Queries \u00b6 Saved SPARQL queries in TriplyDB can be used as data sources. SPARQL queries are very powerful data sources, since they allow complex filters to be expressed. There are 4 SPARQL query forms, with different source extractors that can process their results: Query form Source extractor SPARQL Ask fromJson() , fromXml() SPARQL Construct loadRdf() SPARQL Describe loadRdf() SPARQL Select fromCsv() , fromJson() , fromTsv() , fromXml() SPARQL Ask queries \u00b6 SPARQL Ask queries can return data in either the JSON or the XML format. This allows them to be processed with the extractors fromCsv() and fromXml() . The following code snippet connects to the XML results of a SPARQL Ask query in TriplyDB: fromXml(Source.TriplyDb.query('my-account', 'my-ask-query')), SPARQL Construct and Describe queries \u00b6 SPARQL Construct and Describe queries return data in the RDF format. This allows them to be used with function loadRdf() . The following snippet loads the results of a SPARQL query into the internal RDF store of TriplyETL: loadRdf(Source.TriplyDb.query('my-account', 'my-construct-query')), SPARQL Select queries \u00b6 SPARQL Select queries return data in either the CSV, JSON, TSV, or XML format. This allows them to be used with the following four extractors: fromCsv() , fromJson() , fromTsv() , and fromXml() . The following code snippet connects to the table returned by a SPARQL Select query in TriplyDB: fromCsv(Source.TriplyDb.query('my-account', 'my-select-query')), As with other TriplyDB sources, the account name is optional. When omitted, the user account that is associated with the current API Token is used: loadRdf(Source.TriplyDb.query('my-construct-query')), Versioning \u00b6 In production systems, applications must be able to choose whether they want to use the latest version of a query (acceptance mode), or whether they want to use a specific recent version (production mode), or whether they want to use a specific older version (legacy mode). Versioning is supported by TriplyDB saved queries. When no specific version is specified, a TriplyETL pipeline will use the latest version of a query automatically. In order to use a specific version, the version option can be set to a version number. The following snippet uses a specific version of a query: fromJson(Source.TriplyDb.query('my-query', { version: 2 })), Not specifying the version option automatically uses the latest version. API variables \u00b6 In production systems, applications often need to request distinct information based on a limited set of input variables. This is supported in TriplyDB saved queries which API variables. API variables ensure that the query string is parameterized correctly, while adhering to the RDF and SPARQL standards. The following example binds the ?country variable inside the query string to literal 'Holland' . This allows the results for Holland to be returned: fromCsv( Source.TriplyDb.query( 'information-about-countries', { variables: { country: 'Holland' } } ) ), Pagination \u00b6 When a bare SPARQL endpoint is queried as an online API , there are sometimes issues with retrieving the full result set for larger queries. With TriplyDB saved queries, the process of obtaining all results is abstracted away from the user, with the TriplyETL source performing multiple requests in the background as needed. Result graph \u00b6 It is often useful to store the results of SPARQL Construct and Describe queries in a specific graph. For example, when internal data is enriched with external sources, it is often useful to store the external enrichments in a separate graph. Another example is the use of a query that applies RDF(S) and/or OWL reasoning. In such cases the results of the reasoner may be stored in a specific graph. The following snippet stores the results of the specified construct query in a special enrichment graph: loadRdf( Source.TriplyDb.query('my-query', { toGraph: graph.enrichment }) ) This snippet assumes that the graph names have been declared (see Delcarations ). TriplyDB instance \u00b6 The triplyDb option can be used to specify that a query from a different TriplyDB instance should be used. This option works in the same way as for TriplyDB assets: link","title":"TriplyDB Queries"},{"location":"triply-etl/sources/triplydb-queries/#triplydb-queries","text":"Saved SPARQL queries in TriplyDB can be used as data sources. SPARQL queries are very powerful data sources, since they allow complex filters to be expressed. There are 4 SPARQL query forms, with different source extractors that can process their results: Query form Source extractor SPARQL Ask fromJson() , fromXml() SPARQL Construct loadRdf() SPARQL Describe loadRdf() SPARQL Select fromCsv() , fromJson() , fromTsv() , fromXml()","title":"TriplyDB Queries"},{"location":"triply-etl/sources/triplydb-queries/#sparql-ask-queries","text":"SPARQL Ask queries can return data in either the JSON or the XML format. This allows them to be processed with the extractors fromCsv() and fromXml() . The following code snippet connects to the XML results of a SPARQL Ask query in TriplyDB: fromXml(Source.TriplyDb.query('my-account', 'my-ask-query')),","title":"SPARQL Ask queries"},{"location":"triply-etl/sources/triplydb-queries/#sparql-construct-and-describe-queries","text":"SPARQL Construct and Describe queries return data in the RDF format. This allows them to be used with function loadRdf() . The following snippet loads the results of a SPARQL query into the internal RDF store of TriplyETL: loadRdf(Source.TriplyDb.query('my-account', 'my-construct-query')),","title":"SPARQL Construct and Describe queries"},{"location":"triply-etl/sources/triplydb-queries/#sparql-select-queries","text":"SPARQL Select queries return data in either the CSV, JSON, TSV, or XML format. This allows them to be used with the following four extractors: fromCsv() , fromJson() , fromTsv() , and fromXml() . The following code snippet connects to the table returned by a SPARQL Select query in TriplyDB: fromCsv(Source.TriplyDb.query('my-account', 'my-select-query')), As with other TriplyDB sources, the account name is optional. When omitted, the user account that is associated with the current API Token is used: loadRdf(Source.TriplyDb.query('my-construct-query')),","title":"SPARQL Select queries"},{"location":"triply-etl/sources/triplydb-queries/#versioning","text":"In production systems, applications must be able to choose whether they want to use the latest version of a query (acceptance mode), or whether they want to use a specific recent version (production mode), or whether they want to use a specific older version (legacy mode). Versioning is supported by TriplyDB saved queries. When no specific version is specified, a TriplyETL pipeline will use the latest version of a query automatically. In order to use a specific version, the version option can be set to a version number. The following snippet uses a specific version of a query: fromJson(Source.TriplyDb.query('my-query', { version: 2 })), Not specifying the version option automatically uses the latest version.","title":"Versioning"},{"location":"triply-etl/sources/triplydb-queries/#api-variables","text":"In production systems, applications often need to request distinct information based on a limited set of input variables. This is supported in TriplyDB saved queries which API variables. API variables ensure that the query string is parameterized correctly, while adhering to the RDF and SPARQL standards. The following example binds the ?country variable inside the query string to literal 'Holland' . This allows the results for Holland to be returned: fromCsv( Source.TriplyDb.query( 'information-about-countries', { variables: { country: 'Holland' } } ) ),","title":"API variables"},{"location":"triply-etl/sources/triplydb-queries/#pagination","text":"When a bare SPARQL endpoint is queried as an online API , there are sometimes issues with retrieving the full result set for larger queries. With TriplyDB saved queries, the process of obtaining all results is abstracted away from the user, with the TriplyETL source performing multiple requests in the background as needed.","title":"Pagination"},{"location":"triply-etl/sources/triplydb-queries/#result-graph","text":"It is often useful to store the results of SPARQL Construct and Describe queries in a specific graph. For example, when internal data is enriched with external sources, it is often useful to store the external enrichments in a separate graph. Another example is the use of a query that applies RDF(S) and/or OWL reasoning. In such cases the results of the reasoner may be stored in a specific graph. The following snippet stores the results of the specified construct query in a special enrichment graph: loadRdf( Source.TriplyDb.query('my-query', { toGraph: graph.enrichment }) ) This snippet assumes that the graph names have been declared (see Delcarations ).","title":"Result graph"},{"location":"triply-etl/sources/triplydb-queries/#triplydb-instance","text":"The triplyDb option can be used to specify that a query from a different TriplyDB instance should be used. This option works in the same way as for TriplyDB assets: link","title":"TriplyDB instance"},{"location":"triply-etl/tmp/automation/","text":"On this page: Special key: $environment TriplyETL runs within a Gitlab CI environment ( Figure 1 ). Figure 1: The landing page of a TriplyETL project in Gitlab. Special key: $environment \u00b6 The special key $environment denotes the DTAP environment in which the TriplyETL pipeline is running. This allows special actions to be performed based on whether the pipeline runs in \"Debug\" , \"Test\" , \"Acceptance\" , or \"Production\" mode. See the DTAP documentation for more information.","title":"Automation"},{"location":"triply-etl/tmp/automation/#special-key-environment","text":"The special key $environment denotes the DTAP environment in which the TriplyETL pipeline is running. This allows special actions to be performed based on whether the pipeline runs in \"Debug\" , \"Test\" , \"Acceptance\" , or \"Production\" mode. See the DTAP documentation for more information.","title":"Special key: $environment"},{"location":"triply-etl/tmp/ci-cd/","text":"On this page: How to create a TriplyETL CI pipeline? Modifying a pipeline artifacts: variables: script: only: This document explains how to maintain an ETL that runs in the gitlab CI. How to create a TriplyETL CI pipeline? \u00b6 Use the TriplyETL boilerplate from this repository , specifically the etl folder. Each customer organization in gitlab needs their own 'CI runner'. If you're adding a TriplyETL repo to an existing customer, then you're probably fine. But if this is a newly created organization, you will need to register a runner for this organization. To check whether the customer organization has a runner configured, go to the customer organization Settings in gitlab, then go to CI/CD . The URL should look something like this: https://git.triply.cc/groups/customers/<customer-name>/-/settings/ci_cd . Click Runners and verify that Available runners: is not zero. If you need to register a runner, contact a sysadmin (see here for the list of sysadmins) and ask them to create a runner for your organization. Modifying a pipeline \u00b6 To change what happens in a CI pipeline, all you need to do is modify the .gitlab-ci.yml file of your repository. Below we detail some relevant .gitlab-ci.yml fields that are used in most of our ETL pipelines (see here for the complete gitlab documentation on what all these fields mean) artifacts: \u00b6 Artifacts are files or directories that gitlab will save for you. These files are available on the gitlab pipelines page after a job ran. This is particularly useful for TriplyETL error/log files, as you download these from the gitlab UI. variables: \u00b6 You can define environment variables in several places. In the .gitlab.yml file, you can configure them at a job level or for all jobs. You can also configure them in the gitlab UI in the pipeline schedule form. Variables defined in a pipeline schedule will overwrite variables defined in the .gitlab-ci.yml file (see here for the gitlab documentation on variable precedence). script: \u00b6 This is the code that will run in the job. If you need a job to run two TriplyETL commands after each other, you can easily add another npx etl .... line here. only: \u00b6 This defines when a job should run. If a job does not have an only: configure, it will always run. See here for documentation about the syntax of only ). The boilerplate comes with some example only: rules that look like this: only: variables: - $JOB == \"$CI_JOB_NAME\" This means that we only run this job if there a JOB environment variable that is the same as CI_JOB_NAME . Notice that the CI_JOB_NAME is a default environment variable that gitlab gives us and that equals the name of the job, e.g. production (see other predefined variables here ). If you want to run this specific job, JOB is the environment variable that you should set in the pipeline schedule page. In other words, if you set JOB=production in the pipeline schedule page, then using the above only: rule, only the intended job will run.","title":"Ci cd"},{"location":"triply-etl/tmp/ci-cd/#how-to-create-a-triplyetl-ci-pipeline","text":"Use the TriplyETL boilerplate from this repository , specifically the etl folder. Each customer organization in gitlab needs their own 'CI runner'. If you're adding a TriplyETL repo to an existing customer, then you're probably fine. But if this is a newly created organization, you will need to register a runner for this organization. To check whether the customer organization has a runner configured, go to the customer organization Settings in gitlab, then go to CI/CD . The URL should look something like this: https://git.triply.cc/groups/customers/<customer-name>/-/settings/ci_cd . Click Runners and verify that Available runners: is not zero. If you need to register a runner, contact a sysadmin (see here for the list of sysadmins) and ask them to create a runner for your organization.","title":"How to create a TriplyETL CI pipeline?"},{"location":"triply-etl/tmp/ci-cd/#modifying-a-pipeline","text":"To change what happens in a CI pipeline, all you need to do is modify the .gitlab-ci.yml file of your repository. Below we detail some relevant .gitlab-ci.yml fields that are used in most of our ETL pipelines (see here for the complete gitlab documentation on what all these fields mean)","title":"Modifying a pipeline"},{"location":"triply-etl/tmp/ci-cd/#artifacts","text":"Artifacts are files or directories that gitlab will save for you. These files are available on the gitlab pipelines page after a job ran. This is particularly useful for TriplyETL error/log files, as you download these from the gitlab UI.","title":"artifacts:"},{"location":"triply-etl/tmp/ci-cd/#variables","text":"You can define environment variables in several places. In the .gitlab.yml file, you can configure them at a job level or for all jobs. You can also configure them in the gitlab UI in the pipeline schedule form. Variables defined in a pipeline schedule will overwrite variables defined in the .gitlab-ci.yml file (see here for the gitlab documentation on variable precedence).","title":"variables:"},{"location":"triply-etl/tmp/ci-cd/#script","text":"This is the code that will run in the job. If you need a job to run two TriplyETL commands after each other, you can easily add another npx etl .... line here.","title":"script:"},{"location":"triply-etl/tmp/ci-cd/#only","text":"This defines when a job should run. If a job does not have an only: configure, it will always run. See here for documentation about the syntax of only ). The boilerplate comes with some example only: rules that look like this: only: variables: - $JOB == \"$CI_JOB_NAME\" This means that we only run this job if there a JOB environment variable that is the same as CI_JOB_NAME . Notice that the CI_JOB_NAME is a default environment variable that gitlab gives us and that equals the name of the job, e.g. production (see other predefined variables here ). If you want to run this specific job, JOB is the environment variable that you should set in the pipeline schedule page. In other words, if you set JOB=production in the pipeline schedule page, then using the above only: rule, only the intended job will run.","title":"only:"},{"location":"triply-etl/tmp/context/","text":"On this page: Context Configuring the Context Configuring the standard graph Configuring the well-known IRI prefix Context \u00b6 Configuring the Context \u00b6 The TriplyETL Context is specified when the Etl object is instantiated. This often appears towards the start of a pipeline script. The TriplyETL Context allows the following things to be specified: The data sources that can be used in the ETL. The data destinations where linked data is published to. The named graph in which triple calls with no graph argument add their data. The prefix IRI for blank node-replacing well-known IRIs. Configuring the standard graph \u00b6 When we call triple with 3 arguments, a triple is created and placed in a named graph that is chosen by TriplyETL. You can change the name of this default graph by specifying it in the TriplyETL context. Notice that graph names must be IRIs: const etl = new Etl() Configuring the well-known IRI prefix \u00b6 TriplyDB performs Skolemization, an approach in which blank nodes are systematically replaced by well-known IRIs. TriplyDB chooses a well-known IRI prefix for you, const etl = new Etl({ wellKnownIriPrefix: 'https://triplydb.com/Triply/example/.well-known/genid/', })","title":"Context"},{"location":"triply-etl/tmp/context/#context","text":"","title":"Context"},{"location":"triply-etl/tmp/context/#configuring-the-context","text":"The TriplyETL Context is specified when the Etl object is instantiated. This often appears towards the start of a pipeline script. The TriplyETL Context allows the following things to be specified: The data sources that can be used in the ETL. The data destinations where linked data is published to. The named graph in which triple calls with no graph argument add their data. The prefix IRI for blank node-replacing well-known IRIs.","title":"Configuring the Context"},{"location":"triply-etl/tmp/context/#configuring-the-standard-graph","text":"When we call triple with 3 arguments, a triple is created and placed in a named graph that is chosen by TriplyETL. You can change the name of this default graph by specifying it in the TriplyETL context. Notice that graph names must be IRIs: const etl = new Etl()","title":"Configuring the standard graph"},{"location":"triply-etl/tmp/context/#configuring-the-well-known-iri-prefix","text":"TriplyDB performs Skolemization, an approach in which blank nodes are systematically replaced by well-known IRIs. TriplyDB chooses a well-known IRI prefix for you, const etl = new Etl({ wellKnownIriPrefix: 'https://triplydb.com/Triply/example/.well-known/genid/', })","title":"Configuring the well-known IRI prefix"},{"location":"triply-etl/tmp/copy/","text":"On this page: Copy an existing entry over to a new entry Function signature Copy an existing entry over to a new entry \u00b6 Copying is the act of creating a new thing that is based on a specific existing thing. Function signature \u00b6 The copy function has the following signature: etl.use( copy({ fromKey: 'FROM_KEY', toKey: 'TO_KEY', type: 'VALUE_TYPE', change: value => FUNCTION_BODY}), ) This function copies the value from \u2018foo\u2019 to \u2018bar\u2019. The type key ensures that the value in \u2018foo\u2019 is cast to the specified type prior to being copied. The optional change key allows the cast value to be transformed prior to storing it in \u2018bar\u2019. Leaving the change key out results in a direct copy in which the value is not modified. This function emits an error if fromKey and toKey are the same. If you want to change a value in-place you should use change instead. This function emits an error if toKey already exists. If you want to replace the value in an existing entry then you should use replace instead. The change function only takes the value argument and does not take the context argument. If you need the context argument then they must use add instead.","title":"Copy"},{"location":"triply-etl/tmp/copy/#copy-an-existing-entry-over-to-a-new-entry","text":"Copying is the act of creating a new thing that is based on a specific existing thing.","title":"Copy an existing entry over to a new entry"},{"location":"triply-etl/tmp/copy/#function-signature","text":"The copy function has the following signature: etl.use( copy({ fromKey: 'FROM_KEY', toKey: 'TO_KEY', type: 'VALUE_TYPE', change: value => FUNCTION_BODY}), ) This function copies the value from \u2018foo\u2019 to \u2018bar\u2019. The type key ensures that the value in \u2018foo\u2019 is cast to the specified type prior to being copied. The optional change key allows the cast value to be transformed prior to storing it in \u2018bar\u2019. Leaving the change key out results in a direct copy in which the value is not modified. This function emits an error if fromKey and toKey are the same. If you want to change a value in-place you should use change instead. This function emits an error if toKey already exists. If you want to replace the value in an existing entry then you should use replace instead. The change function only takes the value argument and does not take the context argument. If you need the context argument then they must use add instead.","title":"Function signature"},{"location":"triply-etl/tmp/faq/","text":"On this page: FAQ Why does my pipeline schedule only run an install job? I made a change to the .gitlab-ci.yml file and after I push I see a pipeline failed with status yaml invalid. How can I fix this? Why is my pipeline not running and marked as 'pending'? What do all these $CI_... environment variables mean? What should I do when the pipeline fail when I commit in a personal project? FAQ \u00b6 Why does my pipeline schedule only run an install job? \u00b6 This probably means that none of the only: rules in your .gitlab-ci.yml file match. You should check whether the variables you've set in the pipelines schedules page match with the only: rules in your .gitlab-ci.yml file. I made a change to the .gitlab-ci.yml file and after I push I see a pipeline failed with status yaml invalid . How can I fix this? \u00b6 This error is emitted when the content of your .gitlab-ci.yml file does not follow the Yaml syntax. In GitLab, search for a page called CI Lint. On that page, copy-paste the contents of your .gitlab-ci.yml file into the text field and click validate. GitLab will indicate to you the location in your file where the Yaml syntax is violated, so that you can fix this. Why is my pipeline not running and marked as 'pending'? \u00b6 This probably means that you have not configured an ETL runner for this customer organization yet. See the section about getting started here What do all these $CI_... environment variables mean? \u00b6 These are environment variables added by gitlab. To see what they mean, go to this gitlab documentation page, also mentioned above. What should I do when the pipeline fail when I commit in a personal project? \u00b6 In a personal repository, you have available runners, but shared ones. Thus, your pipelines will fail. This is expected and it is not an issue. You can either ignore the failed pipeline or remove gitlab-ci.yml from the repository.","title":"Faq"},{"location":"triply-etl/tmp/faq/#faq","text":"","title":"FAQ"},{"location":"triply-etl/tmp/faq/#why-does-my-pipeline-schedule-only-run-an-install-job","text":"This probably means that none of the only: rules in your .gitlab-ci.yml file match. You should check whether the variables you've set in the pipelines schedules page match with the only: rules in your .gitlab-ci.yml file.","title":"Why does my pipeline schedule only run an install job?"},{"location":"triply-etl/tmp/faq/#i-made-a-change-to-the-gitlab-ciyml-file-and-after-i-push-i-see-a-pipeline-failed-with-status-yaml-invalid-how-can-i-fix-this","text":"This error is emitted when the content of your .gitlab-ci.yml file does not follow the Yaml syntax. In GitLab, search for a page called CI Lint. On that page, copy-paste the contents of your .gitlab-ci.yml file into the text field and click validate. GitLab will indicate to you the location in your file where the Yaml syntax is violated, so that you can fix this.","title":"I made a change to the .gitlab-ci.yml file and after I push I see a pipeline failed with status yaml invalid. How can I fix this?"},{"location":"triply-etl/tmp/faq/#why-is-my-pipeline-not-running-and-marked-as-pending","text":"This probably means that you have not configured an ETL runner for this customer organization yet. See the section about getting started here","title":"Why is my pipeline not running and marked as 'pending'?"},{"location":"triply-etl/tmp/faq/#what-do-all-these-ci_-environment-variables-mean","text":"These are environment variables added by gitlab. To see what they mean, go to this gitlab documentation page, also mentioned above.","title":"What do all these $CI_... environment variables mean?"},{"location":"triply-etl/tmp/faq/#what-should-i-do-when-the-pipeline-fail-when-i-commit-in-a-personal-project","text":"In a personal repository, you have available runners, but shared ones. Thus, your pipelines will fail. This is expected and it is not an issue. You can either ignore the failed pipeline or remove gitlab-ci.yml from the repository.","title":"What should I do when the pipeline fail when I commit in a personal project?"},{"location":"triply-etl/tmp/getting-started/","text":"On this page: Getting started Transforming RDF data Connect a data source Important terms before starting to work with TriplyETL Middlewares What is a record? What is the store? What is the context(ctx)? A JSON data source An XML data source Getting started \u00b6 Transforming RDF data \u00b6 If you have RDF data that does not need to be transformed, see copying source data . If you have RDF data that does need to be transformed, you can use the following pattern. This example renames the graph. const etl = new Etl({ defaultGraph: graph.model }) etl.use( loadRdf(Source.file(`data/shapes.trig`)), mapQuads( (quad, ctx) => ctx.store.quad( quad.subject, quad.predicate, quad.object, iri(prefix.graph, 'new-graph') ) ), toRdf(Destination.TriplyDb.rdf('my-dataset', remoteOptions)) ) Similarly, you can change all the subject, predicates or objects in your data. Also, you can choose to transform triples of a specific subject, predicate, object or graph name. in this case, you should use: mapQuads( (quad, ctx) => ctx.store.quad( quad.subject, app.prefix.example('new-predicate'), quad.object, quad.graph ), {predicate: app.prefix.example(\"old-predicate\")} ) Connect a data source \u00b6 This section extends the pipeline from the previous section by connecting a data source. TriplyETL can connect to database systems and web APIs, but to keep things simple we will use the following tabular input data from a local file: ID NAME 00001 Anna 00002 Bob 00003 Carol We then perform the following steps to build a pipelines that processes this data source: 1. Create a text file called example.csv in a text editor, and copy/paste the following source data into that file: ID,NAME 00001,Anna 00002,Bob 00003,Carol 2. Open text file main.ts and add the following content: import { Etl, declarePrefix, fromCsv, iri, literal, rdfs, Source, toRdf, triple } from '@triplyetl/etl/generic' import { rdfs } from '@triplyetl/vocabularies' export default async function (): Promise<Etl> { const etl = new Etl({ prefixes: { ex: declarePrefix('https://example.com/'), }, }) etl.use( // Connects the tabular source data to the pipeline. // Every row in the table is processed as a TriplyETL record. fromCsv(Source.file('example.csv')), // Create a linked data statement that is based on the // source data. triple(iri(etl.prefix.ex, 'ID'), rdfs.label, 'NAME'), toRdf(Destination.file('example.ttl')) ) return etl } 3. Transpile the code with: npm run build 4. Run the ETL with: npx etl The TriplyETL script will give you a link to the uploaded dataset. This dataset contains the following graph content: Important terms before starting to work with TriplyETL \u00b6 Middlewares \u00b6 The most common occurrence in ETL are the middlewares. Middlewares are essentially reusable pieces of code that execute a certain long and/or complex piece of functionality. An middleware is a piece of code that transforms a record and can be invoked with etl.use(). Example of middleware function: loadRdf(Source.TriplyDb.query('my-account', 'my-query')), What is a record? \u00b6 TriplyETL doesn't have infinite memory and not all data can be loaded at once. So instead of loading data all at once, first one part of data is processed and written to the file, and then the second one, third one, and so on. These parts are called records. Each record goes through all middlewares before a new record is started. What is the store? \u00b6 As mentioned above, when ETL is running we go through data record by record. Together with the input data we also have output data. Before being written to the final destination (triplyDB or file), output data has to be kept somewhere and that's what store is for. The store is for temporarily storing linked data. Every record has its own store. toRdf reads from the store. etl.use( toRdf(Ratt.Destination.file('example.ttl')) ) What is the context(ctx)? \u00b6 In TriplyETL, the context is an object that represents the current record. The context gives us access to the triple store, the in memory storage of our triples. It also contains utility functions that will be used to modify and transform our source data into linked data. Some examples of ctx: ctx.getString(\"address\") ctx.getIri(...) ctx.getArray(...) ctx.store.addQuad(...) ctx.store.getQuad(...) //etc. A JSON data source \u00b6 The following code snippet uses extractor fromJson() with two inline example records: import { fromJson, logRecord, Etl } from '@triplydb/etl/generic' export default async function (): Promise<Etl> { const etl = new Etl() etl.use( fromJson([ { id: '123', name: 'John' }, { id: '456', name: 'Jane' }, ]), logRecord(), ) return etl } Debug function logRecord() prints the current record to standard output. When this pipeline is run, the two records are printed as follows: { \"id\": \"123\", \"name\": \"John\", \"$recordId\": 1, \"$environment\": \"Development\" } { \"id\": \"456\", \"name\": \"Jane\", \"$recordId\": 2, \"$environment\": \"Development\" } Notice that TriplyETL adds two keys to both records: $recordId and $environment (see Special Key ). An XML data source \u00b6 Now suppose that we change the source system. We no longer use in-line JSON, but will instead use an XML file. The contents of the XML file are as follows: <?xml version=\"1.0\"?> <persons> <person> <id>123</id> <name>John</name> </person> <person> <id>456</id> <name>Jane</name> </person> </persons> Let us change the TriplyETL script to use the XML source connector: import { Etl, fromXml, logRecord, Source } from '@triplyetl/etl/generic' export default async function (): Promise<Etl> { const etl = new Etl() etl.use( fromXml(Source.file('example.xml')), logRecord(), ) return etl } This new script logs the following two records: { \"id\": \"123\", \"name\": \"John\", \"$recordId\": 1, \"$environment\": \"Development\" } { \"id\": \"456\", \"name\": \"Jane\", \"$recordId\": 2, \"$environment\": \"Development\" } Notice that the two records that are logged from an XML source are completely identical to the two records that were previously logged from a JSON source. This is an essential property of TriplyETL: it treats data from any source system in the same way, using the same intermediary record format. This makes it easy to write pipelines that process data from a large number of different data sources. This also makes replacing a data source in one format with a data source in another format a relatively cheap operation. More often than not, only the source connector needs to be changed, and all transformations and assertions remain as they were.","title":"Getting started"},{"location":"triply-etl/tmp/getting-started/#getting-started","text":"","title":"Getting started"},{"location":"triply-etl/tmp/getting-started/#transforming-rdf-data","text":"If you have RDF data that does not need to be transformed, see copying source data . If you have RDF data that does need to be transformed, you can use the following pattern. This example renames the graph. const etl = new Etl({ defaultGraph: graph.model }) etl.use( loadRdf(Source.file(`data/shapes.trig`)), mapQuads( (quad, ctx) => ctx.store.quad( quad.subject, quad.predicate, quad.object, iri(prefix.graph, 'new-graph') ) ), toRdf(Destination.TriplyDb.rdf('my-dataset', remoteOptions)) ) Similarly, you can change all the subject, predicates or objects in your data. Also, you can choose to transform triples of a specific subject, predicate, object or graph name. in this case, you should use: mapQuads( (quad, ctx) => ctx.store.quad( quad.subject, app.prefix.example('new-predicate'), quad.object, quad.graph ), {predicate: app.prefix.example(\"old-predicate\")} )","title":"Transforming RDF data"},{"location":"triply-etl/tmp/getting-started/#connect-a-data-source","text":"This section extends the pipeline from the previous section by connecting a data source. TriplyETL can connect to database systems and web APIs, but to keep things simple we will use the following tabular input data from a local file: ID NAME 00001 Anna 00002 Bob 00003 Carol We then perform the following steps to build a pipelines that processes this data source: 1. Create a text file called example.csv in a text editor, and copy/paste the following source data into that file: ID,NAME 00001,Anna 00002,Bob 00003,Carol 2. Open text file main.ts and add the following content: import { Etl, declarePrefix, fromCsv, iri, literal, rdfs, Source, toRdf, triple } from '@triplyetl/etl/generic' import { rdfs } from '@triplyetl/vocabularies' export default async function (): Promise<Etl> { const etl = new Etl({ prefixes: { ex: declarePrefix('https://example.com/'), }, }) etl.use( // Connects the tabular source data to the pipeline. // Every row in the table is processed as a TriplyETL record. fromCsv(Source.file('example.csv')), // Create a linked data statement that is based on the // source data. triple(iri(etl.prefix.ex, 'ID'), rdfs.label, 'NAME'), toRdf(Destination.file('example.ttl')) ) return etl } 3. Transpile the code with: npm run build 4. Run the ETL with: npx etl The TriplyETL script will give you a link to the uploaded dataset. This dataset contains the following graph content:","title":"Connect a data source"},{"location":"triply-etl/tmp/getting-started/#important-terms-before-starting-to-work-with-triplyetl","text":"","title":"Important terms before starting to work with TriplyETL"},{"location":"triply-etl/tmp/getting-started/#middlewares","text":"The most common occurrence in ETL are the middlewares. Middlewares are essentially reusable pieces of code that execute a certain long and/or complex piece of functionality. An middleware is a piece of code that transforms a record and can be invoked with etl.use(). Example of middleware function: loadRdf(Source.TriplyDb.query('my-account', 'my-query')),","title":"Middlewares"},{"location":"triply-etl/tmp/getting-started/#what-is-a-record","text":"TriplyETL doesn't have infinite memory and not all data can be loaded at once. So instead of loading data all at once, first one part of data is processed and written to the file, and then the second one, third one, and so on. These parts are called records. Each record goes through all middlewares before a new record is started.","title":"What is a record?"},{"location":"triply-etl/tmp/getting-started/#what-is-the-store","text":"As mentioned above, when ETL is running we go through data record by record. Together with the input data we also have output data. Before being written to the final destination (triplyDB or file), output data has to be kept somewhere and that's what store is for. The store is for temporarily storing linked data. Every record has its own store. toRdf reads from the store. etl.use( toRdf(Ratt.Destination.file('example.ttl')) )","title":"What is the store?"},{"location":"triply-etl/tmp/getting-started/#what-is-the-contextctx","text":"In TriplyETL, the context is an object that represents the current record. The context gives us access to the triple store, the in memory storage of our triples. It also contains utility functions that will be used to modify and transform our source data into linked data. Some examples of ctx: ctx.getString(\"address\") ctx.getIri(...) ctx.getArray(...) ctx.store.addQuad(...) ctx.store.getQuad(...) //etc.","title":"What is the context(ctx)?"},{"location":"triply-etl/tmp/getting-started/#a-json-data-source","text":"The following code snippet uses extractor fromJson() with two inline example records: import { fromJson, logRecord, Etl } from '@triplydb/etl/generic' export default async function (): Promise<Etl> { const etl = new Etl() etl.use( fromJson([ { id: '123', name: 'John' }, { id: '456', name: 'Jane' }, ]), logRecord(), ) return etl } Debug function logRecord() prints the current record to standard output. When this pipeline is run, the two records are printed as follows: { \"id\": \"123\", \"name\": \"John\", \"$recordId\": 1, \"$environment\": \"Development\" } { \"id\": \"456\", \"name\": \"Jane\", \"$recordId\": 2, \"$environment\": \"Development\" } Notice that TriplyETL adds two keys to both records: $recordId and $environment (see Special Key ).","title":"A JSON data source"},{"location":"triply-etl/tmp/getting-started/#an-xml-data-source","text":"Now suppose that we change the source system. We no longer use in-line JSON, but will instead use an XML file. The contents of the XML file are as follows: <?xml version=\"1.0\"?> <persons> <person> <id>123</id> <name>John</name> </person> <person> <id>456</id> <name>Jane</name> </person> </persons> Let us change the TriplyETL script to use the XML source connector: import { Etl, fromXml, logRecord, Source } from '@triplyetl/etl/generic' export default async function (): Promise<Etl> { const etl = new Etl() etl.use( fromXml(Source.file('example.xml')), logRecord(), ) return etl } This new script logs the following two records: { \"id\": \"123\", \"name\": \"John\", \"$recordId\": 1, \"$environment\": \"Development\" } { \"id\": \"456\", \"name\": \"Jane\", \"$recordId\": 2, \"$environment\": \"Development\" } Notice that the two records that are logged from an XML source are completely identical to the two records that were previously logged from a JSON source. This is an essential property of TriplyETL: it treats data from any source system in the same way, using the same intermediary record format. This makes it easy to write pipelines that process data from a large number of different data sources. This also makes replacing a data source in one format with a data source in another format a relatively cheap operation. More often than not, only the source connector needs to be changed, and all transformations and assertions remain as they were.","title":"An XML data source"},{"location":"triply-etl/tmp/main-loop/","text":"On this page: The main loop The main loop \u00b6 The following code snippet shows the main TriplyETL loop. Every TriplyETL pipeline consists of such a loop. import { Etl } from '@triplyetl/etl/generic' export default async function (): Promise<Etl> { const etl = new Etl() etl.use( // Etc ) return etl } By adding the following five components, you configure the pipeline to create a linked data knowledge graph for your organization: Declarations declare IRI prefixes, graph names, and vocabularies that are used in the pipeline configuration. Source Connectors connect to the systems that add source data to your knowledge graph. Transformations clean, modify, and enrich the source data. Assertions generate the linked data that goes into the knowledge graph. Validation ensures that the linked data that is added to the knowledge graph follows your data model. Publication makes the linked data knowledge graph available in a triple store. These six components occur in specific places inside the TripleETL main loop, as indicated by the comments in the following code snippet: import { Etl } from '@triplyetl/etl/generic' // 1. Declarations are made before the main loop. export default async function (): Promise<Etl> { const etl = new Etl() etl.use( // 2. Source Connectors appear at the top. // 3. Transformations appear in the middle. // 4. Assertions appear in the middle. // 5. Validation occurs directly before publication. // 6. Publication appears at the bottom. ) return etl }","title":"Main loop"},{"location":"triply-etl/tmp/main-loop/#the-main-loop","text":"The following code snippet shows the main TriplyETL loop. Every TriplyETL pipeline consists of such a loop. import { Etl } from '@triplyetl/etl/generic' export default async function (): Promise<Etl> { const etl = new Etl() etl.use( // Etc ) return etl } By adding the following five components, you configure the pipeline to create a linked data knowledge graph for your organization: Declarations declare IRI prefixes, graph names, and vocabularies that are used in the pipeline configuration. Source Connectors connect to the systems that add source data to your knowledge graph. Transformations clean, modify, and enrich the source data. Assertions generate the linked data that goes into the knowledge graph. Validation ensures that the linked data that is added to the knowledge graph follows your data model. Publication makes the linked data knowledge graph available in a triple store. These six components occur in specific places inside the TripleETL main loop, as indicated by the comments in the following code snippet: import { Etl } from '@triplyetl/etl/generic' // 1. Declarations are made before the main loop. export default async function (): Promise<Etl> { const etl = new Etl() etl.use( // 2. Source Connectors appear at the top. // 3. Transformations appear in the middle. // 4. Assertions appear in the middle. // 5. Validation occurs directly before publication. // 6. Publication appears at the bottom. ) return etl }","title":"The main loop"},{"location":"triply-etl/tmp/source-destination/","text":"On this page: Source destination An easier way to configure graph names and prefixes Source destination \u00b6 An easier way to configure graph names and prefixes \u00b6 Instead of setting the graph name and the prefixes for every ETL, you can use functions for their generation: export function create_prefixes( organization: string = default_organization, dataset: string, host: string = default_host ) { const prefix_base = Ratt.prefixer(`https://${host}/${organization}/${dataset}/`) const prefix_bnode = Ratt.prefixer(prefix_base(`.well-known/genid/`)) const prefix_graph = Ratt.prefixer(prefix_base(`graph/`)) return { bnode: prefix_bnode, graph: prefix_graph, } } For example, if host==='triplydb.com' , organization==='exampleOrganization' and dataset='pokemon' , then the prefix for the blank nodes will be https://triplydb.com/exampleOrganization/pokemon/.well-known/genid/ . Then, similarly, you can use another function for the graph names: export function create_graphs( dataset: string, organization: string = default_organization, host: string = default_host ) { const prefix = create_prefixes(dataset, organization, host) return { default: prefix.graph('default'), metadata: prefix.graph('metadata'), instances: prefix.graph('instances'), instances_report: prefix.graph('instances/report'), shapes: prefix.graph('shapes'), } }","title":"Source destination"},{"location":"triply-etl/tmp/source-destination/#source-destination","text":"","title":"Source destination"},{"location":"triply-etl/tmp/source-destination/#an-easier-way-to-configure-graph-names-and-prefixes","text":"Instead of setting the graph name and the prefixes for every ETL, you can use functions for their generation: export function create_prefixes( organization: string = default_organization, dataset: string, host: string = default_host ) { const prefix_base = Ratt.prefixer(`https://${host}/${organization}/${dataset}/`) const prefix_bnode = Ratt.prefixer(prefix_base(`.well-known/genid/`)) const prefix_graph = Ratt.prefixer(prefix_base(`graph/`)) return { bnode: prefix_bnode, graph: prefix_graph, } } For example, if host==='triplydb.com' , organization==='exampleOrganization' and dataset='pokemon' , then the prefix for the blank nodes will be https://triplydb.com/exampleOrganization/pokemon/.well-known/genid/ . Then, similarly, you can use another function for the graph names: export function create_graphs( dataset: string, organization: string = default_organization, host: string = default_host ) { const prefix = create_prefixes(dataset, organization, host) return { default: prefix.graph('default'), metadata: prefix.graph('metadata'), instances: prefix.graph('instances'), instances_report: prefix.graph('instances/report'), shapes: prefix.graph('shapes'), } }","title":"An easier way to configure graph names and prefixes"},{"location":"triply-etl/tmp/static-dynamic-statements/","text":"On this page: Static and dynamic statements Create dynamic statements Static and dynamic triples When should you use an IRI instead of an URI literal? Limitation of literal() and iri() Static and dynamic statements \u00b6 Create dynamic statements \u00b6 Dynamic statements are statements that are based on some aspect of the source data. We use the following Record as an example: Country Inhabitants France null Germany 83190556 Netherlands 17650200 We start with creating the prefix and term declarations (see the Declare documentation for more information): const base = declarePrefix('https://triplydb.com/Triply/example/') const prefix = { def: declarePrefix(base('def/')), id: declarePrefix(base('id/')), xsd: declarePrefix('http://www.w3.org/2001/XMLSchema#'), } const def = { Country: prefix.def('Country'), name: prefix.def('inhabitants'), } const xsd = { positiveInteger: prefix.xsd('positiveInteger'), string: prefix.xsd('string'), } const input_string = ['Country', 'inhabitants'] With these prefix and term constants in place, a dynamic statement is created as follows: triple( iri(prefix.id, 'Country'), def.inhabitants, literal('Inhabitants', xsd.positiveInteger) ), Notice the following details: - iri() is used to create a dynamic IRI term. - Arguments Country and Inhabitants allow values for these keys to be used from processed Records. - The IRI prefix for the subject term is specified with constant prefix.id . - literal is used to create a dynamic literal term. - For literals a datatype IRI can be specified. If no datatype IRI is specified then the default IRI is xsd.string . iri.hashed can be used instead of iri when the ETL has a high number of blank nodes and they need more than one constant as input to hash a unique IRI. etl.use( triple( iri.hashed(prefix.id, input_string), def.inhabitants, mw.toLiteral('Inhabitants', {datatype: xsd.positiveInteger})), ) Notice the following details: - input_string can pass more than one constant to hash a unique IRI term. Static and dynamic triples \u00b6 Be aware that there are different approaches forstatic anddynamic IRIs: Static IRIs are created with prefix declarations (example [1a]). Dynamic IRIs are created with iri() , iri.hashed and prefix declarations (example [2a]). [1a] prefix.id('person') [2a] iri(prefix.id, 'person'), [3a] iri.hashed(prefix.id, ['person','age']), Notation [1a] creates thestatic* IRI [1b]. This IRI does not depend on the currently processed record. Notation [2a] creates thedynamic* IRI in [2b], assuming the \"person\" key contains the value \"John\" . This IRI depends on the currently processed record. For a different record, IRI [2c] may be created instead (assuming the \"person\" key contains the value \"Jane\" ). Notation [3a] creates thedynamic* IRI in [3b], assuming the \"person\" key contains the value \"Sam\" and the \"age\" key contains the value \"30\" . For a different record, IRI [3c] may be created instead (assuming the \"person\" key contains the value \"Roland\" and \"age\" key contains the value \"20\" ). [1b] id:person [2b] id:John [2c] id:Jane [3b] id:Sam , age: 30 [3c] id:Sam , age: 20 --> When should you use an IRI instead of an URI literal? \u00b6 An IRI is used to identify something, for example the city of Amsterdam. It is expected that accessing it returns linked data. An IRI can be used to make assertions about a subject. On the other hand, a URI is expected to return a non-linked data content, for example an HTML website, and can be used as objects in linked data, for example for inserting further information about the subject resource. In the example below, the subject IRI is described further by the object's URL. <https://dbpedia.org/resource/Amsterdam> rdfs:seeAlso \"https://www.iamsterdam.com\"^^xsd:anyURI. An IRI can be created with iri() , while an URI is created by using literal() . Limitation of literal() and iri() \u00b6 There is a limitation for both literal() and iri() . It is not possible to change the value in the record in the literal() and iri() assertions. The value that is at that moment stored in the record for that key, is then added as either an IRI when called with the iri() function or as a literal when called with the function literal() . The limitation is shown in the example below. In the example we want to round the inhabitants number to the nearest thousand. We can not transform this in the literal() function. Instead we need to add a custom.change() middleware which will execute the transformation. custom.change({ key: 'Inhabitants', type: 'number', change: value => value / 1_000, }), triple( iri(prefix.id, 'Country'), def.name, literal('Inhabitants', xsd.positiveInteger) ),","title":"Static dynamic statements"},{"location":"triply-etl/tmp/static-dynamic-statements/#static-and-dynamic-statements","text":"","title":"Static and dynamic statements"},{"location":"triply-etl/tmp/static-dynamic-statements/#create-dynamic-statements","text":"Dynamic statements are statements that are based on some aspect of the source data. We use the following Record as an example: Country Inhabitants France null Germany 83190556 Netherlands 17650200 We start with creating the prefix and term declarations (see the Declare documentation for more information): const base = declarePrefix('https://triplydb.com/Triply/example/') const prefix = { def: declarePrefix(base('def/')), id: declarePrefix(base('id/')), xsd: declarePrefix('http://www.w3.org/2001/XMLSchema#'), } const def = { Country: prefix.def('Country'), name: prefix.def('inhabitants'), } const xsd = { positiveInteger: prefix.xsd('positiveInteger'), string: prefix.xsd('string'), } const input_string = ['Country', 'inhabitants'] With these prefix and term constants in place, a dynamic statement is created as follows: triple( iri(prefix.id, 'Country'), def.inhabitants, literal('Inhabitants', xsd.positiveInteger) ), Notice the following details: - iri() is used to create a dynamic IRI term. - Arguments Country and Inhabitants allow values for these keys to be used from processed Records. - The IRI prefix for the subject term is specified with constant prefix.id . - literal is used to create a dynamic literal term. - For literals a datatype IRI can be specified. If no datatype IRI is specified then the default IRI is xsd.string . iri.hashed can be used instead of iri when the ETL has a high number of blank nodes and they need more than one constant as input to hash a unique IRI. etl.use( triple( iri.hashed(prefix.id, input_string), def.inhabitants, mw.toLiteral('Inhabitants', {datatype: xsd.positiveInteger})), ) Notice the following details: - input_string can pass more than one constant to hash a unique IRI term.","title":"Create dynamic statements"},{"location":"triply-etl/tmp/static-dynamic-statements/#static-and-dynamic-triples","text":"Be aware that there are different approaches forstatic anddynamic IRIs: Static IRIs are created with prefix declarations (example [1a]). Dynamic IRIs are created with iri() , iri.hashed and prefix declarations (example [2a]). [1a] prefix.id('person') [2a] iri(prefix.id, 'person'), [3a] iri.hashed(prefix.id, ['person','age']), Notation [1a] creates thestatic* IRI [1b]. This IRI does not depend on the currently processed record. Notation [2a] creates thedynamic* IRI in [2b], assuming the \"person\" key contains the value \"John\" . This IRI depends on the currently processed record. For a different record, IRI [2c] may be created instead (assuming the \"person\" key contains the value \"Jane\" ). Notation [3a] creates thedynamic* IRI in [3b], assuming the \"person\" key contains the value \"Sam\" and the \"age\" key contains the value \"30\" . For a different record, IRI [3c] may be created instead (assuming the \"person\" key contains the value \"Roland\" and \"age\" key contains the value \"20\" ). [1b] id:person [2b] id:John [2c] id:Jane [3b] id:Sam , age: 30 [3c] id:Sam , age: 20 -->","title":"Static and dynamic triples"},{"location":"triply-etl/tmp/static-dynamic-statements/#when-should-you-use-an-iri-instead-of-an-uri-literal","text":"An IRI is used to identify something, for example the city of Amsterdam. It is expected that accessing it returns linked data. An IRI can be used to make assertions about a subject. On the other hand, a URI is expected to return a non-linked data content, for example an HTML website, and can be used as objects in linked data, for example for inserting further information about the subject resource. In the example below, the subject IRI is described further by the object's URL. <https://dbpedia.org/resource/Amsterdam> rdfs:seeAlso \"https://www.iamsterdam.com\"^^xsd:anyURI. An IRI can be created with iri() , while an URI is created by using literal() .","title":"When should you use an IRI instead of an URI literal?"},{"location":"triply-etl/tmp/static-dynamic-statements/#limitation-of-literal-and-iri","text":"There is a limitation for both literal() and iri() . It is not possible to change the value in the record in the literal() and iri() assertions. The value that is at that moment stored in the record for that key, is then added as either an IRI when called with the iri() function or as a literal when called with the function literal() . The limitation is shown in the example below. In the example we want to round the inhabitants number to the nearest thousand. We can not transform this in the literal() function. Instead we need to add a custom.change() middleware which will execute the transformation. custom.change({ key: 'Inhabitants', type: 'number', change: value => value / 1_000, }), triple( iri(prefix.id, 'Country'), def.name, literal('Inhabitants', xsd.positiveInteger) ),","title":"Limitation of literal() and iri()"},{"location":"triply-etl/tmp/tmp/","text":"On this page: Create statements Create static statements Create dynamic statements Static and dynamic triples When should you use an IRI instead of an URI (which is a literal)? Limitation of literal, iri and iri.hashed Record IDs Process data conditionally Null values Missing values The empty string Custom functions Tree-shaped data Iterating over lists of objects Index key ($index) Parent key ($parent) Root key ($root) Iterating over lists of primitives Transforming RDF data Create statements \u00b6 After source data is connected and transformed, the RATT Record is ready to be transformed to linked data. Linked data statements are assertions or factual statements that consist of 3 terms (triple) or 4 terms (quadruples). Statements are created with the triple function. Calls to this function are part of the pipeline, and must appear inside the scope of etl.use . Create static statements \u00b6 Static linked data statements are statements that only make use of constant terms (see working with IRIs ). Constant terms are introduced at the beginning of a RATT pipeline, typically prior to the occurrence of the first etl.use scope. The following static statements make use of the constant terms introduced in the section on working with IRIs . etl.use( // \u201cJohn is a person.\u201d triple(ex.john, a, foaf.Person), // \u201cMary is a person.\u201d triple(ex.mary, a, foaf.Person), ) Create dynamic statements \u00b6 Dynamic statements are statements that are based on some aspect of the source data. We use the following RATT Record as an example: Country Inhabitants France null Germany 83190556 Netherlands 17650200 We start with creating the prefix and term declarations (see the section on working with IRIs for more information): const prefix_base = Ratt.prefixer('https://triplydb.com/Triply/example/') const prefix = { def: Ratt.prefixer(prefix_base('def/')), id: Ratt.prefixer(prefix_base('id/')), xsd: Ratt.prefixer('http://www.w3.org/2001/XMLSchema#'), } const def = { Country: prefix.def('Country'), name: prefix.def('inhabitants'), } const xsd = { positiveInteger: prefix.xsd('positiveInteger'), string: prefix.xsd('string'), } const input_string = ['Country', 'inhabitants'] With these prefix and term constants in place, a dynamic statement is created as follows: etl.use( triple( iri('Country', {prefix: prefix.id}), def.inhabitants, literal('Inhabitants', {datatype: xsd.positiveInteger})), ) Notice the following details: - iri is used to create a dynamic IRI term. - Arguments Country and Inhabitants allow values for these keys to be used from processed RATT Records. - The IRI prefix for the subject term is specified with constant prefix.id . - literal is used to create a dynamic literal term. - For literals a datatype IRI can be specified. If no datatype IRI is specified then the default IRI is xsd.string . iri.hashed can be used instead of iri when the ETL has a high number of blank nodes and they need more than one constant as input to hash a unique IRI. etl.use( triple( iri.hashed(prefix.id, input_string), def.inhabitants, mw.toLiteral('Inhabitants', {datatype: xsd.positiveInteger})), ) Notice the following details: - input_string can pass more than one constant to hash a unique IRI term. Static and dynamic triples \u00b6 Be aware that there are different approaches for static and dynamic IRIs: Static IRIs are created with prefix declarations (example [1a]). Dynamic IRIs are created with iri , iri.hashed and prefix declarations (example [2a]). [1a] prefix.id('person') [2a] iri(prefix.id, 'person'), [3a] iri.hashed(prefix.id, ['person','age']), Notation [1a] creates the static IRI [1b]. This IRI does not depend on the currently processed RATT record. Notation [2a] creates the dynamic IRI in [2b], assuming the \"person\" key contains the value \"John\" . This IRI depends on the currently processed RATT record. For a different RATT record, IRI [2c] may be created instead (assuming the \"person\" key contains the value \"Jane\" ). Notation [3a] creates the dynamic IRI in [3b], assuming the \"person\" key contains the value \"Sam\" and the \"age\" key contains the value \"30\" . For a different RATT record, IRI [3c] may be created instead (assuming the \"person\" key contains the value \"Roland\" and \"age\" key contains the value \"20\" ). [1b] id:person [2b] id:John [2c] id:Jane [3b] id:Sam , age: 30 [3c] id:Sam , age: 20 When should you use an IRI instead of an URI (which is a literal)? \u00b6 An IRI is used to identify something, for example the city of Amsterdam. It is expected that accessing it returns linked data. An IRI can be used to make assertions about a subject. On the other hand, a URI is expected to return a non-linked data content, for example an HTML website, and can be used as objects in linked data, for example for inserting further information about the subject resource. In the example below, the subject IRI is described further by the object's URL. <https://dbpedia.org/resource/Amsterdam> rdfs:seeAlso \"https://www.iamsterdam.com\"^^xsd:anyURI. An IRI can be created with iri , while an URI is created by using literal . Limitation of literal , iri and iri.hashed \u00b6 There is a limitation for both literal , iri and iri.hashed . It is not possible to change the value in the record in the literal , iri and iri.hashed middlewares. The value that is at that moment stored in the record for that key, is then added as either an IRI when called with the iri / iri.hashed function or as a literal when called with the function literal . The limitation is shown in the example below. In the example we want to round the inhabitants number to the nearest thousand. We can not transform this in the literal function. Instead we need to add a change middleware which will execute the transformation. etl.use( change({ key: 'Inhabitants', type: 'number', change: (value) => value/1000 }), triple( iri(prefix.id, 'Country'), def.name, literal('Inhabitants', xsd.positiveInteger) ), ) Record IDs \u00b6 If your RATT Records do not contain a unique ID then you can use the recordId entry that RATT adds automatically. These recordId values are unique for every record processed in the same pipeline, but they are not an entry into the RATT Record by default. Record IDs are consistently assigned across runs of the same pipeline. They generate the same output as long as the input does not change. The following example code shows how the record ID can be added to each RATT Record: etl.use( add({ key: 'ID', value: context => app.prefix.observation(context.recordId.toString()) }), triple(iri(prefix.id, key_id), a, def.Country), ) Process data conditionally \u00b6 Source data often contains optional values. These are values that appear in some, but not all records. Source data often contains 'special' values to denote the absence of a value. Common examples are values such as 'NULL' or the empty string ( '' ) or 'clear' outliers such as 9999 for a missing year. We call such values \u2018null values\u2019. The when function supports the creation of triples under certain conditions. The first argument that this function takes establishes whether or not a certain condition is met. After that, one or more additional statement arguments appear that will only be called if the condition is satisfied. The generic structure of when is as follows: etl.use( when( '{condition}', '{statement-1}', '{statement-2}', '{statement-3}', ..., ) ) Notice that it is often useful to specify multiple statements under the same condition: The first statement transforms an optional value, and the second statement uses the transformed optional value to make a triple assertion. The first statement asserts one triple based on the optional value, and the second statement asserts a second triple based on the same optional value. Null values \u00b6 If a key contains a null value in some records, then we need to specifically identify the criteria under which a triple must be added. etl.use( // The source data uses '9999' to denote an unknown creation year. when( context => context.getNumber('CREATED') != 9999), triple( iri(prefix.id, 'ID'), dct.created, literal('CREATED', xsd.gYear))), Notice that the conditional function inside the when function takes the current RATT context as its single argument and returns a Boolean. Missing values \u00b6 If a value is sometimes completely missing from a source data record, then the following construct can be used to only add a triple in case the value is present: etl.use( // The source data does not always include a value for 'zipcode'. when( context => context.isNotEmpty('ZIPCODE'), triple( iri(prefix.id, 'ID'), def.zipcode, literal('ZIPCODE')), ..., ), ) Because missing values are very common in source data, RATT introduces special support for when the value for a specific key is missing. Instead of having to write context => context.isNotEmpty('foo') one can simply write the key name instead. The above example is equivalent to the following: etl.use( // The source data does not always include a value for 'zipcode'. when( 'ZIPCODE', triple( iri(prefix.id, 'ID'), def.zipcode, literal('ZIPCODE')), ..., ), ) It is also possible to check if a value is completely missing from the source data with ctx.isEmpty() A note for finding more methods RATT: One of the many advantages using Typescript is code completion. As such any methods available on a class in Ratt can be accessed using your IDE's intellisense ( ctrl + space in VSCODE). In Ratt the context and mw are two such classes that can be accessed in this way. The empty string \u00b6 Because source data often uses the empty string to signify NULL values, this particular string is treated in a special way by RATT. etl.use( when( key.zipcode, // Skipped for the empty string. ...), ) Notice that it is almost never useful to store the empty string in linked data. So the treatment of the empty string as a NULL value is the correct default behavior. Custom functions \u00b6 If we want to extract a string value from the source data, we can write a custom function which can be used with when . when can receive two parameters: string(a key value) or a function. If when receives a string, it checks whether it is empty or not. But in case of a custom method specific instructions are required. For example, (ctx) => ctx.isNotEmpty('foo') && ctx.getString('foo') === 'foo' Notice details: ctx.isNotEmpty('foo') checks whether the string is empty or not and only if it is not empty, the function moves to the next step ctx.getString('bla') === 'something\u2019 , which is the next step, extracts 'foo' when it fulfills the required criteria Tree-shaped data \u00b6 Tree-shaped data is very common in different source systems. We will use the following JSON source data as an example in this section: { \"metadata\": { \"title\": { \"name\": \"Data about countries.\" } }, \"data\": { \"countries\": [ { \"id\": \"nl\", \"name\": \"The Netherlands\" }, { \"id\": \"de\", \"name\": \"Germany\" } ] } } The principles that are documented in this section can be applied to any form of tree-shaped data. For example, the following XML snippet is very similar to the JSON example: <?xml version=\"1.0\"?> <root> <metadata> <title> <name>Data about countries.</name> </title> </metadata> <data> <countries> <id>nl</id> <name>The Netherlands</name> </countries> <countries> <id>de</id> <name>Germany</name> </countries> </data> </root> Iterating over lists of objects \u00b6 In the previous section, we saw that we were able to assert the name of the first country and the name of the second country. But what do we do if we want to assert the name for every country in the world? And what do we do if some countries have a name in 2 languages, but other countries have a name in 1 or 3 languages? What we need is a simple way to express that we want RATT to make an assertion for every element in a list. RATT uses the forEach function for this purpose. The following code snippet asserts the name for each country in the example data: etl.use( forEach('data.countries', triple( iri(prefix.country, 'id'), rdfs.label, literal('name', 'en'))), ) Notice the following details: - forEach uses the path expression 'data.countries' to identify the list. - Inside the forEach function, each element in the list is made available separately. - This allows the 'id' and 'name' keys to be identified directly. The above code snippet makes one assertion for every element in the \"countries\" list: country:nl rdfs:label 'The Netherlands'@en. country:de rdfs:label 'Germany'@en. Notice that forEach only works for lists whose elements are objects . See Iterating over lists of primitives for dealing with lists that do not contain objects. The elements that forEach iterates over are themselves RATT records. This implies that all functions that work for full RATT records also work for the RATT records inside forEach . The RATT records inside an forEach function are smaller. This allows the regular keys of the iterated-over elements to be accessed directly. In addition to these regular keys, RATT records inside forEach also contain additional keys that simplify common operations. The following subsections explain the following special keys: Index key ( $index ) Parent key ( $parent ) Root key ( $root ) Index key ( $index ) \u00b6 Each RATT record that is made available in forEach contains the $index key. The value of this key is the index of the element in the list. This is the same index that is used to access specific elements in an list, as explained in the section on accessing lists by index . The index key is often useful for assigning a unique subject IRI to every element. Suppose we have the following source data. We do not want to use the values of the \"name\" key for our subject IRI, because these names contain spaces and possibly other problematic characters that make the IRI more difficult to read and use. { \"countries\": [ { \"name\": \"The Netherlands\" }, { \"name\": \"Germany\" }, { \"name\": \"Italy\" }, \u2026 ] } The following code snippet uses the $index key that is made available inside forEach in order to create a unique subject IRI for each country: etl.use( forEach('countries', triple( iri(prefix.country, '$index'), rdfs.label, literal('name', 'en'))), ) This results in the following assertions: country:0 rdfs:label 'The Netherlands'@en. country:1 rdfs:label 'Germany'@en. country:2 rdfs:label 'Italy'@en. Parent key ( $parent ) \u00b6 When forEach iterates through a list of elements, it makes the enclosing parent record available under key $parent . The parent record is the record that directly contains the first key that appears in the path that was specified in forEach . For example, the parent record in the following call is the record that directly contains the \"data\" key: etl.use( forEach('data.countries', \u2026 ) ) The $parent key can be observed when logRecord` is used to print the iterated-over elements to the terminal: etl.use( forEach('data.countries', logRecord()) ) For our example source data, this emits the following 2 RATT records: { \"id\": \"en\", \"name\": \"The Netherlands\", \"$index\": 0, \"$parent\": { \"data\": { \"labels\": [ { \"id\": \"en\", \"name\": \"The Netherlands\", }, { \"id\": \"de\" \"name\": \"Germany\", } ] } }, \"$root\": \"__circular__\" } and: { \"id\": \"de\", \"name\": \"Germany\", \"$index\": 1, \"$parent\": { \"data\": { \"labels\": [ { \"id\": \"en\", \"name\": \"The Netherlands\", }, { \"id\": \"de\" \"name\": \"Germany\", } ] } }, \"$root\": \"__circular__\" } The $root key is explained in the next section . Root key ( $root ) \u00b6 Sometimes it may be necessary to access a part of the original RATT record that is outside of the scope of the forEach call. Every RATT record inside a forEach call contains the \"$root\" key. The value of the root key provides a link to the full RATT record. Because the $root key is part of the linked-to RATT record, it is not possible to print the value of the root key. (This would result in infinite output.) For this reason, the value of the $root key is printed as the special value \"__circular__\" . For the above examples, the parent record and root record are the same, but this is not always the case. Specifically, the parent record and root record are different when forEach calls are nested. The following data contains an inner list (key \"labels\" ) inside an outer list ( \"countries\" ): { \"data\": { \"countries\": [ { \"id\": \"NL\", \"labels\": [ { \"name\": \"The Netherlands\", \"locale\": \"en-us\" }, { \"name\": \"Nederland\", \"locale\": \"nl-nl\" } ] }, { \"id\": \"EN\", \"labels\": [ { \"name\": \"England\", \"locale\": \"en-gb\" } ] } ] } } The following nested forEach call shows the difference between the \"$parent\" key and the $root key. The $parent key allows the individual country objects to be accessed, while the \"$root\" key allows the entire tree to be accessed: etl.use( forEach('data.countries', forEach('labels', logRecord())), ) The following RATT record is printed first (3 records are printed in total). Notice that the value of the outer $parent and \"$root\" keys are now different: - The $parent key allows access to the first country. - The $root key allows access to the full record (describing multiple countries). { \"name\": \"The Netherlands\", \"locale\": \"en-us\", \"$index\": 0, \"$parent\": { \"id\": \"NL\", \"labels\": [ { \"name\": \"The Netherlands\", \"locale\": \"en-us\" }, { \"name\": \"Nederland\", \"locale\": \"nl-nl\" } ], \"$index\": 0, \"$parent\": { \"data\": { \"countries\": [ { \"id\": \"NL\", \"labels\": \"__circular__\" }, { \"id\": \"EN\", \"labels\": [ { \"name\": \"England\", \"locale\": \"en-gb\" } ] } ] } }, \"$root\": \"__circular__\" }, \"$root\": \"__circular__\" } Iterating over lists of primitives \u00b6 In the previous section we showed how to iterate over lists of objects. But what happens if a list does not contain objects but elements of primitive type? Examples include lists of strings or lists of numbers. Function forEach does not work with lists containing primitive types, because it assumes a RATT record structure which can only be provided by objects. Luckily, RATT includes the functions iri.forEach and literal.forEach that can be specifically used to iterate over lists of primitives. etl.use( fromJson({\"id\": \"nl\", \"names\": [\"The Netherlands\", \"Holland\"]}), triple( iri(prefix.country, 'id'), rdfs.label, literal.forEach('names', 'en')), ) This makes the following assertion: country:nl rdfs:label 'The Netherlands'@en, 'Holland'@en. Transforming RDF data \u00b6 If you have RDF data that does not need to be transformed, see copying source data . If you have RDF data that does need to be transformed, you can use the following pattern. This example renames the graph. const app = new Ratt({ defaultGraph: graph.model, prefixes: prefix, sources: { inputFile: Ratt.Source.file(`data/shapes.trig`) }, destinations: { dataset: Ratt.Destination.TriplyDb.rdf(organization, dataset, remoteOptions) }, }) etl.use( loadRdf(app.sources.inputFile), mapQuads( (quad, ctx) => ctx.store.quad( quad.subject, quad.predicate, quad.object, app.prefix.somePrefix(\"graph\") ) ), toRdf(app.destinations.dataset) ) Similarly, you can change all the subject, predicates or objects in your data. Also, you can choose to transform triples of a specific subject, predicate, object or graph name. in this case, you should use: mapQuads( (quad, ctx) => ctx.store.quad( quad.subject, app.prefix.example('new-predicate'), quad.object, quad.graph ), {predicate: app.prefix.example(\"old-predicate\")} )","title":"Tmp"},{"location":"triply-etl/tmp/tmp/#create-statements","text":"After source data is connected and transformed, the RATT Record is ready to be transformed to linked data. Linked data statements are assertions or factual statements that consist of 3 terms (triple) or 4 terms (quadruples). Statements are created with the triple function. Calls to this function are part of the pipeline, and must appear inside the scope of etl.use .","title":"Create statements"},{"location":"triply-etl/tmp/tmp/#static-assertions","text":"Static linked data statements are statements that only make use of constant terms (see working with IRIs ). Constant terms are introduced at the beginning of a RATT pipeline, typically prior to the occurrence of the first etl.use scope. The following static statements make use of the constant terms introduced in the section on working with IRIs . etl.use( // \u201cJohn is a person.\u201d triple(ex.john, a, foaf.Person), // \u201cMary is a person.\u201d triple(ex.mary, a, foaf.Person), )","title":"Create static statements"},{"location":"triply-etl/tmp/tmp/#dynamic-assertions","text":"Dynamic statements are statements that are based on some aspect of the source data. We use the following RATT Record as an example: Country Inhabitants France null Germany 83190556 Netherlands 17650200 We start with creating the prefix and term declarations (see the section on working with IRIs for more information): const prefix_base = Ratt.prefixer('https://triplydb.com/Triply/example/') const prefix = { def: Ratt.prefixer(prefix_base('def/')), id: Ratt.prefixer(prefix_base('id/')), xsd: Ratt.prefixer('http://www.w3.org/2001/XMLSchema#'), } const def = { Country: prefix.def('Country'), name: prefix.def('inhabitants'), } const xsd = { positiveInteger: prefix.xsd('positiveInteger'), string: prefix.xsd('string'), } const input_string = ['Country', 'inhabitants'] With these prefix and term constants in place, a dynamic statement is created as follows: etl.use( triple( iri('Country', {prefix: prefix.id}), def.inhabitants, literal('Inhabitants', {datatype: xsd.positiveInteger})), ) Notice the following details: - iri is used to create a dynamic IRI term. - Arguments Country and Inhabitants allow values for these keys to be used from processed RATT Records. - The IRI prefix for the subject term is specified with constant prefix.id . - literal is used to create a dynamic literal term. - For literals a datatype IRI can be specified. If no datatype IRI is specified then the default IRI is xsd.string . iri.hashed can be used instead of iri when the ETL has a high number of blank nodes and they need more than one constant as input to hash a unique IRI. etl.use( triple( iri.hashed(prefix.id, input_string), def.inhabitants, mw.toLiteral('Inhabitants', {datatype: xsd.positiveInteger})), ) Notice the following details: - input_string can pass more than one constant to hash a unique IRI term.","title":"Create dynamic statements"},{"location":"triply-etl/tmp/tmp/#static-and-dynamic-triples","text":"Be aware that there are different approaches for static and dynamic IRIs: Static IRIs are created with prefix declarations (example [1a]). Dynamic IRIs are created with iri , iri.hashed and prefix declarations (example [2a]). [1a] prefix.id('person') [2a] iri(prefix.id, 'person'), [3a] iri.hashed(prefix.id, ['person','age']), Notation [1a] creates the static IRI [1b]. This IRI does not depend on the currently processed RATT record. Notation [2a] creates the dynamic IRI in [2b], assuming the \"person\" key contains the value \"John\" . This IRI depends on the currently processed RATT record. For a different RATT record, IRI [2c] may be created instead (assuming the \"person\" key contains the value \"Jane\" ). Notation [3a] creates the dynamic IRI in [3b], assuming the \"person\" key contains the value \"Sam\" and the \"age\" key contains the value \"30\" . For a different RATT record, IRI [3c] may be created instead (assuming the \"person\" key contains the value \"Roland\" and \"age\" key contains the value \"20\" ). [1b] id:person [2b] id:John [2c] id:Jane [3b] id:Sam , age: 30 [3c] id:Sam , age: 20","title":"Static and dynamic triples"},{"location":"triply-etl/tmp/tmp/#when-should-you-use-an-iri-instead-of-an-uri-which-is-a-literal","text":"An IRI is used to identify something, for example the city of Amsterdam. It is expected that accessing it returns linked data. An IRI can be used to make assertions about a subject. On the other hand, a URI is expected to return a non-linked data content, for example an HTML website, and can be used as objects in linked data, for example for inserting further information about the subject resource. In the example below, the subject IRI is described further by the object's URL. <https://dbpedia.org/resource/Amsterdam> rdfs:seeAlso \"https://www.iamsterdam.com\"^^xsd:anyURI. An IRI can be created with iri , while an URI is created by using literal .","title":"When should you use an IRI instead of an URI (which is a literal)?"},{"location":"triply-etl/tmp/tmp/#limitation-of-literal-iri-and-irihashed","text":"There is a limitation for both literal , iri and iri.hashed . It is not possible to change the value in the record in the literal , iri and iri.hashed middlewares. The value that is at that moment stored in the record for that key, is then added as either an IRI when called with the iri / iri.hashed function or as a literal when called with the function literal . The limitation is shown in the example below. In the example we want to round the inhabitants number to the nearest thousand. We can not transform this in the literal function. Instead we need to add a change middleware which will execute the transformation. etl.use( change({ key: 'Inhabitants', type: 'number', change: (value) => value/1000 }), triple( iri(prefix.id, 'Country'), def.name, literal('Inhabitants', xsd.positiveInteger) ), )","title":"Limitation of literal, iri and iri.hashed"},{"location":"triply-etl/tmp/tmp/#record-ids","text":"If your RATT Records do not contain a unique ID then you can use the recordId entry that RATT adds automatically. These recordId values are unique for every record processed in the same pipeline, but they are not an entry into the RATT Record by default. Record IDs are consistently assigned across runs of the same pipeline. They generate the same output as long as the input does not change. The following example code shows how the record ID can be added to each RATT Record: etl.use( add({ key: 'ID', value: context => app.prefix.observation(context.recordId.toString()) }), triple(iri(prefix.id, key_id), a, def.Country), )","title":"Record IDs"},{"location":"triply-etl/tmp/tmp/#process-data-conditionally","text":"Source data often contains optional values. These are values that appear in some, but not all records. Source data often contains 'special' values to denote the absence of a value. Common examples are values such as 'NULL' or the empty string ( '' ) or 'clear' outliers such as 9999 for a missing year. We call such values \u2018null values\u2019. The when function supports the creation of triples under certain conditions. The first argument that this function takes establishes whether or not a certain condition is met. After that, one or more additional statement arguments appear that will only be called if the condition is satisfied. The generic structure of when is as follows: etl.use( when( '{condition}', '{statement-1}', '{statement-2}', '{statement-3}', ..., ) ) Notice that it is often useful to specify multiple statements under the same condition: The first statement transforms an optional value, and the second statement uses the transformed optional value to make a triple assertion. The first statement asserts one triple based on the optional value, and the second statement asserts a second triple based on the same optional value.","title":"Process data conditionally"},{"location":"triply-etl/tmp/tmp/#null-values","text":"If a key contains a null value in some records, then we need to specifically identify the criteria under which a triple must be added. etl.use( // The source data uses '9999' to denote an unknown creation year. when( context => context.getNumber('CREATED') != 9999), triple( iri(prefix.id, 'ID'), dct.created, literal('CREATED', xsd.gYear))), Notice that the conditional function inside the when function takes the current RATT context as its single argument and returns a Boolean.","title":"Null values"},{"location":"triply-etl/tmp/tmp/#missing-values","text":"If a value is sometimes completely missing from a source data record, then the following construct can be used to only add a triple in case the value is present: etl.use( // The source data does not always include a value for 'zipcode'. when( context => context.isNotEmpty('ZIPCODE'), triple( iri(prefix.id, 'ID'), def.zipcode, literal('ZIPCODE')), ..., ), ) Because missing values are very common in source data, RATT introduces special support for when the value for a specific key is missing. Instead of having to write context => context.isNotEmpty('foo') one can simply write the key name instead. The above example is equivalent to the following: etl.use( // The source data does not always include a value for 'zipcode'. when( 'ZIPCODE', triple( iri(prefix.id, 'ID'), def.zipcode, literal('ZIPCODE')), ..., ), ) It is also possible to check if a value is completely missing from the source data with ctx.isEmpty() A note for finding more methods RATT: One of the many advantages using Typescript is code completion. As such any methods available on a class in Ratt can be accessed using your IDE's intellisense ( ctrl + space in VSCODE). In Ratt the context and mw are two such classes that can be accessed in this way.","title":"Missing values"},{"location":"triply-etl/tmp/tmp/#the-empty-string","text":"Because source data often uses the empty string to signify NULL values, this particular string is treated in a special way by RATT. etl.use( when( key.zipcode, // Skipped for the empty string. ...), ) Notice that it is almost never useful to store the empty string in linked data. So the treatment of the empty string as a NULL value is the correct default behavior.","title":"The empty string"},{"location":"triply-etl/tmp/tmp/#custom-functions","text":"If we want to extract a string value from the source data, we can write a custom function which can be used with when . when can receive two parameters: string(a key value) or a function. If when receives a string, it checks whether it is empty or not. But in case of a custom method specific instructions are required. For example, (ctx) => ctx.isNotEmpty('foo') && ctx.getString('foo') === 'foo' Notice details: ctx.isNotEmpty('foo') checks whether the string is empty or not and only if it is not empty, the function moves to the next step ctx.getString('bla') === 'something\u2019 , which is the next step, extracts 'foo' when it fulfills the required criteria","title":"Custom functions"},{"location":"triply-etl/tmp/tmp/#tree-shaped-data","text":"Tree-shaped data is very common in different source systems. We will use the following JSON source data as an example in this section: { \"metadata\": { \"title\": { \"name\": \"Data about countries.\" } }, \"data\": { \"countries\": [ { \"id\": \"nl\", \"name\": \"The Netherlands\" }, { \"id\": \"de\", \"name\": \"Germany\" } ] } } The principles that are documented in this section can be applied to any form of tree-shaped data. For example, the following XML snippet is very similar to the JSON example: <?xml version=\"1.0\"?> <root> <metadata> <title> <name>Data about countries.</name> </title> </metadata> <data> <countries> <id>nl</id> <name>The Netherlands</name> </countries> <countries> <id>de</id> <name>Germany</name> </countries> </data> </root>","title":"Tree-shaped data"},{"location":"triply-etl/tmp/tmp/#iterating-over-lists-of-objects","text":"In the previous section, we saw that we were able to assert the name of the first country and the name of the second country. But what do we do if we want to assert the name for every country in the world? And what do we do if some countries have a name in 2 languages, but other countries have a name in 1 or 3 languages? What we need is a simple way to express that we want RATT to make an assertion for every element in a list. RATT uses the forEach function for this purpose. The following code snippet asserts the name for each country in the example data: etl.use( forEach('data.countries', triple( iri(prefix.country, 'id'), rdfs.label, literal('name', 'en'))), ) Notice the following details: - forEach uses the path expression 'data.countries' to identify the list. - Inside the forEach function, each element in the list is made available separately. - This allows the 'id' and 'name' keys to be identified directly. The above code snippet makes one assertion for every element in the \"countries\" list: country:nl rdfs:label 'The Netherlands'@en. country:de rdfs:label 'Germany'@en. Notice that forEach only works for lists whose elements are objects . See Iterating over lists of primitives for dealing with lists that do not contain objects. The elements that forEach iterates over are themselves RATT records. This implies that all functions that work for full RATT records also work for the RATT records inside forEach . The RATT records inside an forEach function are smaller. This allows the regular keys of the iterated-over elements to be accessed directly. In addition to these regular keys, RATT records inside forEach also contain additional keys that simplify common operations. The following subsections explain the following special keys: Index key ( $index ) Parent key ( $parent ) Root key ( $root )","title":"Iterating over lists of objects"},{"location":"triply-etl/tmp/tmp/#index-key-index","text":"Each RATT record that is made available in forEach contains the $index key. The value of this key is the index of the element in the list. This is the same index that is used to access specific elements in an list, as explained in the section on accessing lists by index . The index key is often useful for assigning a unique subject IRI to every element. Suppose we have the following source data. We do not want to use the values of the \"name\" key for our subject IRI, because these names contain spaces and possibly other problematic characters that make the IRI more difficult to read and use. { \"countries\": [ { \"name\": \"The Netherlands\" }, { \"name\": \"Germany\" }, { \"name\": \"Italy\" }, \u2026 ] } The following code snippet uses the $index key that is made available inside forEach in order to create a unique subject IRI for each country: etl.use( forEach('countries', triple( iri(prefix.country, '$index'), rdfs.label, literal('name', 'en'))), ) This results in the following assertions: country:0 rdfs:label 'The Netherlands'@en. country:1 rdfs:label 'Germany'@en. country:2 rdfs:label 'Italy'@en.","title":"Index key ($index)"},{"location":"triply-etl/tmp/tmp/#parent-key-parent","text":"When forEach iterates through a list of elements, it makes the enclosing parent record available under key $parent . The parent record is the record that directly contains the first key that appears in the path that was specified in forEach . For example, the parent record in the following call is the record that directly contains the \"data\" key: etl.use( forEach('data.countries', \u2026 ) ) The $parent key can be observed when logRecord` is used to print the iterated-over elements to the terminal: etl.use( forEach('data.countries', logRecord()) ) For our example source data, this emits the following 2 RATT records: { \"id\": \"en\", \"name\": \"The Netherlands\", \"$index\": 0, \"$parent\": { \"data\": { \"labels\": [ { \"id\": \"en\", \"name\": \"The Netherlands\", }, { \"id\": \"de\" \"name\": \"Germany\", } ] } }, \"$root\": \"__circular__\" } and: { \"id\": \"de\", \"name\": \"Germany\", \"$index\": 1, \"$parent\": { \"data\": { \"labels\": [ { \"id\": \"en\", \"name\": \"The Netherlands\", }, { \"id\": \"de\" \"name\": \"Germany\", } ] } }, \"$root\": \"__circular__\" } The $root key is explained in the next section .","title":"Parent key ($parent)"},{"location":"triply-etl/tmp/tmp/#root-key-root","text":"Sometimes it may be necessary to access a part of the original RATT record that is outside of the scope of the forEach call. Every RATT record inside a forEach call contains the \"$root\" key. The value of the root key provides a link to the full RATT record. Because the $root key is part of the linked-to RATT record, it is not possible to print the value of the root key. (This would result in infinite output.) For this reason, the value of the $root key is printed as the special value \"__circular__\" . For the above examples, the parent record and root record are the same, but this is not always the case. Specifically, the parent record and root record are different when forEach calls are nested. The following data contains an inner list (key \"labels\" ) inside an outer list ( \"countries\" ): { \"data\": { \"countries\": [ { \"id\": \"NL\", \"labels\": [ { \"name\": \"The Netherlands\", \"locale\": \"en-us\" }, { \"name\": \"Nederland\", \"locale\": \"nl-nl\" } ] }, { \"id\": \"EN\", \"labels\": [ { \"name\": \"England\", \"locale\": \"en-gb\" } ] } ] } } The following nested forEach call shows the difference between the \"$parent\" key and the $root key. The $parent key allows the individual country objects to be accessed, while the \"$root\" key allows the entire tree to be accessed: etl.use( forEach('data.countries', forEach('labels', logRecord())), ) The following RATT record is printed first (3 records are printed in total). Notice that the value of the outer $parent and \"$root\" keys are now different: - The $parent key allows access to the first country. - The $root key allows access to the full record (describing multiple countries). { \"name\": \"The Netherlands\", \"locale\": \"en-us\", \"$index\": 0, \"$parent\": { \"id\": \"NL\", \"labels\": [ { \"name\": \"The Netherlands\", \"locale\": \"en-us\" }, { \"name\": \"Nederland\", \"locale\": \"nl-nl\" } ], \"$index\": 0, \"$parent\": { \"data\": { \"countries\": [ { \"id\": \"NL\", \"labels\": \"__circular__\" }, { \"id\": \"EN\", \"labels\": [ { \"name\": \"England\", \"locale\": \"en-gb\" } ] } ] } }, \"$root\": \"__circular__\" }, \"$root\": \"__circular__\" }","title":"Root key ($root)"},{"location":"triply-etl/tmp/tmp/#iterating-over-lists-of-primitives","text":"In the previous section we showed how to iterate over lists of objects. But what happens if a list does not contain objects but elements of primitive type? Examples include lists of strings or lists of numbers. Function forEach does not work with lists containing primitive types, because it assumes a RATT record structure which can only be provided by objects. Luckily, RATT includes the functions iri.forEach and literal.forEach that can be specifically used to iterate over lists of primitives. etl.use( fromJson({\"id\": \"nl\", \"names\": [\"The Netherlands\", \"Holland\"]}), triple( iri(prefix.country, 'id'), rdfs.label, literal.forEach('names', 'en')), ) This makes the following assertion: country:nl rdfs:label 'The Netherlands'@en, 'Holland'@en.","title":"Iterating over lists of primitives"},{"location":"triply-etl/tmp/tmp/#transforming-rdf-data","text":"If you have RDF data that does not need to be transformed, see copying source data . If you have RDF data that does need to be transformed, you can use the following pattern. This example renames the graph. const app = new Ratt({ defaultGraph: graph.model, prefixes: prefix, sources: { inputFile: Ratt.Source.file(`data/shapes.trig`) }, destinations: { dataset: Ratt.Destination.TriplyDb.rdf(organization, dataset, remoteOptions) }, }) etl.use( loadRdf(app.sources.inputFile), mapQuads( (quad, ctx) => ctx.store.quad( quad.subject, quad.predicate, quad.object, app.prefix.somePrefix(\"graph\") ) ), toRdf(app.destinations.dataset) ) Similarly, you can change all the subject, predicates or objects in your data. Also, you can choose to transform triples of a specific subject, predicate, object or graph name. in this case, you should use: mapQuads( (quad, ctx) => ctx.store.quad( quad.subject, app.prefix.example('new-predicate'), quad.object, quad.graph ), {predicate: app.prefix.example(\"old-predicate\")} )","title":"Transforming RDF data"},{"location":"triply-etl/transform/","text":"On this page: Transform Next steps Transform \u00b6 The transform step makes changes to the record : graph LR sources -- 1. Extract --> record record -- 2. Transform --> record record -- 3. Assert --> ld ld -- 4. Enrich --> ld ld -- 5. Validate --> ld ld -- 6. Publish --> destinations linkStyle 1 stroke:red,stroke-width:3px; destinations[(\"D. Destinations\\n(TriplyDB)\")] ld[C. Internal Store] record[B. Record] sources[A. Data Sources] If you do not have a stream of records yet, read the documentation for the Extract step first. Once you have a stream of records, the following transformations are typically needed: Values need to be mapped onto a prepared list of IRIs or literals (e.g. from country names to country-denoting IRIs). Values need to be translated into standards-compliant formats (e.g., from country name to ISO 3166 country codes). Multiple existing values need to be combined into one new value (e.g., street name and house number may be combined into an address). A single value needs to be split into multiple values (e.g., from 'apple, orange' to 'apple' and 'orange' ). Values need to be cleaned because they are dirty in the source (e.g., from '001 ' to 1 ). TriplyETL supports the following four transformation approaches: RATT transformations are a set of commonly used transformation functions that are developed and maintained by Triply. RML can be used to convert non-RDF data into RDF triples. TypeScript can be used to write new customer transformations. XSLT used to transform XML data through the definition of transformation rules in XSLT stylesheets. Next steps \u00b6 The Transform step results in a cleaned and enriched record. The following link documents how you can use the record to make linked data assertions: Step 3. Assert : uses data from the record to make linked data assertions in the internal store.","title":"Overview"},{"location":"triply-etl/transform/#transform","text":"The transform step makes changes to the record : graph LR sources -- 1. Extract --> record record -- 2. Transform --> record record -- 3. Assert --> ld ld -- 4. Enrich --> ld ld -- 5. Validate --> ld ld -- 6. Publish --> destinations linkStyle 1 stroke:red,stroke-width:3px; destinations[(\"D. Destinations\\n(TriplyDB)\")] ld[C. Internal Store] record[B. Record] sources[A. Data Sources] If you do not have a stream of records yet, read the documentation for the Extract step first. Once you have a stream of records, the following transformations are typically needed: Values need to be mapped onto a prepared list of IRIs or literals (e.g. from country names to country-denoting IRIs). Values need to be translated into standards-compliant formats (e.g., from country name to ISO 3166 country codes). Multiple existing values need to be combined into one new value (e.g., street name and house number may be combined into an address). A single value needs to be split into multiple values (e.g., from 'apple, orange' to 'apple' and 'orange' ). Values need to be cleaned because they are dirty in the source (e.g., from '001 ' to 1 ). TriplyETL supports the following four transformation approaches: RATT transformations are a set of commonly used transformation functions that are developed and maintained by Triply. RML can be used to convert non-RDF data into RDF triples. TypeScript can be used to write new customer transformations. XSLT used to transform XML data through the definition of transformation rules in XSLT stylesheets.","title":"Transform"},{"location":"triply-etl/transform/#next-steps","text":"The Transform step results in a cleaned and enriched record. The following link documents how you can use the record to make linked data assertions: Step 3. Assert : uses data from the record to make linked data assertions in the internal store.","title":"Next steps"},{"location":"triply-etl/transform/ratt/","text":"On this page: RATT transformations Overview addHashedIri() Signature Parameters When to use? Example: lazy identifiers Example: dynamic IRI prefix Example: statement reification addIri() Signature Parameters When to use? See also Example: Prefix declaration and local name Example: Absolute IRI addLiteral() When to use Parameters See also Example: Typed literal Example: String literal Example: Language-tagged string Example: Validate usage addSkolemIri() See also Example addTag() Parameters Throws See also Example addValue() Description Parameters Example Example capitalize() Parameters Example: Class IRIs concat() Description Parameters Example copy() Parameters Example Example encodeHtml() Description Parameters Example geojsonToWkt() Parameters GeoJSON and Well-Known Text (WKT) See also Example jpath() Description Use cases Parameters Example lowercase() Description Use cases Parameters Example padEnd() Description Use cases Parameters Example padStart() Description Use cases Example: Fixed-length identifiers Example: Create year literals replace() Description Parameters Example split() Description Whitespace handling Empty values Use cases Parameters Example: Multiple values in singular table cells Example: Split a complex string into components substring() Description Parameters Example translateAll() Description When to use? Parameters Example: Map source data to IRI values Example: Map source data to IRI properties translateSome() Description Parameters Use cases Example tryLiteral() Description Throws Example See also uppercase() Description Parameters Example wkt.addPoint() Description Parameters Example wkt.project() Description Parameters Example RATT transformations \u00b6 RATT transformations are a core set of functions that are commonly used to change the content of TriplyETL Records. RATT transformations started out as TypeScript transformations that turned out to be useful in a wide variety of TriplyETL pipelines. Triply maintains this core set of transformation functions to allow new ETLs to make use of off-the-shelf functionality that has proven useful in the past. Overview \u00b6 The following transformation functions are currently available: Function Description addHashedIri() Creates an IRI with a content-based hash as the local name. addIri() Create a new IRI based on a prefix and a local name. addLiteral() Create a new literal based on a lexical for and a datatype IRI or language tag. addSkolemIri() Create a new IRI with a random local name, which advertises that it may be consistently replaced with blank nodes. addTag() Create a language tag. addValue() Create a TypeScript value. capitalize() Transforms a string value to its capitalized variant. concat() Combine multiple strings into a new string. copy() Copy a value from an old into a new key. decodeHtml() Decode HTML entities that occur in strings. geojsonToWkt() Change GeoJSON strings to WKT strings. jpath() Uses the JSONPath query language to select a value from the record. lowercase() Change strings to their lowercase variants. padEnd() Pad the end of strings. padStart() Pad the start of strings. replace() Replace part of a string. split() Split a string into multiple substrings. substring() Extract a substring from a string. translateAll() Translate all string values to other values. translateSome() Translate some string values to other strings. tryLiteral() Create literals for which the datatype is not know beforehand. uppercase() Change a string to its uppercase variant. wkt.addPoint() Add a geospatial point using the Well-Known Text (WKT) format. wkt.project() Change the projection of a Well-Known Text (WKT) literal from from Coordinate Reference System into another. addHashedIri() \u00b6 Creates an IRI with a content-based hash as the local name. Signature \u00b6 This function has the following signature: addHashedIri({ prefix, content, key }) Parameters \u00b6 prefix is a dynamic or static IRI. content is an array with static value, or a key that contains a dynamic arrays of values. key is a new key where the created IRI is stored. When to use? \u00b6 This function is used under the following circumstances: Something must be identified with an IRI. The thing that must be identified does not have a readily available identifier. The thing that must be identified does have properties whose combination of values is unique, and can therefore act as a composite identifier. This is called a composite key in database theory. The composed URL is not allowed to be skolemised anymore, e.g. the prefix cannot be an IRI where the pathname starts with /.well-known/genid/ . For this purpose, use transformation addskolemiri() instead. Benefits: The created IRIs are the same across different ETL runs over the same source data. This supports important use cases like change managements / versioning and graph signing. Downsides: It can take a lot of time to figure out which set of properties make every IRI unique. In database theory this process is known as composite key detection . Furthermore, keeping the list of identifying properties up-to-date over time poses a maintenance burden. Example: lazy identifiers \u00b6 Some source data does not include good identifiers for all data items. For example, the following source table contains first names and last names of persons, but neither property is by itself unique: First name Last name Jane Doe Jane Smith John Doe In such cases it may be an option to take a combination of columns, and use that combined sequence of columns for identification. The following snippet uses the combination of the first name and last name fields (in that order) to create a locally unique hash. Together with an IRI prefix, this can be used to create globally unique IRIs: fromJson([{ 'First name': 'John', 'Last name': 'Doe' }]), addHashedIri({ prefix: prefix.person, content: ['First name', 'Last name'], key: '_person', }), pairs('_person', [a, sdo.Person], [sdo.givenName, 'First name'], [sdo.familyName, 'Last name'], ), This results in the following linked data: person:70020947bea6c39cccea20d27e30fbdf a sdo:Person; sdo:givenName 'John'; familyName 'Doe'. Or diagrammatically: graph LR person -- a --> Person person -- sdo:givenName --> john person -- sdo:familyName --> doe Person[sdo:Person]:::model doe['Doe']:::data john['John']:::data person([person:70020947bea6c39cccea20d27e30fbdf]):::data classDef model fill:lightblue classDef meta fill:sandybrown Example: dynamic IRI prefix \u00b6 It is possible to specify a dynamic IRI prefix to addHashedIri() . The following code snippet uses a dynamic IRI prefix from the data source record: fromJson([ { prefix: Iri('https://triplydb.com/my-account/my-dataset/id/person/'), name: 'John Doe', }, { prefix: Iri('https://triplydb.com/my-account/my-dataset/id/product/'), name: '123', }, ]), addHashedIri({ prefix: 'prefix', content: ['name'], key: '_subject', }), triple('_subject', a, owl.NamedIndividual), This results in the following linked data: prefix person: <https://triplydb.com/my-account/my-dataset/id/person/> prefix product: <https://triplydb.com/my-account/my-dataset/id/product/> person:76f294ac31199b65ec25048439b66f78 a owl:NamedIndividual. product:9154deaa364b289c6b012e99f947f30e a owl:NamedIndividual. Example: statement reification \u00b6 The RDF standard allows individual statements to be identified by a node. This approach is called statement reification and can be used to assert metadata about statements or can represent modalities such as probability and belief. The following snippet uses addHashedIri() to create a unique identifier for each reified statement: fromJson([{ id: '1', name: 'John Doe' }]), // Step 1. Create the subject, predicate, and object terms. addIri({ prefix: prefix.person, content: 'id', key: 'subject' }), addIri({ prefix: prefix.def, content: str('name'), key: 'predicate' }), addLiteral({ content: 'name', key: 'object' }), // Step 2. Create the triple statement. triple('subject', 'predicate', 'object'), // Step 3. Create the reified statement. addHashedIri({ prefix: prefix.statement, content: ['subject', 'predicate', 'object'], key: 'statement', }), pairs('statement', [a, rdf.Statement], [rdf.subject, 'subject'], [rdf.predicate, 'predicate'], [rdf.object, 'object'], ), This results in the following linked data: person:1 def:name 'John Doe'. statement:549decc4c44204a907aa32b4cc9bfaba a rdf:Statement; rdf:subject person:1; rdf:predicate def:name; rdf:object 'John Doe'. Or diagrammatically: graph TB person --- name name --> johndoe statement -- a --> Statement statement -- rdf:subject --> person statement -- rdf:predicate --> name statement -- rdf:object --> johndoe Statement[rdf:Statement]:::model person([person:1]):::data name[def:name]:::model johndoe([John Doe]):::data statement([statement:549decc4c44204a907aa32b4cc9bfaba]):::meta classDef model fill:lightblue classDef meta fill:sandybrown addIri() \u00b6 Creates an IRI based on a specified local name. Signature \u00b6 addIri({ prefix, content, key }) Parameters \u00b6 prefix Optionally, a static or dynamic IRI. This IRI will appear before the local name specified for the content parameter. If the prefix parameter is absent, parameter content is must contain an absolute IRI. content A string, or a key that contains a string. If the prefix parameter is specified, content specifies the IRI local name that appears after the IRI prefix. If the prefix argument is not specified, content is assumed to encode a full absolute IRI. key A new key where the created IRI is stored. When to use? \u00b6 This transformation can be used in the following two ways: 1. By using an IRI prefix and a local name. 2. By using a full absolute IRI. 3. The composed URL is not allowed to be skolemised anymore, e.g. the prefix cannot be an IRI where the pathname starts with /.well-known/genid/ . For this purpose, use transformation addskolemiri() instead. See also \u00b6 If the created IRI is used exactly once, it is often better to use inline function iri() instead. Example: Prefix declaration and local name \u00b6 The following snippet creates an IRI based on the specified IRI prefix and local name: addIri({ prefix: prefix.person, content: 'username', key: '_person', }), triple('_person', a, sdo.Person), This results in the following linked data assertions: person:johndoe a sdo:Person. Or diagrammatically: graph LR johndoe -- a --> Person Person[sdo:Person]:::model johndoe(person:johndoe):::data The following snippet makes the same assertion, but uses assertion iri() instead of transformation addIri() : triple(iri(prefix.person, 'username'), a, sdo.Person), Example: Absolute IRI \u00b6 The following snippet creates the same IRI, but does not use a predefined prefix IRI: addIri({ content: 'https://example.com/id/person/johndoe', key: '_person', }), triple('_person', a, sdo.Person), Or diagrammatically: graph LR johndoe -- a --> Person Person[sdo:Person]:::model johndoe(https://example.com/id/person/johndoe):::data The following snippet uses assertion iri() instead of transformation addIri() : triple(iri('https://example.com/id/person/johndoe'), a, sdo.Person), addLiteral() \u00b6 Creates an new literal and adds it to the Record under the specified key. This transformation can be used in the following 3 ways: If a datatype (key: datatype ) is given, a typed literal is created. If a language tag (key: languageTag ) is given, a language-tagged string (datatype rdf:langString ) is created. If neither a datatype not a language tag is given, a literal with datatype xsd:string is created. When to use \u00b6 This transformation is typically used when: The same literal occurs in two or more statement assertions (function triple() or quad() ). This avoids having to specify the same literal multiple times using function literal() . The datatype or language tag is derived from the source data record. Parameters \u00b6 content A key that contains a string value, or a string specified with function str() . datatype Optionally, a key that stores an IRI or a static IRI. language Optionally, a language tag from the lang object, or a key that stores such a language tag. validate Optionally provide a single validator condition or an array of validator conditions that the literal content should hold to, will return a boolean and throw and error when a validator condition does not hold. key A new key where the created literal is stored. See also \u00b6 If the created literal is used exactly once, it is often better to use the inline function literal() instead. Example: Typed literal \u00b6 The following snippet asserts a triple with a typed literal with datatype IRI xsd:date : fromJson([{ id: '123', date: '2022-01-30' }]), addLiteral({ content: 'date', datatype: xsd.date, key: '_dateCreated', }), triple(iri(prefix.book, 'id'), sdo.dateCreated, '_dateCreated'), This makes the following linked data assertion: book:123 sdo:dateCreated '2022-30-01'^^xsd:date. Notice that the same linked data could have been asserted with the following use the the literal() assertion middleware: fromJson([{ id: '123', date: '2022-01-30' }]), triple(iri(prefix.book, 'id'), sdo.dateCreated, literal('date', xsd.date)), Example: String literal \u00b6 The following snippet asserts a triple with a string literal in the object position: fromJson([{name: 'London'}]), addLiteral({ content: 'name', key: '_name', }), triple(iri(prefix.city, '_name'), skos.prefLabel, '_name') This makes the following assertion: city:London sdo:name 'London'. The literal 'London' has type xsd:string . This is the standard datatype IRI for typed literals in the linked data languages (i.e. Turtle, TriG, and SPARQL). Notice that the same linked data could have been asserted with the following snippet, where the string value 'London' is automatically cast into a string literal: fromJson([{ name: 'London' }]), triple(iri(prefix.city, 'name'), skos.prefLabel, 'name'), Example: Language-tagged string \u00b6 The following snippet asserts a triple with a language-tagged string in the object position: fromJson([{ name: 'London' }]), addLiteral({ content: 'name', language: language.en, key: '_name', }), triple(iri(prefix.city, 'name'), skos.prefLabel, '_name'), This results in the following linked data assertion: city:London skos:prefLabel 'London'@en. Notice that the same linked data could have been asserted with the following use the the literal() assertion middleware: fromJson([{ name: 'London' }]), triple(iri(prefix.city, 'name'), skos.prefLabel, literal('name', lang['en'])), Example: Validate usage \u00b6 The following snippet asserts a triple of a person's email, with the email address being validated in the object position, and should throw an error when the record contains an invalid email address: fromJson([{ name: \"John\", email: 'john.appleseed@example.com' }, {name: 'NA', email: 'notAnEmail' } ]), addLiteral({ content: 'email', validate: isEmail(), key: '_email', }), triple(iri(prefix.person, 'name'), foaf.mbox, '_email'), This results in the following error for the second record: ERROR (Record #2) String \"notAnEmail\" is not an email address. Notice that when using only correct email addresses: fromJson([{ name: \"John\", email: 'john.appleseed@example.com' }, { name: \"Lisa\", email: 'lisa.appleseed@example.com' } ]), addLiteral({ content: 'email', validate: isEmail(), key: '_email', }), triple(iri(prefix.person, 'name'), foaf.mbox, '_email'), It results in the following correct linked data assertion: person:John foaf:mbox \"john.appleseed@example.com\" person:Lisa foaf:mbox \"lisa.appleseed@example.com\" addSkolemIri() \u00b6 Creates a globally unique IRI that is intended to be used as a blank node identifier. Blank nodes are nodes without identification. It relatively difficult to work which such nodes in graph data, since they cannot be identified or dereferenced online. For this reason TriplyETL uses Skolem IRIs to denote blank nodes. This allows blank nodes to be identified and dereferenced. This Skolemization approach is part of the RDF standard. Skolem IRIs are random IRIs whose root path starts with .well-known/genid/ . This makes it easy to distinguish them from other random IRIs that are not used to denote blank nodes. prefix A IRI or a key that contains an IRI whose path starts with .well-known/genid/ . key A new key where the created IRI is stored. See also \u00b6 Tne Skolemization section in the RDF standard explains what Skolem IRIs are and how they should be used. Example \u00b6 The following snippet uses a hashed IRI to create a predictable identifier for a geospatial feature, and a Skolem IRI to create an unpredictable identifier for the geometry. The snippet includes the prefix declarations to illustrate that the path of the Skolem IRI must start with .well-known/genid. . const base = 'https://example.com/' const prefix = { feature: declarePrefix(base('id/feature/')), skolem: declarePrefix(base('.well-known/genid/')), } // Etc fromJson([{ point: 'Point(1.1 2.2)' }]), addHashedIri({ prefix: prefix.feature, content: 'point', key: '_feature', }), addSkolemIri({ prefix: prefix.skolem, key: '_geometry', }), triple('_feature', geo.hasGeometry, '_geometry'), triple('_geometry', geo.asWKT, literal('point', geo.wktLiteral)), This results in the following linked data assertions: feature:22238008e490f725979118f8f2dd9b5a geo:hasGeometry skolem:0cf4b63252a0476a8afc20735aa03da6. skolem:0cf4b63252a0476a8afc20735aa03da6 geo:asWKT 'Point(1.1 2.2)'^^geo:wktLiteral. Notice that the feature IRI will be the same across ELT runs if the source data stays the same, but the Skolem will always be different. Since the Skolem IRIs can be identified by the start of their path ( .well-known/genid/ ), the same linked data assertions can be displayed as follows: feature:22238008e490f725979118f8f2dd9b5a geo:hasGeometry [ geo:asWKT 'Point(1.1 2.2)'^^geo:wktLiteral ]. addTag() \u00b6 This middleware creates a language tag based on a given string value. Parameters \u00b6 content A string value that encodes a language tag according to the IANA language subtag registry. key A new key where the language tag will be stored. Throws \u00b6 An error is emitted if the given string value does not follow the language tag format, or denotes a language tag that is not currently registered. See also \u00b6 The language tag format is defined in the IETF BCP 47 standard (RFC 5646) . Language tags are registered in the IANA language subtag registry . Example \u00b6 The following snippet created a language tag for the Dutch language as spoken in The Netherlands, and uses it to assert a language-tagged string: fromJson([{ label: 'Amsterdam' }]), addTag({ content: 'nl-nl', key: 'lang', }), triple(iri(prefix.city, 'label'), rdfs.label, literal('label', 'lang')), addValue() \u00b6 This middleware allows any value to be added to the Record. Description \u00b6 This middleware is useful for data that is not present in the source data record, but must be used in one or more assertions. Parameters \u00b6 content Any value that can be represented in TypeScript. key A new key where the value is stored. Example \u00b6 The following snippet starts out with an empty source record ( {} ), and adds a new data key to it. The added value is an array that contains a string and a number (in that order). This new value is used in the triple assertion, where 'data[0]' extracts the string element and 'data[1]' extracts the number elements. fromJson([{}]), addValue({ content: ['johndoe', 22], key: 'data', }), triple(iri(prefix.person, 'data[0]'), foaf.age, 'data[1]'), This results in the following linked data assertion: person:johndoe foaf:age 22. Example \u00b6 The following snippet adds a key called _startDate that either contains the start date as specified in the data source record, or the value 'unknown' : fromJson([ { id: '123', start: '2022-02-12' }, { id: '456' }, ]), ifElse({ if: 'start', then: addLiteral({ content: 'start', datatype: xsd.date, key: '_start', }), }, { else: addValue({ content: 'unknown', key: '_start', }), }), triple(iri(prefix.event, 'id'), sdo.startDate, '_start'), This results in the following linked data assertions: event:123 sdo:startDate '2022-02-12'^^xsd:date. event:456 sdo:startDate 'unknown'. capitalize() \u00b6 Transforms a string value to its capitalized variant. If the first character of a string has an uppercase variant, then that variant is used. If the first character does not have an uppercase variant -- because the character is already uppercase or is a punctuation character -- then the string remains unchanged. This transformation can uppercase the first character in any language; the Unicode Default Case Conversion algorithm is used. Parameters \u00b6 content A key that contains a string value. key A new key where the capitalized result is stored. Example: Class IRIs \u00b6 According to convention, classes in linked data are denoted by IRIs whose local name starts with a capital letter. The following source data contains nice values for the type key, but they do not start with a capital letter yet. The following snippet capitalizes the values of the type keys, and uses them to create class IRIs. fromJson([ { id: '1', type: 'location' }, { id: '2', type: 'person' }, ]), capitalize({ content: 'type', key: '_type', }), triple(iri(prefix.id, 'id'), a, iri(prefix.def, '_type')), This results in the following linked data assertions: id:1 a def:Location. id:2 a def:Person. concat() \u00b6 Description \u00b6 Concatenates an array of strings into one new string. An optionally specified separator is placed in between every two consecutive string values. Parameters \u00b6 content An array of key that contain a string and/or strings specified with assertion str() . separator Optionally, the string that is places between every two consecutive string values. key A new key where the concatenated string is stored. Example \u00b6 The following snippet concatenates the first and last name of a person (in that order), using a space separator. fromJson([{ id: '1', first: 'John', last: 'Doe' }]), concat({ content: ['first', 'last'], separator: ' ', key: '_name', }), triple(iri(prefix.person, 'id'), foaf.name, '_name'), This results in the following linked data assertion: person:1 foaf:name 'John Doe'. copy() \u00b6 Makes a plain copy from the value stored in the given key to a new key. Parameters \u00b6 content A value of any type, or a key that contains a value of any type. type Optionally, the name of the TypeScript type of the value. The default value is 'string' . key A new key where the plain copy is stored. Example \u00b6 Plain copies can be used to abbreviate long keys, especially in tree-shaped data like JSON or XML. In the following example, values stored in a long nested key are copies into a short and descriptive key. This is even more useful if the key is used many times in the script. copy({ content: 'record[0].family[0].children.child[0].id.$text', key: 'childId', }), Example \u00b6 Since plain copies introduce a new name for an existing value, the new name can be used to store extra information about the value. The following example stores an English name, if available; or a Dutch name, if available; or no name at all. This is a relatively complex example that can only be accomplished by copying the names for the encountered languages under descriptive key names. fromJson([ { id: '1', names: [ { language: 'en', value: 'London' }, { language: 'fr', value: 'Londres' }, { language: 'nl', value: 'Londen' }, ], }, { id: '2', names: [ { language: 'fr', value: 'Paris' }, { language: 'nl', value: 'Parijs' }, ], }, ]), forEach('names', [ _switch('language', // Plain copy of the English label, if available. ['en', copy({ content: 'value', key: '$parent.en' })], // Plain copy of the Dutch label, if available. ['nl', copy({ content: 'value', key: '$parent.nl' })], ), ]), ifElse({ // Prefer an English label over a Dutch label. if: 'en', then: copy({ content: 'en', key: '_preferredName' }), }, { // If there is no English label, a Dutch label is a second-best option. if: 'nl', then: copy({ content: 'nl', key: '_preferredName' }), }), // If there is either an English or a Dutch label, assert it. when('_preferredName', [ triple(iri(prefix.city, 'id'), rdfs.label, '_preferredName'), ]), This results in the following linked data assertions: city:1 rdfs:label 'London'. city:2 rdfs:label 'Parijs'. encodeHtml() \u00b6 Description \u00b6 This transformation decodes any HTML entities that appear in a given key. The following HTML entities are common in source data: HTML entity Decoded &amp; & &gt; > &lt; < You do not need to use this transformation if you want to assert literals with datatype IRI rdf:HTML . HTML entities are meaningful in HTML, so there they should be preserved. Parameters \u00b6 content A key in the Record that contains string values with HTML entities. key A new key where the decoded content is stored. Example \u00b6 The following snippet takes HTML texts from the source data and asserts them as regular text literals. Since HTML entities are meaningless in regular text, decodeHtml is used to denote these entities. fromJson([ { id: '1', label: 'A&amp;B' }, { id: '2', label: '1 &lt; 2' }, ]), decodeHtml({ content: 'label', key: '_label', }), triple(iri(prefix.id, 'id'), rdfs.label, '_label'), This results in the following linked data assertions: id:1 rdfs:label 'A&B'. id:2 rdfs:label '1 < 2'. geojsonToWkt() \u00b6 Transforms GeoJSON objects to their corresponding Well-Known Text (WKT) serialization strings. Parameters \u00b6 content A key that stores a GeoJSON object. crs Optionally, an IRI that denotes a Coordinate Reference System (CRS). You can use IRIs from the epsg object. If absent, uses https://epsg.io/4326 as the CRS. key A new key where the WKT serialization string is stored GeoJSON and Well-Known Text (WKT) \u00b6 According to the GeoJSON standard , the only Coordinate Reference System (CRS) that is allowed to be used is EPSG:4326/WGS84. In practice, source data sometimes (incorrectly) stores GeoJSON formatted data in other CRSes. An example of this is the GISCO dataset of the European Union, which uses the EPSG:3857 CRS. For cases like these, the optional crs parameter comes in handy. See also \u00b6 The GeoJSON format is standardized in RFC 7946 . The Well-Known Text (WKT) serialization format is standardized as part of ISO/IEC 13249-3:2016 standard . Example \u00b6 The following snippet converts GeoJSON objects that denote traffic light locations to their GeoSPARQL representation. fromJson([ { id: '123', geometry: { type: 'Point', coordinates: [6.256, 48.480], }, }, ]), addIri({ prefix: prefix.feature, content: 'id', key: '_feature', }), geojsonToWkt({ content: 'geometry', crs: epsg[3857], key: '_wkt', }), addHashedIri({ prefix: prefix.geometry, content: '_wkt', key: '_geometry' }), pairs('_feature', [a, def.TrafficLight], [geo.hasGeometry, '_geometry'], ), pairs('_geometry', [a, geo.Geometry], [geo.asWKT, literal('_wkt', geo.wktLiteral)], ), This results in the following linked data assertions: feature:123 a def:TrafficLight; geo:hasGeometry geometry:197e6376c2bd8192c24911f88c330606. geometry:197e6376c2bd8192c24911f88c330606 a geo:Geometry; geo:asWKT 'Point(6.256 48.480)'^^geo:wktLiteral. Or diagrammatically: graph LR feature -- a --> TrafficLight feature -- geo:hasGeometry --> geometry geometry -- a --> Geometry geometry -- geo:asWKT --> wkt Geometry[geo:Geometry]:::model TrafficLight[def:TrafficLight]:::model feature(feature:123):::data geometry(geometry:197e6376c2bd8192c24911f88c330606):::data wkt(\"'Point(6.256 48.480)'^^geo:wktLiteral\"):::data jpath() \u00b6 Description \u00b6 Filters a value based on a JSON Path expression. JSON Path is a query language for JSON. For the syntax of JSON Path expressions, please visit the JSON Path documentation page . Use cases \u00b6 This function simplifies the complex key specification to filter specific values. It can only be used for an object of a triple to create a literal. The result of a function must have a fundamental type. Parameters \u00b6 value A JSON Path expression. Example \u00b6 The following examples will create a literal based on key value : If key 'ISO_639-2' exists: fromJson({ language: [ { 'ISO_639-1': 'en', lcid: 2057, value: 'Paris' }, { 'ISO_639-1': 'nl', 'ISO_639-2': 'nld', lcid: 1043, value: 'Parijs' }, ], }), triple( iri(prefix.city, '$recordId'), rdfs.label, literal(jpath(\"$.language[?(@['ISO_639-2'])].value\"), language.nl) ), If key 'ISO_639-1' is equal to nl : fromJson({ language: [ { 'ISO_639-1': 'en', lcid: 2057, value: 'Paris' }, { 'ISO_639-1': 'nl', 'ISO_639-2': 'nld', lcid: 1043, value: 'Parijs' }, ], }), triple( iri(prefix.city, '$recordId'), rdfs.label, literal(jpath(\"$.language[?(@['ISO_639-1'] =='nl')].value\"), language.nl) ), If key 'lcid' is lower than 1,100: fromJson({ language: [ { 'ISO_639-1': 'en', lcid: 2057, value: 'Paris' }, { 'ISO_639-1': 'nl', 'ISO_639-2': 'nld', lcid: 1043, value: 'Parijs' }, ], }), triple( iri(prefix.city, '$recordId'), rdfs.label, literal(jpath('$.language[?(@.lcid < 1100)].value'), language.nl) ), All three examples generate the following linked data: record:1 rdfs:label 'Parijs'@nl. lowercase() \u00b6 Description \u00b6 Translates a string value to its lowercase variant. This middleware can lowercase strings in any language; the Unicode Default Case Conversion algorithm is used. Use cases \u00b6 Older data formats sometimes use uppercase letters for header names or codes. The lowercase transformation middleware may be used to change such string values to lowercase. Parameters \u00b6 content A key that contains a string value. key A new key where the lowercase variant is stored. Example \u00b6 The following snippet starts out with header values that use uppercase characters exclusively. The lowerCase transformation is used to create lowercase names that can be used to create property IRIs. fromJson([ { from: '1', rel: 'PARENT', to: '2' }, { from: '2', rel: 'CHILD', to: '1' }, ]), lowercase({ content: 'rel', key: '_relationship', }), triple( iri(prefix.id, 'from'), iri(prefix.def, '_relationship'), iri(prefix.id, 'to'), ), This results in the following linked data assertions: id:1 def:parent id:2. id:2 def:child id:1. padEnd() \u00b6 Description \u00b6 Adds a given padding string zero or more times to the end of a string value, until the resulting string value is exactly a given number of characters long. Use cases \u00b6 This transformation is useful for identifiers that must have fixed length and that may be suffixed by zero's. Parameters \u00b6 content A key that contains a string value. If the key contains a numeric value, that value is first cast to string. padString The string that is added to the end of the string value in key content , until the result string has exactly targetLength characters. Can be a static string or a key. targetLength The exact number of characters that the resulting string should have. The string value is copied over as-is when targetLength is smaller than or equal to the length of the string value in key content . This includes cases where targetLength is negative or zero. key A new key where the padded string is stored. Example \u00b6 The following snippet processes identifiers of varying length, and ensures that they have the same length after suffixing '0' characters. fromJson([ { id: '16784' }, { id: '129' }, ]), padEnd({ content: 'id', padString: '0', targetLength: 6, key: '_id', }), This results in the following two Records: [ { \"id\": \"16784\", \"_id\": \"167840\" }, { \"id\": \"129\", \"_id\": \"129000\" } ] padStart() \u00b6 Description \u00b6 Adds a given padding string zero or more times in front of a string value, until the resulting string value is exactly a given number of characters long. Use cases \u00b6 This transformation is useful for identifiers that must have fixed length and that may be prepended by zero's. If key content contains a numeric value, then that value is first cast to string. content A key that contains a string value. padString The string that is added in front of the string value in key content , until the result string has exactly targetLength characters. targetLength The exact number of characters that the resulting string should have. The string value is copied over as-is when targetLength is smaller than or equal to the length of the string value in key content . This includes cases where targetLength is negative or zero. key A new key where the lowercased string is stored. Example: Fixed-length identifiers \u00b6 The following snippet processes identifiers of varying length, and ensures that they have the same length after prefixing '0' characters. fromJson([ { id: '16784' }, { id: '129' }, ]), padStart({ content: 'id', padString: '0', targetLength: 6, key: '_id', }), This results in the following two records: [ { \"id\": \"16784\", \"_id\": \"016784\" }, { \"id\": \"129\", \"_id\": \"000129\" } ] Example: Create year literals \u00b6 In order to create standards-conforming temporal literal, we need to pad the year component to be at least 4 decimal digits long. (This requirement is defined in the XML Schema Datatypes 1.1: Part 2 Datatypes standard.) Suppose that the source data looks as follows: Artifact Year 0001 612 0002 1702 We can ensure that all years have at least 4 decimal digits by calling the following function: padStart({ content: 'Year', padString: '0', targetLength: 4, key: '_lexicalForm', }), triple( iri(prefix.id, 'Artifact'), dct.created, literal('_lexicalForm', xsd.gYear), ), This makes the following linked data assertions: id:0001 dct:created '0612'^^xsd:gYear. id:0002 dct:created '1702'^^xsd:gYear. replace() \u00b6 Description \u00b6 Performs a regular expression replacement to the given input string, and stores the result in a new key. Parameters \u00b6 content A key that contains a string value, or a static string specified with assertion str() . from A JavaScript Regular Expression . to Optionally, a string that replaces potential matches of the Regular Expression ( from ). Use $1 , $2 , etc. to insert matches. If absent, the empty string is used. key A new key where the result of the replacement is stored. Example \u00b6 Suppose the source data contains date/time strings, but only the date component is needed: { \"created\": \"2020-01-02T00:00:00.0Z\" } It is possible to extract only the date part (everything up to the T ) in the following way: replace({ content: 'created', from: /^([^T]*).*$/, to: '$1', key: '_created', }), triple('_creativeWork', dct.created, literal('_created', xsd.date)), This results in the following Record: { \"created\": \"2020-01-02T00:00:00.0Z\", \"_created\": \"2020-01-02\" } split() \u00b6 Description \u00b6 Splits a string into an array of strings, and stores that array in a new key. Whitespace handling \u00b6 This transformation removes any trailing whitespace that remains after the strings are split. This ensures that irregular use of whitespace in the source data is taken care of automatically. Empty values \u00b6 This transformation removes any elements of the splitted string that are empty (after trimmimng). To keep empty entries, use the `` flag. Use cases \u00b6 The transformation is used when: - Tabular source data encodes multiple values inside singular cells. (Such concatenated storage inside cells is a data quality issue, because the table format cannot guarantee that the separator character does not (accidentally) occur inside individual values inside a cell. Tree-shaped source formats are able to store multiple values for the same key reliably, e.g. JSON and XML.) - Source data contains complex string values that can be decomposed into stand-alone components with distinct meaning. Parameters \u00b6 content A key that stores a string, or a string specified with assertion str() . separator A string or a regular expression that is used to separate the content. key A new key where the array of split strings is stored. keepEmptyEntities A boolean flag indicating if the empty values of a splitted string should be kept or not. By default empty values are removed. Example: Multiple values in singular table cells \u00b6 Tabular formats are unable to store more than one value in a cell. Because of this limitation, tabular data sources sometimes encode multiple values in cells by encoding them into one string. A separator character is typically used to distinguish between these multiple values. Suppose that the source data looks as follows: Parent Child John Jane, Jake , Kate ,, The following transformation splits the cells that encode zero or more children for each parent: split({ content: 'Child', separator: ',', key: 'Children', }), This results in the following transformed records: { \"Parent\": \"John\", \"Child\": \"Jane, Jake , \", \"Children\": [ \"Jane\", \"Jake\" ] } { \"Parent\": \"Kate\", \"Child\": \",, \", \"Children\": [] } Notice that trailing whitespace and empty values are dealt with automatically. Since the split() transformation always results in an array of strings, we can use the term assertion iris() afterwards: split({ content: 'children', separator: ',', key: '_children', }), triple( iri(prefix.person, 'parent'), sdo.children, iris(prefix.person, '_children') ), This results in the following linked data assertions: person:johndoe sdo:children person:janedoe, person:jakedoe. Example: Split a complex string into components \u00b6 The following snippet uses a regular expression to split a KIX code. (A KIX code is a standardized format for representing postal addresses in The Netherlands.) fromJson([{ id: '1', KIX: '1231FZ13Xhs' }]), split({ content: 'KIX', separator: /^(\\d{4}[A-Z]{2})(\\d{1,5})(?:X(.{1,6}))/, key: 'KIX_components', }), triple(iri(prefix.id, 'id'), sdo.postalCode, 'KIX_components[1]'), This results in the following record: { \"id\": \"1\", \"KIX\": \"1231FZ13Xhs\", \"KIX_components\": [\"\", \"1231FZ\", \"13\", \"hs\", \"\"] } And in the following linked data assertion: id:1 sdo:postalCode '1231FZ'. substring() \u00b6 Description \u00b6 This middleware takes a substring from the input string and stores the result in a new key. Parameters \u00b6 content A key that stores a string value, or a string specified with assertion str() . start The index of the first character that is included in the substring. The first character has index 0. end Optionally, the index of the first character that is excluded from the substring. If absent, the substring ends at the end of the source string. key The new key in which the substring is stored. Example \u00b6 The Library of Congress MARC format stores the type of record in the sixth character that appears in the leader key. We use substring() to extract this character, and then use transformation translateAll() to map them to a corresponding class IRI: substring({ content: 'metadata.marc:record.marc:leader.$text', start: 6, end: 7, key: '_typeOfRecord', }), translateAll({ content: '_typeOfRecord', table: { a: dcm.Text, k: dcm.StillImage, }, key: '_class', }), triple('_iri', a, '_class'), translateAll() \u00b6 Description \u00b6 Translates all dynamic strings from a specific key to new values of an arbitrary type To , according to a specified translation table. Since this function translates all values, the mapped values can have any type T ; they do not need to be strings. For example, this allows strings to be translated to IRIs or to literals. When to use? \u00b6 This approach is used when: The set of source data values is small. The set of source data values is known ahead of time. The corresponding linked data terms are known ahead of time. The appearance of a new value is considered to be an error in the source data. Parameters \u00b6 content A key that contains a string value. table A translation table from strings to values of some arbitrary type T . nulls Optionally, a list of string values that are considered denote NULL values in the source data. When a NULL value is encountered, the special value undefined is added for the target key . default Optionally, a default value or a default value-determining function that is used for string values that are neither in the translation table ( table ) nor in the NULL values list ( nulls ). The function must return a value of type T . Use of a default value value is equivalent to using the following value-determining function: _ => value . key A new key where the results of the translation are stored. Example: Map source data to IRI values \u00b6 Suppose that source data contains country names. In linked data we want to use IRIs to denote countries, so that we can link additional information. Since the list of countries that appears in the source data is not that long, we can specify a translation table from names to IRIs by hand: change.translateAll({ content: 'country', table: { 'Belgium': country.be, 'Germany': country.de, 'England': country.gb, ..., }, nulls: ['Unknown'], key: '_country', }), when('country', [ triple('_country', a, sdo.Country), ]), Example: Map source data to IRI properties \u00b6 When we relate a creative work to its creator, we sometimes know whether the creator was the actor, architect, author, etc. of the creative work. But in other cases we only know that there is a generic creator relationship. The Library of Congress Relators vocabulary allows us to express specific and generic predicates of this kind. transform.translateAll({ table: { 'actor': rel.act, 'architect': rel.arc, 'author': rel.aut, ..., }, default: rel.oth, // generic relator key: '_relator', }), triple('_creativeWork', '_relator', '_creator'), translateSome() \u00b6 Description \u00b6 Translates some strings, according to the specified translation table, to other strings. Strings that are not translated according to the translation table are copied over as-is. Parameters \u00b6 content A key that contains a string value. table A translation table that specifies translations from strings to strings. key A new key where the translated strings are stored. Use cases \u00b6 Source data often contains some strings that are correct and some that are incorrect. For example, if source data contains a key with city names, some of the names may be misspelled. In such cases, translateSome() can be used to translate the incorrect strings into correct ones. A translateSome() transformation is often performed directly before a translateAll() transformation. The former ensures that all string values are correct (e.g. fixing typo's in city names); the latter ensures that all strings are mapped onto IRIs (e.g. city names mapped onto city-denoting IRIs). Example \u00b6 The following example fixes an encoding issue that occurs in the source data: transform.translateSome({ content: 'name', table: { 'Frysl\ufffd\ufffdn': 'Frysl\u00e2n', // Other entries for typographic fixes go here. ..., }, key: '_name', }), tryLiteral() \u00b6 Description \u00b6 This transformation is used when string values must be mapped onto literals with varying datatype IRIs. The datatype IRIs that could apply are specified in a list. The specified datatype IRIs are tried out from left to right. The first datatype IRI that results in a valid literal is chosen. content A key that contains a string value, or a string value specified with assertion str() . datatypes An array of two or more datatype IRIs. key A new key where the created literal is stored. Throws \u00b6 An exception is emitted if a string value does not belong to the lexical space of any of the specified datatype IRIs. Example \u00b6 A literal is valid if the given string value appears in the lexical space of a specific datatype IRI. This is best explained with an example: tryLiteral({ content: 'date', datatypes: [xsd.date, xsd.gYearMonth, xsd.gYear], key: '_publicationDate', }), Source data in key 'date' Result in key '_date' '1900-01-02' '1900-01-02'^^xsd:date '1900' '1900'^^xsd:gYear '02-01-1900' An error is emitted. If we do not want to emit errors for string values that cannot be satisfy any of the specified datatype IRIs, we may choose to include xsd.string as the last datatype IRI in the list. Do notice however that this will result in dates that cannot be compared on a timeline, since they were not transformed to an XSD date/time datatype. See also \u00b6 You only need to use tryLiteral() if the datatype IRI varies from record to record. If the datatype IRI is the same for every record, then the regular assertion function literal() should be used instead. uppercase() \u00b6 Description \u00b6 Translates a string value to its uppercase variant. This middleware can uppercase strings in any language; the Unicode Default Case Conversion algorithm is used for this. Parameters \u00b6 content A key that contains a string value. key A new key where the uppercase variant is stored. Example \u00b6 In the following example, the string in the key 'countryCode' becomes the uppercase string: fromJson({ place: 'Amsterdam', countryCode: 'nl' }), uppercase({ content: 'countryCode', key: '_countryCode' }), triple(iri(prefix.id, 'place'), iri(prefix.geonames, str('countryCode')), '_countryCode') This results in the following linked data assertion: city:Amsterdam geonames:countryCode \"NL\" wkt.addPoint() \u00b6 Description \u00b6 Creates a Well-Known Text (WKT) serialization string from the corresponding geospatial point. Parameters \u00b6 latitude A key or a string assertion ( str() ) with latitude. longitude A key or a string assertion ( str() ) with longitude. crs Optionally, an IRI that denotes a Coordinate Reference System (CRS). You can use IRIs from the epsg object. If absent, uses EPSG:4326/WGS84 as the CRS. key A new key where the WKT string is stored. Example \u00b6 The following example creates a WKT literal from the geo coordinates of Amsterdam: fromJson({ place: 'Amsterdam', lat: 52.37308, long: 4.89245 }), wkt.addPoint({ latitude: 'lat', longitude: 'long', key: '_point' }), triple(iri(prefix.city, 'place'), geo.asWKT, '_point'), This results in the following record of the key '_point' : { \"_point\": { \"termType\": \"Literal\", \"value\": \"Point (52.37308 4.89245)\", \"language\": \"\", \"datatype\": { \"termType\": \"NamedNode\", \"value\": \"http://www.opengis.net/ont/geosparql#wktLiteral\", \"validationStatus\": \"canonical\" }, \"validationStatus\": \"canonical\" } } And in the following linked data assertion: city:Amstedam geo:asWKT \"Point (52.37308 4.89245)\"^^geo:wktLiteral wkt.project() \u00b6 Description \u00b6 Converts the projection of a Well-Known Text (WKT) literal from one Coordinate Reference System to another one. Parameters \u00b6 content An array of keys or numbers. key A new key where the new projection is stored. fromCrs : an IRI that denotes a Coordinate Reference System (CRS) of the content . toCrs : Optionally, an IRI that denotes a Coordinate Reference System (CRS) we want to convert to. If absent, uses EPSG:4326/WGS84 as the CRS. Example \u00b6 The following example converts an array with latitude and longitude in content key from Dutch grid coordinates (Rijksdriehoeks-coordinates) to WGS84 coordinates. fromJson({ place: 'Amsterdam', lat: 121307, long: 487360 }), wkt.project({ content: ['lat', 'long'], key: '_coordinates', fromCrs: epsg[666], toCrs: epsg[4326] }), This results in the following record of the key '_coordinates' : { \"_coordinates\": [ 52.374671935135474, 4.892803721020475 ] } We can now use the converted result to create a WKT Point() using addPoint() : wkt.addPoint({ latitude: '_coordinates[0]', longitude: '_coordinates[1]', key: '_point' }), triple(iri(prefix.id, 'place'), geo.asWKT, '_point') This code snippet creates the following linked data assertion: city:Amstedam geo:asWKT \"Point (52.374671935135474 4.892803721020475)\"^^geo:asWKT","title":"RATT"},{"location":"triply-etl/transform/ratt/#ratt-transformations","text":"RATT transformations are a core set of functions that are commonly used to change the content of TriplyETL Records. RATT transformations started out as TypeScript transformations that turned out to be useful in a wide variety of TriplyETL pipelines. Triply maintains this core set of transformation functions to allow new ETLs to make use of off-the-shelf functionality that has proven useful in the past.","title":"RATT transformations"},{"location":"triply-etl/transform/ratt/#overview","text":"The following transformation functions are currently available: Function Description addHashedIri() Creates an IRI with a content-based hash as the local name. addIri() Create a new IRI based on a prefix and a local name. addLiteral() Create a new literal based on a lexical for and a datatype IRI or language tag. addSkolemIri() Create a new IRI with a random local name, which advertises that it may be consistently replaced with blank nodes. addTag() Create a language tag. addValue() Create a TypeScript value. capitalize() Transforms a string value to its capitalized variant. concat() Combine multiple strings into a new string. copy() Copy a value from an old into a new key. decodeHtml() Decode HTML entities that occur in strings. geojsonToWkt() Change GeoJSON strings to WKT strings. jpath() Uses the JSONPath query language to select a value from the record. lowercase() Change strings to their lowercase variants. padEnd() Pad the end of strings. padStart() Pad the start of strings. replace() Replace part of a string. split() Split a string into multiple substrings. substring() Extract a substring from a string. translateAll() Translate all string values to other values. translateSome() Translate some string values to other strings. tryLiteral() Create literals for which the datatype is not know beforehand. uppercase() Change a string to its uppercase variant. wkt.addPoint() Add a geospatial point using the Well-Known Text (WKT) format. wkt.project() Change the projection of a Well-Known Text (WKT) literal from from Coordinate Reference System into another.","title":"Overview"},{"location":"triply-etl/transform/ratt/#addhashediri","text":"Creates an IRI with a content-based hash as the local name.","title":"addHashedIri()"},{"location":"triply-etl/transform/ratt/#signature","text":"This function has the following signature: addHashedIri({ prefix, content, key })","title":"Signature"},{"location":"triply-etl/transform/ratt/#parameters","text":"prefix is a dynamic or static IRI. content is an array with static value, or a key that contains a dynamic arrays of values. key is a new key where the created IRI is stored.","title":"Parameters"},{"location":"triply-etl/transform/ratt/#when-to-use","text":"This function is used under the following circumstances: Something must be identified with an IRI. The thing that must be identified does not have a readily available identifier. The thing that must be identified does have properties whose combination of values is unique, and can therefore act as a composite identifier. This is called a composite key in database theory. The composed URL is not allowed to be skolemised anymore, e.g. the prefix cannot be an IRI where the pathname starts with /.well-known/genid/ . For this purpose, use transformation addskolemiri() instead. Benefits: The created IRIs are the same across different ETL runs over the same source data. This supports important use cases like change managements / versioning and graph signing. Downsides: It can take a lot of time to figure out which set of properties make every IRI unique. In database theory this process is known as composite key detection . Furthermore, keeping the list of identifying properties up-to-date over time poses a maintenance burden.","title":"When to use?"},{"location":"triply-etl/transform/ratt/#example-lazy-identifiers","text":"Some source data does not include good identifiers for all data items. For example, the following source table contains first names and last names of persons, but neither property is by itself unique: First name Last name Jane Doe Jane Smith John Doe In such cases it may be an option to take a combination of columns, and use that combined sequence of columns for identification. The following snippet uses the combination of the first name and last name fields (in that order) to create a locally unique hash. Together with an IRI prefix, this can be used to create globally unique IRIs: fromJson([{ 'First name': 'John', 'Last name': 'Doe' }]), addHashedIri({ prefix: prefix.person, content: ['First name', 'Last name'], key: '_person', }), pairs('_person', [a, sdo.Person], [sdo.givenName, 'First name'], [sdo.familyName, 'Last name'], ), This results in the following linked data: person:70020947bea6c39cccea20d27e30fbdf a sdo:Person; sdo:givenName 'John'; familyName 'Doe'. Or diagrammatically: graph LR person -- a --> Person person -- sdo:givenName --> john person -- sdo:familyName --> doe Person[sdo:Person]:::model doe['Doe']:::data john['John']:::data person([person:70020947bea6c39cccea20d27e30fbdf]):::data classDef model fill:lightblue classDef meta fill:sandybrown","title":"Example: lazy identifiers"},{"location":"triply-etl/transform/ratt/#example-dynamic-iri-prefix","text":"It is possible to specify a dynamic IRI prefix to addHashedIri() . The following code snippet uses a dynamic IRI prefix from the data source record: fromJson([ { prefix: Iri('https://triplydb.com/my-account/my-dataset/id/person/'), name: 'John Doe', }, { prefix: Iri('https://triplydb.com/my-account/my-dataset/id/product/'), name: '123', }, ]), addHashedIri({ prefix: 'prefix', content: ['name'], key: '_subject', }), triple('_subject', a, owl.NamedIndividual), This results in the following linked data: prefix person: <https://triplydb.com/my-account/my-dataset/id/person/> prefix product: <https://triplydb.com/my-account/my-dataset/id/product/> person:76f294ac31199b65ec25048439b66f78 a owl:NamedIndividual. product:9154deaa364b289c6b012e99f947f30e a owl:NamedIndividual.","title":"Example: dynamic IRI prefix"},{"location":"triply-etl/transform/ratt/#example-statement-reification","text":"The RDF standard allows individual statements to be identified by a node. This approach is called statement reification and can be used to assert metadata about statements or can represent modalities such as probability and belief. The following snippet uses addHashedIri() to create a unique identifier for each reified statement: fromJson([{ id: '1', name: 'John Doe' }]), // Step 1. Create the subject, predicate, and object terms. addIri({ prefix: prefix.person, content: 'id', key: 'subject' }), addIri({ prefix: prefix.def, content: str('name'), key: 'predicate' }), addLiteral({ content: 'name', key: 'object' }), // Step 2. Create the triple statement. triple('subject', 'predicate', 'object'), // Step 3. Create the reified statement. addHashedIri({ prefix: prefix.statement, content: ['subject', 'predicate', 'object'], key: 'statement', }), pairs('statement', [a, rdf.Statement], [rdf.subject, 'subject'], [rdf.predicate, 'predicate'], [rdf.object, 'object'], ), This results in the following linked data: person:1 def:name 'John Doe'. statement:549decc4c44204a907aa32b4cc9bfaba a rdf:Statement; rdf:subject person:1; rdf:predicate def:name; rdf:object 'John Doe'. Or diagrammatically: graph TB person --- name name --> johndoe statement -- a --> Statement statement -- rdf:subject --> person statement -- rdf:predicate --> name statement -- rdf:object --> johndoe Statement[rdf:Statement]:::model person([person:1]):::data name[def:name]:::model johndoe([John Doe]):::data statement([statement:549decc4c44204a907aa32b4cc9bfaba]):::meta classDef model fill:lightblue classDef meta fill:sandybrown","title":"Example: statement reification"},{"location":"triply-etl/transform/ratt/#addiri","text":"Creates an IRI based on a specified local name.","title":"addIri()"},{"location":"triply-etl/transform/ratt/#signature_1","text":"addIri({ prefix, content, key })","title":"Signature"},{"location":"triply-etl/transform/ratt/#parameters_1","text":"prefix Optionally, a static or dynamic IRI. This IRI will appear before the local name specified for the content parameter. If the prefix parameter is absent, parameter content is must contain an absolute IRI. content A string, or a key that contains a string. If the prefix parameter is specified, content specifies the IRI local name that appears after the IRI prefix. If the prefix argument is not specified, content is assumed to encode a full absolute IRI. key A new key where the created IRI is stored.","title":"Parameters"},{"location":"triply-etl/transform/ratt/#when-to-use_1","text":"This transformation can be used in the following two ways: 1. By using an IRI prefix and a local name. 2. By using a full absolute IRI. 3. The composed URL is not allowed to be skolemised anymore, e.g. the prefix cannot be an IRI where the pathname starts with /.well-known/genid/ . For this purpose, use transformation addskolemiri() instead.","title":"When to use?"},{"location":"triply-etl/transform/ratt/#see-also","text":"If the created IRI is used exactly once, it is often better to use inline function iri() instead.","title":"See also"},{"location":"triply-etl/transform/ratt/#example-prefix-declaration-and-local-name","text":"The following snippet creates an IRI based on the specified IRI prefix and local name: addIri({ prefix: prefix.person, content: 'username', key: '_person', }), triple('_person', a, sdo.Person), This results in the following linked data assertions: person:johndoe a sdo:Person. Or diagrammatically: graph LR johndoe -- a --> Person Person[sdo:Person]:::model johndoe(person:johndoe):::data The following snippet makes the same assertion, but uses assertion iri() instead of transformation addIri() : triple(iri(prefix.person, 'username'), a, sdo.Person),","title":"Example: Prefix declaration and local name"},{"location":"triply-etl/transform/ratt/#example-absolute-iri","text":"The following snippet creates the same IRI, but does not use a predefined prefix IRI: addIri({ content: 'https://example.com/id/person/johndoe', key: '_person', }), triple('_person', a, sdo.Person), Or diagrammatically: graph LR johndoe -- a --> Person Person[sdo:Person]:::model johndoe(https://example.com/id/person/johndoe):::data The following snippet uses assertion iri() instead of transformation addIri() : triple(iri('https://example.com/id/person/johndoe'), a, sdo.Person),","title":"Example: Absolute IRI"},{"location":"triply-etl/transform/ratt/#addliteral","text":"Creates an new literal and adds it to the Record under the specified key. This transformation can be used in the following 3 ways: If a datatype (key: datatype ) is given, a typed literal is created. If a language tag (key: languageTag ) is given, a language-tagged string (datatype rdf:langString ) is created. If neither a datatype not a language tag is given, a literal with datatype xsd:string is created.","title":"addLiteral()"},{"location":"triply-etl/transform/ratt/#when-to-use_2","text":"This transformation is typically used when: The same literal occurs in two or more statement assertions (function triple() or quad() ). This avoids having to specify the same literal multiple times using function literal() . The datatype or language tag is derived from the source data record.","title":"When to use"},{"location":"triply-etl/transform/ratt/#parameters_2","text":"content A key that contains a string value, or a string specified with function str() . datatype Optionally, a key that stores an IRI or a static IRI. language Optionally, a language tag from the lang object, or a key that stores such a language tag. validate Optionally provide a single validator condition or an array of validator conditions that the literal content should hold to, will return a boolean and throw and error when a validator condition does not hold. key A new key where the created literal is stored.","title":"Parameters"},{"location":"triply-etl/transform/ratt/#see-also_1","text":"If the created literal is used exactly once, it is often better to use the inline function literal() instead.","title":"See also"},{"location":"triply-etl/transform/ratt/#example-typed-literal","text":"The following snippet asserts a triple with a typed literal with datatype IRI xsd:date : fromJson([{ id: '123', date: '2022-01-30' }]), addLiteral({ content: 'date', datatype: xsd.date, key: '_dateCreated', }), triple(iri(prefix.book, 'id'), sdo.dateCreated, '_dateCreated'), This makes the following linked data assertion: book:123 sdo:dateCreated '2022-30-01'^^xsd:date. Notice that the same linked data could have been asserted with the following use the the literal() assertion middleware: fromJson([{ id: '123', date: '2022-01-30' }]), triple(iri(prefix.book, 'id'), sdo.dateCreated, literal('date', xsd.date)),","title":"Example: Typed literal"},{"location":"triply-etl/transform/ratt/#example-string-literal","text":"The following snippet asserts a triple with a string literal in the object position: fromJson([{name: 'London'}]), addLiteral({ content: 'name', key: '_name', }), triple(iri(prefix.city, '_name'), skos.prefLabel, '_name') This makes the following assertion: city:London sdo:name 'London'. The literal 'London' has type xsd:string . This is the standard datatype IRI for typed literals in the linked data languages (i.e. Turtle, TriG, and SPARQL). Notice that the same linked data could have been asserted with the following snippet, where the string value 'London' is automatically cast into a string literal: fromJson([{ name: 'London' }]), triple(iri(prefix.city, 'name'), skos.prefLabel, 'name'),","title":"Example: String literal"},{"location":"triply-etl/transform/ratt/#example-language-tagged-string","text":"The following snippet asserts a triple with a language-tagged string in the object position: fromJson([{ name: 'London' }]), addLiteral({ content: 'name', language: language.en, key: '_name', }), triple(iri(prefix.city, 'name'), skos.prefLabel, '_name'), This results in the following linked data assertion: city:London skos:prefLabel 'London'@en. Notice that the same linked data could have been asserted with the following use the the literal() assertion middleware: fromJson([{ name: 'London' }]), triple(iri(prefix.city, 'name'), skos.prefLabel, literal('name', lang['en'])),","title":"Example: Language-tagged string"},{"location":"triply-etl/transform/ratt/#example-validate-usage","text":"The following snippet asserts a triple of a person's email, with the email address being validated in the object position, and should throw an error when the record contains an invalid email address: fromJson([{ name: \"John\", email: 'john.appleseed@example.com' }, {name: 'NA', email: 'notAnEmail' } ]), addLiteral({ content: 'email', validate: isEmail(), key: '_email', }), triple(iri(prefix.person, 'name'), foaf.mbox, '_email'), This results in the following error for the second record: ERROR (Record #2) String \"notAnEmail\" is not an email address. Notice that when using only correct email addresses: fromJson([{ name: \"John\", email: 'john.appleseed@example.com' }, { name: \"Lisa\", email: 'lisa.appleseed@example.com' } ]), addLiteral({ content: 'email', validate: isEmail(), key: '_email', }), triple(iri(prefix.person, 'name'), foaf.mbox, '_email'), It results in the following correct linked data assertion: person:John foaf:mbox \"john.appleseed@example.com\" person:Lisa foaf:mbox \"lisa.appleseed@example.com\"","title":"Example: Validate usage"},{"location":"triply-etl/transform/ratt/#addskolemiri","text":"Creates a globally unique IRI that is intended to be used as a blank node identifier. Blank nodes are nodes without identification. It relatively difficult to work which such nodes in graph data, since they cannot be identified or dereferenced online. For this reason TriplyETL uses Skolem IRIs to denote blank nodes. This allows blank nodes to be identified and dereferenced. This Skolemization approach is part of the RDF standard. Skolem IRIs are random IRIs whose root path starts with .well-known/genid/ . This makes it easy to distinguish them from other random IRIs that are not used to denote blank nodes. prefix A IRI or a key that contains an IRI whose path starts with .well-known/genid/ . key A new key where the created IRI is stored.","title":"addSkolemIri()"},{"location":"triply-etl/transform/ratt/#see-also_2","text":"Tne Skolemization section in the RDF standard explains what Skolem IRIs are and how they should be used.","title":"See also"},{"location":"triply-etl/transform/ratt/#example","text":"The following snippet uses a hashed IRI to create a predictable identifier for a geospatial feature, and a Skolem IRI to create an unpredictable identifier for the geometry. The snippet includes the prefix declarations to illustrate that the path of the Skolem IRI must start with .well-known/genid. . const base = 'https://example.com/' const prefix = { feature: declarePrefix(base('id/feature/')), skolem: declarePrefix(base('.well-known/genid/')), } // Etc fromJson([{ point: 'Point(1.1 2.2)' }]), addHashedIri({ prefix: prefix.feature, content: 'point', key: '_feature', }), addSkolemIri({ prefix: prefix.skolem, key: '_geometry', }), triple('_feature', geo.hasGeometry, '_geometry'), triple('_geometry', geo.asWKT, literal('point', geo.wktLiteral)), This results in the following linked data assertions: feature:22238008e490f725979118f8f2dd9b5a geo:hasGeometry skolem:0cf4b63252a0476a8afc20735aa03da6. skolem:0cf4b63252a0476a8afc20735aa03da6 geo:asWKT 'Point(1.1 2.2)'^^geo:wktLiteral. Notice that the feature IRI will be the same across ELT runs if the source data stays the same, but the Skolem will always be different. Since the Skolem IRIs can be identified by the start of their path ( .well-known/genid/ ), the same linked data assertions can be displayed as follows: feature:22238008e490f725979118f8f2dd9b5a geo:hasGeometry [ geo:asWKT 'Point(1.1 2.2)'^^geo:wktLiteral ].","title":"Example"},{"location":"triply-etl/transform/ratt/#addtag","text":"This middleware creates a language tag based on a given string value.","title":"addTag()"},{"location":"triply-etl/transform/ratt/#parameters_3","text":"content A string value that encodes a language tag according to the IANA language subtag registry. key A new key where the language tag will be stored.","title":"Parameters"},{"location":"triply-etl/transform/ratt/#throws","text":"An error is emitted if the given string value does not follow the language tag format, or denotes a language tag that is not currently registered.","title":"Throws"},{"location":"triply-etl/transform/ratt/#see-also_3","text":"The language tag format is defined in the IETF BCP 47 standard (RFC 5646) . Language tags are registered in the IANA language subtag registry .","title":"See also"},{"location":"triply-etl/transform/ratt/#example_1","text":"The following snippet created a language tag for the Dutch language as spoken in The Netherlands, and uses it to assert a language-tagged string: fromJson([{ label: 'Amsterdam' }]), addTag({ content: 'nl-nl', key: 'lang', }), triple(iri(prefix.city, 'label'), rdfs.label, literal('label', 'lang')),","title":"Example"},{"location":"triply-etl/transform/ratt/#addvalue","text":"This middleware allows any value to be added to the Record.","title":"addValue()"},{"location":"triply-etl/transform/ratt/#description","text":"This middleware is useful for data that is not present in the source data record, but must be used in one or more assertions.","title":"Description"},{"location":"triply-etl/transform/ratt/#parameters_4","text":"content Any value that can be represented in TypeScript. key A new key where the value is stored.","title":"Parameters"},{"location":"triply-etl/transform/ratt/#example_2","text":"The following snippet starts out with an empty source record ( {} ), and adds a new data key to it. The added value is an array that contains a string and a number (in that order). This new value is used in the triple assertion, where 'data[0]' extracts the string element and 'data[1]' extracts the number elements. fromJson([{}]), addValue({ content: ['johndoe', 22], key: 'data', }), triple(iri(prefix.person, 'data[0]'), foaf.age, 'data[1]'), This results in the following linked data assertion: person:johndoe foaf:age 22.","title":"Example"},{"location":"triply-etl/transform/ratt/#example_3","text":"The following snippet adds a key called _startDate that either contains the start date as specified in the data source record, or the value 'unknown' : fromJson([ { id: '123', start: '2022-02-12' }, { id: '456' }, ]), ifElse({ if: 'start', then: addLiteral({ content: 'start', datatype: xsd.date, key: '_start', }), }, { else: addValue({ content: 'unknown', key: '_start', }), }), triple(iri(prefix.event, 'id'), sdo.startDate, '_start'), This results in the following linked data assertions: event:123 sdo:startDate '2022-02-12'^^xsd:date. event:456 sdo:startDate 'unknown'.","title":"Example"},{"location":"triply-etl/transform/ratt/#capitalize","text":"Transforms a string value to its capitalized variant. If the first character of a string has an uppercase variant, then that variant is used. If the first character does not have an uppercase variant -- because the character is already uppercase or is a punctuation character -- then the string remains unchanged. This transformation can uppercase the first character in any language; the Unicode Default Case Conversion algorithm is used.","title":"capitalize()"},{"location":"triply-etl/transform/ratt/#parameters_5","text":"content A key that contains a string value. key A new key where the capitalized result is stored.","title":"Parameters"},{"location":"triply-etl/transform/ratt/#example-class-iris","text":"According to convention, classes in linked data are denoted by IRIs whose local name starts with a capital letter. The following source data contains nice values for the type key, but they do not start with a capital letter yet. The following snippet capitalizes the values of the type keys, and uses them to create class IRIs. fromJson([ { id: '1', type: 'location' }, { id: '2', type: 'person' }, ]), capitalize({ content: 'type', key: '_type', }), triple(iri(prefix.id, 'id'), a, iri(prefix.def, '_type')), This results in the following linked data assertions: id:1 a def:Location. id:2 a def:Person.","title":"Example: Class IRIs"},{"location":"triply-etl/transform/ratt/#concat","text":"","title":"concat()"},{"location":"triply-etl/transform/ratt/#description_1","text":"Concatenates an array of strings into one new string. An optionally specified separator is placed in between every two consecutive string values.","title":"Description"},{"location":"triply-etl/transform/ratt/#parameters_6","text":"content An array of key that contain a string and/or strings specified with assertion str() . separator Optionally, the string that is places between every two consecutive string values. key A new key where the concatenated string is stored.","title":"Parameters"},{"location":"triply-etl/transform/ratt/#example_4","text":"The following snippet concatenates the first and last name of a person (in that order), using a space separator. fromJson([{ id: '1', first: 'John', last: 'Doe' }]), concat({ content: ['first', 'last'], separator: ' ', key: '_name', }), triple(iri(prefix.person, 'id'), foaf.name, '_name'), This results in the following linked data assertion: person:1 foaf:name 'John Doe'.","title":"Example"},{"location":"triply-etl/transform/ratt/#copy","text":"Makes a plain copy from the value stored in the given key to a new key.","title":"copy()"},{"location":"triply-etl/transform/ratt/#parameters_7","text":"content A value of any type, or a key that contains a value of any type. type Optionally, the name of the TypeScript type of the value. The default value is 'string' . key A new key where the plain copy is stored.","title":"Parameters"},{"location":"triply-etl/transform/ratt/#example_5","text":"Plain copies can be used to abbreviate long keys, especially in tree-shaped data like JSON or XML. In the following example, values stored in a long nested key are copies into a short and descriptive key. This is even more useful if the key is used many times in the script. copy({ content: 'record[0].family[0].children.child[0].id.$text', key: 'childId', }),","title":"Example"},{"location":"triply-etl/transform/ratt/#example_6","text":"Since plain copies introduce a new name for an existing value, the new name can be used to store extra information about the value. The following example stores an English name, if available; or a Dutch name, if available; or no name at all. This is a relatively complex example that can only be accomplished by copying the names for the encountered languages under descriptive key names. fromJson([ { id: '1', names: [ { language: 'en', value: 'London' }, { language: 'fr', value: 'Londres' }, { language: 'nl', value: 'Londen' }, ], }, { id: '2', names: [ { language: 'fr', value: 'Paris' }, { language: 'nl', value: 'Parijs' }, ], }, ]), forEach('names', [ _switch('language', // Plain copy of the English label, if available. ['en', copy({ content: 'value', key: '$parent.en' })], // Plain copy of the Dutch label, if available. ['nl', copy({ content: 'value', key: '$parent.nl' })], ), ]), ifElse({ // Prefer an English label over a Dutch label. if: 'en', then: copy({ content: 'en', key: '_preferredName' }), }, { // If there is no English label, a Dutch label is a second-best option. if: 'nl', then: copy({ content: 'nl', key: '_preferredName' }), }), // If there is either an English or a Dutch label, assert it. when('_preferredName', [ triple(iri(prefix.city, 'id'), rdfs.label, '_preferredName'), ]), This results in the following linked data assertions: city:1 rdfs:label 'London'. city:2 rdfs:label 'Parijs'.","title":"Example"},{"location":"triply-etl/transform/ratt/#encodehtml","text":"","title":"encodeHtml()"},{"location":"triply-etl/transform/ratt/#description_2","text":"This transformation decodes any HTML entities that appear in a given key. The following HTML entities are common in source data: HTML entity Decoded &amp; & &gt; > &lt; < You do not need to use this transformation if you want to assert literals with datatype IRI rdf:HTML . HTML entities are meaningful in HTML, so there they should be preserved.","title":"Description"},{"location":"triply-etl/transform/ratt/#parameters_8","text":"content A key in the Record that contains string values with HTML entities. key A new key where the decoded content is stored.","title":"Parameters"},{"location":"triply-etl/transform/ratt/#example_7","text":"The following snippet takes HTML texts from the source data and asserts them as regular text literals. Since HTML entities are meaningless in regular text, decodeHtml is used to denote these entities. fromJson([ { id: '1', label: 'A&amp;B' }, { id: '2', label: '1 &lt; 2' }, ]), decodeHtml({ content: 'label', key: '_label', }), triple(iri(prefix.id, 'id'), rdfs.label, '_label'), This results in the following linked data assertions: id:1 rdfs:label 'A&B'. id:2 rdfs:label '1 < 2'.","title":"Example"},{"location":"triply-etl/transform/ratt/#geojsontowkt","text":"Transforms GeoJSON objects to their corresponding Well-Known Text (WKT) serialization strings.","title":"geojsonToWkt()"},{"location":"triply-etl/transform/ratt/#parameters_9","text":"content A key that stores a GeoJSON object. crs Optionally, an IRI that denotes a Coordinate Reference System (CRS). You can use IRIs from the epsg object. If absent, uses https://epsg.io/4326 as the CRS. key A new key where the WKT serialization string is stored","title":"Parameters"},{"location":"triply-etl/transform/ratt/#geojson-and-well-known-text-wkt","text":"According to the GeoJSON standard , the only Coordinate Reference System (CRS) that is allowed to be used is EPSG:4326/WGS84. In practice, source data sometimes (incorrectly) stores GeoJSON formatted data in other CRSes. An example of this is the GISCO dataset of the European Union, which uses the EPSG:3857 CRS. For cases like these, the optional crs parameter comes in handy.","title":"GeoJSON and Well-Known Text (WKT)"},{"location":"triply-etl/transform/ratt/#see-also_4","text":"The GeoJSON format is standardized in RFC 7946 . The Well-Known Text (WKT) serialization format is standardized as part of ISO/IEC 13249-3:2016 standard .","title":"See also"},{"location":"triply-etl/transform/ratt/#example_8","text":"The following snippet converts GeoJSON objects that denote traffic light locations to their GeoSPARQL representation. fromJson([ { id: '123', geometry: { type: 'Point', coordinates: [6.256, 48.480], }, }, ]), addIri({ prefix: prefix.feature, content: 'id', key: '_feature', }), geojsonToWkt({ content: 'geometry', crs: epsg[3857], key: '_wkt', }), addHashedIri({ prefix: prefix.geometry, content: '_wkt', key: '_geometry' }), pairs('_feature', [a, def.TrafficLight], [geo.hasGeometry, '_geometry'], ), pairs('_geometry', [a, geo.Geometry], [geo.asWKT, literal('_wkt', geo.wktLiteral)], ), This results in the following linked data assertions: feature:123 a def:TrafficLight; geo:hasGeometry geometry:197e6376c2bd8192c24911f88c330606. geometry:197e6376c2bd8192c24911f88c330606 a geo:Geometry; geo:asWKT 'Point(6.256 48.480)'^^geo:wktLiteral. Or diagrammatically: graph LR feature -- a --> TrafficLight feature -- geo:hasGeometry --> geometry geometry -- a --> Geometry geometry -- geo:asWKT --> wkt Geometry[geo:Geometry]:::model TrafficLight[def:TrafficLight]:::model feature(feature:123):::data geometry(geometry:197e6376c2bd8192c24911f88c330606):::data wkt(\"'Point(6.256 48.480)'^^geo:wktLiteral\"):::data","title":"Example"},{"location":"triply-etl/transform/ratt/#jpath","text":"","title":"jpath()"},{"location":"triply-etl/transform/ratt/#description_3","text":"Filters a value based on a JSON Path expression. JSON Path is a query language for JSON. For the syntax of JSON Path expressions, please visit the JSON Path documentation page .","title":"Description"},{"location":"triply-etl/transform/ratt/#use-cases","text":"This function simplifies the complex key specification to filter specific values. It can only be used for an object of a triple to create a literal. The result of a function must have a fundamental type.","title":"Use cases"},{"location":"triply-etl/transform/ratt/#parameters_10","text":"value A JSON Path expression.","title":"Parameters"},{"location":"triply-etl/transform/ratt/#example_9","text":"The following examples will create a literal based on key value : If key 'ISO_639-2' exists: fromJson({ language: [ { 'ISO_639-1': 'en', lcid: 2057, value: 'Paris' }, { 'ISO_639-1': 'nl', 'ISO_639-2': 'nld', lcid: 1043, value: 'Parijs' }, ], }), triple( iri(prefix.city, '$recordId'), rdfs.label, literal(jpath(\"$.language[?(@['ISO_639-2'])].value\"), language.nl) ), If key 'ISO_639-1' is equal to nl : fromJson({ language: [ { 'ISO_639-1': 'en', lcid: 2057, value: 'Paris' }, { 'ISO_639-1': 'nl', 'ISO_639-2': 'nld', lcid: 1043, value: 'Parijs' }, ], }), triple( iri(prefix.city, '$recordId'), rdfs.label, literal(jpath(\"$.language[?(@['ISO_639-1'] =='nl')].value\"), language.nl) ), If key 'lcid' is lower than 1,100: fromJson({ language: [ { 'ISO_639-1': 'en', lcid: 2057, value: 'Paris' }, { 'ISO_639-1': 'nl', 'ISO_639-2': 'nld', lcid: 1043, value: 'Parijs' }, ], }), triple( iri(prefix.city, '$recordId'), rdfs.label, literal(jpath('$.language[?(@.lcid < 1100)].value'), language.nl) ), All three examples generate the following linked data: record:1 rdfs:label 'Parijs'@nl.","title":"Example"},{"location":"triply-etl/transform/ratt/#lowercase","text":"","title":"lowercase()"},{"location":"triply-etl/transform/ratt/#description_4","text":"Translates a string value to its lowercase variant. This middleware can lowercase strings in any language; the Unicode Default Case Conversion algorithm is used.","title":"Description"},{"location":"triply-etl/transform/ratt/#use-cases_1","text":"Older data formats sometimes use uppercase letters for header names or codes. The lowercase transformation middleware may be used to change such string values to lowercase.","title":"Use cases"},{"location":"triply-etl/transform/ratt/#parameters_11","text":"content A key that contains a string value. key A new key where the lowercase variant is stored.","title":"Parameters"},{"location":"triply-etl/transform/ratt/#example_10","text":"The following snippet starts out with header values that use uppercase characters exclusively. The lowerCase transformation is used to create lowercase names that can be used to create property IRIs. fromJson([ { from: '1', rel: 'PARENT', to: '2' }, { from: '2', rel: 'CHILD', to: '1' }, ]), lowercase({ content: 'rel', key: '_relationship', }), triple( iri(prefix.id, 'from'), iri(prefix.def, '_relationship'), iri(prefix.id, 'to'), ), This results in the following linked data assertions: id:1 def:parent id:2. id:2 def:child id:1.","title":"Example"},{"location":"triply-etl/transform/ratt/#padend","text":"","title":"padEnd()"},{"location":"triply-etl/transform/ratt/#description_5","text":"Adds a given padding string zero or more times to the end of a string value, until the resulting string value is exactly a given number of characters long.","title":"Description"},{"location":"triply-etl/transform/ratt/#use-cases_2","text":"This transformation is useful for identifiers that must have fixed length and that may be suffixed by zero's.","title":"Use cases"},{"location":"triply-etl/transform/ratt/#parameters_12","text":"content A key that contains a string value. If the key contains a numeric value, that value is first cast to string. padString The string that is added to the end of the string value in key content , until the result string has exactly targetLength characters. Can be a static string or a key. targetLength The exact number of characters that the resulting string should have. The string value is copied over as-is when targetLength is smaller than or equal to the length of the string value in key content . This includes cases where targetLength is negative or zero. key A new key where the padded string is stored.","title":"Parameters"},{"location":"triply-etl/transform/ratt/#example_11","text":"The following snippet processes identifiers of varying length, and ensures that they have the same length after suffixing '0' characters. fromJson([ { id: '16784' }, { id: '129' }, ]), padEnd({ content: 'id', padString: '0', targetLength: 6, key: '_id', }), This results in the following two Records: [ { \"id\": \"16784\", \"_id\": \"167840\" }, { \"id\": \"129\", \"_id\": \"129000\" } ]","title":"Example"},{"location":"triply-etl/transform/ratt/#padstart","text":"","title":"padStart()"},{"location":"triply-etl/transform/ratt/#description_6","text":"Adds a given padding string zero or more times in front of a string value, until the resulting string value is exactly a given number of characters long.","title":"Description"},{"location":"triply-etl/transform/ratt/#use-cases_3","text":"This transformation is useful for identifiers that must have fixed length and that may be prepended by zero's. If key content contains a numeric value, then that value is first cast to string. content A key that contains a string value. padString The string that is added in front of the string value in key content , until the result string has exactly targetLength characters. targetLength The exact number of characters that the resulting string should have. The string value is copied over as-is when targetLength is smaller than or equal to the length of the string value in key content . This includes cases where targetLength is negative or zero. key A new key where the lowercased string is stored.","title":"Use cases"},{"location":"triply-etl/transform/ratt/#example-fixed-length-identifiers","text":"The following snippet processes identifiers of varying length, and ensures that they have the same length after prefixing '0' characters. fromJson([ { id: '16784' }, { id: '129' }, ]), padStart({ content: 'id', padString: '0', targetLength: 6, key: '_id', }), This results in the following two records: [ { \"id\": \"16784\", \"_id\": \"016784\" }, { \"id\": \"129\", \"_id\": \"000129\" } ]","title":"Example: Fixed-length identifiers"},{"location":"triply-etl/transform/ratt/#example-create-year-literals","text":"In order to create standards-conforming temporal literal, we need to pad the year component to be at least 4 decimal digits long. (This requirement is defined in the XML Schema Datatypes 1.1: Part 2 Datatypes standard.) Suppose that the source data looks as follows: Artifact Year 0001 612 0002 1702 We can ensure that all years have at least 4 decimal digits by calling the following function: padStart({ content: 'Year', padString: '0', targetLength: 4, key: '_lexicalForm', }), triple( iri(prefix.id, 'Artifact'), dct.created, literal('_lexicalForm', xsd.gYear), ), This makes the following linked data assertions: id:0001 dct:created '0612'^^xsd:gYear. id:0002 dct:created '1702'^^xsd:gYear.","title":"Example: Create year literals"},{"location":"triply-etl/transform/ratt/#replace","text":"","title":"replace()"},{"location":"triply-etl/transform/ratt/#description_7","text":"Performs a regular expression replacement to the given input string, and stores the result in a new key.","title":"Description"},{"location":"triply-etl/transform/ratt/#parameters_13","text":"content A key that contains a string value, or a static string specified with assertion str() . from A JavaScript Regular Expression . to Optionally, a string that replaces potential matches of the Regular Expression ( from ). Use $1 , $2 , etc. to insert matches. If absent, the empty string is used. key A new key where the result of the replacement is stored.","title":"Parameters"},{"location":"triply-etl/transform/ratt/#example_12","text":"Suppose the source data contains date/time strings, but only the date component is needed: { \"created\": \"2020-01-02T00:00:00.0Z\" } It is possible to extract only the date part (everything up to the T ) in the following way: replace({ content: 'created', from: /^([^T]*).*$/, to: '$1', key: '_created', }), triple('_creativeWork', dct.created, literal('_created', xsd.date)), This results in the following Record: { \"created\": \"2020-01-02T00:00:00.0Z\", \"_created\": \"2020-01-02\" }","title":"Example"},{"location":"triply-etl/transform/ratt/#split","text":"","title":"split()"},{"location":"triply-etl/transform/ratt/#description_8","text":"Splits a string into an array of strings, and stores that array in a new key.","title":"Description"},{"location":"triply-etl/transform/ratt/#whitespace-handling","text":"This transformation removes any trailing whitespace that remains after the strings are split. This ensures that irregular use of whitespace in the source data is taken care of automatically.","title":"Whitespace handling"},{"location":"triply-etl/transform/ratt/#empty-values","text":"This transformation removes any elements of the splitted string that are empty (after trimmimng). To keep empty entries, use the `` flag.","title":"Empty values"},{"location":"triply-etl/transform/ratt/#use-cases_4","text":"The transformation is used when: - Tabular source data encodes multiple values inside singular cells. (Such concatenated storage inside cells is a data quality issue, because the table format cannot guarantee that the separator character does not (accidentally) occur inside individual values inside a cell. Tree-shaped source formats are able to store multiple values for the same key reliably, e.g. JSON and XML.) - Source data contains complex string values that can be decomposed into stand-alone components with distinct meaning.","title":"Use cases"},{"location":"triply-etl/transform/ratt/#parameters_14","text":"content A key that stores a string, or a string specified with assertion str() . separator A string or a regular expression that is used to separate the content. key A new key where the array of split strings is stored. keepEmptyEntities A boolean flag indicating if the empty values of a splitted string should be kept or not. By default empty values are removed.","title":"Parameters"},{"location":"triply-etl/transform/ratt/#example-multiple-values-in-singular-table-cells","text":"Tabular formats are unable to store more than one value in a cell. Because of this limitation, tabular data sources sometimes encode multiple values in cells by encoding them into one string. A separator character is typically used to distinguish between these multiple values. Suppose that the source data looks as follows: Parent Child John Jane, Jake , Kate ,, The following transformation splits the cells that encode zero or more children for each parent: split({ content: 'Child', separator: ',', key: 'Children', }), This results in the following transformed records: { \"Parent\": \"John\", \"Child\": \"Jane, Jake , \", \"Children\": [ \"Jane\", \"Jake\" ] } { \"Parent\": \"Kate\", \"Child\": \",, \", \"Children\": [] } Notice that trailing whitespace and empty values are dealt with automatically. Since the split() transformation always results in an array of strings, we can use the term assertion iris() afterwards: split({ content: 'children', separator: ',', key: '_children', }), triple( iri(prefix.person, 'parent'), sdo.children, iris(prefix.person, '_children') ), This results in the following linked data assertions: person:johndoe sdo:children person:janedoe, person:jakedoe.","title":"Example: Multiple values in singular table cells"},{"location":"triply-etl/transform/ratt/#example-split-a-complex-string-into-components","text":"The following snippet uses a regular expression to split a KIX code. (A KIX code is a standardized format for representing postal addresses in The Netherlands.) fromJson([{ id: '1', KIX: '1231FZ13Xhs' }]), split({ content: 'KIX', separator: /^(\\d{4}[A-Z]{2})(\\d{1,5})(?:X(.{1,6}))/, key: 'KIX_components', }), triple(iri(prefix.id, 'id'), sdo.postalCode, 'KIX_components[1]'), This results in the following record: { \"id\": \"1\", \"KIX\": \"1231FZ13Xhs\", \"KIX_components\": [\"\", \"1231FZ\", \"13\", \"hs\", \"\"] } And in the following linked data assertion: id:1 sdo:postalCode '1231FZ'.","title":"Example: Split a complex string into components"},{"location":"triply-etl/transform/ratt/#substring","text":"","title":"substring()"},{"location":"triply-etl/transform/ratt/#description_9","text":"This middleware takes a substring from the input string and stores the result in a new key.","title":"Description"},{"location":"triply-etl/transform/ratt/#parameters_15","text":"content A key that stores a string value, or a string specified with assertion str() . start The index of the first character that is included in the substring. The first character has index 0. end Optionally, the index of the first character that is excluded from the substring. If absent, the substring ends at the end of the source string. key The new key in which the substring is stored.","title":"Parameters"},{"location":"triply-etl/transform/ratt/#example_13","text":"The Library of Congress MARC format stores the type of record in the sixth character that appears in the leader key. We use substring() to extract this character, and then use transformation translateAll() to map them to a corresponding class IRI: substring({ content: 'metadata.marc:record.marc:leader.$text', start: 6, end: 7, key: '_typeOfRecord', }), translateAll({ content: '_typeOfRecord', table: { a: dcm.Text, k: dcm.StillImage, }, key: '_class', }), triple('_iri', a, '_class'),","title":"Example"},{"location":"triply-etl/transform/ratt/#translateall","text":"","title":"translateAll()"},{"location":"triply-etl/transform/ratt/#description_10","text":"Translates all dynamic strings from a specific key to new values of an arbitrary type To , according to a specified translation table. Since this function translates all values, the mapped values can have any type T ; they do not need to be strings. For example, this allows strings to be translated to IRIs or to literals.","title":"Description"},{"location":"triply-etl/transform/ratt/#when-to-use_3","text":"This approach is used when: The set of source data values is small. The set of source data values is known ahead of time. The corresponding linked data terms are known ahead of time. The appearance of a new value is considered to be an error in the source data.","title":"When to use?"},{"location":"triply-etl/transform/ratt/#parameters_16","text":"content A key that contains a string value. table A translation table from strings to values of some arbitrary type T . nulls Optionally, a list of string values that are considered denote NULL values in the source data. When a NULL value is encountered, the special value undefined is added for the target key . default Optionally, a default value or a default value-determining function that is used for string values that are neither in the translation table ( table ) nor in the NULL values list ( nulls ). The function must return a value of type T . Use of a default value value is equivalent to using the following value-determining function: _ => value . key A new key where the results of the translation are stored.","title":"Parameters"},{"location":"triply-etl/transform/ratt/#example-map-source-data-to-iri-values","text":"Suppose that source data contains country names. In linked data we want to use IRIs to denote countries, so that we can link additional information. Since the list of countries that appears in the source data is not that long, we can specify a translation table from names to IRIs by hand: change.translateAll({ content: 'country', table: { 'Belgium': country.be, 'Germany': country.de, 'England': country.gb, ..., }, nulls: ['Unknown'], key: '_country', }), when('country', [ triple('_country', a, sdo.Country), ]),","title":"Example: Map source data to IRI values"},{"location":"triply-etl/transform/ratt/#example-map-source-data-to-iri-properties","text":"When we relate a creative work to its creator, we sometimes know whether the creator was the actor, architect, author, etc. of the creative work. But in other cases we only know that there is a generic creator relationship. The Library of Congress Relators vocabulary allows us to express specific and generic predicates of this kind. transform.translateAll({ table: { 'actor': rel.act, 'architect': rel.arc, 'author': rel.aut, ..., }, default: rel.oth, // generic relator key: '_relator', }), triple('_creativeWork', '_relator', '_creator'),","title":"Example: Map source data to IRI properties"},{"location":"triply-etl/transform/ratt/#translatesome","text":"","title":"translateSome()"},{"location":"triply-etl/transform/ratt/#description_11","text":"Translates some strings, according to the specified translation table, to other strings. Strings that are not translated according to the translation table are copied over as-is.","title":"Description"},{"location":"triply-etl/transform/ratt/#parameters_17","text":"content A key that contains a string value. table A translation table that specifies translations from strings to strings. key A new key where the translated strings are stored.","title":"Parameters"},{"location":"triply-etl/transform/ratt/#use-cases_5","text":"Source data often contains some strings that are correct and some that are incorrect. For example, if source data contains a key with city names, some of the names may be misspelled. In such cases, translateSome() can be used to translate the incorrect strings into correct ones. A translateSome() transformation is often performed directly before a translateAll() transformation. The former ensures that all string values are correct (e.g. fixing typo's in city names); the latter ensures that all strings are mapped onto IRIs (e.g. city names mapped onto city-denoting IRIs).","title":"Use cases"},{"location":"triply-etl/transform/ratt/#example_14","text":"The following example fixes an encoding issue that occurs in the source data: transform.translateSome({ content: 'name', table: { 'Frysl\ufffd\ufffdn': 'Frysl\u00e2n', // Other entries for typographic fixes go here. ..., }, key: '_name', }),","title":"Example"},{"location":"triply-etl/transform/ratt/#tryliteral","text":"","title":"tryLiteral()"},{"location":"triply-etl/transform/ratt/#description_12","text":"This transformation is used when string values must be mapped onto literals with varying datatype IRIs. The datatype IRIs that could apply are specified in a list. The specified datatype IRIs are tried out from left to right. The first datatype IRI that results in a valid literal is chosen. content A key that contains a string value, or a string value specified with assertion str() . datatypes An array of two or more datatype IRIs. key A new key where the created literal is stored.","title":"Description"},{"location":"triply-etl/transform/ratt/#throws_1","text":"An exception is emitted if a string value does not belong to the lexical space of any of the specified datatype IRIs.","title":"Throws"},{"location":"triply-etl/transform/ratt/#example_15","text":"A literal is valid if the given string value appears in the lexical space of a specific datatype IRI. This is best explained with an example: tryLiteral({ content: 'date', datatypes: [xsd.date, xsd.gYearMonth, xsd.gYear], key: '_publicationDate', }), Source data in key 'date' Result in key '_date' '1900-01-02' '1900-01-02'^^xsd:date '1900' '1900'^^xsd:gYear '02-01-1900' An error is emitted. If we do not want to emit errors for string values that cannot be satisfy any of the specified datatype IRIs, we may choose to include xsd.string as the last datatype IRI in the list. Do notice however that this will result in dates that cannot be compared on a timeline, since they were not transformed to an XSD date/time datatype.","title":"Example"},{"location":"triply-etl/transform/ratt/#see-also_5","text":"You only need to use tryLiteral() if the datatype IRI varies from record to record. If the datatype IRI is the same for every record, then the regular assertion function literal() should be used instead.","title":"See also"},{"location":"triply-etl/transform/ratt/#uppercase","text":"","title":"uppercase()"},{"location":"triply-etl/transform/ratt/#description_13","text":"Translates a string value to its uppercase variant. This middleware can uppercase strings in any language; the Unicode Default Case Conversion algorithm is used for this.","title":"Description"},{"location":"triply-etl/transform/ratt/#parameters_18","text":"content A key that contains a string value. key A new key where the uppercase variant is stored.","title":"Parameters"},{"location":"triply-etl/transform/ratt/#example_16","text":"In the following example, the string in the key 'countryCode' becomes the uppercase string: fromJson({ place: 'Amsterdam', countryCode: 'nl' }), uppercase({ content: 'countryCode', key: '_countryCode' }), triple(iri(prefix.id, 'place'), iri(prefix.geonames, str('countryCode')), '_countryCode') This results in the following linked data assertion: city:Amsterdam geonames:countryCode \"NL\"","title":"Example"},{"location":"triply-etl/transform/ratt/#wktAddPoint()","text":"","title":"wkt.addPoint()"},{"location":"triply-etl/transform/ratt/#description_14","text":"Creates a Well-Known Text (WKT) serialization string from the corresponding geospatial point.","title":"Description"},{"location":"triply-etl/transform/ratt/#parameters_19","text":"latitude A key or a string assertion ( str() ) with latitude. longitude A key or a string assertion ( str() ) with longitude. crs Optionally, an IRI that denotes a Coordinate Reference System (CRS). You can use IRIs from the epsg object. If absent, uses EPSG:4326/WGS84 as the CRS. key A new key where the WKT string is stored.","title":"Parameters"},{"location":"triply-etl/transform/ratt/#example_17","text":"The following example creates a WKT literal from the geo coordinates of Amsterdam: fromJson({ place: 'Amsterdam', lat: 52.37308, long: 4.89245 }), wkt.addPoint({ latitude: 'lat', longitude: 'long', key: '_point' }), triple(iri(prefix.city, 'place'), geo.asWKT, '_point'), This results in the following record of the key '_point' : { \"_point\": { \"termType\": \"Literal\", \"value\": \"Point (52.37308 4.89245)\", \"language\": \"\", \"datatype\": { \"termType\": \"NamedNode\", \"value\": \"http://www.opengis.net/ont/geosparql#wktLiteral\", \"validationStatus\": \"canonical\" }, \"validationStatus\": \"canonical\" } } And in the following linked data assertion: city:Amstedam geo:asWKT \"Point (52.37308 4.89245)\"^^geo:wktLiteral","title":"Example"},{"location":"triply-etl/transform/ratt/#wktProject()","text":"","title":"wkt.project()"},{"location":"triply-etl/transform/ratt/#description_15","text":"Converts the projection of a Well-Known Text (WKT) literal from one Coordinate Reference System to another one.","title":"Description"},{"location":"triply-etl/transform/ratt/#parameters_20","text":"content An array of keys or numbers. key A new key where the new projection is stored. fromCrs : an IRI that denotes a Coordinate Reference System (CRS) of the content . toCrs : Optionally, an IRI that denotes a Coordinate Reference System (CRS) we want to convert to. If absent, uses EPSG:4326/WGS84 as the CRS.","title":"Parameters"},{"location":"triply-etl/transform/ratt/#example_18","text":"The following example converts an array with latitude and longitude in content key from Dutch grid coordinates (Rijksdriehoeks-coordinates) to WGS84 coordinates. fromJson({ place: 'Amsterdam', lat: 121307, long: 487360 }), wkt.project({ content: ['lat', 'long'], key: '_coordinates', fromCrs: epsg[666], toCrs: epsg[4326] }), This results in the following record of the key '_coordinates' : { \"_coordinates\": [ 52.374671935135474, 4.892803721020475 ] } We can now use the converted result to create a WKT Point() using addPoint() : wkt.addPoint({ latitude: '_coordinates[0]', longitude: '_coordinates[1]', key: '_point' }), triple(iri(prefix.id, 'place'), geo.asWKT, '_point') This code snippet creates the following linked data assertion: city:Amstedam geo:asWKT \"Point (52.374671935135474 4.892803721020475)\"^^geo:asWKT","title":"Example"},{"location":"triply-etl/transform/rml/","text":"On this page: RML Transformations Configuration components A simple example RML Transformations \u00b6 The RDF Mapping Language (RML) is an ETL configuration language for linked data. RML mappings are applied to the TriplyETL Record. Configuration components \u00b6 RML mappings contain the following configuration components: Logical Source : Defines the source of the data to be transformed. It includes information about the data format (csv, xml, json, etc.), location, and access methods. Logical sources can represent various types of data sources, such as files, databases, or web services. Triples Map : The mapping rules that are used to convert data from a Logical Source to linked data. It defines how data should be transformed and specifies the subject, predicate and object terms of the generated statements. Subject Map : The part of the Triples Map that defines how the subjects of the generated linked data statements must be constructed. It specifies the subject's term type, which can be blank node, IRI or literal. It often includes the class of which the subject term is an instance. Predicate Object Map : The part of the Triples Map that defines how the predicate and objects are mapped. A simple example \u00b6 The following full TriplyETL script applies the RML mappings specified in map.trig to the in-line specified source data record: import { logQuads } from '@triplyetl/etl/debug' import { Etl, fromJson, Source } from '@triplyetl/etl/generic' import { map } from '@triplyetl/etl/rml' export default async function (): Promise<Etl> { const etl = new Etl() etl.use( fromJson([{ name: 'John' }]), map(Source.file('map.trig')), logQuads(), ) return etl } The contents of file map.trig specify how the data will be mapped: prefix ql: <http://semweb.mmlab.be/ns/ql#> prefix rml: <http://semweb.mmlab.be/ns/rml#> prefix rr: <http://www.w3.org/ns/r2rml#> prefix sdo: <https://schema.org/> [] rml:logicalSource [ rml:source '$Record.json'; rml:referenceFormulation ql:JSONPath; rml:iterator '$' ]; rr:subjectMap [ rr:termType rr:BlankNode; rr:class sdo:Person ]; rr:predicateObjectMap [ rr:predicate sdo:firstName; rr:objectMap [ rml:reference 'name' ] ]. The Logical Source component specifies that the TriplyETL Record should be used: [] rml:logicalSource [ rml:source '$Record.json'; rml:referenceFormulation ql:JSONPath; rml:iterator '$' ]; The Subject Map specifies that the subject term is a blank node that is an instance of class sdo:Person : [] rr:subjectMap [ rr:termType rr:BlankNode; rr:class sdo:Person ]; The Predicate Object Map specifies that the value of key 'name' should be used together with the property sdo:firstName : [] rr:predicateObjectMap [ rr:predicate sdo:firstName; rr:objectMap [ rml:reference 'name' ] ]. Running the TriplyETL script results in the following linked data: <https://triplydb.com/.well-known/genid/1703545835347b1_b0> a sdo:Person; sdo:firstName 'John'.","title":"RML"},{"location":"triply-etl/transform/rml/#rml-transformations","text":"The RDF Mapping Language (RML) is an ETL configuration language for linked data. RML mappings are applied to the TriplyETL Record.","title":"RML Transformations"},{"location":"triply-etl/transform/rml/#configuration-components","text":"RML mappings contain the following configuration components: Logical Source : Defines the source of the data to be transformed. It includes information about the data format (csv, xml, json, etc.), location, and access methods. Logical sources can represent various types of data sources, such as files, databases, or web services. Triples Map : The mapping rules that are used to convert data from a Logical Source to linked data. It defines how data should be transformed and specifies the subject, predicate and object terms of the generated statements. Subject Map : The part of the Triples Map that defines how the subjects of the generated linked data statements must be constructed. It specifies the subject's term type, which can be blank node, IRI or literal. It often includes the class of which the subject term is an instance. Predicate Object Map : The part of the Triples Map that defines how the predicate and objects are mapped.","title":"Configuration components"},{"location":"triply-etl/transform/rml/#a-simple-example","text":"The following full TriplyETL script applies the RML mappings specified in map.trig to the in-line specified source data record: import { logQuads } from '@triplyetl/etl/debug' import { Etl, fromJson, Source } from '@triplyetl/etl/generic' import { map } from '@triplyetl/etl/rml' export default async function (): Promise<Etl> { const etl = new Etl() etl.use( fromJson([{ name: 'John' }]), map(Source.file('map.trig')), logQuads(), ) return etl } The contents of file map.trig specify how the data will be mapped: prefix ql: <http://semweb.mmlab.be/ns/ql#> prefix rml: <http://semweb.mmlab.be/ns/rml#> prefix rr: <http://www.w3.org/ns/r2rml#> prefix sdo: <https://schema.org/> [] rml:logicalSource [ rml:source '$Record.json'; rml:referenceFormulation ql:JSONPath; rml:iterator '$' ]; rr:subjectMap [ rr:termType rr:BlankNode; rr:class sdo:Person ]; rr:predicateObjectMap [ rr:predicate sdo:firstName; rr:objectMap [ rml:reference 'name' ] ]. The Logical Source component specifies that the TriplyETL Record should be used: [] rml:logicalSource [ rml:source '$Record.json'; rml:referenceFormulation ql:JSONPath; rml:iterator '$' ]; The Subject Map specifies that the subject term is a blank node that is an instance of class sdo:Person : [] rr:subjectMap [ rr:termType rr:BlankNode; rr:class sdo:Person ]; The Predicate Object Map specifies that the value of key 'name' should be used together with the property sdo:firstName : [] rr:predicateObjectMap [ rr:predicate sdo:firstName; rr:objectMap [ rml:reference 'name' ] ]. Running the TriplyETL script results in the following linked data: <https://triplydb.com/.well-known/genid/1703545835347b1_b0> a sdo:Person; sdo:firstName 'John'.","title":"A simple example"},{"location":"triply-etl/transform/typescript/","text":"On this page: TypeScript Context Function custom.add() Function signature Error conditions See also Example: Numeric calculations Function custom.change() Function signature Error conditions Example: Numeric calculation Example: Cast numeric data Example: Variant type Example: String or object custom.replace() Function signature Error conditions See also TypeScript \u00b6 The vast majority of ETLs can be written with the core set of RATT Transformations . But sometimes a custom transformation is necessary that cannot be handled by this core set. For such circumstances, TriplyETL allows a custom TypeScript function to be written. Notice that the use of a custom TypeScript function should be somewhat uncommon. The vast majority of real-world transformations should be supported by the core set of RATT Transformations. Context \u00b6 Custom TypeScript functions have access to various resources inside the TriplyETL. These resources include, but are not limited to, the full Record and the full Internal Store. TriplyETL refers to these resources as the Context . context.app The TriplyETL pipeline object. context.getX Tetrieves the value of a specific key in the Record and assumes it has type X , e.g. getAny() , getNumber() , getString() . context.record The current Record. context.store The Internal Store. Function custom.add() \u00b6 Adds a new entry to the Record, based on more than one existing entry. The value of the entry is the result of an arbitrary TypeScript function that has access to the full Context . Function signature \u00b6 The custom.add function has the following signature: etl.use( custom.add({ value: context => FUNCTION_BODY, key: 'NEW_KEY', }), ) The function can be configured in the following ways: - FUNCTION_BODY the body of a function, taking the Context as its input parameter ( context) and ending with a return statement returning the newly added value. - NEW_KEY must be the name of a new entry in the Record. Error conditions \u00b6 This function emits an error if NEW_KEY already exists in the current Record. See also \u00b6 Notice that it is bad practice to use custom.add() for adding a new entry that is based on exactly one existing entry. In such cases, the use of function custom.copy() Example: Numeric calculations \u00b6 Suppose the source data contains a numeric balance and a numeric rate. We can use function custom.add() to calculate the interest and store it in a new key: import { Etl, fromJson } from '@triplyetl/etl/generic' import { custom, logRecord } from '@triplyetl/etl/ratt' export default async function (): Promise<Etl> { const etl = new Etl() etl.use( fromJson([ { balance: 100, rate: 0.1 }, { balance: 200, rate: 0.2 } ]), custom.add({ value: context => context.getNumber('balance') * context.getNumber('rate'), key: 'interest', }), logRecord(), ) return etl } This prints the following two records: { \"balance\": 100, \"rate\": 0.1, \"interest\": 10 } { \"balance\": 200, \"rate\": 0.2, \"interest\": 40 } Function custom.change() \u00b6 Changes an existing entry in the Record. The change function takes the old value and returns the new value. Function signature \u00b6 This function has the following signature: etl.use( custom.change({ key: 'KEY_NAME', type: 'VALUE_TYPE', change: value => FUNCTION_BODY, }), ) The function can be configured in the following way: - KEY_NAME must be the name of a key in the record. - VALUE_TYPE must be one of the following type-denoting strings: - 'array' an array whose elements have type any . - 'boolean' a Boolean value ( true or false ). - 'iri' a universal identifier / IRI term. - 'literal' an RDF literal term. - 'number' a natural number or floating-point number. - 'string' a sequence of characters. - 'unknown' an unknown type. - FUNCTION_BODY a function body that returns the new value. Error conditions \u00b6 This function emits an error if the specified key ( KEY_NAME ) does not exist in the RATT record. Use custom.copy() if you want to create a new entry based on an existing one. Example: Numeric calculation \u00b6 Suppose the source data contains a balance in thousands. We can use function custom.change() to multiply the balance inplace: import { Etl, fromJson } from '@triplyetl/etl/generic' import { custom, logRecord } from '@triplyetl/etl/ratt' export default async function (): Promise<Etl> { const etl = new Etl() etl.use( fromJson([{ balance: 100 }, { balance: 200 }]), custom.change({ change: value => 1_000 * value, type: 'number', key: 'balance', }), logRecord(), ) return etl } This prints the following two records: { \"balance\": 100000 } { \"balance\": 200000 } Notice that the values for the balance keys were changed. Example: Cast numeric data \u00b6 Some source data formats are unable to represent numeric data. A good example are the CSV and TSV formats, where every cell value is represented as a string. If such a source data format that cannot represent numeric data is used, it is often useful to explicitly cast string values to numbers. For example, assume the following input table that uses strings to encode the number of inhabitants for each country: Country Inhabitants France '' Germany '83190556' Italy 'empty' Netherlands '17650200' We can use the custom.change() function to cast the values stored in the 'Inhabitants' key to numbers in the following way: custom.change({ change: value => +(value as number), type: 'unknown', key: 'Inhabitants', }), Notice that the type must be set to 'unknown' because a string is not allowed to be cast to a number in TypeScript (because not every string can be cast to a number). After custom.change() has been applied, the record looks as follows: Country Inhabitants France 0 Germany 83190556 Italy null Netherlands 17650200 Notice that strings that encode a number are correctly transformed, and non-empty strings that do not encode a number are transformed to null . Most of the time, this is the behavior that you want in a linked data pipeline. Also notice that the empty string is cast to the number zero. Most of the time, this is not what you want. If you want to prevent this transformation from happening, and you almost certainly do, you must process the source data conditionally, using control structures . Example: Variant type \u00b6 A variant is a value that does not always have the same type. Variants may appear in dirty source data, where a value is sometimes given in one way and sometimes in another. In such cases, the type parameter must be set to 'unknown' . Inside the body of the change function we first cast the value to a variant type. In TypeScript the notation for this is a sequence of types separated by the pipe ( | ) character. Finally, the typeof operator is used to clean the source data to a uniform type that is easier to process in the rest of the ETL. The following code snippet processes source data where the balance is sometimes specified as a number and sometimes as a string: import { Etl, fromJson } from '@triplyetl/etl/generic' import { custom, logRecord } from '@triplyetl/etl/ratt' export default async function (): Promise<Etl> { const etl = new Etl() etl.use( fromJson([{ balance: 100 }, { balance: \"200\" }]), custom.change({ key: 'balance', type: 'unknown', change: value => { const tmp = value as number | string switch (typeof tmp) { case 'number': return value as number case 'string': return parseInt(value as string) } }, }), logRecord(), ) return etl } This prints the following two records, where the balance is now always a number that can be uniformly processed: { \"balance\": 100 } { \"balance\": 200 } Example: String or object \u00b6 In the following example the name of a person is sometimes given as a plain string and sometimes as an object with a fistName and a lastName key: The following function transforms this variant to a uniform string type: import { Etl, fromJson } from '@triplyetl/etl/generic' import { custom, logRecord } from '@triplyetl/etl/ratt' export default async function (): Promise<Etl> { const etl = new Etl() etl.use( fromJson([ { name: 'John Doe' }, { name: { firstName: 'Jane', lastName: 'Doe' } } ]), custom.change({ key: 'name', type: 'unknown', change: value => { const tmp = value as { firstName: string, lastName: string } | string switch (typeof tmp) { case 'string': return tmp case 'object': return tmp.firstName + ' ' + tmp.lastName } }, }), logRecord(), ) return etl } This print the following two records that can now be uniformly processed: { \"name\": \"John Doe\" } { \"name\": \"Jane Doe\" } custom.replace() \u00b6 Replaces the value of an existing key based on the value from another key. Function signature \u00b6 The custom.replace() function has the following signature: etl.use( custom.replace({ fromKey: 'FROM_KEY', type: 'VALUE_TYPE', change?: value => FUNCTION_BODY, toKey: 'FROM_TYPE', }), ) fromKey is the name of the key whose value is going to be used to replace the old value with. type is the name of the type of the value in fromKey . The change key optionally specifies a function that takes the cast value of fromKey and that returns the value that will be stored in toKey . If the change function is not specified, it is identical to value => value . toKey is the name of the existing key whose value is going to be replaced. Error conditions \u00b6 This function emits an error under the following conditions: - fromKey does not specify a key name that exists in the current Record. - toKey does not specify a key name that exists in the current Record. - fromKey and toKey are the same. See also \u00b6 If fromKey and toKey are the same, then function custom.change() must be used instead.","title":"TypeScript"},{"location":"triply-etl/transform/typescript/#typescript","text":"The vast majority of ETLs can be written with the core set of RATT Transformations . But sometimes a custom transformation is necessary that cannot be handled by this core set. For such circumstances, TriplyETL allows a custom TypeScript function to be written. Notice that the use of a custom TypeScript function should be somewhat uncommon. The vast majority of real-world transformations should be supported by the core set of RATT Transformations.","title":"TypeScript"},{"location":"triply-etl/transform/typescript/#context","text":"Custom TypeScript functions have access to various resources inside the TriplyETL. These resources include, but are not limited to, the full Record and the full Internal Store. TriplyETL refers to these resources as the Context . context.app The TriplyETL pipeline object. context.getX Tetrieves the value of a specific key in the Record and assumes it has type X , e.g. getAny() , getNumber() , getString() . context.record The current Record. context.store The Internal Store.","title":"Context"},{"location":"triply-etl/transform/typescript/#function-customadd","text":"Adds a new entry to the Record, based on more than one existing entry. The value of the entry is the result of an arbitrary TypeScript function that has access to the full Context .","title":"Function custom.add()"},{"location":"triply-etl/transform/typescript/#function-signature","text":"The custom.add function has the following signature: etl.use( custom.add({ value: context => FUNCTION_BODY, key: 'NEW_KEY', }), ) The function can be configured in the following ways: - FUNCTION_BODY the body of a function, taking the Context as its input parameter ( context) and ending with a return statement returning the newly added value. - NEW_KEY must be the name of a new entry in the Record.","title":"Function signature"},{"location":"triply-etl/transform/typescript/#error-conditions","text":"This function emits an error if NEW_KEY already exists in the current Record.","title":"Error conditions"},{"location":"triply-etl/transform/typescript/#see-also","text":"Notice that it is bad practice to use custom.add() for adding a new entry that is based on exactly one existing entry. In such cases, the use of function custom.copy()","title":"See also"},{"location":"triply-etl/transform/typescript/#example-numeric-calculations","text":"Suppose the source data contains a numeric balance and a numeric rate. We can use function custom.add() to calculate the interest and store it in a new key: import { Etl, fromJson } from '@triplyetl/etl/generic' import { custom, logRecord } from '@triplyetl/etl/ratt' export default async function (): Promise<Etl> { const etl = new Etl() etl.use( fromJson([ { balance: 100, rate: 0.1 }, { balance: 200, rate: 0.2 } ]), custom.add({ value: context => context.getNumber('balance') * context.getNumber('rate'), key: 'interest', }), logRecord(), ) return etl } This prints the following two records: { \"balance\": 100, \"rate\": 0.1, \"interest\": 10 } { \"balance\": 200, \"rate\": 0.2, \"interest\": 40 }","title":"Example: Numeric calculations"},{"location":"triply-etl/transform/typescript/#function-customchange","text":"Changes an existing entry in the Record. The change function takes the old value and returns the new value.","title":"Function custom.change()"},{"location":"triply-etl/transform/typescript/#function-signature_1","text":"This function has the following signature: etl.use( custom.change({ key: 'KEY_NAME', type: 'VALUE_TYPE', change: value => FUNCTION_BODY, }), ) The function can be configured in the following way: - KEY_NAME must be the name of a key in the record. - VALUE_TYPE must be one of the following type-denoting strings: - 'array' an array whose elements have type any . - 'boolean' a Boolean value ( true or false ). - 'iri' a universal identifier / IRI term. - 'literal' an RDF literal term. - 'number' a natural number or floating-point number. - 'string' a sequence of characters. - 'unknown' an unknown type. - FUNCTION_BODY a function body that returns the new value.","title":"Function signature"},{"location":"triply-etl/transform/typescript/#error-conditions_1","text":"This function emits an error if the specified key ( KEY_NAME ) does not exist in the RATT record. Use custom.copy() if you want to create a new entry based on an existing one.","title":"Error conditions"},{"location":"triply-etl/transform/typescript/#example-numeric-calculation","text":"Suppose the source data contains a balance in thousands. We can use function custom.change() to multiply the balance inplace: import { Etl, fromJson } from '@triplyetl/etl/generic' import { custom, logRecord } from '@triplyetl/etl/ratt' export default async function (): Promise<Etl> { const etl = new Etl() etl.use( fromJson([{ balance: 100 }, { balance: 200 }]), custom.change({ change: value => 1_000 * value, type: 'number', key: 'balance', }), logRecord(), ) return etl } This prints the following two records: { \"balance\": 100000 } { \"balance\": 200000 } Notice that the values for the balance keys were changed.","title":"Example: Numeric calculation"},{"location":"triply-etl/transform/typescript/#example-cast-numeric-data","text":"Some source data formats are unable to represent numeric data. A good example are the CSV and TSV formats, where every cell value is represented as a string. If such a source data format that cannot represent numeric data is used, it is often useful to explicitly cast string values to numbers. For example, assume the following input table that uses strings to encode the number of inhabitants for each country: Country Inhabitants France '' Germany '83190556' Italy 'empty' Netherlands '17650200' We can use the custom.change() function to cast the values stored in the 'Inhabitants' key to numbers in the following way: custom.change({ change: value => +(value as number), type: 'unknown', key: 'Inhabitants', }), Notice that the type must be set to 'unknown' because a string is not allowed to be cast to a number in TypeScript (because not every string can be cast to a number). After custom.change() has been applied, the record looks as follows: Country Inhabitants France 0 Germany 83190556 Italy null Netherlands 17650200 Notice that strings that encode a number are correctly transformed, and non-empty strings that do not encode a number are transformed to null . Most of the time, this is the behavior that you want in a linked data pipeline. Also notice that the empty string is cast to the number zero. Most of the time, this is not what you want. If you want to prevent this transformation from happening, and you almost certainly do, you must process the source data conditionally, using control structures .","title":"Example: Cast numeric data"},{"location":"triply-etl/transform/typescript/#example-variant-type","text":"A variant is a value that does not always have the same type. Variants may appear in dirty source data, where a value is sometimes given in one way and sometimes in another. In such cases, the type parameter must be set to 'unknown' . Inside the body of the change function we first cast the value to a variant type. In TypeScript the notation for this is a sequence of types separated by the pipe ( | ) character. Finally, the typeof operator is used to clean the source data to a uniform type that is easier to process in the rest of the ETL. The following code snippet processes source data where the balance is sometimes specified as a number and sometimes as a string: import { Etl, fromJson } from '@triplyetl/etl/generic' import { custom, logRecord } from '@triplyetl/etl/ratt' export default async function (): Promise<Etl> { const etl = new Etl() etl.use( fromJson([{ balance: 100 }, { balance: \"200\" }]), custom.change({ key: 'balance', type: 'unknown', change: value => { const tmp = value as number | string switch (typeof tmp) { case 'number': return value as number case 'string': return parseInt(value as string) } }, }), logRecord(), ) return etl } This prints the following two records, where the balance is now always a number that can be uniformly processed: { \"balance\": 100 } { \"balance\": 200 }","title":"Example: Variant type"},{"location":"triply-etl/transform/typescript/#example-string-or-object","text":"In the following example the name of a person is sometimes given as a plain string and sometimes as an object with a fistName and a lastName key: The following function transforms this variant to a uniform string type: import { Etl, fromJson } from '@triplyetl/etl/generic' import { custom, logRecord } from '@triplyetl/etl/ratt' export default async function (): Promise<Etl> { const etl = new Etl() etl.use( fromJson([ { name: 'John Doe' }, { name: { firstName: 'Jane', lastName: 'Doe' } } ]), custom.change({ key: 'name', type: 'unknown', change: value => { const tmp = value as { firstName: string, lastName: string } | string switch (typeof tmp) { case 'string': return tmp case 'object': return tmp.firstName + ' ' + tmp.lastName } }, }), logRecord(), ) return etl } This print the following two records that can now be uniformly processed: { \"name\": \"John Doe\" } { \"name\": \"Jane Doe\" }","title":"Example: String or object"},{"location":"triply-etl/transform/typescript/#customreplace","text":"Replaces the value of an existing key based on the value from another key.","title":"custom.replace()"},{"location":"triply-etl/transform/typescript/#function-signature_2","text":"The custom.replace() function has the following signature: etl.use( custom.replace({ fromKey: 'FROM_KEY', type: 'VALUE_TYPE', change?: value => FUNCTION_BODY, toKey: 'FROM_TYPE', }), ) fromKey is the name of the key whose value is going to be used to replace the old value with. type is the name of the type of the value in fromKey . The change key optionally specifies a function that takes the cast value of fromKey and that returns the value that will be stored in toKey . If the change function is not specified, it is identical to value => value . toKey is the name of the existing key whose value is going to be replaced.","title":"Function signature"},{"location":"triply-etl/transform/typescript/#error-conditions_2","text":"This function emits an error under the following conditions: - fromKey does not specify a key name that exists in the current Record. - toKey does not specify a key name that exists in the current Record. - fromKey and toKey are the same.","title":"Error conditions"},{"location":"triply-etl/transform/typescript/#see-also_1","text":"If fromKey and toKey are the same, then function custom.change() must be used instead.","title":"See also"},{"location":"triply-etl/transform/xslt/","text":"On this page: XSLT (Extensible Stylesheet Language Transformations) Example Input XML file (books.xml) XSLT Stylesheet (books-to-rdf.xsl) Output RDF (result.rdf) after applying XSLT Using XSLT in TriplyETL XSLT (Extensible Stylesheet Language Transformations) \u00b6 XSLT (Extensible Stylesheet Language Transformations) is a language used to transform and manipulate XML data. With XSLT, you have the capability to create rules and transformations that convert data documents into different formats or structures. Example \u00b6 Here's an example of an XML file, an XSLT stylesheet, and the resulting output in RDF format after applying the XSLT transformation. In this example, we'll transform a simple XML representation of books into RDF triples. Input XML file (books.xml) \u00b6 <?xml version=\"1.0\" encoding=\"UTF-8\"?> <library> <book> <title>The Great Gatsby</title> <author>F. Scott Fitzgerald</author> </book> <book> <title>1984</title> <author>George Orwell</author> </book> </library> XSLT Stylesheet (books-to-rdf.xsl) \u00b6 <?xml version=\"1.0\" encoding=\"UTF-8\"?> <xsl:stylesheet version=\"1.0\" xmlns:xsl=\"http://www.w3.org/1999/XSL/Transform\"> <xsl:template match=\"/\"> <rdf:RDF xmlns:rdf=\"http://www.w3.org/1999/02/22-rdf-syntax-ns#\"> <xsl:for-each select=\"library/book\"> <rdf:Description> <rdf:type rdf:resource=\"http://example.org/book\" /> <rdf:Description> <rdf:type rdf:resource=\"http://example.org/author\" /> <rdf:value> <xsl:value-of select=\"author\" /> </rdf:value> </rdf:Description> <rdf:title> <xsl:value-of select=\"title\" /> </rdf:title> </rdf:Description> </xsl:for-each> </rdf:RDF> </xsl:template> </xsl:stylesheet> Output RDF (result.rdf) after applying XSLT \u00b6 <rdf:RDF xmlns:rdf=\"http://www.w3.org/1999/02/22-rdf-syntax-ns#\"> <rdf:Description> <rdf:type rdf:resource=\"http://example.org/book\" /> <rdf:Description> <rdf:type rdf:resource=\"http://example.org/author\" /> <rdf:value>F. Scott Fitzgerald</rdf:value> </rdf:Description> <rdf:title>The Great Gatsby</rdf:title> </rdf:Description> <rdf:Description> <rdf:type rdf:resource=\"http://example.org/book\" /> <rdf:Description> <rdf:type rdf:resource=\"http://example.org/author\" /> <rdf:value>George Orwell</rdf:value> </rdf:Description> <rdf:title>1984</rdf:title> </rdf:Description> </rdf:RDF> Using XSLT in TriplyETL \u00b6 In TriplyETL, XSLT processing is supported in the fromXML() and loadRdf() middlewares by providing an optional Source.file() to the stylesheet parameter that uses an XSL-XML Stylesheet. Below we will explain in steps how it can be used: 1. Create your XSLT stylesheet: First, you need to create an XSLT stylesheet. This stylesheet defines the rules for transforming your XML data. It should have a .xslt or .xsl file extension. You can create this stylesheet using any text editor or XML/XSLT development tool. 2. Load the data and apply XSLT transformation using either fromXml() or loadRdf() : fromXml() is used to load and transform xml to xml with different structure: fromXml(Source.file(xml), { selectors: 'rdf:RDF.sdo:Person', stylesheet: Source.file(stylesheet), }) loadRdf() is used to load and transform xml to xml/rdf to internal store: loadRdf(Source.file(xml), { contentType: 'application/rdf+xml', stylesheet: Source.file(xsl), }),","title":"XSLT"},{"location":"triply-etl/transform/xslt/#xslt-extensible-stylesheet-language-transformations","text":"XSLT (Extensible Stylesheet Language Transformations) is a language used to transform and manipulate XML data. With XSLT, you have the capability to create rules and transformations that convert data documents into different formats or structures.","title":"XSLT (Extensible Stylesheet Language Transformations)"},{"location":"triply-etl/transform/xslt/#example","text":"Here's an example of an XML file, an XSLT stylesheet, and the resulting output in RDF format after applying the XSLT transformation. In this example, we'll transform a simple XML representation of books into RDF triples.","title":"Example"},{"location":"triply-etl/transform/xslt/#input-xml-file-booksxml","text":"<?xml version=\"1.0\" encoding=\"UTF-8\"?> <library> <book> <title>The Great Gatsby</title> <author>F. Scott Fitzgerald</author> </book> <book> <title>1984</title> <author>George Orwell</author> </book> </library>","title":"Input XML file (books.xml)"},{"location":"triply-etl/transform/xslt/#xslt-stylesheet-books-to-rdfxsl","text":"<?xml version=\"1.0\" encoding=\"UTF-8\"?> <xsl:stylesheet version=\"1.0\" xmlns:xsl=\"http://www.w3.org/1999/XSL/Transform\"> <xsl:template match=\"/\"> <rdf:RDF xmlns:rdf=\"http://www.w3.org/1999/02/22-rdf-syntax-ns#\"> <xsl:for-each select=\"library/book\"> <rdf:Description> <rdf:type rdf:resource=\"http://example.org/book\" /> <rdf:Description> <rdf:type rdf:resource=\"http://example.org/author\" /> <rdf:value> <xsl:value-of select=\"author\" /> </rdf:value> </rdf:Description> <rdf:title> <xsl:value-of select=\"title\" /> </rdf:title> </rdf:Description> </xsl:for-each> </rdf:RDF> </xsl:template> </xsl:stylesheet>","title":"XSLT Stylesheet (books-to-rdf.xsl)"},{"location":"triply-etl/transform/xslt/#output-rdf-resultrdf-after-applying-xslt","text":"<rdf:RDF xmlns:rdf=\"http://www.w3.org/1999/02/22-rdf-syntax-ns#\"> <rdf:Description> <rdf:type rdf:resource=\"http://example.org/book\" /> <rdf:Description> <rdf:type rdf:resource=\"http://example.org/author\" /> <rdf:value>F. Scott Fitzgerald</rdf:value> </rdf:Description> <rdf:title>The Great Gatsby</rdf:title> </rdf:Description> <rdf:Description> <rdf:type rdf:resource=\"http://example.org/book\" /> <rdf:Description> <rdf:type rdf:resource=\"http://example.org/author\" /> <rdf:value>George Orwell</rdf:value> </rdf:Description> <rdf:title>1984</rdf:title> </rdf:Description> </rdf:RDF>","title":"Output RDF (result.rdf) after applying XSLT"},{"location":"triply-etl/transform/xslt/#using-xslt-in-triplyetl","text":"In TriplyETL, XSLT processing is supported in the fromXML() and loadRdf() middlewares by providing an optional Source.file() to the stylesheet parameter that uses an XSL-XML Stylesheet. Below we will explain in steps how it can be used: 1. Create your XSLT stylesheet: First, you need to create an XSLT stylesheet. This stylesheet defines the rules for transforming your XML data. It should have a .xslt or .xsl file extension. You can create this stylesheet using any text editor or XML/XSLT development tool. 2. Load the data and apply XSLT transformation using either fromXml() or loadRdf() : fromXml() is used to load and transform xml to xml with different structure: fromXml(Source.file(xml), { selectors: 'rdf:RDF.sdo:Person', stylesheet: Source.file(stylesheet), }) loadRdf() is used to load and transform xml to xml/rdf to internal store: loadRdf(Source.file(xml), { contentType: 'application/rdf+xml', stylesheet: Source.file(xsl), }),","title":"Using XSLT in TriplyETL"},{"location":"triply-etl/validate/","text":"On this page: Validate Validate \u00b6 The Validate step ensures that the linked data a pipeline produces conforms to the requirements specified in the data model. Every ETL should include the Validate step to ensure that only valid data is published in knowledge graphs. graph LR sources -- 1. Extract --> record record -- 2. Transform --> record record -- 3. Assert --> ld ld -- 4. Enrich --> ld ld -- 5. Validate --> ld ld -- 6. Publish --> destinations linkStyle 4 stroke:red,stroke-width:3px; destinations[(\"D. Destinations\\n(TriplyDB)\")] ld[C. Internal Store] record[B. Record] sources[A. Data Sources] TriplyETL supports the following two approaches for validating linked data: Graph Comparison uses one or more manually created 'gold records'. Graph comparison ensures that these records are transformed in the intended way by the ETL pipeline. SHACL Validation uses a generic data model. SHACL Validation ensures that each individual record is processed in accordance with the generic data model. Notice that it is possible to combine these two approaches in the same ETL pipeline: you can use graph comparison to test for specific conformities, and use SHACL to test for generic conformities.","title":"Overview"},{"location":"triply-etl/validate/#validate","text":"The Validate step ensures that the linked data a pipeline produces conforms to the requirements specified in the data model. Every ETL should include the Validate step to ensure that only valid data is published in knowledge graphs. graph LR sources -- 1. Extract --> record record -- 2. Transform --> record record -- 3. Assert --> ld ld -- 4. Enrich --> ld ld -- 5. Validate --> ld ld -- 6. Publish --> destinations linkStyle 4 stroke:red,stroke-width:3px; destinations[(\"D. Destinations\\n(TriplyDB)\")] ld[C. Internal Store] record[B. Record] sources[A. Data Sources] TriplyETL supports the following two approaches for validating linked data: Graph Comparison uses one or more manually created 'gold records'. Graph comparison ensures that these records are transformed in the intended way by the ETL pipeline. SHACL Validation uses a generic data model. SHACL Validation ensures that each individual record is processed in accordance with the generic data model. Notice that it is possible to combine these two approaches in the same ETL pipeline: you can use graph comparison to test for specific conformities, and use SHACL to test for generic conformities.","title":"Validate"},{"location":"triply-etl/validate/graph-comparison/","text":"On this page: Graph Comparison Graph comparison failure Graph comparison success Graph comparison for validation Step 1: Identify representative records Step 2: Manually create linked data Step 3: Implement the ETL Step 4. Call the graph comparison function Full script Options See also Graph Comparison \u00b6 Graph comparison is an approach for validating that the data produced by TriplyETL is identical to one or more manually specified graphs. Graph comparison failure \u00b6 The following full TriplyETL script shows how graph comparison failure is detected in TriplyETL: import { compareGraphs, Etl, loadRdf, Source } from '@triplyetl/etl/generic' export default async function (): Promise<Etl> { const etl = new Etl() etl.use( loadRdf(Source.string(`[]<p><o>.`)), compareGraphs(Source.string(`[]<p><o>. []<p><o>.`)), ) return etl } Function loadRdf() loads the following linked data into the Internal Store: graph LR _:1 -- p --> o. Function compareGraphs() compares the contents of the Internal Store to the following linked data: graph LR _:2a -- p --> o. _:2b -- p --> o. Notice that these two graphs are not isomorphic: It is possible to map _:2a and _:2b onto _:1 , but it is not possible to map _:1 onto both _:2a and _:2b . As a result, graph comparison will fail and the ETL will be halted. Graph comparison success \u00b6 The following full TriplyETL script shows how graph comparison success is detected in TriplyETL: import { compareGraphs, Etl, loadRdf, Source } from '@triplyetl/etl/generic' export default async function (): Promise<Etl> { const etl = new Etl() etl.use( loadRdf(Source.string(`<s><p><o>.`)), compareGraphs(Source.string(`<s><p><o>. <s><p><o>.`)), ) return etl } This example is similar to the previous example, but uses names for the subject terms ( <s> ). Because of this, graph comparison now succeeds. Graph comparison for validation \u00b6 The two examples that were given before illustrate how graph comparison works. But they do not show how graph comparison can be used as a validation approach in TriplyETL pipelines. In order to do so, the following steps must be followed: Identify representative records. Select a limited number of records from your data sources that together are representative for the full data source systems. This often includes typical records, where all/most expected data items are included, as well as atypical records that are uncommon but valid. There must be reasonable confidence that an ETL that produces the correct results for these selected records, will also produce correct results for all other records. Create target linked data. For the representative records identified in step 1, manually create the linked data that the ETL should generate for them. The manually created linked data must be included in the ETL repository, for example by including various TriG files. Implement the ETL. The ETL configuration must be implemented in a generic way that works for all records. Any of the supported ETL configuration languages can be used for this: JSON-LD, RATT, RML, SHACL, SPARQL, or XSLT. Call the graph comparison function. After the ETL has been performed for a specific record, call the graph comparison function to validate whether the linked data in the Internal Store is isomorphic to the linked data that was manually specified. Step 1: Identify representative records \u00b6 To keep our example simple, we use a data source with three simple JSON records: fromJson([ { id: '1', price: 15 }, { id: '2', price: 12 }, { color: 'red', id: '3', price: 16 }, ]), We choose records 1 and 3 as the representative ones. Together these two records use all properties that occur in the source data. These records also allow us to test whether the optional color property is handled correctly. Step 2: Manually create linked data \u00b6 For each record selected in Step 1, we create the linked data that must be generated by TriplyETL: prefix sdo: <https://schema.org/> [] sdo:value 15. and: prefix sdo: <https://schema.org/> [] sdo:color 'red'; sdo:value 16. Step 3: Implement the ETL \u00b6 We use RATT to implement the assertions: addSkolemIri({ key: '_product' }), triple('_product', sdo.price, 'price'), when('width', triple('_product', sdo.color, 'color')), Step 4. Call the graph comparison function \u00b6 Since comparison graphs must be created by hand, only a small number of records will have a corresponding comparison graph. The graph comparison call must therefore only be performed for some records. We use the switch control structure to determine whether a record is eligible for graph comparison. Full script \u00b6 By completing the 4 steps, we end up with the following full TriplyETL script that applies graph comparison to a limited number of representative records: import { _switch, compareGraphs, Etl, fromJson, Source, when } from '@triplyetl/etl/generic' import { addSkolemIri, triple } from '@triplyetl/etl/ratt' import { sdo } from '@triplyetl/vocabularies' export default async function (): Promise<Etl> { const etl = new Etl() etl.use( fromJson([ { id: '1', price: 15 }, { id: '2', price: 12 }, { color: 'red', id: '3', price: 16 }, ]), addSkolemIri({ key: '_product' }), triple('_product', sdo.price, 'price'), when('width', triple('_product', sdo.color, 'color')), _switch('id', ['1', compareGraphs(Source.string(` prefix sdo: <https://schema.org/> [] sdo:value 15.`))], ['3', compareGraphs(Source.string(` prefix sdo: <https://schema.org/> [] sdo:color 'red'; sdo:value 16.`))], ), ) return etl } This script succeeds, since the linked data generated by the ETL is isomorphic to the manually specified data. Options \u00b6 contentType specifies the RDF format in which the comparison graphs are serialized. The following values are supported: \"application/json\" \"application/ld+json\" \"application/n-quads\" \"application/n-triples\" \"application/rdf+xml\" \"application/trig\" \"text/html\" \"text/n3\"; \"text/turtle\" defaultGraph specifies the named graph in which the comparison graph must be loaded. This is only useful if the chosen RDF serialization format cannot express named graphs. key unknown See also \u00b6 Graph comparison makes use of graph isomorphism, which is part of the RDF 1.1 Semantics standard ( external link ).","title":"Graph Comparison"},{"location":"triply-etl/validate/graph-comparison/#graph-comparison","text":"Graph comparison is an approach for validating that the data produced by TriplyETL is identical to one or more manually specified graphs.","title":"Graph Comparison"},{"location":"triply-etl/validate/graph-comparison/#graph-comparison-failure","text":"The following full TriplyETL script shows how graph comparison failure is detected in TriplyETL: import { compareGraphs, Etl, loadRdf, Source } from '@triplyetl/etl/generic' export default async function (): Promise<Etl> { const etl = new Etl() etl.use( loadRdf(Source.string(`[]<p><o>.`)), compareGraphs(Source.string(`[]<p><o>. []<p><o>.`)), ) return etl } Function loadRdf() loads the following linked data into the Internal Store: graph LR _:1 -- p --> o. Function compareGraphs() compares the contents of the Internal Store to the following linked data: graph LR _:2a -- p --> o. _:2b -- p --> o. Notice that these two graphs are not isomorphic: It is possible to map _:2a and _:2b onto _:1 , but it is not possible to map _:1 onto both _:2a and _:2b . As a result, graph comparison will fail and the ETL will be halted.","title":"Graph comparison failure"},{"location":"triply-etl/validate/graph-comparison/#graph-comparison-success","text":"The following full TriplyETL script shows how graph comparison success is detected in TriplyETL: import { compareGraphs, Etl, loadRdf, Source } from '@triplyetl/etl/generic' export default async function (): Promise<Etl> { const etl = new Etl() etl.use( loadRdf(Source.string(`<s><p><o>.`)), compareGraphs(Source.string(`<s><p><o>. <s><p><o>.`)), ) return etl } This example is similar to the previous example, but uses names for the subject terms ( <s> ). Because of this, graph comparison now succeeds.","title":"Graph comparison success"},{"location":"triply-etl/validate/graph-comparison/#graph-comparison-for-validation","text":"The two examples that were given before illustrate how graph comparison works. But they do not show how graph comparison can be used as a validation approach in TriplyETL pipelines. In order to do so, the following steps must be followed: Identify representative records. Select a limited number of records from your data sources that together are representative for the full data source systems. This often includes typical records, where all/most expected data items are included, as well as atypical records that are uncommon but valid. There must be reasonable confidence that an ETL that produces the correct results for these selected records, will also produce correct results for all other records. Create target linked data. For the representative records identified in step 1, manually create the linked data that the ETL should generate for them. The manually created linked data must be included in the ETL repository, for example by including various TriG files. Implement the ETL. The ETL configuration must be implemented in a generic way that works for all records. Any of the supported ETL configuration languages can be used for this: JSON-LD, RATT, RML, SHACL, SPARQL, or XSLT. Call the graph comparison function. After the ETL has been performed for a specific record, call the graph comparison function to validate whether the linked data in the Internal Store is isomorphic to the linked data that was manually specified.","title":"Graph comparison for validation"},{"location":"triply-etl/validate/graph-comparison/#step-1-identify-representative-records","text":"To keep our example simple, we use a data source with three simple JSON records: fromJson([ { id: '1', price: 15 }, { id: '2', price: 12 }, { color: 'red', id: '3', price: 16 }, ]), We choose records 1 and 3 as the representative ones. Together these two records use all properties that occur in the source data. These records also allow us to test whether the optional color property is handled correctly.","title":"Step 1: Identify representative records"},{"location":"triply-etl/validate/graph-comparison/#step-2-manually-create-linked-data","text":"For each record selected in Step 1, we create the linked data that must be generated by TriplyETL: prefix sdo: <https://schema.org/> [] sdo:value 15. and: prefix sdo: <https://schema.org/> [] sdo:color 'red'; sdo:value 16.","title":"Step 2: Manually create linked data"},{"location":"triply-etl/validate/graph-comparison/#step-3-implement-the-etl","text":"We use RATT to implement the assertions: addSkolemIri({ key: '_product' }), triple('_product', sdo.price, 'price'), when('width', triple('_product', sdo.color, 'color')),","title":"Step 3: Implement the ETL"},{"location":"triply-etl/validate/graph-comparison/#step-4-call-the-graph-comparison-function","text":"Since comparison graphs must be created by hand, only a small number of records will have a corresponding comparison graph. The graph comparison call must therefore only be performed for some records. We use the switch control structure to determine whether a record is eligible for graph comparison.","title":"Step 4. Call the graph comparison function"},{"location":"triply-etl/validate/graph-comparison/#full-script","text":"By completing the 4 steps, we end up with the following full TriplyETL script that applies graph comparison to a limited number of representative records: import { _switch, compareGraphs, Etl, fromJson, Source, when } from '@triplyetl/etl/generic' import { addSkolemIri, triple } from '@triplyetl/etl/ratt' import { sdo } from '@triplyetl/vocabularies' export default async function (): Promise<Etl> { const etl = new Etl() etl.use( fromJson([ { id: '1', price: 15 }, { id: '2', price: 12 }, { color: 'red', id: '3', price: 16 }, ]), addSkolemIri({ key: '_product' }), triple('_product', sdo.price, 'price'), when('width', triple('_product', sdo.color, 'color')), _switch('id', ['1', compareGraphs(Source.string(` prefix sdo: <https://schema.org/> [] sdo:value 15.`))], ['3', compareGraphs(Source.string(` prefix sdo: <https://schema.org/> [] sdo:color 'red'; sdo:value 16.`))], ), ) return etl } This script succeeds, since the linked data generated by the ETL is isomorphic to the manually specified data.","title":"Full script"},{"location":"triply-etl/validate/graph-comparison/#options","text":"contentType specifies the RDF format in which the comparison graphs are serialized. The following values are supported: \"application/json\" \"application/ld+json\" \"application/n-quads\" \"application/n-triples\" \"application/rdf+xml\" \"application/trig\" \"text/html\" \"text/n3\"; \"text/turtle\" defaultGraph specifies the named graph in which the comparison graph must be loaded. This is only useful if the chosen RDF serialization format cannot express named graphs. key unknown","title":"Options"},{"location":"triply-etl/validate/graph-comparison/#see-also","text":"Graph comparison makes use of graph isomorphism, which is part of the RDF 1.1 Semantics standard ( external link ).","title":"See also"},{"location":"triply-etl/validate/shacl/","text":"On this page: SHACL Validation Prerequisites A complete example Step 1: Source data Step 2: Target data (informal) Step 3: Information Model (informal) Step 4: Transformation Step 5: Information Model (formal) Step 6: Use the validate() function Step 7: Fix the validation error Option 1: Change the source data Option 2: Change the transformation and/or assertions Option 3: Change the Information Model Reflections on which option to choose SHACL Validation \u00b6 This page documents how SHACL is used to validate linked data in the internal store of your ETL pipeline. Prerequisites \u00b6 SHACL Validation can be used when the following preconditions are met: A data model that uses SHACL constraints. Some data must be asserted in the internal store. If your internal store is still empty, you can read the Assert documentation on how to add assertions to that store. The function for running SHACL Validation is imported as follows: import { validate } from '@triplyetl/etl/shacl' A complete example \u00b6 We use the following full TriplyETL script to explain the validation feature. Do not worry about the length of the script; we will go through each part step-by-step. import { Etl, Source, declarePrefix, fromJson, toTriplyDb } from '@triplyetl/etl/generic' import { iri, pairs } from '@triplyetl/etl/ratt' import { validate } from '@triplyetl/etl/shacl' import { a, foaf } from '@triplyetl/vocabularies' const prefix = { id: declarePrefix('https://triplydb.com/Triply/example/id/'), } export default async function (): Promise<Etl> { const etl = new Etl() etl.use( fromJson([{ age: 'twelve', id: '1' }]), pairs(iri(prefix.id, 'id'), [a, foaf.Person], [foaf.age, 'age'], ), validate(Source.string(` prefix foaf: <http://xmlns.com/foaf/0.1/> prefix rdf: <http://www.w3.org/1999/02/22-rdf-syntax-ns#> prefix sh: <http://www.w3.org/ns/shacl#> prefix shp: <https://triplydb.com/Triply/example/model/shp/> prefix xsd: <http://www.w3.org/2001/XMLSchema#> shp:Person a sh:NodeShape; sh:closed true; sh:ignoredProperties ( rdf:type ); sh:property shp:Person_age; sh:targetClass foaf:Person. shp:Person_age a sh:PropertyShape; sh:datatype xsd:nonNegativeInteger; sh:maxCount 1; sh:minCount 1; sh:path foaf:age.` )), toTriplyDb({ dataset: 'test' }), ) return etl } Step 1: Source data \u00b6 In our example we are using the following source data that records the age of a person: { \"age\": \"twelve\", \"id\": \"id\" } In our example the data source is inline JSON , but notice that any source format could have been used: fromJson([{ age: 'twelve', id: '1' }]), Step 2: Target data (informal) \u00b6 Based on the source data in Step 1, we want to publish the following linked data in TriplyDB: id:123 a foaf:Person; foaf:age 'twelve'. Step 3: Information Model (informal) \u00b6 Our intended target data in Step 2 looks ok at first glance. But we want to specify the requirements for our data in generic terms. Such a specification is called an Information Model . An Information Model is a generic specification of the requirements for our data. It is common to illustrate an Information Model with a picture: classDiagram class foaf_Person { foaf_age: xsd_nonNegativeInteger [1..1] } This Information Model specifies that instances of class foaf:Person must have exactly one value for the foaf:age property. Values for this property must have datatype xsd:nonNegativeInteger . Step 4: Transformation \u00b6 We now have source data (Step 1), and a fair intuition about our target data (Step 2), and an Information Model (Step 3). We can automate the mapping from source to target data with an Assertion : etl.use( fromJson([{ age: 'twelve', id: '1' }]), pairs(iri(prefix.id, 'id'), [a, foaf.Person], [foaf.age, 'age'], ), ) That looks about right: we create instances of class foaf:Person and triples that assert a foaf:age property for each such person. However, a linked data expert may notice that the value 'twelve' from the source data will not be transformed into a non-negative integer ( xsd:nonNegativeInteger ). Indeed, our 'age' assertion will create a literal with datatype xsd:string . Oops, that violates the Information Model! How can we automate such checks? The above example is relatively simple, so a linked data expert may notice the error and fix it. But what happens when the ETL configuration is hundreds of lines long and is spread across multiple files? What happens when there is a large number of classes, and each class has a large number of properties? What if some of the properties are required, while others are optional? Etc. Obviously, any real-world ETL will quickly become too complex to validate by hand. For this reason, TriplyETL provides automated validation. Triply considers having an automated validation step best practice for any ETL. This is the case even for small and simple ETLs, since they tend to grow into complex ones some day. Step 5: Information Model (formal) \u00b6 The linked data ecosystem includes the SHACL standard for encoding Information Models. SHACL allows us to formally express the picture from Step 3. The model is itself expressed in linked data: prefix foaf: <http://xmlns.com/foaf/0.1/> prefix rdf: <http://www.w3.org/1999/02/22-rdf-syntax-ns#> prefix sh: <http://www.w3.org/ns/shacl#> prefix shp: <https://triplydb.com/Triply/example/model/shp/> prefix xsd: <http://www.w3.org/2001/XMLSchema#> shp:Person a sh:NodeShape; sh:closed true; sh:ignoredProperties ( rdf:type ); sh:property shp:Person_age; sh:targetClass foaf:Person. shp:Person_age a sh:PropertyShape; sh:datatype xsd:nonNegativeInteger; sh:maxCount 1; sh:minCount 1; sh:path foaf:age. Notice the following details: We enforce a Closed World Semantics (CWA) in our Information Models with the sh:closed property. If a property is not explicitly specified in our Information Model, it is not allowed to be used with instance data. We create IRIs in the dedicated shp: namespace for nodes in the Information Model. Elements in our Information Model are always in a one-to-one correspondence with elements in our Knowledge Model: Node shapes such as shp:Person relate to a specific class such as foaf:Person . Property shapes such as shp:Person_age relate to a specific property such as foaf:age . Step 6: Use the validate() function \u00b6 TriplyETL has a dedicated function that can be used to automatically enforce Information Models such as the one expressed in Step 5. Since the Information Model is relatively small, it can be specified in-line using the string source type . Larger models will probably be stored in a separate file or in a TriplyDB graph or asset. validate(Source.string(` prefix foaf: <http://xmlns.com/foaf/0.1/> prefix rdf: <http://www.w3.org/1999/02/22-rdf-syntax-ns#> prefix sh: <http://www.w3.org/ns/shacl#> prefix shp: <https://triplydb.com/Triply/example/model/shp/> prefix xsd: <http://www.w3.org/2001/XMLSchema#> shp:Person a sh:NodeShape; sh:closed true; sh:ignoredProperties ( rdf:type ); sh:property shp:Person_age; sh:targetClass foaf:Person. shp:Person_age a sh:PropertyShape; sh:datatype xsd:nonNegativeInteger; sh:maxCount 1; sh:minCount 1; sh:path foaf:age.` )), When we run the validate() function at the end of our ETL script, we will receive the following error: ERROR (Record #1) SHACL Violation on node id:1 for path foaf:age, source shape shp:Person_age: 1. Value does not have datatype xsd:nonNegativeInteger Oops! The value for the foaf:age property has an incorrect datatype. This is indeed the automated check and feedback that we want. Notice that the requirement that was violated ( shp:Person_age ) is mentioned in the notification. If we want to learn more, we can look up this node in our Information Model. If we want to take a look at a concrete example in our instance data, we can also take look at node id:1 which is also mentioned in the notification. The snippet below uses a file called model.trig as the Information Model and stores the report in a graph called https://example.org . validate(Source.file('static/model.trig'), { graph: \"https://example.org\" }) If we want to upload the report to TriplyDB, we can do this like in the example below. validate(Source.file('static/model.trig'), { graph: 'https://example.org' }) toRdf(Destination.TriplyDb.rdf('my-account', 'my-dataset'), { includeGraphs: ['https://example.org'] })) Step 7: Fix the validation error \u00b6 Now that we receive the automated validation error in Step 6, we can look for ways to fix our ETL. Let us take one more look at our current assertions: etl.run( fromJson([{ age: 'twelve', id: '1' }]), pairs(iri(prefix.id, 'id'), [a, foaf.Person], [foaf.age, 'age'], ), ) We could change the term assertion for the value of foaf:age to explicitly make use of the xsd:nonNegativeInteger datatype: literal('age', xsd.nonNegativeInteger) But that would not work in TriplyETL: the Triply software (luckily!) does not allow us to create incorrect linked data. Since the following literal would be incorrect, TriplyETL does not even allow us to assert it: 'twelve'^^xsd:nonNegativeInteger Well, it is nice that TriplyETL does not allow us to create incorrect data. But how can we fix the issue at hand? How can we create linked data that follows our Information Model? As in any ETL error, there are 3 possible solutions: Change the data in the source system. Change the ETL transformations and/or assertions. Change the Information Model. Option 1: Change the source data \u00b6 In this case, changing the data in the source system seem the most logical. After all, there may be multiple ways in which the age of a person can be described using one or more English words. Expressing ages numerically is a good idea in general, since it will make the source data easier to interpret. Option 2: Change the transformation and/or assertions \u00b6 Alternatively, it is possible to transform English words that denote numbers to their corresponding numeric values. Since people can get up to one hundred years old, or even older, there are many words that we must consider and transform. This can be done with the translateAll() transformation : translateAll({ content: 'age', table: { 'one': 1, ... 'twelve': 12, ..., 'one hundred': 100, ..., }, key: '_age', }), pairs(iri(prefix.id, 'id'), [a, foaf.Person], [foaf.age, literal('_age', xsd.nonNegativeInteger)], ), But even the above transformation may not suffice. The same number can be expressed in multiple ways in natural language, so the mapping will never be truly complete and reliable. This seems to be the worst of the three options in this case. Option 3: Change the Information Model \u00b6 Finally, we could loosen the Information Model. For example, we could change the datatype to check for strings: shp:Person_age sh:datatype xsd:string. But that would invalidate ETLs that generate numeric ages for persons, even though that seems perfectly fine, if not better than generating strings. Also, this would allow literals like 'abc' to pass validation as a legal value for foaf:age . Alternatively, we can remove the sh:datatype requirement from our Information Model entirely. That would allow either string-based ages or numeric ages to be specified. But now even weirder values for age, e.g. '2023-01-01'^^xsd:date , would be considered valid values for age. Reflections on which option to choose \u00b6 Notice that TriplyETL does not tell you which of the 3 options you should follow in order to fix issues in your ETL. After all, creating an ETL requires domain knowledge based on which you weight the pros and const of different options. However, TriplyETL does give you the tools to discover issues that prompt you to come up with such solutions. And once you have decided on a specific solution, TriplyETL provides you with the tools to implement it.","title":"SHACL"},{"location":"triply-etl/validate/shacl/#shacl-validation","text":"This page documents how SHACL is used to validate linked data in the internal store of your ETL pipeline.","title":"SHACL Validation"},{"location":"triply-etl/validate/shacl/#prerequisites","text":"SHACL Validation can be used when the following preconditions are met: A data model that uses SHACL constraints. Some data must be asserted in the internal store. If your internal store is still empty, you can read the Assert documentation on how to add assertions to that store. The function for running SHACL Validation is imported as follows: import { validate } from '@triplyetl/etl/shacl'","title":"Prerequisites"},{"location":"triply-etl/validate/shacl/#a-complete-example","text":"We use the following full TriplyETL script to explain the validation feature. Do not worry about the length of the script; we will go through each part step-by-step. import { Etl, Source, declarePrefix, fromJson, toTriplyDb } from '@triplyetl/etl/generic' import { iri, pairs } from '@triplyetl/etl/ratt' import { validate } from '@triplyetl/etl/shacl' import { a, foaf } from '@triplyetl/vocabularies' const prefix = { id: declarePrefix('https://triplydb.com/Triply/example/id/'), } export default async function (): Promise<Etl> { const etl = new Etl() etl.use( fromJson([{ age: 'twelve', id: '1' }]), pairs(iri(prefix.id, 'id'), [a, foaf.Person], [foaf.age, 'age'], ), validate(Source.string(` prefix foaf: <http://xmlns.com/foaf/0.1/> prefix rdf: <http://www.w3.org/1999/02/22-rdf-syntax-ns#> prefix sh: <http://www.w3.org/ns/shacl#> prefix shp: <https://triplydb.com/Triply/example/model/shp/> prefix xsd: <http://www.w3.org/2001/XMLSchema#> shp:Person a sh:NodeShape; sh:closed true; sh:ignoredProperties ( rdf:type ); sh:property shp:Person_age; sh:targetClass foaf:Person. shp:Person_age a sh:PropertyShape; sh:datatype xsd:nonNegativeInteger; sh:maxCount 1; sh:minCount 1; sh:path foaf:age.` )), toTriplyDb({ dataset: 'test' }), ) return etl }","title":"A complete example"},{"location":"triply-etl/validate/shacl/#step-1-source-data","text":"In our example we are using the following source data that records the age of a person: { \"age\": \"twelve\", \"id\": \"id\" } In our example the data source is inline JSON , but notice that any source format could have been used: fromJson([{ age: 'twelve', id: '1' }]),","title":"Step 1: Source data"},{"location":"triply-etl/validate/shacl/#step-2-target-data-informal","text":"Based on the source data in Step 1, we want to publish the following linked data in TriplyDB: id:123 a foaf:Person; foaf:age 'twelve'.","title":"Step 2: Target data (informal)"},{"location":"triply-etl/validate/shacl/#step-3-information-model-informal","text":"Our intended target data in Step 2 looks ok at first glance. But we want to specify the requirements for our data in generic terms. Such a specification is called an Information Model . An Information Model is a generic specification of the requirements for our data. It is common to illustrate an Information Model with a picture: classDiagram class foaf_Person { foaf_age: xsd_nonNegativeInteger [1..1] } This Information Model specifies that instances of class foaf:Person must have exactly one value for the foaf:age property. Values for this property must have datatype xsd:nonNegativeInteger .","title":"Step 3: Information Model (informal)"},{"location":"triply-etl/validate/shacl/#step-4-transformation","text":"We now have source data (Step 1), and a fair intuition about our target data (Step 2), and an Information Model (Step 3). We can automate the mapping from source to target data with an Assertion : etl.use( fromJson([{ age: 'twelve', id: '1' }]), pairs(iri(prefix.id, 'id'), [a, foaf.Person], [foaf.age, 'age'], ), ) That looks about right: we create instances of class foaf:Person and triples that assert a foaf:age property for each such person. However, a linked data expert may notice that the value 'twelve' from the source data will not be transformed into a non-negative integer ( xsd:nonNegativeInteger ). Indeed, our 'age' assertion will create a literal with datatype xsd:string . Oops, that violates the Information Model! How can we automate such checks? The above example is relatively simple, so a linked data expert may notice the error and fix it. But what happens when the ETL configuration is hundreds of lines long and is spread across multiple files? What happens when there is a large number of classes, and each class has a large number of properties? What if some of the properties are required, while others are optional? Etc. Obviously, any real-world ETL will quickly become too complex to validate by hand. For this reason, TriplyETL provides automated validation. Triply considers having an automated validation step best practice for any ETL. This is the case even for small and simple ETLs, since they tend to grow into complex ones some day.","title":"Step 4: Transformation"},{"location":"triply-etl/validate/shacl/#step-5-information-model-formal","text":"The linked data ecosystem includes the SHACL standard for encoding Information Models. SHACL allows us to formally express the picture from Step 3. The model is itself expressed in linked data: prefix foaf: <http://xmlns.com/foaf/0.1/> prefix rdf: <http://www.w3.org/1999/02/22-rdf-syntax-ns#> prefix sh: <http://www.w3.org/ns/shacl#> prefix shp: <https://triplydb.com/Triply/example/model/shp/> prefix xsd: <http://www.w3.org/2001/XMLSchema#> shp:Person a sh:NodeShape; sh:closed true; sh:ignoredProperties ( rdf:type ); sh:property shp:Person_age; sh:targetClass foaf:Person. shp:Person_age a sh:PropertyShape; sh:datatype xsd:nonNegativeInteger; sh:maxCount 1; sh:minCount 1; sh:path foaf:age. Notice the following details: We enforce a Closed World Semantics (CWA) in our Information Models with the sh:closed property. If a property is not explicitly specified in our Information Model, it is not allowed to be used with instance data. We create IRIs in the dedicated shp: namespace for nodes in the Information Model. Elements in our Information Model are always in a one-to-one correspondence with elements in our Knowledge Model: Node shapes such as shp:Person relate to a specific class such as foaf:Person . Property shapes such as shp:Person_age relate to a specific property such as foaf:age .","title":"Step 5: Information Model (formal)"},{"location":"triply-etl/validate/shacl/#step-6-use-the-validate-function","text":"TriplyETL has a dedicated function that can be used to automatically enforce Information Models such as the one expressed in Step 5. Since the Information Model is relatively small, it can be specified in-line using the string source type . Larger models will probably be stored in a separate file or in a TriplyDB graph or asset. validate(Source.string(` prefix foaf: <http://xmlns.com/foaf/0.1/> prefix rdf: <http://www.w3.org/1999/02/22-rdf-syntax-ns#> prefix sh: <http://www.w3.org/ns/shacl#> prefix shp: <https://triplydb.com/Triply/example/model/shp/> prefix xsd: <http://www.w3.org/2001/XMLSchema#> shp:Person a sh:NodeShape; sh:closed true; sh:ignoredProperties ( rdf:type ); sh:property shp:Person_age; sh:targetClass foaf:Person. shp:Person_age a sh:PropertyShape; sh:datatype xsd:nonNegativeInteger; sh:maxCount 1; sh:minCount 1; sh:path foaf:age.` )), When we run the validate() function at the end of our ETL script, we will receive the following error: ERROR (Record #1) SHACL Violation on node id:1 for path foaf:age, source shape shp:Person_age: 1. Value does not have datatype xsd:nonNegativeInteger Oops! The value for the foaf:age property has an incorrect datatype. This is indeed the automated check and feedback that we want. Notice that the requirement that was violated ( shp:Person_age ) is mentioned in the notification. If we want to learn more, we can look up this node in our Information Model. If we want to take a look at a concrete example in our instance data, we can also take look at node id:1 which is also mentioned in the notification. The snippet below uses a file called model.trig as the Information Model and stores the report in a graph called https://example.org . validate(Source.file('static/model.trig'), { graph: \"https://example.org\" }) If we want to upload the report to TriplyDB, we can do this like in the example below. validate(Source.file('static/model.trig'), { graph: 'https://example.org' }) toRdf(Destination.TriplyDb.rdf('my-account', 'my-dataset'), { includeGraphs: ['https://example.org'] }))","title":"Step 6: Use the validate() function"},{"location":"triply-etl/validate/shacl/#step-7-fix-the-validation-error","text":"Now that we receive the automated validation error in Step 6, we can look for ways to fix our ETL. Let us take one more look at our current assertions: etl.run( fromJson([{ age: 'twelve', id: '1' }]), pairs(iri(prefix.id, 'id'), [a, foaf.Person], [foaf.age, 'age'], ), ) We could change the term assertion for the value of foaf:age to explicitly make use of the xsd:nonNegativeInteger datatype: literal('age', xsd.nonNegativeInteger) But that would not work in TriplyETL: the Triply software (luckily!) does not allow us to create incorrect linked data. Since the following literal would be incorrect, TriplyETL does not even allow us to assert it: 'twelve'^^xsd:nonNegativeInteger Well, it is nice that TriplyETL does not allow us to create incorrect data. But how can we fix the issue at hand? How can we create linked data that follows our Information Model? As in any ETL error, there are 3 possible solutions: Change the data in the source system. Change the ETL transformations and/or assertions. Change the Information Model.","title":"Step 7: Fix the validation error"},{"location":"triply-etl/validate/shacl/#option-1-change-the-source-data","text":"In this case, changing the data in the source system seem the most logical. After all, there may be multiple ways in which the age of a person can be described using one or more English words. Expressing ages numerically is a good idea in general, since it will make the source data easier to interpret.","title":"Option 1: Change the source data"},{"location":"triply-etl/validate/shacl/#option-2-change-the-transformation-andor-assertions","text":"Alternatively, it is possible to transform English words that denote numbers to their corresponding numeric values. Since people can get up to one hundred years old, or even older, there are many words that we must consider and transform. This can be done with the translateAll() transformation : translateAll({ content: 'age', table: { 'one': 1, ... 'twelve': 12, ..., 'one hundred': 100, ..., }, key: '_age', }), pairs(iri(prefix.id, 'id'), [a, foaf.Person], [foaf.age, literal('_age', xsd.nonNegativeInteger)], ), But even the above transformation may not suffice. The same number can be expressed in multiple ways in natural language, so the mapping will never be truly complete and reliable. This seems to be the worst of the three options in this case.","title":"Option 2: Change the transformation and/or assertions"},{"location":"triply-etl/validate/shacl/#option-3-change-the-information-model","text":"Finally, we could loosen the Information Model. For example, we could change the datatype to check for strings: shp:Person_age sh:datatype xsd:string. But that would invalidate ETLs that generate numeric ages for persons, even though that seems perfectly fine, if not better than generating strings. Also, this would allow literals like 'abc' to pass validation as a legal value for foaf:age . Alternatively, we can remove the sh:datatype requirement from our Information Model entirely. That would allow either string-based ages or numeric ages to be specified. But now even weirder values for age, e.g. '2023-01-01'^^xsd:date , would be considered valid values for age.","title":"Option 3: Change the Information Model"},{"location":"triply-etl/validate/shacl/#reflections-on-which-option-to-choose","text":"Notice that TriplyETL does not tell you which of the 3 options you should follow in order to fix issues in your ETL. After all, creating an ETL requires domain knowledge based on which you weight the pros and const of different options. However, TriplyETL does give you the tools to discover issues that prompt you to come up with such solutions. And once you have decided on a specific solution, TriplyETL provides you with the tools to implement it.","title":"Reflections on which option to choose"},{"location":"triplydb-changelog/","text":"On this page: 26.2.200 26.2.100 26.1.200 26.1.100 25.12.100 25.11.200 25.11.100 25.10.301 25.10.300 25.10.200 25.10.100 25.9.100 25.8.200 25.08.100 25.7.100 25.6.200 25.6.100 25.5.100 25.5.100 25.4.300 25.4.200 25.4.100 25.3.200 25.3.100 25.2.200 25.2.100 25.1.200 25.01.1 24.12.2 24.12.1 24.11.2 24.11.1 24.10.2 24.10.1 24.09.2 24.09.1 24.08.2 24.08.1 24.08.0 24.07.1 24.07.0 24.06.1 24.06.0 24.05.1 24.05.0 24.04.1 24.04.0 24.03.1 24.03.0 24.02.2 24.02.1 24.02.0 24.01.0 23.12.1 23.12.0 23.11.1 23.11.0 26.2.200 \u00b6 Release date: 2026-02-18 Features #11150 [Data Editor] Improved the SKOS setup experience. When no SKOS concepts are present in a dataset, users now see guidance on how to get started with SKOS navigation. #11069 [Console] Improved loading performance of the admin queries table. Issues fixed #11175 [Data Upload] Fixed an issue where importing data from a URL could fail because an already decompressed file was incorrectly treated as compressed. #10653 [SPARQL] Fixed an issue where introspection functions (such as tf:authenticated_user_url() ) did not work in SPARQL UPDATE queries. These functions now work in any WHERE clause. #10440 [SPARQL] Fixed an issue where the SERVICE clause did not work in SPARQL UPDATE queries, even though the same clause worked correctly in SELECT queries. 26.2.100 \u00b6 Release date: 2026-02-04 Features #11097 [Dataset Settings] Added support for disabling features on a dataset level. Dataset owners can now hide specific features such as the SPARQL IDE, LD-Browser, data editor, assets, services, and the triples table from the dataset interface to simplify the user experience. #11005 [Data Editor] Next to editing skos hierarchies, the dataset editor now supports editing and navigating resources from any data model. #11010 [Data Editor] Improved instance search to support searching by IRI in addition to labels. When a class has many instances, the system now fetches and displays labels properly in the outline. #10838 [Data Editor] Improved layout behavior for the Data Model and Data Editor pages. Menu items are now only shown when there is relevant content for the user to see or edit. #10962 [Tags] Supports tagging queries, datasets and stories, supporting more flexibility in grouping such items. Searching by tag is possible in the faceted search for datasets, queries, and stories. Issues fixed #11094 [Forms] In some cases, the UI does not properly show an error message when there is wrong user input. Users now receive clear feedback when form values are invalid. [SPARQL] Fixed an issue SERVICE and some SPARQL functions were not returning the correct results for SPARQL UPDATE queries. [API] Fixed an API validation issue, where requests to create a saved query were wrongly rejected. 26.1.200 \u00b6 Release date: 2026-01-21 Features #10979 [Tabular uploads] Improved the CSV, TSV and XML import representation, using the facade-x representation. This is documented here . Issues fixed [Large file uploads] Large file uploads in TriplyDB use the TUS protocol . Some TUS clients use the legacy x-patch response header, which caused issues. TriplyDB now supports this x-patch header in TUS uploads, improving compatibility with additional client libraries such as the Java client. #11089 Fixed an issue where uploading a file from URL could show a download progress of more than 100%. #10705 [SPARQL] Improved error handling when federated SPARQL queries (using the service <...> { ... } clause) encounter timeouts or errors at third-party endpoints. Users now receive clearer error messages. 26.1.100 \u00b6 Release date: 2026-01-07 Issues fixed #11065 [Editor] Fixed issue where too many triples were incorrectly displayed as changed in the editor history view. #11064 [Dataset Settings] Fixed UI error when removing all prefix declarations from dataset settings. #11035 [Editor] Fixed issue where autocomplete results weren't shown. #11033 Fixed form issue, where an error message was not shown to the user 25.12.100 \u00b6 Release date: 2025-12-10 Features None Issues fixed #10251 [Editor] Tree structure in the editor now automatically updates after modifying concept hierarchies, eliminating the need to refresh the browser to see changes reflected in the tree view. #11024 [Data Model] Resolved inconsistent language display where Dutch and English labels were mixed within the same view. The system now consistently applies language preferences for labels and descriptions. #11036 [Data Model] Fixed a crash on the Data Model page caused by ambiguous tree root structures in certain datasets. 25.11.200 \u00b6 Release date: 2025-11-27 Issues fixed #10861 [Speedy] Fixed intermittent 502 Bad Gateway errors when executing SPARQL queries with complex function calls. Server errors are now properly handled and reported with clear error messages. 25.11.100 \u00b6 Release date: 2025-11-13 Issues fixed #10920 [SPARQL IDE] Certain pivot visualization errors would make the SPARQL IDE unrecoverable without refreshing the page. Fixed serialization handling for RDF literals containing unusual Unicode line-break characters. 25.10.301 \u00b6 Release date: 2025-11-06 Patch release for: #10872 [Data model] The search & find functionality may return incomplete results #10933 [Data editor] Copying a resource does not always work, making it impossible to copy an existing resource 25.10.300 \u00b6 Release date: 2025-10-29 Features #10885 [Tabular uploads] Introduced a new CSV/TSV import mechanism that preserves all information from the source files, enabling lossless roundtrip conversions between CSV and RDF representations. The new approach properly handles edge cases like duplicate column headers, multiple CSV files collapsing into the same graph, and hierarchical header structures. #10742 [Data Model] Merged the separate property type and type fields into a single unified type selector. The system automatically infers whether a value should be a literal or IRI based on the selected datatype or class, reducing the number of fields users need to interact with while maintaining full functionality. #10670 [Speedy] Made SPARQL graph management operations more scalable: Operations now operate on graphs of any size. Issues fixed #10916 [Speedy] Fixed query job failures for certain queries that processed very large CONSTRUCT query results. Query jobs now robustly handles arbitrary result set sizes to generate hundreds of millions of triples without memory errors. #10892 [Data Editor] Fixed an issue where editing an instance with a root-only prefix would append a random ID to the IRI, causing the instance to appear as lost. The editor now preserves the original root IRI during edits, preventing unexpected IRI modifications. 25.10.200 \u00b6 Release date: 2025-10-16 Features #10827 [Data Model] The side panel on the data model page can now be resized for better workspace customization. Issues fixed #10862 [Speedy] Cache not invalidated after update to global functions dataset - query caches are now properly invalidated when global SPARQL functions are modified. #10842 [Editor] Allowed languages feature (sh:languageIn) is not always correctly applied - language selector now correctly displays and applies configured language constraints from SHACL shapes. #10835 [Editor] Copy feature doesn't validate copied resource before saving - the copy functionality now validates resources using the same constraints as creating and editing resources. #10829 [Data Model] Properties appear as inherited - properties are no longer incorrectly marked as inherited when they belong to the current class. #10777 [Data model]: Update failures during data model operations are now properly reported to users with error messages. #10665 [Editor] In the Editor properties with same sh:path shown multiple times - properties are now deduplicated by path, showing each unique property only once regardless of how many SHACL shapes constrain it. 25.10.100 \u00b6 Release date: 2025-10-02 Features #10790 [Editor] Support providing both instructions and descriptions of form fields - forms now support short instructions (displayed as helper text) and detailed descriptions (displayed behind an info button) using the new triply:hint property for instructions and existing description properties for detailed explanations. #10753 [Editor] Add Numeric editor - numeric fields now use an appropriately sized editor with validation, replacing the oversized string input previously used for numbers. #10706 [Data Model] Update styling of other forms - class and constraint forms now use the improved styling introduced for property forms, with better organization of fields including collapsible advanced settings. #10587 [Editor] Ability to rollback a revision of a resource - resources can now be reverted to previous states from the history view, creating a new revision that restores the selected historical state. Issues fixed #10672 [Editor] Required Boolean properties must be set to 'true' - boolean fields are now automatically displayed with checkboxes defaulting to false, allowing forms to be saved with either true or false values. #10428 [SPARQL] Incorrect handling of unbound parameters in SPARQL Functions - fixed issue where unbound parameters in SPARQL functions were not properly handled, particularly in switch statements with default expressions. 25.9.100 \u00b6 Release date: 2025-09-18 Features #10317 [Editor] The history view of a resource is more intuitive - the difference between revisions uses human readable labels as much as possible, and the differences between revisions are clearer. #10752 [Editor] Add prefix support - IRIs in the editor now apply prefixes for better readability #10743 [Editor] A SPARQL viewer - SPARQL literals are now displayed with syntax highlighting using a dedicated viewer #10738 [Editor] Enable map view of wkt literals - geometry data can now be displayed on a map in the Editor #10633 [Data model] Support sh:languageIn and sh:uniqueLang for properties with the rdf:langString datatype - enhanced language validation for multilingual content Issues fixed #10775 [Editor] Automatically focusing the browser to a reported error in a form may not always work #10770 Support re-using SPARQL functions between datasets - the dataset to re-use can be configured by a site administrator #10703 [Editor] Searching for a selected item closes the tree - fixed search behavior that incorrectly collapsed the tree view #10301 [Editor] Sort autocomplete lists alphabetically - autocomplete suggestions are now properly sorted for better usability 25.8.200 \u00b6 Release date: 2025-08-28 Features #10630 [Editor] Support sh:nodeKind sh:IRI in Editor and Data model #10605 [Editor] Data model: support sh:hasValue and sh:in #10590 [Editor] Improve the handling of errors in the form #10302 [Editor] Automatically create default fields when applicable #10213 Support all sparql update graph management operations #9522 Optimize performance for SHACL SPARQL functions Issues fixed #10722 In atypical cases, you may get upload errors saying the graph name already exists #10711 [Editor] Copy operations use the wrong IRI #10700 SPARQL drop all fails #10694 [Data Model] The instances tab may contain duplicate results #10689 [Editor Dialog] Stem is used incorrectly for nested nodes #10687 [Data Model] Stem disappears from class #10659 [Editor View] Editor and Data model does not update after adding/removing graphs #10648 [Editor View] SHACL SPARQL constraints are not checking the class #10644 [Editor View] Inconsistent GUI state after deleting an instance #10607 anyURI literals are not validated validated #10418 Some escape charactes do not work in Regex function #9267 [TriplyDB-JS] Can't update dataset used by saved query 25.08.100 \u00b6 Release date: 2025-08-06 Features #10604 Editor: support sh:hasValue - when a new field value is added, it gets the value specified with sh:hasValue by default #10603 Editor: support sh:in - the Editor shows a select field in the form when sh:in is used to specify a list of allowed values #10309 Editor: Expand all offspring of one top level SKOS concept - allows SHIFT + click to expand one whole top level concept and its offspring #10226 Api | Allow SPARQL-Update queries in run-path of saved queries Issues fixed #10697 [Data Model] Editing a class label, deletes its order #10649 [LD Table] Dragging and dropping terms no longer works #10639 [Data Model View] Min count is displayed as max count #10121 Concatenation functions are too slow due to reordering 25.7.100 \u00b6 Release date: 2025-07-16 Features #10261 [Editor] Support entering language-tagged strings #10543 [Editor] Support collapsing and expanding groups #10240 [Editor] Automatically fill in the range class when adding a new instance #10449 Support long running select-queries in query jobs #10557 Improve user experience when copying IRIs in LD-Browser #10542 [SPARQL-IDE] Report errors in the IDE when a query includes expressions (e.g. BIND(..) as ?x ) that reference a variable that are out of scope #10478 Improve performance for autocompleting terms in e.g. the SPARQL IDE or data stories. #10600 Improve error messages when using SHACL functions, reporting whether a function does not exist, or whether a wrong number of arguments is used. Issues fixed #10432 [SPARQL-IDE, LD-Browser] SVGs not always rendered correctly in SPARQL #9967 [SPARQL-update] Concurrent requests may result in wrong modifications #10489 [LD-Table] Filtering on literals may return 0 autocompletion results #10522 Dataset sidebar behaves odd on page refresh #10191 An atypical query job never starts #10545 An atypical query always returns an error 25.6.200 \u00b6 Release date: 2025-06-26 Features #10308 Added visual cue for readonly properties in the editor #10300 Display modification dates for saved query versions Issues fixed #10380 Intermediate changes in TriplyDB may interrupt service updates #10438 Copying a sparql query as curl command may result in an invalid shell command #10512 Atypical queries that run as query job consistently fail after 5 minutes 25.6.100 \u00b6 Release date: 2025-06-05 Issues fixed - #10477 Cannot access index information for elasticsearch - #10380 Intermediate changes in TriplyDB may interrupt service updates - #10438 Copying a sparql query as curl command may result in an invalid shell command 25.5.100 \u00b6 Release date: 2025-05-14 Features #10058 Apply stricter content-security-policies in the UI, making the UI more robust against illegal use and cross site scripting #6218 Make API error messages more consistent Issues fixed - #10386 Uploading CSVs may result in errors - #10369 Improve visual queues in the UI for which resources are public, internal or private - #10429 API requests are dropped in rare cases (0.0002% of all requests) 25.5.100 \u00b6 Release date: 2025-05-14 Features #10058 Apply stricter content-security-policies in the UI, making the UI more robust against illegal use and cross site scripting #6218 Make API error messages more consistent Issues fixed - #10386 Uploading CSVs may result in errors - #10369 Improve visual queues in the UI for which resources are public, internal or private - #10429 API requests are dropped in rare cases (0.0002% of all requests) 25.4.300 \u00b6 Release date: 2025-04-28 Features #10095 Improved speedy performance when querying more graphs. Depending on the query and datastructure, you may get performance improvements of around 20%, with outliers to 100%. Issues fixed #10353 [Editor] Editor may show outdated views with simultaneous errors #10288 [Editor] sh:prefixes are unrecognized in the SHACL SPARQL constraints model #10359 [SPARQL IDE] Opening a chart visualization via a URL may result in an error 25.4.200 \u00b6 Release date: 2025-04-16 Features #10297 Support rendering SVGs in LD-Browser and SPARQL-IDE Issues fixed #10205 [Import] Parallel uploads my result in errors #10348 [SPARQL IDE] Interactions with IDE become sluggish for large query #10277 User is not always notified when an import from URL fails 25.4.100 \u00b6 Release date: 2025-04-03 Features #9776 [Query jobs] Insights into the error encountered by a query job #9935 [Editor] Allow a SKOS tree hierarchy without concept schemes #10272 [Assets] Gracefully handle uploading an asset with an existing name in the GUI #10054 [Output] Ability to export SPARQL results in more readable turtle format #9797 Upgrade elasticsearch from version 7 to version 8 #9768 [SPARQL UPDATE] Performance improvement when using mutations frequently #10175 [Import] Support for CSV, TSV and more use cases within those formats Issues fixed #9765 [Import] Namespaces are not resolved for uploaded XML files 25.3.200 \u00b6 Release date: 2025-03-20 Features #10129 [Editor] Display a list of concept schemes when no concept is selected #9768 [Graphs] General performance improvements #10189 [SAML] Allow email-based user lookup Issues fixed #10273 [Editor] Oops something went wrong when using the editor #10274 [Editor] Wrong title for property edit dialog #10250 [Editor] Server error when a field was not filled in #10110 [Editor] Super class / parent class cannot be cleared #10269 [Editor] Wrong title in staging dialog #10228 [Triples] Adding statements to the default graph does nothing #10221 [Speedy] Internal performance improvement #10138 [Speedy] Performance improvement for a queries containing multiple SHACL SPARQL functions #10276 [Speedy] 500 error for non-compliant literal #9794 [Data upload] CSV parser throws 500 error when delimiter is not consistent 25.3.100 \u00b6 Release date: 2025-03-06 Features #9896 [Editor] Improved changelog readability #10229 [Datasets] Improved metadata display, added changed date #10150 [Authentication] Improved SAML management #10184 [Dataset] Configuration of auto-generated ID in dataset settings #10131 [Triples] Improvement while filtering and creating terms Issues fixed #10123 [Data stories] Unexpected crashes within visualization. #10116 [Saved queries] API variable dialog, autocomplete display. #10169 [SPARQL-IDE] Autocompletion for literals is missing #10036 [Speedy] Union variables are not translated correctly in SHACL SPARQL functions #10106 [Speedy] GeoSPARQL functions do not work on polygons in different CRSes #10200 [Speedy] Values clause with 2 terminating bindings does not terminate 25.2.200 \u00b6 Release date: 2025-02-20 Features #10037 [SPARQL-IDE] Query autoformat indentation behavior, we use a simple and concise indentation scheme. #10067 [Editor] Configure read-only fields for the SKOS editor. #10052 [Editor] Configure search fields in the data model so that users can search on user defined labels. #10078 [Editor] SKOS Search performance improvements. #10041 [Assets] See who uploaded an asset. #9945 [Speedy] Performance of 20% of our benchmark queries increased by about 40% Issues fixed #10115 [Saved Queries] Autocomplete UI showed multi line options. #10144 [Saved Queries] \"Save as new version\" appeared not to work (while it did). #10057 [Speedy] SPARQL query with SHACL SPARQL functions return a 500 error #10161 [Speedy] encode_for_uri is implemented incorrect #10163 [Speedy] SHACL rules can not have a limit 25.2.100 \u00b6 Release date: 2025-02-06 Features #10094 [Editor] Show all SPARQL based constraint errors in editor #10079 [Editor] Show SPARQL based constraint errors at bottom of the form #10085 [Editor] Remember previous concept scheme hierarchy in the skos tree 25.1.200 \u00b6 Release date: 2025-01-23 Features New version scheme as of this release. Instead of 25.01.2 , we now release 25.1.200 . A patch release will be released as 25.1.201 #9581 Added the ability to restore queries that would have been lost due to navigating away from (or refreshing) the query page #3170 Account API tokens are now copied to the clipboard when clicked Improved the usability of admin tables Issues fixed #9716 [Speedy] Unexpected group_concat result when pre-bindings are used #10066 [Speedy] SHACL SPARQL function is executed even when the argument to it is unbound #9171 [SPARQL-IDE] Unexpected indentation after semi-colon #10045 [SPARQL-IDE] Empty lines are introduced when adding prefix declarations 25.01.1 \u00b6 Release date: 2025-01-09 Issues fixed #9929 Query jobs may incidentally hang #10062 Speedy queries that use federation may now work in firewalled on-premise environments #9999 Default variable values of saved queries are not always applied in query jobs 24.12.2 \u00b6 Release date: 2024-12-18 Issues fixed #10006 Cancelling a linked data upload may result in an error #9711 [SaaS] Incidental requests are dropped 24.12.1 \u00b6 Release date: 2024-12-05 Issues fixed #9888 [Query job] Query job for very simple construct query may fail #9959 [SPARQL IDE] SPARQL IDE may show same autocomplete suggestion multiple times #9954 [Query job] Query jobs may get sometimes inadvertently cancelled #9927 [Stories] Stories with many queries are slow to load in the browser 24.11.2 \u00b6 Release date: 2024-11-22 Issues fixed #9017 [Speedy] Some queries take too long when reordering/optimizing a query #9780 [SaaS] Enable OCSP stapling for TLS 24.11.1 \u00b6 Release date: 2024-11-08 Issues fixed #8878 Include correct dataset modification date for JSON schema descriptions #9694 Speedy SPARQL endpoints are not included in the dataset NDE descriptions #9807 Parsing of XML content assigns an incorrect datatype #9752 Parsing of XML is incorrect for escaped ampersand #9718 Unintentional grey bar after renaming a graph #9767 [Speedy] Incorrect n-triples serialization when handling blank nodes from external SPARQL endpoints #9557 [SPARQL-IDE] Yellow marker in SPARQL Editor is incorrect. This feature has been disabled until we improve some of the rough edges in future releases. #9791 [SPARQL-IDE] Error when tying to view a populated query from a gallery visualization with a query variable #9774 [SPARQL-IDE] Copy shortened URL doesn't work for webkit browsers 24.10.2 \u00b6 Release date: 2024-10-25 Issues fixed #9739 [Query jobs] Query job artifacts, exposed to system administrators, are incomplete #9740 [Query jobs] Cannot start a query job for queries with a default variable #9701 [Query jobs] A query job with invalid graph name fails at a late stage, where we expect it to fail early 24.10.1 \u00b6 Release date: 2024-10-11 Issues fixed #9695 [Speedy] ORDER BY doesn't order by the second operand if the first one is an error #9658 [SPARQL-IDE] Capture position doesn't work with height settings for stories 24.09.2 \u00b6 Release date: 2024-09-27 Features #8471 Admins on a TriplyDB instance can restrict users to only one active session #9148 [SPARQL-IDE] Added support for overlaying 3D tiles, from a URL, in the Geo visualization Issues fixed #9646 Base URI when uploading data always uses the uploading account instead of the target account #9561 Saving a new version of a saved query does not always save its visualization configuration #9600 When editing query metadata, the service \"Speedy\" is not always visible #9633 [Speedy] Regression, introduced in version 24.09.1, causing fatal error when executing certain queries 24.09.1 \u00b6 Features #9535 [SPARQL-IDE] The camera point-of-view can be saved as part of a query, in the geo visualization. This enables more precise use in stories #9527 [Speedy] Improved the user message when there are errors within a SHACL SPARQL function Issues fixed #9556 / #9550 [Speedy] Reordering of queries with both an OPTIONAL and ORDER BY clause did not always return the correct result #9579 [Speedy] Incorrect coordinate transformation from epsg:7415 to epsg:28992 when using geof:transform GeoSPARQL function #9528 [Speedy] Incorrect results when using a SHACL SPARQL function due to reordering #9569 [Speedy] Incorrect results when using a SHACL SPARQL function with blank nodes #9523 [SPARQL-IDE] Unable to export certain chart visualizations to SVG 24.08.2 \u00b6 Features #8929 Added the display name of the user who created a version to the list of query versions #9399 The class frequencies have been added to Graphs page of a dataset. The Class frequency tab under Insights is still available but is deprecated. This will be removed in a future release #9400 The Copy dataset button has been moved from the Dataset panel to a drop-down menu instead. This makes it consistent with actions on saved queries and stories Issues fixed #9565 Failing uploads aren't reported to users in real-time #9507 [SPARQL-IDE] The edit dialog of the Google chart visualization has incorrect styling #9271 [SPARQL-IDE] Incorrect coloring for SPARQL variables that are used after the WHERE clause and are not projected outward 24.08.1 \u00b6 Features #9388 Improved navigation of the admin settings pages #9453 [SPARQL-IDE] Increased the maximum pitch in the geo visualization #8495 [Speedy] Support more Coordinate Reference Systems Issues fixed #9483 When paginating over the results of a saved query, in certain cases the limit of a subselect is wrongfully removed 24.08.0 \u00b6 Issues fixed #9328 Captions to query visualizations in stories are too wide #9428 [SPARQL-IDE] API variables that are used in the query string are incorrectly colored 24.07.1 \u00b6 Features #8928 [SPARQL-IDE] Improved usability of the gallery visualization with customization of keywords #8587 [Speedy] Improved the performance of queries that use arbitrary length path matching by 5-100x #9035 [Speedy] Added support for federating to internal or private Speedy SPARQL endpoints on the same instance, given the necessary permissions #9380 Improved the look and feel of the dataset panel to accommodate upcoming features Issues fixed #9371 Changing a query in a data story does not update the query result #9385 [Speedy] Using SHACL SPARQL functions in combination with a LIMIT clause returned an error #9395 [Speedy] In a rare instance, query reordering produces incorrect results 24.07.0 \u00b6 Features #9191 [Speedy] Added support for using SHACL SPARQL functions defined within a dataset #7003 [Speedy] Added support for the x and q flags, in the regex() and replace() functions #9051 [SPARQL-IDE] Added a button to auto-format SPARQL queries Issues fixed #9330 Using saved query variables sometimes creates a syntactically invalid query #9173 Unclear error message when upload JSON-LD files with an invalid IRI 24.06.1 \u00b6 Features #9122 [Speedy] Added support for the units vocabulary in the GeoSPARQL functions geof:distance and geof:buffer Issues fixed #9298 Unable to upload files from a URL with many redirects #9312 SPARQL results in CSV and TSV formats incorrectly handle escape characters in literals #9237 [SPARQL-IDE] Repair messages do not signify their actions well enough #9122 [Speedy] GeoSPARQL functions geof:sfWithin , geof:sfOverlaps and geof:sfTouches did not support complex polygons 24.06.0 \u00b6 Issues fixed #9212 [SPARQL-IDE] Unused variables are sometimes displayed incorrectly on first load #9214 [SPARQL-IDE] Unexpected syntax error for a valid query 24.05.1 \u00b6 Features #9081 [SPARQL-IDE] Support for a JSON-LD visualization is now more prominent #9146 [SPARQL-IDE] Added informative hints for SPARQL queries that return 0 results Issues fixed #9083 Parallel LD-uploads cause conflicting graphs 24.05.0 \u00b6 Features #9063 [Speedy] Added support for geof:buffer GeoSPARQL functions Issues fixed #8960 Inconsistent formatting notation for numbers #9174 [SPARQL IDE] Saved Query editor crashes when a JSON-LD Frame is applied 24.04.1 \u00b6 Features #9136 [SPARQL-IDE] Added support for rendering gLTF 3D models #9059 - #9062 [Speedy] Added support for geof:distance , geof:minZ , geof:maxZ and geof:boundary GeoSPARQL functions Issues fixed #9114 TriplyDB reports valid IRIs as invalid when they contain atypical characters #9152 Virtuoso SPARQL service exhibits different querying behaviour after sync operation #9160 [SPARQL-IDE] Chart visualization not working for certain SPARQL responses 24.04.0 \u00b6 Features #9050 Added a button to display the available keyboard shortcuts for the SPARQL-IDE #9055 - #9058 [Speedy] Added support for geof:sfWithin , geof:sfOverlaps , geof:sfTouches and geof:aggBoundingBox GeoSPARQL functions Issues fixed #9048 [SPARQL-IDE] Non-string literals were not accepted as valid #9005 [SPARQL-IDE] Variable auto-complete not working in BIND clause #9068 [SPARQL-IDE] Query can be executed when there is no service available #9053 [SPARQL-IDE] Saved query view shows \"large result\" dialog when not applicable 24.03.1 \u00b6 Features #8580 Added saved query execution status in the TriplyDB administrator view #8798 Improved the performance when navigating to most queries and stories, by proactively caching query results #8580 [Speedy] Added support for xsd:negativeinteger , xsd:positiveInteger , xsd:nonNegativeInteger and xsd:nonPositiveInteger casting functions #8681 [Speedy] Improved query performance when federating to a public dataset on the same TriplyDB instance #9000 [SPARQL-IDE] Unreferenced variables are now identifiable by their colour #9028 [SPARQL-IDE] Improved the rendering of polygons on a map and map interactions on mobile devices Issues fixed #8601 [SPARQL-IDE] Unexpected auto-complete after operator #8954 [SPARQL-IDE] SPARQL IDE shows 2D data in tilted view #9004 [SPARQL-IDE] LD-frame editor doesn't show icon to display the entire query #9006 [SPARQL-IDE] Auto-indent references the next line instead of the previous line #9029 [SPARQL-IDE] Unactionable warning for some plugins in stories 24.03.0 \u00b6 Issues fixed #8600 [SPARQL-IDE] Automatically inserted brackets caused syntax errors #8999 [SPARQL-IDE] Editor inserts duplicate prefix declarations when comments are used #8780 [Speedy] Queries with LIMIT statements took longer to execute than expected 24.02.2 \u00b6 Features #8659 [SPARQL-IDE] Show a notification when a SPARQL result set contains unrecognized geographic shapes #8868 [Speedy] Improved the performance of some aggregates queries. #8834 / #8892 [SPARQL-IDE] More errors are now validated by the SPARQL IDE. For example, nested aggregates ( count(count(...)) ) now report as an error. #8834 TriplyDB supports query annotations. An TriplyDB SPARQL annotation looks like this: #! cache: false This annotation makes ensure that the TriplyDB cache is bypassed. Issues fixed #8913 [Speedy] Some arithmetic SPARQL functions return 0 results #8598 [SPARQL-IDE] Triggering context menu behaves odd when one is already open #8660 [SPARQL-IDE] QGIS does not recognized the an exported shapefile #8918 Some small services fail to consistently start 24.02.1 \u00b6 Features #8795 Support use of the <style></style> attribute in markdown and HTML (used in dataset/account/query descriptions, or by the SPARQL IDE) #8796 Support different size dimensions for story elements Issues fixed #8720 Invalid saved-query variables are not validated in the stories UI #8792 [SPARQL-IDE] A combination of the pivot table and google charts visualization may not render #8779 [SPARQL-IDE] Multiline errors are not rendered correctly #8690 [Speedy] Some atypical queries with large group-by's may result in an error #8606 [Speedy] Some valid regular expressions throw an error #8765 [Speedy] Federating to virtuoso does not work in some cases #8749 Syncing a service may fail when there are many concurrent requests #8686 Syncing a service may fail after a service is renamed #8793 [SPARQL IDE] The gallery image-widget result (populated by the ?imageWidget variable) is not shown when printing a story 24.02.0 \u00b6 Features #7616 Render skolemized IRIs better #8728 [SPARQL IDE] Improved ui for rendering grouped geo shapes Issues fixed Speedy may return too many results when using a FROM clause #8695 #8602 #8603 #8770 [SPARQL IDE] Fixed UX issues with tabs and autocompletion 24.01.0 \u00b6 Features - #8502 [SPARQL IDE] Add confirmation mechanisms in the browser to avoid the browser rendering too much information. This avoids issues where the browser is rendered unresponsive Issues fixed #8584 Insufficient request validation using the saved-query API #8092 Dataset metadata may report wrong number of statements for atypical uploads #8364 Uploads with combinations of atypical invalid IRIs may result in an error #8697 [SPARQL IDE] Changing the network visualization may result in an client-side error #8588 Saved queries with an LD-Frame always show up as modified and in draft state 23.12.1 \u00b6 Features #4209 Add queries overview page for TriplyDB administrators #8494 Improve UX for service selection in saved queries by removing the option for selecting one specific service. This option was unintuitive and not recommended. Instead, using a service type is recommended (and not the only available option). #8558 #8556 :[SPARQL speedy] Improve performance of queries with filter not exists and optionals by 30% to 180%. Issues fixed #8584 Uninformative error message when using terms autocompletion API #8512 Uninformative error message when requesting elasticsearch mapping information during a sync operation 23.12.0 \u00b6 Features #8224 [SPARQL IDE] Replace the current SPARQL editor with SPARQL IDE. The new SPARQL IDE will be gradually enabled on all TriplyDB deployments. The editor and result visualization have a slightly different look Added shortcuts for powerusers (press <ctrl>-? on the SPARQL IDE page to show them) Performance improvements when writing larger queries Consolidated the visualizations: the geo 3d plugin is now combined with the regular geo plugin, and the markup visualization is now part of the gallery visualization. The new editor is backwards compatible with the old editor. The geo-events plugin (where geographic information can be rendered as a timeline) is deprecated and not present in the SPARQL IDE. #8420 #8457 [SPARQL speedy] Improve performance of most SPARQL queries with 40% to 200% #8504 Improve UX for service selection in saved queries: the type of a manually created service has precendence now over speedy #8456 Support in the UI for deleting all dataset assets #8481 Include link to changelog in the footer of all TriplyDB pages 23.11.1 \u00b6 Features #8266 Automatic date detection and indexing in Elasticsearch Issues fixed #8459 Unable to upload instance logo #8444 Virtuoso service becomes unresponsive for atypical SPARQL query #8414 [SPARQL speedy] Querying non-existent graph may result in an error #8415 [SPARQL speedy] Query with service clause and filter may result in an error #8256 Jena concistently fails to start for a specific dataset #8371 The LD-Browser does not render an image, even though an image is present in the describe result. 23.11.0 \u00b6 Features #8324 Added quick-actions for changing the saved-query access level from the stories page. These quick-actions are shown when the story access level is incompatible with the saved-query access level (e.g., the story is public, but a saved-query is private) #8308 [SPARQL Speedy] Support for the geof:area function #8309 [SPARQL Speedy] Support for the geof:transform function Issues fixed #8352 Setting custom mapping in Elasticsearch may result in default mappings getting ignored #8326 Setting invalid custom mappings for Elasticsearch results in uninformative error #8256 Jena concistently fails to start for a specific dataset #8371 The LD-Browser does not render an image, even though an image is present in the describe result.","title":"Changelog"},{"location":"triplydb-changelog/#26.2.200","text":"Release date: 2026-02-18 Features #11150 [Data Editor] Improved the SKOS setup experience. When no SKOS concepts are present in a dataset, users now see guidance on how to get started with SKOS navigation. #11069 [Console] Improved loading performance of the admin queries table. Issues fixed #11175 [Data Upload] Fixed an issue where importing data from a URL could fail because an already decompressed file was incorrectly treated as compressed. #10653 [SPARQL] Fixed an issue where introspection functions (such as tf:authenticated_user_url() ) did not work in SPARQL UPDATE queries. These functions now work in any WHERE clause. #10440 [SPARQL] Fixed an issue where the SERVICE clause did not work in SPARQL UPDATE queries, even though the same clause worked correctly in SELECT queries.","title":"26.2.200"},{"location":"triplydb-changelog/#26.2.100","text":"Release date: 2026-02-04 Features #11097 [Dataset Settings] Added support for disabling features on a dataset level. Dataset owners can now hide specific features such as the SPARQL IDE, LD-Browser, data editor, assets, services, and the triples table from the dataset interface to simplify the user experience. #11005 [Data Editor] Next to editing skos hierarchies, the dataset editor now supports editing and navigating resources from any data model. #11010 [Data Editor] Improved instance search to support searching by IRI in addition to labels. When a class has many instances, the system now fetches and displays labels properly in the outline. #10838 [Data Editor] Improved layout behavior for the Data Model and Data Editor pages. Menu items are now only shown when there is relevant content for the user to see or edit. #10962 [Tags] Supports tagging queries, datasets and stories, supporting more flexibility in grouping such items. Searching by tag is possible in the faceted search for datasets, queries, and stories. Issues fixed #11094 [Forms] In some cases, the UI does not properly show an error message when there is wrong user input. Users now receive clear feedback when form values are invalid. [SPARQL] Fixed an issue SERVICE and some SPARQL functions were not returning the correct results for SPARQL UPDATE queries. [API] Fixed an API validation issue, where requests to create a saved query were wrongly rejected.","title":"26.2.100"},{"location":"triplydb-changelog/#26.1.200","text":"Release date: 2026-01-21 Features #10979 [Tabular uploads] Improved the CSV, TSV and XML import representation, using the facade-x representation. This is documented here . Issues fixed [Large file uploads] Large file uploads in TriplyDB use the TUS protocol . Some TUS clients use the legacy x-patch response header, which caused issues. TriplyDB now supports this x-patch header in TUS uploads, improving compatibility with additional client libraries such as the Java client. #11089 Fixed an issue where uploading a file from URL could show a download progress of more than 100%. #10705 [SPARQL] Improved error handling when federated SPARQL queries (using the service <...> { ... } clause) encounter timeouts or errors at third-party endpoints. Users now receive clearer error messages.","title":"26.1.200"},{"location":"triplydb-changelog/#26.1.100","text":"Release date: 2026-01-07 Issues fixed #11065 [Editor] Fixed issue where too many triples were incorrectly displayed as changed in the editor history view. #11064 [Dataset Settings] Fixed UI error when removing all prefix declarations from dataset settings. #11035 [Editor] Fixed issue where autocomplete results weren't shown. #11033 Fixed form issue, where an error message was not shown to the user","title":"26.1.100"},{"location":"triplydb-changelog/#25.12.100","text":"Release date: 2025-12-10 Features None Issues fixed #10251 [Editor] Tree structure in the editor now automatically updates after modifying concept hierarchies, eliminating the need to refresh the browser to see changes reflected in the tree view. #11024 [Data Model] Resolved inconsistent language display where Dutch and English labels were mixed within the same view. The system now consistently applies language preferences for labels and descriptions. #11036 [Data Model] Fixed a crash on the Data Model page caused by ambiguous tree root structures in certain datasets.","title":"25.12.100"},{"location":"triplydb-changelog/#25.11.200","text":"Release date: 2025-11-27 Issues fixed #10861 [Speedy] Fixed intermittent 502 Bad Gateway errors when executing SPARQL queries with complex function calls. Server errors are now properly handled and reported with clear error messages.","title":"25.11.200"},{"location":"triplydb-changelog/#25.11.100","text":"Release date: 2025-11-13 Issues fixed #10920 [SPARQL IDE] Certain pivot visualization errors would make the SPARQL IDE unrecoverable without refreshing the page. Fixed serialization handling for RDF literals containing unusual Unicode line-break characters.","title":"25.11.100"},{"location":"triplydb-changelog/#25.10.301","text":"Release date: 2025-11-06 Patch release for: #10872 [Data model] The search & find functionality may return incomplete results #10933 [Data editor] Copying a resource does not always work, making it impossible to copy an existing resource","title":"25.10.301"},{"location":"triplydb-changelog/#25.10.300","text":"Release date: 2025-10-29 Features #10885 [Tabular uploads] Introduced a new CSV/TSV import mechanism that preserves all information from the source files, enabling lossless roundtrip conversions between CSV and RDF representations. The new approach properly handles edge cases like duplicate column headers, multiple CSV files collapsing into the same graph, and hierarchical header structures. #10742 [Data Model] Merged the separate property type and type fields into a single unified type selector. The system automatically infers whether a value should be a literal or IRI based on the selected datatype or class, reducing the number of fields users need to interact with while maintaining full functionality. #10670 [Speedy] Made SPARQL graph management operations more scalable: Operations now operate on graphs of any size. Issues fixed #10916 [Speedy] Fixed query job failures for certain queries that processed very large CONSTRUCT query results. Query jobs now robustly handles arbitrary result set sizes to generate hundreds of millions of triples without memory errors. #10892 [Data Editor] Fixed an issue where editing an instance with a root-only prefix would append a random ID to the IRI, causing the instance to appear as lost. The editor now preserves the original root IRI during edits, preventing unexpected IRI modifications.","title":"25.10.300"},{"location":"triplydb-changelog/#25.10.200","text":"Release date: 2025-10-16 Features #10827 [Data Model] The side panel on the data model page can now be resized for better workspace customization. Issues fixed #10862 [Speedy] Cache not invalidated after update to global functions dataset - query caches are now properly invalidated when global SPARQL functions are modified. #10842 [Editor] Allowed languages feature (sh:languageIn) is not always correctly applied - language selector now correctly displays and applies configured language constraints from SHACL shapes. #10835 [Editor] Copy feature doesn't validate copied resource before saving - the copy functionality now validates resources using the same constraints as creating and editing resources. #10829 [Data Model] Properties appear as inherited - properties are no longer incorrectly marked as inherited when they belong to the current class. #10777 [Data model]: Update failures during data model operations are now properly reported to users with error messages. #10665 [Editor] In the Editor properties with same sh:path shown multiple times - properties are now deduplicated by path, showing each unique property only once regardless of how many SHACL shapes constrain it.","title":"25.10.200"},{"location":"triplydb-changelog/#25.10.100","text":"Release date: 2025-10-02 Features #10790 [Editor] Support providing both instructions and descriptions of form fields - forms now support short instructions (displayed as helper text) and detailed descriptions (displayed behind an info button) using the new triply:hint property for instructions and existing description properties for detailed explanations. #10753 [Editor] Add Numeric editor - numeric fields now use an appropriately sized editor with validation, replacing the oversized string input previously used for numbers. #10706 [Data Model] Update styling of other forms - class and constraint forms now use the improved styling introduced for property forms, with better organization of fields including collapsible advanced settings. #10587 [Editor] Ability to rollback a revision of a resource - resources can now be reverted to previous states from the history view, creating a new revision that restores the selected historical state. Issues fixed #10672 [Editor] Required Boolean properties must be set to 'true' - boolean fields are now automatically displayed with checkboxes defaulting to false, allowing forms to be saved with either true or false values. #10428 [SPARQL] Incorrect handling of unbound parameters in SPARQL Functions - fixed issue where unbound parameters in SPARQL functions were not properly handled, particularly in switch statements with default expressions.","title":"25.10.100"},{"location":"triplydb-changelog/#25.9.100","text":"Release date: 2025-09-18 Features #10317 [Editor] The history view of a resource is more intuitive - the difference between revisions uses human readable labels as much as possible, and the differences between revisions are clearer. #10752 [Editor] Add prefix support - IRIs in the editor now apply prefixes for better readability #10743 [Editor] A SPARQL viewer - SPARQL literals are now displayed with syntax highlighting using a dedicated viewer #10738 [Editor] Enable map view of wkt literals - geometry data can now be displayed on a map in the Editor #10633 [Data model] Support sh:languageIn and sh:uniqueLang for properties with the rdf:langString datatype - enhanced language validation for multilingual content Issues fixed #10775 [Editor] Automatically focusing the browser to a reported error in a form may not always work #10770 Support re-using SPARQL functions between datasets - the dataset to re-use can be configured by a site administrator #10703 [Editor] Searching for a selected item closes the tree - fixed search behavior that incorrectly collapsed the tree view #10301 [Editor] Sort autocomplete lists alphabetically - autocomplete suggestions are now properly sorted for better usability","title":"25.9.100"},{"location":"triplydb-changelog/#25.8.200","text":"Release date: 2025-08-28 Features #10630 [Editor] Support sh:nodeKind sh:IRI in Editor and Data model #10605 [Editor] Data model: support sh:hasValue and sh:in #10590 [Editor] Improve the handling of errors in the form #10302 [Editor] Automatically create default fields when applicable #10213 Support all sparql update graph management operations #9522 Optimize performance for SHACL SPARQL functions Issues fixed #10722 In atypical cases, you may get upload errors saying the graph name already exists #10711 [Editor] Copy operations use the wrong IRI #10700 SPARQL drop all fails #10694 [Data Model] The instances tab may contain duplicate results #10689 [Editor Dialog] Stem is used incorrectly for nested nodes #10687 [Data Model] Stem disappears from class #10659 [Editor View] Editor and Data model does not update after adding/removing graphs #10648 [Editor View] SHACL SPARQL constraints are not checking the class #10644 [Editor View] Inconsistent GUI state after deleting an instance #10607 anyURI literals are not validated validated #10418 Some escape charactes do not work in Regex function #9267 [TriplyDB-JS] Can't update dataset used by saved query","title":"25.8.200"},{"location":"triplydb-changelog/#25.08.100","text":"Release date: 2025-08-06 Features #10604 Editor: support sh:hasValue - when a new field value is added, it gets the value specified with sh:hasValue by default #10603 Editor: support sh:in - the Editor shows a select field in the form when sh:in is used to specify a list of allowed values #10309 Editor: Expand all offspring of one top level SKOS concept - allows SHIFT + click to expand one whole top level concept and its offspring #10226 Api | Allow SPARQL-Update queries in run-path of saved queries Issues fixed #10697 [Data Model] Editing a class label, deletes its order #10649 [LD Table] Dragging and dropping terms no longer works #10639 [Data Model View] Min count is displayed as max count #10121 Concatenation functions are too slow due to reordering","title":"25.08.100"},{"location":"triplydb-changelog/#25.7.100","text":"Release date: 2025-07-16 Features #10261 [Editor] Support entering language-tagged strings #10543 [Editor] Support collapsing and expanding groups #10240 [Editor] Automatically fill in the range class when adding a new instance #10449 Support long running select-queries in query jobs #10557 Improve user experience when copying IRIs in LD-Browser #10542 [SPARQL-IDE] Report errors in the IDE when a query includes expressions (e.g. BIND(..) as ?x ) that reference a variable that are out of scope #10478 Improve performance for autocompleting terms in e.g. the SPARQL IDE or data stories. #10600 Improve error messages when using SHACL functions, reporting whether a function does not exist, or whether a wrong number of arguments is used. Issues fixed #10432 [SPARQL-IDE, LD-Browser] SVGs not always rendered correctly in SPARQL #9967 [SPARQL-update] Concurrent requests may result in wrong modifications #10489 [LD-Table] Filtering on literals may return 0 autocompletion results #10522 Dataset sidebar behaves odd on page refresh #10191 An atypical query job never starts #10545 An atypical query always returns an error","title":"25.7.100"},{"location":"triplydb-changelog/#25.6.200","text":"Release date: 2025-06-26 Features #10308 Added visual cue for readonly properties in the editor #10300 Display modification dates for saved query versions Issues fixed #10380 Intermediate changes in TriplyDB may interrupt service updates #10438 Copying a sparql query as curl command may result in an invalid shell command #10512 Atypical queries that run as query job consistently fail after 5 minutes","title":"25.6.200"},{"location":"triplydb-changelog/#25.6.100","text":"Release date: 2025-06-05 Issues fixed - #10477 Cannot access index information for elasticsearch - #10380 Intermediate changes in TriplyDB may interrupt service updates - #10438 Copying a sparql query as curl command may result in an invalid shell command","title":"25.6.100"},{"location":"triplydb-changelog/#25.5.100","text":"Release date: 2025-05-14 Features #10058 Apply stricter content-security-policies in the UI, making the UI more robust against illegal use and cross site scripting #6218 Make API error messages more consistent Issues fixed - #10386 Uploading CSVs may result in errors - #10369 Improve visual queues in the UI for which resources are public, internal or private - #10429 API requests are dropped in rare cases (0.0002% of all requests)","title":"25.5.100"},{"location":"triplydb-changelog/#25.5.100","text":"Release date: 2025-05-14 Features #10058 Apply stricter content-security-policies in the UI, making the UI more robust against illegal use and cross site scripting #6218 Make API error messages more consistent Issues fixed - #10386 Uploading CSVs may result in errors - #10369 Improve visual queues in the UI for which resources are public, internal or private - #10429 API requests are dropped in rare cases (0.0002% of all requests)","title":"25.5.100"},{"location":"triplydb-changelog/#25.4.300","text":"Release date: 2025-04-28 Features #10095 Improved speedy performance when querying more graphs. Depending on the query and datastructure, you may get performance improvements of around 20%, with outliers to 100%. Issues fixed #10353 [Editor] Editor may show outdated views with simultaneous errors #10288 [Editor] sh:prefixes are unrecognized in the SHACL SPARQL constraints model #10359 [SPARQL IDE] Opening a chart visualization via a URL may result in an error","title":"25.4.300"},{"location":"triplydb-changelog/#25.4.200","text":"Release date: 2025-04-16 Features #10297 Support rendering SVGs in LD-Browser and SPARQL-IDE Issues fixed #10205 [Import] Parallel uploads my result in errors #10348 [SPARQL IDE] Interactions with IDE become sluggish for large query #10277 User is not always notified when an import from URL fails","title":"25.4.200"},{"location":"triplydb-changelog/#25.4.100","text":"Release date: 2025-04-03 Features #9776 [Query jobs] Insights into the error encountered by a query job #9935 [Editor] Allow a SKOS tree hierarchy without concept schemes #10272 [Assets] Gracefully handle uploading an asset with an existing name in the GUI #10054 [Output] Ability to export SPARQL results in more readable turtle format #9797 Upgrade elasticsearch from version 7 to version 8 #9768 [SPARQL UPDATE] Performance improvement when using mutations frequently #10175 [Import] Support for CSV, TSV and more use cases within those formats Issues fixed #9765 [Import] Namespaces are not resolved for uploaded XML files","title":"25.4.100"},{"location":"triplydb-changelog/#25.3.200","text":"Release date: 2025-03-20 Features #10129 [Editor] Display a list of concept schemes when no concept is selected #9768 [Graphs] General performance improvements #10189 [SAML] Allow email-based user lookup Issues fixed #10273 [Editor] Oops something went wrong when using the editor #10274 [Editor] Wrong title for property edit dialog #10250 [Editor] Server error when a field was not filled in #10110 [Editor] Super class / parent class cannot be cleared #10269 [Editor] Wrong title in staging dialog #10228 [Triples] Adding statements to the default graph does nothing #10221 [Speedy] Internal performance improvement #10138 [Speedy] Performance improvement for a queries containing multiple SHACL SPARQL functions #10276 [Speedy] 500 error for non-compliant literal #9794 [Data upload] CSV parser throws 500 error when delimiter is not consistent","title":"25.3.200"},{"location":"triplydb-changelog/#25.3.100","text":"Release date: 2025-03-06 Features #9896 [Editor] Improved changelog readability #10229 [Datasets] Improved metadata display, added changed date #10150 [Authentication] Improved SAML management #10184 [Dataset] Configuration of auto-generated ID in dataset settings #10131 [Triples] Improvement while filtering and creating terms Issues fixed #10123 [Data stories] Unexpected crashes within visualization. #10116 [Saved queries] API variable dialog, autocomplete display. #10169 [SPARQL-IDE] Autocompletion for literals is missing #10036 [Speedy] Union variables are not translated correctly in SHACL SPARQL functions #10106 [Speedy] GeoSPARQL functions do not work on polygons in different CRSes #10200 [Speedy] Values clause with 2 terminating bindings does not terminate","title":"25.3.100"},{"location":"triplydb-changelog/#25.2.200","text":"Release date: 2025-02-20 Features #10037 [SPARQL-IDE] Query autoformat indentation behavior, we use a simple and concise indentation scheme. #10067 [Editor] Configure read-only fields for the SKOS editor. #10052 [Editor] Configure search fields in the data model so that users can search on user defined labels. #10078 [Editor] SKOS Search performance improvements. #10041 [Assets] See who uploaded an asset. #9945 [Speedy] Performance of 20% of our benchmark queries increased by about 40% Issues fixed #10115 [Saved Queries] Autocomplete UI showed multi line options. #10144 [Saved Queries] \"Save as new version\" appeared not to work (while it did). #10057 [Speedy] SPARQL query with SHACL SPARQL functions return a 500 error #10161 [Speedy] encode_for_uri is implemented incorrect #10163 [Speedy] SHACL rules can not have a limit","title":"25.2.200"},{"location":"triplydb-changelog/#25.2.100","text":"Release date: 2025-02-06 Features #10094 [Editor] Show all SPARQL based constraint errors in editor #10079 [Editor] Show SPARQL based constraint errors at bottom of the form #10085 [Editor] Remember previous concept scheme hierarchy in the skos tree","title":"25.2.100"},{"location":"triplydb-changelog/#25.1.200","text":"Release date: 2025-01-23 Features New version scheme as of this release. Instead of 25.01.2 , we now release 25.1.200 . A patch release will be released as 25.1.201 #9581 Added the ability to restore queries that would have been lost due to navigating away from (or refreshing) the query page #3170 Account API tokens are now copied to the clipboard when clicked Improved the usability of admin tables Issues fixed #9716 [Speedy] Unexpected group_concat result when pre-bindings are used #10066 [Speedy] SHACL SPARQL function is executed even when the argument to it is unbound #9171 [SPARQL-IDE] Unexpected indentation after semi-colon #10045 [SPARQL-IDE] Empty lines are introduced when adding prefix declarations","title":"25.1.200"},{"location":"triplydb-changelog/#25.01.1","text":"Release date: 2025-01-09 Issues fixed #9929 Query jobs may incidentally hang #10062 Speedy queries that use federation may now work in firewalled on-premise environments #9999 Default variable values of saved queries are not always applied in query jobs","title":"25.01.1"},{"location":"triplydb-changelog/#24.12.2","text":"Release date: 2024-12-18 Issues fixed #10006 Cancelling a linked data upload may result in an error #9711 [SaaS] Incidental requests are dropped","title":"24.12.2"},{"location":"triplydb-changelog/#24.12.1","text":"Release date: 2024-12-05 Issues fixed #9888 [Query job] Query job for very simple construct query may fail #9959 [SPARQL IDE] SPARQL IDE may show same autocomplete suggestion multiple times #9954 [Query job] Query jobs may get sometimes inadvertently cancelled #9927 [Stories] Stories with many queries are slow to load in the browser","title":"24.12.1"},{"location":"triplydb-changelog/#24.11.2","text":"Release date: 2024-11-22 Issues fixed #9017 [Speedy] Some queries take too long when reordering/optimizing a query #9780 [SaaS] Enable OCSP stapling for TLS","title":"24.11.2"},{"location":"triplydb-changelog/#24.11.1","text":"Release date: 2024-11-08 Issues fixed #8878 Include correct dataset modification date for JSON schema descriptions #9694 Speedy SPARQL endpoints are not included in the dataset NDE descriptions #9807 Parsing of XML content assigns an incorrect datatype #9752 Parsing of XML is incorrect for escaped ampersand #9718 Unintentional grey bar after renaming a graph #9767 [Speedy] Incorrect n-triples serialization when handling blank nodes from external SPARQL endpoints #9557 [SPARQL-IDE] Yellow marker in SPARQL Editor is incorrect. This feature has been disabled until we improve some of the rough edges in future releases. #9791 [SPARQL-IDE] Error when tying to view a populated query from a gallery visualization with a query variable #9774 [SPARQL-IDE] Copy shortened URL doesn't work for webkit browsers","title":"24.11.1"},{"location":"triplydb-changelog/#24.10.2","text":"Release date: 2024-10-25 Issues fixed #9739 [Query jobs] Query job artifacts, exposed to system administrators, are incomplete #9740 [Query jobs] Cannot start a query job for queries with a default variable #9701 [Query jobs] A query job with invalid graph name fails at a late stage, where we expect it to fail early","title":"24.10.2"},{"location":"triplydb-changelog/#24.10.1","text":"Release date: 2024-10-11 Issues fixed #9695 [Speedy] ORDER BY doesn't order by the second operand if the first one is an error #9658 [SPARQL-IDE] Capture position doesn't work with height settings for stories","title":"24.10.1"},{"location":"triplydb-changelog/#24.09.2","text":"Release date: 2024-09-27 Features #8471 Admins on a TriplyDB instance can restrict users to only one active session #9148 [SPARQL-IDE] Added support for overlaying 3D tiles, from a URL, in the Geo visualization Issues fixed #9646 Base URI when uploading data always uses the uploading account instead of the target account #9561 Saving a new version of a saved query does not always save its visualization configuration #9600 When editing query metadata, the service \"Speedy\" is not always visible #9633 [Speedy] Regression, introduced in version 24.09.1, causing fatal error when executing certain queries","title":"24.09.2"},{"location":"triplydb-changelog/#24.09.1","text":"Features #9535 [SPARQL-IDE] The camera point-of-view can be saved as part of a query, in the geo visualization. This enables more precise use in stories #9527 [Speedy] Improved the user message when there are errors within a SHACL SPARQL function Issues fixed #9556 / #9550 [Speedy] Reordering of queries with both an OPTIONAL and ORDER BY clause did not always return the correct result #9579 [Speedy] Incorrect coordinate transformation from epsg:7415 to epsg:28992 when using geof:transform GeoSPARQL function #9528 [Speedy] Incorrect results when using a SHACL SPARQL function due to reordering #9569 [Speedy] Incorrect results when using a SHACL SPARQL function with blank nodes #9523 [SPARQL-IDE] Unable to export certain chart visualizations to SVG","title":"24.09.1"},{"location":"triplydb-changelog/#24.08.2","text":"Features #8929 Added the display name of the user who created a version to the list of query versions #9399 The class frequencies have been added to Graphs page of a dataset. The Class frequency tab under Insights is still available but is deprecated. This will be removed in a future release #9400 The Copy dataset button has been moved from the Dataset panel to a drop-down menu instead. This makes it consistent with actions on saved queries and stories Issues fixed #9565 Failing uploads aren't reported to users in real-time #9507 [SPARQL-IDE] The edit dialog of the Google chart visualization has incorrect styling #9271 [SPARQL-IDE] Incorrect coloring for SPARQL variables that are used after the WHERE clause and are not projected outward","title":"24.08.2"},{"location":"triplydb-changelog/#24.08.1","text":"Features #9388 Improved navigation of the admin settings pages #9453 [SPARQL-IDE] Increased the maximum pitch in the geo visualization #8495 [Speedy] Support more Coordinate Reference Systems Issues fixed #9483 When paginating over the results of a saved query, in certain cases the limit of a subselect is wrongfully removed","title":"24.08.1"},{"location":"triplydb-changelog/#24.08.0","text":"Issues fixed #9328 Captions to query visualizations in stories are too wide #9428 [SPARQL-IDE] API variables that are used in the query string are incorrectly colored","title":"24.08.0"},{"location":"triplydb-changelog/#24.07.1","text":"Features #8928 [SPARQL-IDE] Improved usability of the gallery visualization with customization of keywords #8587 [Speedy] Improved the performance of queries that use arbitrary length path matching by 5-100x #9035 [Speedy] Added support for federating to internal or private Speedy SPARQL endpoints on the same instance, given the necessary permissions #9380 Improved the look and feel of the dataset panel to accommodate upcoming features Issues fixed #9371 Changing a query in a data story does not update the query result #9385 [Speedy] Using SHACL SPARQL functions in combination with a LIMIT clause returned an error #9395 [Speedy] In a rare instance, query reordering produces incorrect results","title":"24.07.1"},{"location":"triplydb-changelog/#24.07.0","text":"Features #9191 [Speedy] Added support for using SHACL SPARQL functions defined within a dataset #7003 [Speedy] Added support for the x and q flags, in the regex() and replace() functions #9051 [SPARQL-IDE] Added a button to auto-format SPARQL queries Issues fixed #9330 Using saved query variables sometimes creates a syntactically invalid query #9173 Unclear error message when upload JSON-LD files with an invalid IRI","title":"24.07.0"},{"location":"triplydb-changelog/#24.06.1","text":"Features #9122 [Speedy] Added support for the units vocabulary in the GeoSPARQL functions geof:distance and geof:buffer Issues fixed #9298 Unable to upload files from a URL with many redirects #9312 SPARQL results in CSV and TSV formats incorrectly handle escape characters in literals #9237 [SPARQL-IDE] Repair messages do not signify their actions well enough #9122 [Speedy] GeoSPARQL functions geof:sfWithin , geof:sfOverlaps and geof:sfTouches did not support complex polygons","title":"24.06.1"},{"location":"triplydb-changelog/#24.06.0","text":"Issues fixed #9212 [SPARQL-IDE] Unused variables are sometimes displayed incorrectly on first load #9214 [SPARQL-IDE] Unexpected syntax error for a valid query","title":"24.06.0"},{"location":"triplydb-changelog/#24.05.1","text":"Features #9081 [SPARQL-IDE] Support for a JSON-LD visualization is now more prominent #9146 [SPARQL-IDE] Added informative hints for SPARQL queries that return 0 results Issues fixed #9083 Parallel LD-uploads cause conflicting graphs","title":"24.05.1"},{"location":"triplydb-changelog/#24.05.0","text":"Features #9063 [Speedy] Added support for geof:buffer GeoSPARQL functions Issues fixed #8960 Inconsistent formatting notation for numbers #9174 [SPARQL IDE] Saved Query editor crashes when a JSON-LD Frame is applied","title":"24.05.0"},{"location":"triplydb-changelog/#24.04.1","text":"Features #9136 [SPARQL-IDE] Added support for rendering gLTF 3D models #9059 - #9062 [Speedy] Added support for geof:distance , geof:minZ , geof:maxZ and geof:boundary GeoSPARQL functions Issues fixed #9114 TriplyDB reports valid IRIs as invalid when they contain atypical characters #9152 Virtuoso SPARQL service exhibits different querying behaviour after sync operation #9160 [SPARQL-IDE] Chart visualization not working for certain SPARQL responses","title":"24.04.1"},{"location":"triplydb-changelog/#24.04.0","text":"Features #9050 Added a button to display the available keyboard shortcuts for the SPARQL-IDE #9055 - #9058 [Speedy] Added support for geof:sfWithin , geof:sfOverlaps , geof:sfTouches and geof:aggBoundingBox GeoSPARQL functions Issues fixed #9048 [SPARQL-IDE] Non-string literals were not accepted as valid #9005 [SPARQL-IDE] Variable auto-complete not working in BIND clause #9068 [SPARQL-IDE] Query can be executed when there is no service available #9053 [SPARQL-IDE] Saved query view shows \"large result\" dialog when not applicable","title":"24.04.0"},{"location":"triplydb-changelog/#24.03.1","text":"Features #8580 Added saved query execution status in the TriplyDB administrator view #8798 Improved the performance when navigating to most queries and stories, by proactively caching query results #8580 [Speedy] Added support for xsd:negativeinteger , xsd:positiveInteger , xsd:nonNegativeInteger and xsd:nonPositiveInteger casting functions #8681 [Speedy] Improved query performance when federating to a public dataset on the same TriplyDB instance #9000 [SPARQL-IDE] Unreferenced variables are now identifiable by their colour #9028 [SPARQL-IDE] Improved the rendering of polygons on a map and map interactions on mobile devices Issues fixed #8601 [SPARQL-IDE] Unexpected auto-complete after operator #8954 [SPARQL-IDE] SPARQL IDE shows 2D data in tilted view #9004 [SPARQL-IDE] LD-frame editor doesn't show icon to display the entire query #9006 [SPARQL-IDE] Auto-indent references the next line instead of the previous line #9029 [SPARQL-IDE] Unactionable warning for some plugins in stories","title":"24.03.1"},{"location":"triplydb-changelog/#24.03.0","text":"Issues fixed #8600 [SPARQL-IDE] Automatically inserted brackets caused syntax errors #8999 [SPARQL-IDE] Editor inserts duplicate prefix declarations when comments are used #8780 [Speedy] Queries with LIMIT statements took longer to execute than expected","title":"24.03.0"},{"location":"triplydb-changelog/#24.02.2","text":"Features #8659 [SPARQL-IDE] Show a notification when a SPARQL result set contains unrecognized geographic shapes #8868 [Speedy] Improved the performance of some aggregates queries. #8834 / #8892 [SPARQL-IDE] More errors are now validated by the SPARQL IDE. For example, nested aggregates ( count(count(...)) ) now report as an error. #8834 TriplyDB supports query annotations. An TriplyDB SPARQL annotation looks like this: #! cache: false This annotation makes ensure that the TriplyDB cache is bypassed. Issues fixed #8913 [Speedy] Some arithmetic SPARQL functions return 0 results #8598 [SPARQL-IDE] Triggering context menu behaves odd when one is already open #8660 [SPARQL-IDE] QGIS does not recognized the an exported shapefile #8918 Some small services fail to consistently start","title":"24.02.2"},{"location":"triplydb-changelog/#24.02.1","text":"Features #8795 Support use of the <style></style> attribute in markdown and HTML (used in dataset/account/query descriptions, or by the SPARQL IDE) #8796 Support different size dimensions for story elements Issues fixed #8720 Invalid saved-query variables are not validated in the stories UI #8792 [SPARQL-IDE] A combination of the pivot table and google charts visualization may not render #8779 [SPARQL-IDE] Multiline errors are not rendered correctly #8690 [Speedy] Some atypical queries with large group-by's may result in an error #8606 [Speedy] Some valid regular expressions throw an error #8765 [Speedy] Federating to virtuoso does not work in some cases #8749 Syncing a service may fail when there are many concurrent requests #8686 Syncing a service may fail after a service is renamed #8793 [SPARQL IDE] The gallery image-widget result (populated by the ?imageWidget variable) is not shown when printing a story","title":"24.02.1"},{"location":"triplydb-changelog/#24.02.0","text":"Features #7616 Render skolemized IRIs better #8728 [SPARQL IDE] Improved ui for rendering grouped geo shapes Issues fixed Speedy may return too many results when using a FROM clause #8695 #8602 #8603 #8770 [SPARQL IDE] Fixed UX issues with tabs and autocompletion","title":"24.02.0"},{"location":"triplydb-changelog/#24.01.0","text":"Features - #8502 [SPARQL IDE] Add confirmation mechanisms in the browser to avoid the browser rendering too much information. This avoids issues where the browser is rendered unresponsive Issues fixed #8584 Insufficient request validation using the saved-query API #8092 Dataset metadata may report wrong number of statements for atypical uploads #8364 Uploads with combinations of atypical invalid IRIs may result in an error #8697 [SPARQL IDE] Changing the network visualization may result in an client-side error #8588 Saved queries with an LD-Frame always show up as modified and in draft state","title":"24.01.0"},{"location":"triplydb-changelog/#23.12.1","text":"Features #4209 Add queries overview page for TriplyDB administrators #8494 Improve UX for service selection in saved queries by removing the option for selecting one specific service. This option was unintuitive and not recommended. Instead, using a service type is recommended (and not the only available option). #8558 #8556 :[SPARQL speedy] Improve performance of queries with filter not exists and optionals by 30% to 180%. Issues fixed #8584 Uninformative error message when using terms autocompletion API #8512 Uninformative error message when requesting elasticsearch mapping information during a sync operation","title":"23.12.1"},{"location":"triplydb-changelog/#23.12.0","text":"Features #8224 [SPARQL IDE] Replace the current SPARQL editor with SPARQL IDE. The new SPARQL IDE will be gradually enabled on all TriplyDB deployments. The editor and result visualization have a slightly different look Added shortcuts for powerusers (press <ctrl>-? on the SPARQL IDE page to show them) Performance improvements when writing larger queries Consolidated the visualizations: the geo 3d plugin is now combined with the regular geo plugin, and the markup visualization is now part of the gallery visualization. The new editor is backwards compatible with the old editor. The geo-events plugin (where geographic information can be rendered as a timeline) is deprecated and not present in the SPARQL IDE. #8420 #8457 [SPARQL speedy] Improve performance of most SPARQL queries with 40% to 200% #8504 Improve UX for service selection in saved queries: the type of a manually created service has precendence now over speedy #8456 Support in the UI for deleting all dataset assets #8481 Include link to changelog in the footer of all TriplyDB pages","title":"23.12.0"},{"location":"triplydb-changelog/#23.11.1","text":"Features #8266 Automatic date detection and indexing in Elasticsearch Issues fixed #8459 Unable to upload instance logo #8444 Virtuoso service becomes unresponsive for atypical SPARQL query #8414 [SPARQL speedy] Querying non-existent graph may result in an error #8415 [SPARQL speedy] Query with service clause and filter may result in an error #8256 Jena concistently fails to start for a specific dataset #8371 The LD-Browser does not render an image, even though an image is present in the describe result.","title":"23.11.1"},{"location":"triplydb-changelog/#23.11.0","text":"Features #8324 Added quick-actions for changing the saved-query access level from the stories page. These quick-actions are shown when the story access level is incompatible with the saved-query access level (e.g., the story is public, but a saved-query is private) #8308 [SPARQL Speedy] Support for the geof:area function #8309 [SPARQL Speedy] Support for the geof:transform function Issues fixed #8352 Setting custom mapping in Elasticsearch may result in default mappings getting ignored #8326 Setting invalid custom mappings for Elasticsearch results in uninformative error #8256 Jena concistently fails to start for a specific dataset #8371 The LD-Browser does not render an image, even though an image is present in the describe result.","title":"23.11.0"},{"location":"triplydb-js/","text":"On this page: Overview TriplyDB.js is the official programming library for interacting with TriplyDB . TriplyDB.js allows you to automate operations that would otherwise be performed in the TriplyDB GUI. TriplyDB.js is implemented in TypeScript . TypeScript is a type-safe language that transpiles to JavaScript . This allows you to use TriplyDB.js in web browsers as well as on servers (using Node.js ). TriplyDB.js is open source and its source code is published on GitHub . Please contact support@triply.cc for questions and suggestions. Overview \u00b6 TriplyDB.js contains several classes, each with their own methods. The documentation for every method includes at least one code example. These code examples can be run by inserting them into the following overall script. Notice that process.env.TOKEN picks up an API token that is stored in the environment variable called TOKEN . Follow the steps on this page to create a new API token in the TriplyDB GUI. import App from '@triply/triplydb' const triply = App.get({ token: process.env.TOKEN }) async function run() { // This is where the code examples in this reference section should be placed. } run().catch(e => { console.error(e) process.exit(1) }) process.on('uncaughtException', function (e) { console.error('Uncaught exception', e) process.exit(1) }) process.on('unhandledRejection', (reason, p) => { console.error('Unhandled Rejection at: Promise', p, 'reason:', reason) process.exit(1) }) The following sections document the various TriplyDB.js classes. Each class comes with its own methods. Classes are related through methods that connect them. For example, calling the getAccount method on a App object returns an Account object.","title":"Overview"},{"location":"triplydb-js/#overview","text":"TriplyDB.js contains several classes, each with their own methods. The documentation for every method includes at least one code example. These code examples can be run by inserting them into the following overall script. Notice that process.env.TOKEN picks up an API token that is stored in the environment variable called TOKEN . Follow the steps on this page to create a new API token in the TriplyDB GUI. import App from '@triply/triplydb' const triply = App.get({ token: process.env.TOKEN }) async function run() { // This is where the code examples in this reference section should be placed. } run().catch(e => { console.error(e) process.exit(1) }) process.on('uncaughtException', function (e) { console.error('Uncaught exception', e) process.exit(1) }) process.on('unhandledRejection', (reason, p) => { console.error('Unhandled Rejection at: Promise', p, 'reason:', reason) process.exit(1) }) The following sections document the various TriplyDB.js classes. Each class comes with its own methods. Classes are related through methods that connect them. For example, calling the getAccount method on a App object returns an Account object.","title":"Overview"},{"location":"triplydb-js/account/","text":"On this page: Account Account.addDataset(name: string, metadata?: object) Access restrictions Arguments Examples See also Account.addQuery(name: string, metadata: object) Arguments Example Account.addStory(name: string, metadata?: object) Required Optional Examples Account.asOrganization() Examples Alternatives See also Account.asUser() Examples Alternatives See also Account.ensureDataset(name: string, metadata?: object) Example See also Account.getDataset(name: string) Examples See also Account.getDatasets() Access restrictions Examples Account.getInfo() Examples Account.getPinnedItems() Order considerations Examples See also Account.getQuery(name: string) Examples See also Account.getQueries() Access restrictions Examples See also Account.ensureStory(name: string, metadata: object) Optional Account.addStory(name: string, newStoryOptions?: object) Required Optional Account.getStory(name: string) Examples See also Account.getStories() Examples See also Account.pinItems(items: array[Dataset|Story|Query]) Account.setAvatar(file: string) Examples Account.update(metadata: object) Account \u00b6 Instances of the Account class denote TriplyDB accounts. Accounts can be either organizations ( Organization ) or users ( User ). Account objects are obtained by calling the following method: App.getAccount(name?: string) Account.addDataset(name: string, metadata?: object) \u00b6 Adds a new TriplyDB dataset with the given name to the current account. The optional metadata argument is used to specify the metadata for the dataset. Access restrictions \u00b6 Creating a new dataset only succeeds if an API token is configured that provides write access to the current account. The default access level for a newly created dataset is private . If you want to publish a dataset with a different access level, you must specify the accessLevel key in the metadata argument. Arguments \u00b6 The name argument specifies the URL-friendly name of the new dataset. The name must only contain alphanumeric characters and hyphens ( [A-Za-z0-9\\-] ). The full URL of the newly created dataset has the following structure: https://{host}/{account}/{dataset} The metadata argument optionally specifies the access level and other important metadata: accessLevel The access level of the dataset. The following values are supported: 'private' (default) The dataset can only be accessed by organization members. 'internal' The dataset can only be accessed by users that are logged into the TriplyDB server. 'public' The dataset can be accessed by everybody. When no access level is specified, the most restrictive access level ( private ) is used. description The human-readable description of the dataset. This description can make use of Markdown. displayName The human-readable name of the dataset. This name may contain spaces and other characters that are not allowed in the URL-friendly name. license The license of the dataset. The following license strings are currently supported: 'CC-BY-SA' 'CC0 1.0' 'GFDL' 'ODC-By' 'ODC-ODbL' 'PDDL' 'None' (default) prefixes The IRI prefix declarations that are configured for the dataset. This is specified as a dictionary object whose keys are aliases and whose values are IRI prefixes. Examples \u00b6 The following snippet creates a new dataset called 'iris' under the account called 'Triply' : The dataset has private access, because the access level is not specified explicitly. The dataset has a description. The dataset has a display name. The dataset has the PDDL license. const account = await triply.getAccount('Triply') const dataset = await account.addDataset('iris', { description: 'A multivariate dataset that quantifies morphologic variation of Iris flowers.', displayName: 'Iris', license: 'PDDL', name: 'iris', prefixes: { def: 'https://triplydb.com/Triply/iris/def/', id: 'https://triplydb.com/Triply/iris/id/', }, }) See also \u00b6 This method returns a dataset object. See the Dataset section for an overview of the methods that can be called on such objects. Account.addQuery(name: string, metadata: object) \u00b6 Adds a new SPARQL query to the account. Arguments \u00b6 Required: name: string The URL-friendly name of the new query. The name must only contain alphanumeric characters and hyphens ( [A-Za-z0-9\\-] ). dataset: Dataset An instance of class Dataset that the current API token gives access to. serviceType: \"speedy\" | \"virtuoso\" | \"jena\" | \"blazegraph\" A service type that will be used as a preferred SPARQL service to execute the query against. If serviceType is \"virtuoso\", \"jena\" or \"blazegraph\", that service is expected to exist in the dataset before the query can be successfully run. See Dataset queryString: string The SPARQL query string (e.g., 'select * { ?s ?p ?o }' ). Optional: accessLevel The access level of the query. If none is set it defaults to 'private' . The following values are supported: 'private' The query can only be accessed by the Account object for which it is created. 'internal' The query can only be accessed by people who are logged into the TriplyDB server. 'public' The query can be accessed by everybody. description: string A human-readable description of the query. displayName: string The human-readable name of the query. This name may include spaces and other characters that are not allowed in the URL-friendly name . output: string The visualization plugin that is used to display the result set of the query. If none is set it will either set ldFrame if provided or default to table . 'boolean' The boolean view is a special view for ask queries. The value is either 'true' or 'false', and is visualized as `X` (False) or `V` (True). 'gallery' The gallery view allows SPARQL results to be displayed in an HTML gallery. 'gchart' The gchart renders geographical, temporal and numerical data in interactive charts such as bar-, line- and pie charts. 'geo' The geo allows SPARQL results that contain GeoSPARQL semantics to be automatically interpreted and displayed on a 2D map. 'geoEvents' The geoEvents plugin renders geographical events as a story map. 'geo3d' The geo3d allows SPARQL results that contain GeoSPARQL semantics to be automatically interpreted and displayed on a 3D globe. 'markup' The markup can be used to render a variety of markup languages. This requires the use of the `?markup` variable to identify which variable to render. 'network' The network renders SPARQL Construct results in a graph representation. The maximum amount of results that can be visualized is 1.000 due to performance. 'pivot' The pivot view renders SPARQL results in an interactive pivot table where you are able to aggregate the results by dragging your binding variables to columns or rows. 'response' The response view shows the body of the response and offers a easy way to download the result as a file. 'table' The table view allows SPARQL results to be displayed in a table. Each column in the table corresponds to a variable that belongs to the outer projection. 'timeline' The timeline timeline renders the SPARQL results on a Timeline. ldFrame: object JSON LD frame object used to transform plain JSON LD into a framed JSON. Will be used only if an output is not provided. variables: Variable[] A list of objects with the following keys: IRI variable An object of the form Variable (see below) Instances of Variable are objects that can have the following keys: Required: name: string A SPARQL variable name. The variable name must appear in the query string. The question mark ( ? ) or dollar sign ( $ ) is not included. termType: 'Literal'|'NamedNode' The kind of variable. This must be either 'Literal' for literals or 'NamedNode' for IRIs. Optional: allowedValues: string[] The list of string values that is allowed for this variable. datatype: string (if termType='Literal' ) The datatype IRI for the literal variable. language: string (if termType='Literal' ) The language tag for the literal variable. Setting this implies that the dataset IRI is rdf:langString . defaultValue: string The default string value for the required: boolean Whether a query request must include an explicit value for this variable. The default value is false . Example \u00b6 The following snippet creates a query with the given query string: const triply = App.get({ token: process.env.TOKEN }) const user = await triply.getUser() const myDataset = await user.getDataset('my-dataset') const query = await user.addQuery('my-query', { dataset: myDataset, serviceType: \"speedy\", queryString: 'select (count(*) as ?n) { ?s ?p ?o. }', output: 'response', }) Account.addStory(name: string, metadata?: object) \u00b6 Adds a new data story. Required \u00b6 name: string The URL-friendly name of the data story. The name must only contain alphanumeric characters and hyphens ( [A-Za-z0-9\\-] ). Optional \u00b6 accessLevel The access level of the dataset. If none is given the default of 'private' is used. The following values are supported: 'private' The dataset can only be accessed by the Account object for which it is created. 'internal' The dataset can only be accessed by people who are logged into the TriplyDB server. 'public' The dataset can be accessed by everybody. content: StoryElementUpdate[] A list of story elements. The building blocks of the Story. displayName: string The human-readable name of the data story. This name may include spaces and other characters that are not allowed in the URL-friendly name. A story element is an object with the following keys: caption: string The caption is an explanatory text about a specific query. id: string Each Story element gets an Id when it is created. When you want to update a Story element you will need this Id. The Id is only required when updating an element and not needed when adding an object. paragraph: string The Markdown content of a story paragraph. Only allowed when the type is set to 'paragraph' query: Query An instance of class Query . queryVersion: number The version that is used of the specified query. type Either 'paragraph' or 'query' . Examples \u00b6 Example 1 - creates a new story that has access level 'private' : const triply = App.get({ token: process.env.TOKEN }) const user = await triply.getUser() const newStory = await user.addStory('name-of-story') Example 2 - creates a new story that has access level 'public' : const triply = App.get({ token: process.env.TOKEN }) const user = await triply.getUser() const newStory = await user.addStory('name-of-story', { accessLevel: 'public', }) Account.asOrganization() \u00b6 Casts the TriplyDB account object to its corresponding organization object. Class Organization is a specialization of class Account . Calling this method on an Organization object does nothing. Examples \u00b6 The following snippet retrieves the account named 'Triply' and casts it to an organization: const account = await triply.getAccount('Triply') const organization = account.asOrganization() Alternatives \u00b6 This method is not needed if the organization is directly retrieved with the specialization method App.getOrganization(name: string) . The following snippet returns the same result as the above example, but in a more direct way: const organization = await triply.getOrganization('Triply') See also \u00b6 This method returns an organization object. See class Organization for an overview of the methods that can be called on such objects. Account.asUser() \u00b6 Casts the TriplyDB account object to its corresponding user object. Class User is a specialization of class Account . Calling this method on a User object does nothing. Examples \u00b6 The following snippet retrieves the account that represents the current user, and casts it to a user object: const account = await triply.getAccount() const user = account.asUser() Alternatives \u00b6 This method is not needed if the user is directly retrieved with the specialization method App.getUser(name?: string) . The following snippet returns the same result as the above example, but in a more direct way: const user = await triply.getUser() See also \u00b6 This method returns an organization object. See class Organization for an overview of the methods that can be called on such objects. Account.ensureDataset(name: string, metadata?: object) \u00b6 Ensures the existence of a dataset with the given name and with the specified metadata if given. Calling this method ensures that the necessary changes (if any) are made in the connected-to TriplyDB server that result in an end state in which a dataset with the given name and metadata exists. This method is useful in practice, because it removes the burden on the programmer to have to write custom code for checking for the existence of a dataset, and conditionally create a new dataset or make metadata changes to an existing dataset. The changes made as a result of calling this method depend on the current state of the connected-to TriplyDB server: If this account does not yet have a dataset with the given name , then the behavior is identical to calling Account.addDataset(name: string, metadata?: object) with the same arguments. If this account already has a dataset with the given name and with the same metadata , then this method makes no changes. Example \u00b6 const account = await triply.getAccount('Triply') const myDataset = await account.ensureDataset(`my-dataset`, { license: 'PDDL', }) See also \u00b6 The meaning of the argument to this method are identical to those of the Account.addDataset(name: string, metadata?: object) method. Account.getDataset(name: string) \u00b6 Returns the dataset with the given name that is published by this account. Examples \u00b6 The following snippet prints the name of the Iris dataset that is published by the Triply account: const account = await triply.getAccount('Triply') const dataset = await triply.getDataset('iris') console.log((await dataset.getInfo()).name) See also \u00b6 This method returns a dataset object. See class Dataset for an overview of the methods that can be called on such objects. Account.getDatasets() \u00b6 Returns an async iterator over the accessible datasets for the current account. Access restrictions \u00b6 The iterator only includes datasets that are accessible for the current connection with a TriplyDB server: If no API token is configured, the iterator will include all and only public datasets belonging to this account. If an API token is configured, the iterator will include all public and internal datasets belonging to this account, and will include all private datasets belonging to this account if the API token gives read access to the account. Examples \u00b6 The following snippet prints the names of all accessible dataset under the Triply account: const account = await triply.getAccount('Triply') for await (const dataset of account.getDatasets()) { console.log((await dataset.getInfo()).name) } The following snippet prints the list of names of all accessible datasets under the Triply account: const account = await triply.getAccount('Triply') console.log(await account.getDatasets().toArray()) Account.getInfo() \u00b6 Returns information about this account. Information is returned in a dictionary object. Individual keys can be accessed for specific information values. The information object for accounts includes the following keys: avatarUrl A URL to the account image. accountName The URL-friendly name of the account. name The human-readable display name of the account description The human-readable description of the account. createdAt The date and time on which the account was created. datasetCount The number of datasets for the account. queryCount The number of queries for the account. storyCount The number of stories for the account pinnedDatasets An array containing the pinned dataset for the account. pinnedItems An array containing the pinned items (datasets, stories and queries) for the account. type The account type: either organization or user . role The role of the account orgs An array of organizations of which the account is a member. Email address The email address of the account. updatedAt The date and time on which the account was last updated. lastActivity The date and time on which the account was last online on TriplyDB. Examples \u00b6 The following snippet prints the full information object for the account called \u2018Triply\u2019: const account = await triply.getAccount('Triply') console.log(await account.getInfo()) The output for this snippet can look as follows: { 'accountName': 'Triply', 'avatarUrl': 'https://www.gravatar.com/avatar/9bc28997dd1074e405e1c66196d5e117?d=mm', 'createdAt': 'Mon Mar 19 2018 14:39:18 GMT+0000 (Coordinated Universal Time)', 'datasetCount': 16, 'name': 'Triply', 'queryCount': 37, 'storyCount': 7, 'type': 'org', 'updatedAt': 'Tue Nov 27 2018 09:29:38 GMT+0000 (Coordinated Universal Time)' } The following snippet prints the name of the account called \u2018Triply\u2019: const account = await triply.getAccount('Triply') console.log((await account.getInfo()).name) Account.getPinnedItems() \u00b6 Returns the list of datasets, stories and queries that are pinned for the current account. A pinned item is an item that is displayed in a prominent way on the account web page. Order considerations \u00b6 The order in which the pinned datasets are returned reflects the order in which they appear on the organization homepage (from top-left to bottom-right). Examples \u00b6 The following snippet prints the names of the items that are pinned on the Triply account page: const account = await triply.getAccount('Triply') for await (const item of account.getPinnedItems()) { console.log((await item.getInfo()).name) } See also \u00b6 This method returns various types of objects. Each class has different functionalities: See class Dataset for an overview of the methods for dataset objects. See class Query for an overview of the methods for query objects. See class Story for an overview of the methods for story objects. Account.getQuery(name: string) \u00b6 Returns the TriplyDB query with the given name . Examples \u00b6 The following snippet prints the query string for a query called animal-gallery that belongs to the account called Triply : const account = await triply.getAccount('Triply') const query = await account.getQuery('animal-gallery') console.log((await query.getInfo()).requestConfig?.payload.query) See also \u00b6 See class Query for an overview of the methods for query objects. Account.getQueries() \u00b6 Returns an async iterator over the accessible queries that belong to the account. Access restrictions \u00b6 The iterator only includes datasets that are accessible for the current connection with a TriplyDB server: If no API token is configured, the iterator will include all and only public queries belonging to this account. If an API token is configured, the iterator will include all public and internal queries that belong to this account, and will include all private queries that belong to this account if the API token gives read access to the account. Examples \u00b6 The following snippet prints the names of the queries that belong to the account called Triply : const account = await triply.getAccount('Triply') for await (const query of account.getQueries()) { console.log((await query.getInfo()).name) } See also \u00b6 See class Query for an overview of the methods for query objects. Account.ensureStory(name: string, metadata: object) \u00b6 Ensures the existence of a story with the given name and with the specified metadata , if given. Calling this method ensures that the necessary changes (if any) are made in the connected-to TriplyDB server that result in an end state in which a story with the given name and metadata exists. This method is useful in practice, because it removes the burden on the programmer to have to write custom code for checking for the existence of a story, and conditionally create a new story or make metadata changes to an existing story. The changes made as a result of calling this method depend on the current state of the connected-to TriplyDB server: If this account does not yet have a story with the given name , then the behavior is identical to calling Account.addStory(name: string, metadata?: object) with the same arguments. If this account already has a story with the given name and with the same metadata , then this method returns that story. Optional \u00b6 displayName Accepts a string value to be used as the display name for the story. accessLevel Accepts either of the following values: 'private' (default), 'internal' , 'public' . content Accepts a list of StoryElementUpdate objects, defined below. Note: If no accessLevel is specified, the default used is 'private'. Examples Example 1: To ensure a Story only requires a name of type string. It's access level will default to private await someUser.ensureStory(`someStoryName`) Example 2: Ensure a Story setting it's accessLevel and displayName . await someUser.ensureStory(`someStoryName`, { accessLevel: 'public', displayName: `This is a Story`, }) Account.addStory(name: string, newStoryOptions?: object) \u00b6 Required \u00b6 Adds and returns the TriplyDB story with the given name . Optional \u00b6 The optional new story object that can be passed accepts the following properties: displayName Accepts a string value to be used as a display name for the story accessLevel Sets the access level for the story. Accepts either of the following: 'private' (default), 'internal' , 'public' . If no accesslevel is specified, the default value private is used. Examples : Example 1 - creates a newStory that is 'private' const newStory = await someUser.addStory('name-of-story') Example 2 - creates a newStory that is 'public' const newStory = await someUser.addStory('name-of-story', { accessLevel: 'public', }) Account.getStory(name: string) \u00b6 Returns the TriplyDB story with the given name . Examples \u00b6 The following snippet prints the paragraphs in the story called the-iris-dataset that is published under the account called Triply . Stories are sequences of paragraphs and queries. This program prints the paragraphs in the sequence in which they appear in the story. const account = await triply.getAccount('Triply') const story = await account.getStory('the-iris-dataset') See also \u00b6 See class Story for an overview of the methods for story objects. Account.getStories() \u00b6 Returns an iterator with the TriplyDB stories that belong to the account. Examples \u00b6 The following snippet prints the names of the queries that belong to the Triply account: const account = await triply.getAccount('Triply') for await (const story of account.getStories()) { console.log((await story.getInfo()).name) } See also \u00b6 See class Story for an overview of the methods for story objects. Account.pinItems(items: array[Dataset|Story|Query]) \u00b6 Pins the given datasets, stores, and/or queries to the home page of this account. The pinned elements can be seen by people who visit the account online. They are also included in the account metadata. const triply = App.get({ token: process.env.TOKEN }) const user = await triply.getUser() const query = await user.getQuery('name-of-query') const newStory = await user.getStory('name-of-story') user.pinItems([query,newStory]) Account.setAvatar(file: string) \u00b6 Sets a new image that characterizes this account. A circular version of this image is displayed inside the TriplyDB GUI. This image is also published as part of account metadata. Examples \u00b6 The following snippet uploads the local image in file logo.svg and set it as the characterizing image for the Triply account: const account = await triply.getAccount('Triply') await account.setAvatar('logo.svg') Account.update(metadata: object) \u00b6 Updates the metadata for this account. To update the metadata profile with information within the metadata itself, we need the following steps: Obtain the relevant piece of information as a variable/const: getObject() Update the metadata profile with the obtained information stored in the variable/const: update() getObject() Define a constant ( const ) and assign it to ctx.store.getObjects() . The arguments for the function will be the subject, predicate, and graph. The function retrieves the object so the other 3 parts of a quad need to be specified. update() Update the relevant part of the metadata profile with the corresponding piece of information. .update({}) Example If one wants to update the display name of a metadata profile with the object of the following triple within the metadata: <https://example.org/example> <https://schema.org/name> 'Example Name'@en async (ctx) => { // Fetch displayName const displayName = ctx.store .getObjects( 'https://example.org/example', 'https://schema.org/name', graph.metadata ) .find( (node) => node.termType === 'Literal' && node.language === 'en' )?.value; // Specify the environment variable, if necessary const _dataset = process.env['MODE'] === 'Production' ? (await app.triplyDb.getOrganization(organization)).getDataset(dataset) : (await app.triplyDb.getUser()).getDataset(organization + '-' + dataset) // Update the display name if (displayName) await (await _dataset).update({ displayName }) }; The metadata object for accounts can include the following keys: accountName The URL-friendly name of the account. name The human-readable display name of the account description The human-readable description of the account. pinnedItems An array containing the pinned items (datasets, stories and queries) for the account. Email address The email address of the account.","title":"Account"},{"location":"triplydb-js/account/#account","text":"Instances of the Account class denote TriplyDB accounts. Accounts can be either organizations ( Organization ) or users ( User ). Account objects are obtained by calling the following method: App.getAccount(name?: string)","title":"Account"},{"location":"triplydb-js/account/#accountadddatasetname-string-metadata-object","text":"Adds a new TriplyDB dataset with the given name to the current account. The optional metadata argument is used to specify the metadata for the dataset.","title":"Account.addDataset(name: string, metadata?: object)"},{"location":"triplydb-js/account/#access-restrictions","text":"Creating a new dataset only succeeds if an API token is configured that provides write access to the current account. The default access level for a newly created dataset is private . If you want to publish a dataset with a different access level, you must specify the accessLevel key in the metadata argument.","title":"Access restrictions"},{"location":"triplydb-js/account/#arguments","text":"The name argument specifies the URL-friendly name of the new dataset. The name must only contain alphanumeric characters and hyphens ( [A-Za-z0-9\\-] ). The full URL of the newly created dataset has the following structure: https://{host}/{account}/{dataset} The metadata argument optionally specifies the access level and other important metadata: accessLevel The access level of the dataset. The following values are supported: 'private' (default) The dataset can only be accessed by organization members. 'internal' The dataset can only be accessed by users that are logged into the TriplyDB server. 'public' The dataset can be accessed by everybody. When no access level is specified, the most restrictive access level ( private ) is used. description The human-readable description of the dataset. This description can make use of Markdown. displayName The human-readable name of the dataset. This name may contain spaces and other characters that are not allowed in the URL-friendly name. license The license of the dataset. The following license strings are currently supported: 'CC-BY-SA' 'CC0 1.0' 'GFDL' 'ODC-By' 'ODC-ODbL' 'PDDL' 'None' (default) prefixes The IRI prefix declarations that are configured for the dataset. This is specified as a dictionary object whose keys are aliases and whose values are IRI prefixes.","title":"Arguments"},{"location":"triplydb-js/account/#examples","text":"The following snippet creates a new dataset called 'iris' under the account called 'Triply' : The dataset has private access, because the access level is not specified explicitly. The dataset has a description. The dataset has a display name. The dataset has the PDDL license. const account = await triply.getAccount('Triply') const dataset = await account.addDataset('iris', { description: 'A multivariate dataset that quantifies morphologic variation of Iris flowers.', displayName: 'Iris', license: 'PDDL', name: 'iris', prefixes: { def: 'https://triplydb.com/Triply/iris/def/', id: 'https://triplydb.com/Triply/iris/id/', }, })","title":"Examples"},{"location":"triplydb-js/account/#see-also","text":"This method returns a dataset object. See the Dataset section for an overview of the methods that can be called on such objects.","title":"See also"},{"location":"triplydb-js/account/#accountaddqueryname-string-metadata-object","text":"Adds a new SPARQL query to the account.","title":"Account.addQuery(name: string, metadata: object)"},{"location":"triplydb-js/account/#arguments_1","text":"Required: name: string The URL-friendly name of the new query. The name must only contain alphanumeric characters and hyphens ( [A-Za-z0-9\\-] ). dataset: Dataset An instance of class Dataset that the current API token gives access to. serviceType: \"speedy\" | \"virtuoso\" | \"jena\" | \"blazegraph\" A service type that will be used as a preferred SPARQL service to execute the query against. If serviceType is \"virtuoso\", \"jena\" or \"blazegraph\", that service is expected to exist in the dataset before the query can be successfully run. See Dataset queryString: string The SPARQL query string (e.g., 'select * { ?s ?p ?o }' ). Optional: accessLevel The access level of the query. If none is set it defaults to 'private' . The following values are supported: 'private' The query can only be accessed by the Account object for which it is created. 'internal' The query can only be accessed by people who are logged into the TriplyDB server. 'public' The query can be accessed by everybody. description: string A human-readable description of the query. displayName: string The human-readable name of the query. This name may include spaces and other characters that are not allowed in the URL-friendly name . output: string The visualization plugin that is used to display the result set of the query. If none is set it will either set ldFrame if provided or default to table . 'boolean' The boolean view is a special view for ask queries. The value is either 'true' or 'false', and is visualized as `X` (False) or `V` (True). 'gallery' The gallery view allows SPARQL results to be displayed in an HTML gallery. 'gchart' The gchart renders geographical, temporal and numerical data in interactive charts such as bar-, line- and pie charts. 'geo' The geo allows SPARQL results that contain GeoSPARQL semantics to be automatically interpreted and displayed on a 2D map. 'geoEvents' The geoEvents plugin renders geographical events as a story map. 'geo3d' The geo3d allows SPARQL results that contain GeoSPARQL semantics to be automatically interpreted and displayed on a 3D globe. 'markup' The markup can be used to render a variety of markup languages. This requires the use of the `?markup` variable to identify which variable to render. 'network' The network renders SPARQL Construct results in a graph representation. The maximum amount of results that can be visualized is 1.000 due to performance. 'pivot' The pivot view renders SPARQL results in an interactive pivot table where you are able to aggregate the results by dragging your binding variables to columns or rows. 'response' The response view shows the body of the response and offers a easy way to download the result as a file. 'table' The table view allows SPARQL results to be displayed in a table. Each column in the table corresponds to a variable that belongs to the outer projection. 'timeline' The timeline timeline renders the SPARQL results on a Timeline. ldFrame: object JSON LD frame object used to transform plain JSON LD into a framed JSON. Will be used only if an output is not provided. variables: Variable[] A list of objects with the following keys: IRI variable An object of the form Variable (see below) Instances of Variable are objects that can have the following keys: Required: name: string A SPARQL variable name. The variable name must appear in the query string. The question mark ( ? ) or dollar sign ( $ ) is not included. termType: 'Literal'|'NamedNode' The kind of variable. This must be either 'Literal' for literals or 'NamedNode' for IRIs. Optional: allowedValues: string[] The list of string values that is allowed for this variable. datatype: string (if termType='Literal' ) The datatype IRI for the literal variable. language: string (if termType='Literal' ) The language tag for the literal variable. Setting this implies that the dataset IRI is rdf:langString . defaultValue: string The default string value for the required: boolean Whether a query request must include an explicit value for this variable. The default value is false .","title":"Arguments"},{"location":"triplydb-js/account/#example","text":"The following snippet creates a query with the given query string: const triply = App.get({ token: process.env.TOKEN }) const user = await triply.getUser() const myDataset = await user.getDataset('my-dataset') const query = await user.addQuery('my-query', { dataset: myDataset, serviceType: \"speedy\", queryString: 'select (count(*) as ?n) { ?s ?p ?o. }', output: 'response', })","title":"Example"},{"location":"triplydb-js/account/#accountaddstoryname-string-metadata-object","text":"Adds a new data story.","title":"Account.addStory(name: string, metadata?: object)"},{"location":"triplydb-js/account/#required","text":"name: string The URL-friendly name of the data story. The name must only contain alphanumeric characters and hyphens ( [A-Za-z0-9\\-] ).","title":"Required"},{"location":"triplydb-js/account/#optional","text":"accessLevel The access level of the dataset. If none is given the default of 'private' is used. The following values are supported: 'private' The dataset can only be accessed by the Account object for which it is created. 'internal' The dataset can only be accessed by people who are logged into the TriplyDB server. 'public' The dataset can be accessed by everybody. content: StoryElementUpdate[] A list of story elements. The building blocks of the Story. displayName: string The human-readable name of the data story. This name may include spaces and other characters that are not allowed in the URL-friendly name. A story element is an object with the following keys: caption: string The caption is an explanatory text about a specific query. id: string Each Story element gets an Id when it is created. When you want to update a Story element you will need this Id. The Id is only required when updating an element and not needed when adding an object. paragraph: string The Markdown content of a story paragraph. Only allowed when the type is set to 'paragraph' query: Query An instance of class Query . queryVersion: number The version that is used of the specified query. type Either 'paragraph' or 'query' .","title":"Optional"},{"location":"triplydb-js/account/#examples_1","text":"Example 1 - creates a new story that has access level 'private' : const triply = App.get({ token: process.env.TOKEN }) const user = await triply.getUser() const newStory = await user.addStory('name-of-story') Example 2 - creates a new story that has access level 'public' : const triply = App.get({ token: process.env.TOKEN }) const user = await triply.getUser() const newStory = await user.addStory('name-of-story', { accessLevel: 'public', })","title":"Examples"},{"location":"triplydb-js/account/#accountasorganization","text":"Casts the TriplyDB account object to its corresponding organization object. Class Organization is a specialization of class Account . Calling this method on an Organization object does nothing.","title":"Account.asOrganization()"},{"location":"triplydb-js/account/#examples_2","text":"The following snippet retrieves the account named 'Triply' and casts it to an organization: const account = await triply.getAccount('Triply') const organization = account.asOrganization()","title":"Examples"},{"location":"triplydb-js/account/#alternatives","text":"This method is not needed if the organization is directly retrieved with the specialization method App.getOrganization(name: string) . The following snippet returns the same result as the above example, but in a more direct way: const organization = await triply.getOrganization('Triply')","title":"Alternatives"},{"location":"triplydb-js/account/#see-also_1","text":"This method returns an organization object. See class Organization for an overview of the methods that can be called on such objects.","title":"See also"},{"location":"triplydb-js/account/#accountasuser","text":"Casts the TriplyDB account object to its corresponding user object. Class User is a specialization of class Account . Calling this method on a User object does nothing.","title":"Account.asUser()"},{"location":"triplydb-js/account/#examples_3","text":"The following snippet retrieves the account that represents the current user, and casts it to a user object: const account = await triply.getAccount() const user = account.asUser()","title":"Examples"},{"location":"triplydb-js/account/#alternatives_1","text":"This method is not needed if the user is directly retrieved with the specialization method App.getUser(name?: string) . The following snippet returns the same result as the above example, but in a more direct way: const user = await triply.getUser()","title":"Alternatives"},{"location":"triplydb-js/account/#see-also_2","text":"This method returns an organization object. See class Organization for an overview of the methods that can be called on such objects.","title":"See also"},{"location":"triplydb-js/account/#accountensuredatasetname-string-metadata-object","text":"Ensures the existence of a dataset with the given name and with the specified metadata if given. Calling this method ensures that the necessary changes (if any) are made in the connected-to TriplyDB server that result in an end state in which a dataset with the given name and metadata exists. This method is useful in practice, because it removes the burden on the programmer to have to write custom code for checking for the existence of a dataset, and conditionally create a new dataset or make metadata changes to an existing dataset. The changes made as a result of calling this method depend on the current state of the connected-to TriplyDB server: If this account does not yet have a dataset with the given name , then the behavior is identical to calling Account.addDataset(name: string, metadata?: object) with the same arguments. If this account already has a dataset with the given name and with the same metadata , then this method makes no changes.","title":"Account.ensureDataset(name: string, metadata?: object)"},{"location":"triplydb-js/account/#example_1","text":"const account = await triply.getAccount('Triply') const myDataset = await account.ensureDataset(`my-dataset`, { license: 'PDDL', })","title":"Example"},{"location":"triplydb-js/account/#see-also_3","text":"The meaning of the argument to this method are identical to those of the Account.addDataset(name: string, metadata?: object) method.","title":"See also"},{"location":"triplydb-js/account/#accountgetdatasetname-string","text":"Returns the dataset with the given name that is published by this account.","title":"Account.getDataset(name: string)"},{"location":"triplydb-js/account/#examples_4","text":"The following snippet prints the name of the Iris dataset that is published by the Triply account: const account = await triply.getAccount('Triply') const dataset = await triply.getDataset('iris') console.log((await dataset.getInfo()).name)","title":"Examples"},{"location":"triplydb-js/account/#see-also_4","text":"This method returns a dataset object. See class Dataset for an overview of the methods that can be called on such objects.","title":"See also"},{"location":"triplydb-js/account/#accountgetdatasets","text":"Returns an async iterator over the accessible datasets for the current account.","title":"Account.getDatasets()"},{"location":"triplydb-js/account/#access-restrictions_1","text":"The iterator only includes datasets that are accessible for the current connection with a TriplyDB server: If no API token is configured, the iterator will include all and only public datasets belonging to this account. If an API token is configured, the iterator will include all public and internal datasets belonging to this account, and will include all private datasets belonging to this account if the API token gives read access to the account.","title":"Access restrictions"},{"location":"triplydb-js/account/#examples_5","text":"The following snippet prints the names of all accessible dataset under the Triply account: const account = await triply.getAccount('Triply') for await (const dataset of account.getDatasets()) { console.log((await dataset.getInfo()).name) } The following snippet prints the list of names of all accessible datasets under the Triply account: const account = await triply.getAccount('Triply') console.log(await account.getDatasets().toArray())","title":"Examples"},{"location":"triplydb-js/account/#accountgetinfo","text":"Returns information about this account. Information is returned in a dictionary object. Individual keys can be accessed for specific information values. The information object for accounts includes the following keys: avatarUrl A URL to the account image. accountName The URL-friendly name of the account. name The human-readable display name of the account description The human-readable description of the account. createdAt The date and time on which the account was created. datasetCount The number of datasets for the account. queryCount The number of queries for the account. storyCount The number of stories for the account pinnedDatasets An array containing the pinned dataset for the account. pinnedItems An array containing the pinned items (datasets, stories and queries) for the account. type The account type: either organization or user . role The role of the account orgs An array of organizations of which the account is a member. Email address The email address of the account. updatedAt The date and time on which the account was last updated. lastActivity The date and time on which the account was last online on TriplyDB.","title":"Account.getInfo()"},{"location":"triplydb-js/account/#examples_6","text":"The following snippet prints the full information object for the account called \u2018Triply\u2019: const account = await triply.getAccount('Triply') console.log(await account.getInfo()) The output for this snippet can look as follows: { 'accountName': 'Triply', 'avatarUrl': 'https://www.gravatar.com/avatar/9bc28997dd1074e405e1c66196d5e117?d=mm', 'createdAt': 'Mon Mar 19 2018 14:39:18 GMT+0000 (Coordinated Universal Time)', 'datasetCount': 16, 'name': 'Triply', 'queryCount': 37, 'storyCount': 7, 'type': 'org', 'updatedAt': 'Tue Nov 27 2018 09:29:38 GMT+0000 (Coordinated Universal Time)' } The following snippet prints the name of the account called \u2018Triply\u2019: const account = await triply.getAccount('Triply') console.log((await account.getInfo()).name)","title":"Examples"},{"location":"triplydb-js/account/#accountgetpinneditems","text":"Returns the list of datasets, stories and queries that are pinned for the current account. A pinned item is an item that is displayed in a prominent way on the account web page.","title":"Account.getPinnedItems()"},{"location":"triplydb-js/account/#order-considerations","text":"The order in which the pinned datasets are returned reflects the order in which they appear on the organization homepage (from top-left to bottom-right).","title":"Order considerations"},{"location":"triplydb-js/account/#examples_7","text":"The following snippet prints the names of the items that are pinned on the Triply account page: const account = await triply.getAccount('Triply') for await (const item of account.getPinnedItems()) { console.log((await item.getInfo()).name) }","title":"Examples"},{"location":"triplydb-js/account/#see-also_5","text":"This method returns various types of objects. Each class has different functionalities: See class Dataset for an overview of the methods for dataset objects. See class Query for an overview of the methods for query objects. See class Story for an overview of the methods for story objects.","title":"See also"},{"location":"triplydb-js/account/#accountgetqueryname-string","text":"Returns the TriplyDB query with the given name .","title":"Account.getQuery(name: string)"},{"location":"triplydb-js/account/#examples_8","text":"The following snippet prints the query string for a query called animal-gallery that belongs to the account called Triply : const account = await triply.getAccount('Triply') const query = await account.getQuery('animal-gallery') console.log((await query.getInfo()).requestConfig?.payload.query)","title":"Examples"},{"location":"triplydb-js/account/#see-also_6","text":"See class Query for an overview of the methods for query objects.","title":"See also"},{"location":"triplydb-js/account/#accountgetqueries","text":"Returns an async iterator over the accessible queries that belong to the account.","title":"Account.getQueries()"},{"location":"triplydb-js/account/#access-restrictions_2","text":"The iterator only includes datasets that are accessible for the current connection with a TriplyDB server: If no API token is configured, the iterator will include all and only public queries belonging to this account. If an API token is configured, the iterator will include all public and internal queries that belong to this account, and will include all private queries that belong to this account if the API token gives read access to the account.","title":"Access restrictions"},{"location":"triplydb-js/account/#examples_9","text":"The following snippet prints the names of the queries that belong to the account called Triply : const account = await triply.getAccount('Triply') for await (const query of account.getQueries()) { console.log((await query.getInfo()).name) }","title":"Examples"},{"location":"triplydb-js/account/#see-also_7","text":"See class Query for an overview of the methods for query objects.","title":"See also"},{"location":"triplydb-js/account/#accountensurestoryname-string-metadata-object","text":"Ensures the existence of a story with the given name and with the specified metadata , if given. Calling this method ensures that the necessary changes (if any) are made in the connected-to TriplyDB server that result in an end state in which a story with the given name and metadata exists. This method is useful in practice, because it removes the burden on the programmer to have to write custom code for checking for the existence of a story, and conditionally create a new story or make metadata changes to an existing story. The changes made as a result of calling this method depend on the current state of the connected-to TriplyDB server: If this account does not yet have a story with the given name , then the behavior is identical to calling Account.addStory(name: string, metadata?: object) with the same arguments. If this account already has a story with the given name and with the same metadata , then this method returns that story.","title":"Account.ensureStory(name: string, metadata: object)"},{"location":"triplydb-js/account/#optional_1","text":"displayName Accepts a string value to be used as the display name for the story. accessLevel Accepts either of the following values: 'private' (default), 'internal' , 'public' . content Accepts a list of StoryElementUpdate objects, defined below. Note: If no accessLevel is specified, the default used is 'private'. Examples Example 1: To ensure a Story only requires a name of type string. It's access level will default to private await someUser.ensureStory(`someStoryName`) Example 2: Ensure a Story setting it's accessLevel and displayName . await someUser.ensureStory(`someStoryName`, { accessLevel: 'public', displayName: `This is a Story`, })","title":"Optional"},{"location":"triplydb-js/account/#accountaddstoryname-string-newstoryoptions-object","text":"","title":"Account.addStory(name: string, newStoryOptions?: object)"},{"location":"triplydb-js/account/#required_1","text":"Adds and returns the TriplyDB story with the given name .","title":"Required"},{"location":"triplydb-js/account/#optional_2","text":"The optional new story object that can be passed accepts the following properties: displayName Accepts a string value to be used as a display name for the story accessLevel Sets the access level for the story. Accepts either of the following: 'private' (default), 'internal' , 'public' . If no accesslevel is specified, the default value private is used. Examples : Example 1 - creates a newStory that is 'private' const newStory = await someUser.addStory('name-of-story') Example 2 - creates a newStory that is 'public' const newStory = await someUser.addStory('name-of-story', { accessLevel: 'public', })","title":"Optional"},{"location":"triplydb-js/account/#accountgetstoryname-string","text":"Returns the TriplyDB story with the given name .","title":"Account.getStory(name: string)"},{"location":"triplydb-js/account/#examples_10","text":"The following snippet prints the paragraphs in the story called the-iris-dataset that is published under the account called Triply . Stories are sequences of paragraphs and queries. This program prints the paragraphs in the sequence in which they appear in the story. const account = await triply.getAccount('Triply') const story = await account.getStory('the-iris-dataset')","title":"Examples"},{"location":"triplydb-js/account/#see-also_8","text":"See class Story for an overview of the methods for story objects.","title":"See also"},{"location":"triplydb-js/account/#accountgetstories","text":"Returns an iterator with the TriplyDB stories that belong to the account.","title":"Account.getStories()"},{"location":"triplydb-js/account/#examples_11","text":"The following snippet prints the names of the queries that belong to the Triply account: const account = await triply.getAccount('Triply') for await (const story of account.getStories()) { console.log((await story.getInfo()).name) }","title":"Examples"},{"location":"triplydb-js/account/#see-also_9","text":"See class Story for an overview of the methods for story objects.","title":"See also"},{"location":"triplydb-js/account/#accountpinitemsitems-arraydatasetstoryquery","text":"Pins the given datasets, stores, and/or queries to the home page of this account. The pinned elements can be seen by people who visit the account online. They are also included in the account metadata. const triply = App.get({ token: process.env.TOKEN }) const user = await triply.getUser() const query = await user.getQuery('name-of-query') const newStory = await user.getStory('name-of-story') user.pinItems([query,newStory])","title":"Account.pinItems(items: array[Dataset|Story|Query])"},{"location":"triplydb-js/account/#accountsetavatarfile-string","text":"Sets a new image that characterizes this account. A circular version of this image is displayed inside the TriplyDB GUI. This image is also published as part of account metadata.","title":"Account.setAvatar(file: string)"},{"location":"triplydb-js/account/#examples_12","text":"The following snippet uploads the local image in file logo.svg and set it as the characterizing image for the Triply account: const account = await triply.getAccount('Triply') await account.setAvatar('logo.svg')","title":"Examples"},{"location":"triplydb-js/account/#accountupdatemetadata-object","text":"Updates the metadata for this account. To update the metadata profile with information within the metadata itself, we need the following steps: Obtain the relevant piece of information as a variable/const: getObject() Update the metadata profile with the obtained information stored in the variable/const: update() getObject() Define a constant ( const ) and assign it to ctx.store.getObjects() . The arguments for the function will be the subject, predicate, and graph. The function retrieves the object so the other 3 parts of a quad need to be specified. update() Update the relevant part of the metadata profile with the corresponding piece of information. .update({}) Example If one wants to update the display name of a metadata profile with the object of the following triple within the metadata: <https://example.org/example> <https://schema.org/name> 'Example Name'@en async (ctx) => { // Fetch displayName const displayName = ctx.store .getObjects( 'https://example.org/example', 'https://schema.org/name', graph.metadata ) .find( (node) => node.termType === 'Literal' && node.language === 'en' )?.value; // Specify the environment variable, if necessary const _dataset = process.env['MODE'] === 'Production' ? (await app.triplyDb.getOrganization(organization)).getDataset(dataset) : (await app.triplyDb.getUser()).getDataset(organization + '-' + dataset) // Update the display name if (displayName) await (await _dataset).update({ displayName }) }; The metadata object for accounts can include the following keys: accountName The URL-friendly name of the account. name The human-readable display name of the account description The human-readable description of the account. pinnedItems An array containing the pinned items (datasets, stories and queries) for the account. Email address The email address of the account.","title":"Account.update(metadata: object)"},{"location":"triplydb-js/app/","text":"On this page: App App.getAccount(name?: string) Examples See also App.getAccounts() Example App.getInfo() Examples App.getOrganization(name: string) Examples Alternatives See also App.getUser(name?: string) Examples Alternatives See also App.isCompatibleWith(minimumVersion: string) Arguments See also App \u00b6 Instances of the App class are specific application connections that are set-up with a TriplyDB server. Connections to TriplyDB servers can be created with and without setting an API token. When no API token is set, the connection can be used to perform read-only operations over public data. When an API token is set, the connection can be used to perform read/write operations over public/private data the API token grants access to. The following snippet creates an instance of the App object that establishes read-only access to the TriplyDB server at https://triplydb.com : import App from '@triply/triplydb' const triply = App.get({ url: 'https://api.triplydb.com' }) Notice that the URL must point to the API of the TriplyDB server that the App object connects to. The API URL is typically created by adding the api. subdomain in front of the server's host name. For example, since [1] is the web-based GUI for the TriplyDB server, then [2] is the corresponding API for that instance. [1] https://triplydb.com [2] https://api.triplydb.com When an API token is specified, the operations that can be performed through the App object are determined by: The access level of the token: either \u201cRead access\u201d, \u201cWrite access\u201d, or \u201cManagement access\u201d. The credentials of the user account for which the API token is created. When a user is a member of an organization, she has access to all its datasets, stories, and queries; a user always has access to her own datasets, stores and queries. The following token access levels are available: 1. \u201cRead access\u201d allows: Read operations over data with access level \u201cPublic\u201d. Read operations over data with access level \u201cInternal\u201d. Read operations over data with access level \u201cPrivate\u201d that belongs to the user who created the token. Read operations over data with access level \u201cPrivate\u201d that belongs to organizations to which the user who created the token is a member. 2. \u201cWrite access\u201d allows: All operations allows by \u201cRead access\u201d. Write operations over data that has access setting \u201cInternal\u201d. Write operations over data 3. \u201cManagement access\u201d allows the following operations to be performed: creating organizations, adding/removing members to/from organizations. The following creates an App object with an API token that is made available through an environment variable: import App from '@triply/triplydb' const triply = App.get({ token: process.env.TOKEN }) It is typical for one TriplyDB.js script to have exactly one App object. App.getAccount(name?: string) \u00b6 Returns the TriplyDB account with the given name . If name is omitted, the TriplyDB account that is associated with the current API token is returned. Examples \u00b6 The following snippet returns the account called 'Triply' . const account = await triply.getAccount('Triply') The following snippet returns the current account. This is the account for which the currently configured API token was created. const account = await triply.getAccount() See also \u00b6 This method returns an account object. See class Account for an overview of the methods that can be called on such objects. Class Account has two specializations: class Organization and class User . In line with these class specializations, there are also two method specializations: Method App.getOrganization(name: string) returns an organization object. Method App.getUser(name?: string) returns a user object. App.getAccounts() \u00b6 Returns an async iterator over all accounts in the TriplyDB server. Example \u00b6 The following snippet prints the display names for all accounts in the TriplyDB server at https://triplydb.com : const triply = App.get({ url: 'https://api.triplydb.com' }) for await (const account of triply.getAccounts()) { console.log((await account.getInfo()).name) } The following snippet returns an array that contains all account objects: console.log(await triply.getAccounts().toArray()) See class Account for an overview of the methods that can be used with account objects. App.getInfo() \u00b6 Returns information about the TriplyDB server that the App is connected to. Information is returned in a dictionary object. Individual keys can be accessed for specific information values. Examples \u00b6 The following snippet prints the contact email for the TriplyDB server: console.log((await triply.getInfo()).contactEmail) The following snippet returns an object describing the used TriplyDB server: console.log(await triply.getInfo()) App.getOrganization(name: string) \u00b6 Returns the TriplyDB organization with the given name . This method is similar to App.getAccount(name?: string) , but differs in the following ways: This method only works for accounts that represent TriplyDB organizations. This method returns an organization object. Class Organization is a specialization of class Account . Examples \u00b6 The following snippet returns the organization called 'Triply' : const organization = await triply.getOrganization('Triply') See class Organization for an overview of the methods that can be used with organization objects. Alternatives \u00b6 This method is a shorthand for calling the following two methods: Call method App.getAccount(name?: string) to retrieve an account object. Then call method Account.asOrganization() to cast the account object into an organization object. The following snippet returns the same result as the previous example, but uses two methods instead of one: const account = await triply.getAccount('Triply') const organization = account.asOrganization() See also \u00b6 This method returns an organization object. See class Organization for an overview of the methods that can be called on such objects. App.getUser(name?: string) \u00b6 Returns the TriplyDB user with the given name . If name is omitted, the TriplyDB user that is associated with the current API token is returned. This only works if an API token is configured for the current App object. Examples \u00b6 The following snippet returns the user with name 'somebody' : const user = await triply.getUser('somebody') The following snippet returns the user for whom the API token was created. This only works if an API token was configured when the App object was created: const me = await triply.getUser() Alternatives \u00b6 This method is a shorthand for the following two methods: Call method App.getAccount() to retrieve an account object. Then call method Account.asUser() to cast the account object into a user object. The following snippet returns the same result as the previous examples, but uses two methods instead of one: const account = await triply.getAccount('somebody') const user = account.asUser() See also \u00b6 This method returns a user object. See class User for an overview of the methods that can be called on such objects. App.isCompatibleWith(minimumVersion: string) \u00b6 Succeeds if and only if the currently connected to TriplyDB server has a version that is identical to or higher than the given minimum version. Arguments \u00b6 Argument minimumVersion must be a string that uses Semantic Versioning. For example '1.2.3' . See also \u00b6 To inspect the current version of the connected-to TriplyDB server, use App.getInfo() .","title":"App"},{"location":"triplydb-js/app/#app","text":"Instances of the App class are specific application connections that are set-up with a TriplyDB server. Connections to TriplyDB servers can be created with and without setting an API token. When no API token is set, the connection can be used to perform read-only operations over public data. When an API token is set, the connection can be used to perform read/write operations over public/private data the API token grants access to. The following snippet creates an instance of the App object that establishes read-only access to the TriplyDB server at https://triplydb.com : import App from '@triply/triplydb' const triply = App.get({ url: 'https://api.triplydb.com' }) Notice that the URL must point to the API of the TriplyDB server that the App object connects to. The API URL is typically created by adding the api. subdomain in front of the server's host name. For example, since [1] is the web-based GUI for the TriplyDB server, then [2] is the corresponding API for that instance. [1] https://triplydb.com [2] https://api.triplydb.com When an API token is specified, the operations that can be performed through the App object are determined by: The access level of the token: either \u201cRead access\u201d, \u201cWrite access\u201d, or \u201cManagement access\u201d. The credentials of the user account for which the API token is created. When a user is a member of an organization, she has access to all its datasets, stories, and queries; a user always has access to her own datasets, stores and queries. The following token access levels are available: 1. \u201cRead access\u201d allows: Read operations over data with access level \u201cPublic\u201d. Read operations over data with access level \u201cInternal\u201d. Read operations over data with access level \u201cPrivate\u201d that belongs to the user who created the token. Read operations over data with access level \u201cPrivate\u201d that belongs to organizations to which the user who created the token is a member. 2. \u201cWrite access\u201d allows: All operations allows by \u201cRead access\u201d. Write operations over data that has access setting \u201cInternal\u201d. Write operations over data 3. \u201cManagement access\u201d allows the following operations to be performed: creating organizations, adding/removing members to/from organizations. The following creates an App object with an API token that is made available through an environment variable: import App from '@triply/triplydb' const triply = App.get({ token: process.env.TOKEN }) It is typical for one TriplyDB.js script to have exactly one App object.","title":"App"},{"location":"triplydb-js/app/#appgetaccountname-string","text":"Returns the TriplyDB account with the given name . If name is omitted, the TriplyDB account that is associated with the current API token is returned.","title":"App.getAccount(name?: string)"},{"location":"triplydb-js/app/#examples","text":"The following snippet returns the account called 'Triply' . const account = await triply.getAccount('Triply') The following snippet returns the current account. This is the account for which the currently configured API token was created. const account = await triply.getAccount()","title":"Examples"},{"location":"triplydb-js/app/#see-also","text":"This method returns an account object. See class Account for an overview of the methods that can be called on such objects. Class Account has two specializations: class Organization and class User . In line with these class specializations, there are also two method specializations: Method App.getOrganization(name: string) returns an organization object. Method App.getUser(name?: string) returns a user object.","title":"See also"},{"location":"triplydb-js/app/#appgetaccounts","text":"Returns an async iterator over all accounts in the TriplyDB server.","title":"App.getAccounts()"},{"location":"triplydb-js/app/#example","text":"The following snippet prints the display names for all accounts in the TriplyDB server at https://triplydb.com : const triply = App.get({ url: 'https://api.triplydb.com' }) for await (const account of triply.getAccounts()) { console.log((await account.getInfo()).name) } The following snippet returns an array that contains all account objects: console.log(await triply.getAccounts().toArray()) See class Account for an overview of the methods that can be used with account objects.","title":"Example"},{"location":"triplydb-js/app/#appgetinfo","text":"Returns information about the TriplyDB server that the App is connected to. Information is returned in a dictionary object. Individual keys can be accessed for specific information values.","title":"App.getInfo()"},{"location":"triplydb-js/app/#examples_1","text":"The following snippet prints the contact email for the TriplyDB server: console.log((await triply.getInfo()).contactEmail) The following snippet returns an object describing the used TriplyDB server: console.log(await triply.getInfo())","title":"Examples"},{"location":"triplydb-js/app/#appgetorganizationname-string","text":"Returns the TriplyDB organization with the given name . This method is similar to App.getAccount(name?: string) , but differs in the following ways: This method only works for accounts that represent TriplyDB organizations. This method returns an organization object. Class Organization is a specialization of class Account .","title":"App.getOrganization(name: string)"},{"location":"triplydb-js/app/#examples_2","text":"The following snippet returns the organization called 'Triply' : const organization = await triply.getOrganization('Triply') See class Organization for an overview of the methods that can be used with organization objects.","title":"Examples"},{"location":"triplydb-js/app/#alternatives","text":"This method is a shorthand for calling the following two methods: Call method App.getAccount(name?: string) to retrieve an account object. Then call method Account.asOrganization() to cast the account object into an organization object. The following snippet returns the same result as the previous example, but uses two methods instead of one: const account = await triply.getAccount('Triply') const organization = account.asOrganization()","title":"Alternatives"},{"location":"triplydb-js/app/#see-also_1","text":"This method returns an organization object. See class Organization for an overview of the methods that can be called on such objects.","title":"See also"},{"location":"triplydb-js/app/#appgetusername-string","text":"Returns the TriplyDB user with the given name . If name is omitted, the TriplyDB user that is associated with the current API token is returned. This only works if an API token is configured for the current App object.","title":"App.getUser(name?: string)"},{"location":"triplydb-js/app/#examples_3","text":"The following snippet returns the user with name 'somebody' : const user = await triply.getUser('somebody') The following snippet returns the user for whom the API token was created. This only works if an API token was configured when the App object was created: const me = await triply.getUser()","title":"Examples"},{"location":"triplydb-js/app/#alternatives_1","text":"This method is a shorthand for the following two methods: Call method App.getAccount() to retrieve an account object. Then call method Account.asUser() to cast the account object into a user object. The following snippet returns the same result as the previous examples, but uses two methods instead of one: const account = await triply.getAccount('somebody') const user = account.asUser()","title":"Alternatives"},{"location":"triplydb-js/app/#see-also_2","text":"This method returns a user object. See class User for an overview of the methods that can be called on such objects.","title":"See also"},{"location":"triplydb-js/app/#appiscompatiblewithminimumversion-string","text":"Succeeds if and only if the currently connected to TriplyDB server has a version that is identical to or higher than the given minimum version.","title":"App.isCompatibleWith(minimumVersion: string)"},{"location":"triplydb-js/app/#arguments","text":"Argument minimumVersion must be a string that uses Semantic Versioning. For example '1.2.3' .","title":"Arguments"},{"location":"triplydb-js/app/#see-also_3","text":"To inspect the current version of the connected-to TriplyDB server, use App.getInfo() .","title":"See also"},{"location":"triplydb-js/asset/","text":"On this page: Asset Asset.addVersion(path: File | string) Example Asset.delete() Example Asset.getInfo(version?: number) Examples Asset.getVersionInfo(version: number) Examples Asset.selectVersion(version: number) Example Asset.toFile(path: string, version?: number) Example Asset.toStream(version?: number) Example Asset \u00b6 Not all data can be stored as RDF data. For example images and video files use a binary format. Such files can also be stored in TriplyDB as Assets and can be integrated into the Knowledge Graph. Each asset has a specific identifier that can be used in the Knowledge Graph. An asset is always uploaded per dataset, for which the function uploadAsset() is used. see Dataset.uploadAsset() for uploading an asset. If the asset already has been created following functions can retrieve it from the dataset. Dataset.getAsset(assetName: string, versionNumber?: number) Dataset.getAssets() TriplyDB.js supports several functions to manipulate an asset on TriplyDB. Asset.addVersion(path: File | string) \u00b6 Update an asset with a new version of the document using the addVersion function. The input of this function is a path to the file location that you want to update the asset with. The file you want to add as a new version does not in any ways have to correspond to the asset. Example \u00b6 The following snippet uploads the an file my-file.pdf and upload it as the new version of the asset: const triply = App.get({ token: process.env.TOKEN }) const user = await triply.getAccount() const dataset = await user.getDataset('my-dataset') const asset = await dataset.getAsset('my-asset') await asset.addVersion('my-file.pdf') Asset.delete() \u00b6 To delete an asset with all of its versions execute the delete() function. Example \u00b6 The following snippet uploads the an file my-file.pdf and upload it as the new version of the asset: const triply = App.get({ token: process.env.TOKEN }) const user = await triply.getAccount() const dataset = await user.getDataset('my-dataset') const asset = await dataset.getAsset('my-asset') await asset.delete() Asset.getInfo(version?: number) \u00b6 Returns information about this asset. Information is returned in a dictionary object. Individual keys can be accessed for specific information values. Optionally you can give the version number to retrieve the assetInfo of a particular version. The information object for assets includes the following keys: assetName The URL-friendly name of the asset. identifier The hexadecimal identifier of the asset createdAt The date and time on which the asset was created. url The url of the asset. versions An array containing all versions of the asset. uploadedAt The date and time on which the asset was uploaded. fileSize Number with the bytesize of the asset Examples \u00b6 The following snippet prints the full information object for the asset called \u2018my-asset\u2019: const triply = App.get({ token: process.env.TOKEN }) const user = await triply.getAccount() const dataset = await user.getDataset('my-dataset') const asset = await dataset.getAsset('my-asset') console.log(await asset.getInfo()) Asset.getVersionInfo(version: number) \u00b6 Returns version specific information about this asset. Information is returned in a dictionary object. Individual keys can be accessed for specific information values. The version specific information object for assets includes the following keys: id The hexadecimal identifier of the asset fileSize Number with the bytesize of the asset url The url of the asset. uploadedAt The date and time on which the asset was uploaded. Examples \u00b6 The following snippet prints the version information object for the asset called \u2018my-asset\u2019 at version 1 : const triply = App.get({ token: process.env.TOKEN }) const user = await triply.getAccount() const dataset = await user.getDataset('my-dataset') const asset = await dataset.getAsset('my-asset') console.log(await asset.getVersionInfo(1)) Asset.selectVersion(version: number) \u00b6 With the selectVersion() function you can select a specific version of an Asset. Each version corresponds to a iteration of the file that is added as an asset. The argument of the selectVersion() function is a number of the version you want to retrieve. Example \u00b6 To select the first asset from the list of assets use the selectVersion with the argument 1 . const triply = App.get({ token: process.env.TOKEN }) const user = await triply.getAccount() const dataset = await user.getDataset('my-dataset') const asset = await dataset.getAsset('my-asset') const versionedAsset = asset.selectVersion(1) Asset.toFile(path: string, version?: number) \u00b6 The binary representation of an asset can be retrieved and written to file via the asset.toFile() function. This function takes as input a string path to the download location and optionally a version number. Example \u00b6 To download the latest version of my-asset asset to the file my-file-location.txt . const triply = App.get({ token: process.env.TOKEN }) const user = await triply.getAccount() const dataset = await user.getDataset('my-dataset') const asset = await dataset.getAsset('my-asset') asset.toFile('my-file-location.txt') Asset.toStream(version?: number) \u00b6 If instead of downloading the asset to a file for later usage you want to directly use the asset. The toStream() functionality is available. This downloads the asset as a stream for use in a script. The toStream() has as optional argument a version number. Example \u00b6 To get the latest version of my-asset asset as a stream available. const triply = App.get({ token: process.env.TOKEN }) const user = await triply.getAccount() const dataset = await user.getDataset('my-dataset') const asset = await dataset.getAsset('my-asset') asset.toStream()","title":"Asset"},{"location":"triplydb-js/asset/#asset","text":"Not all data can be stored as RDF data. For example images and video files use a binary format. Such files can also be stored in TriplyDB as Assets and can be integrated into the Knowledge Graph. Each asset has a specific identifier that can be used in the Knowledge Graph. An asset is always uploaded per dataset, for which the function uploadAsset() is used. see Dataset.uploadAsset() for uploading an asset. If the asset already has been created following functions can retrieve it from the dataset. Dataset.getAsset(assetName: string, versionNumber?: number) Dataset.getAssets() TriplyDB.js supports several functions to manipulate an asset on TriplyDB.","title":"Asset"},{"location":"triplydb-js/asset/#assetaddversionpath-file-string","text":"Update an asset with a new version of the document using the addVersion function. The input of this function is a path to the file location that you want to update the asset with. The file you want to add as a new version does not in any ways have to correspond to the asset.","title":"Asset.addVersion(path: File | string)"},{"location":"triplydb-js/asset/#example","text":"The following snippet uploads the an file my-file.pdf and upload it as the new version of the asset: const triply = App.get({ token: process.env.TOKEN }) const user = await triply.getAccount() const dataset = await user.getDataset('my-dataset') const asset = await dataset.getAsset('my-asset') await asset.addVersion('my-file.pdf')","title":"Example"},{"location":"triplydb-js/asset/#assetdelete","text":"To delete an asset with all of its versions execute the delete() function.","title":"Asset.delete()"},{"location":"triplydb-js/asset/#example_1","text":"The following snippet uploads the an file my-file.pdf and upload it as the new version of the asset: const triply = App.get({ token: process.env.TOKEN }) const user = await triply.getAccount() const dataset = await user.getDataset('my-dataset') const asset = await dataset.getAsset('my-asset') await asset.delete()","title":"Example"},{"location":"triplydb-js/asset/#assetgetinfoversion-number","text":"Returns information about this asset. Information is returned in a dictionary object. Individual keys can be accessed for specific information values. Optionally you can give the version number to retrieve the assetInfo of a particular version. The information object for assets includes the following keys: assetName The URL-friendly name of the asset. identifier The hexadecimal identifier of the asset createdAt The date and time on which the asset was created. url The url of the asset. versions An array containing all versions of the asset. uploadedAt The date and time on which the asset was uploaded. fileSize Number with the bytesize of the asset","title":"Asset.getInfo(version?: number)"},{"location":"triplydb-js/asset/#examples","text":"The following snippet prints the full information object for the asset called \u2018my-asset\u2019: const triply = App.get({ token: process.env.TOKEN }) const user = await triply.getAccount() const dataset = await user.getDataset('my-dataset') const asset = await dataset.getAsset('my-asset') console.log(await asset.getInfo())","title":"Examples"},{"location":"triplydb-js/asset/#assetgetversioninfoversion-number","text":"Returns version specific information about this asset. Information is returned in a dictionary object. Individual keys can be accessed for specific information values. The version specific information object for assets includes the following keys: id The hexadecimal identifier of the asset fileSize Number with the bytesize of the asset url The url of the asset. uploadedAt The date and time on which the asset was uploaded.","title":"Asset.getVersionInfo(version: number)"},{"location":"triplydb-js/asset/#examples_1","text":"The following snippet prints the version information object for the asset called \u2018my-asset\u2019 at version 1 : const triply = App.get({ token: process.env.TOKEN }) const user = await triply.getAccount() const dataset = await user.getDataset('my-dataset') const asset = await dataset.getAsset('my-asset') console.log(await asset.getVersionInfo(1))","title":"Examples"},{"location":"triplydb-js/asset/#assetselectversionversion-number","text":"With the selectVersion() function you can select a specific version of an Asset. Each version corresponds to a iteration of the file that is added as an asset. The argument of the selectVersion() function is a number of the version you want to retrieve.","title":"Asset.selectVersion(version: number)"},{"location":"triplydb-js/asset/#example_2","text":"To select the first asset from the list of assets use the selectVersion with the argument 1 . const triply = App.get({ token: process.env.TOKEN }) const user = await triply.getAccount() const dataset = await user.getDataset('my-dataset') const asset = await dataset.getAsset('my-asset') const versionedAsset = asset.selectVersion(1)","title":"Example"},{"location":"triplydb-js/asset/#assettofilepath-string-version-number","text":"The binary representation of an asset can be retrieved and written to file via the asset.toFile() function. This function takes as input a string path to the download location and optionally a version number.","title":"Asset.toFile(path: string, version?: number)"},{"location":"triplydb-js/asset/#example_3","text":"To download the latest version of my-asset asset to the file my-file-location.txt . const triply = App.get({ token: process.env.TOKEN }) const user = await triply.getAccount() const dataset = await user.getDataset('my-dataset') const asset = await dataset.getAsset('my-asset') asset.toFile('my-file-location.txt')","title":"Example"},{"location":"triplydb-js/asset/#assettostreamversion-number","text":"If instead of downloading the asset to a file for later usage you want to directly use the asset. The toStream() functionality is available. This downloads the asset as a stream for use in a script. The toStream() has as optional argument a version number.","title":"Asset.toStream(version?: number)"},{"location":"triplydb-js/asset/#example_4","text":"To get the latest version of my-asset asset as a stream available. const triply = App.get({ token: process.env.TOKEN }) const user = await triply.getAccount() const dataset = await user.getDataset('my-dataset') const asset = await dataset.getAsset('my-asset') asset.toStream()","title":"Example"},{"location":"triplydb-js/dataset/","text":"On this page: Dataset Dataset.addPrefixes(prefixes: object) Examples Dataset.ensureService(name: string, metadata?: object) Required Optional: metadata Dataset.addService(name: string, metadata?: object) Arguments Required Optional Examples See also Dataset.clear(...resourceType: string) Arguments Examples Dataset.copy(account: string, dataset: string) Examples Dataset.delete() Examples See also Dataset.deleteGraph(name: string) Examples Dataset.describe(iri: string|NamedNode) Examples Dataset.getAsset(name: string, version?: number) Examples Dataset.getAssets() Examples Dataset.getGraph(name: string) Examples Dataset.getGraphs() Examples Dataset.getInfo() Examples Dataset.getPrefixes() Examples Dataset.getService(name: string) Examples Dataset.getServices() Examples Dataset.getStatements({subject?: string, predicate?: string, object?: string, graph?: string}) Arguments Example Get the data locally Dataset.graphsToFile(destinationPath: string, arguments?: object) Optional Examples Dataset.graphsToStore(graph?: Graph) Optional Examples Dataset.graphsToStream(type: 'compressed' | 'rdf-js', arguments?: object) Optional Examples Dataset.importFromDataset(fromDataset: Dataset, arguments?: object) Required Optional Examples Dataset.importFromFiles(files: list(string || File), defaultsConfig?: object) Required Supported files Examples Dataset.importFromStore(store: n3.Store, defaultsConfig?: object) Examples Dataset.importFromUrls(urls: list(string), defaultsConfig?: object) Required Examples Dataset.removeAllGraphs() Examples Dataset.removePrefixes(prefixes: string[]) Examples Dataset.renameGraph(from: string, to: string) Examples Dataset.update(metadata: object) Arguments Example Dataset.uploadAsset( filePath: string, opts?: {mode?: 'throw-if-exists'| 'replace-if-exists'| 'append-version', name?: string}) User cases Examples Dataset \u00b6 The Dataset class represents a TriplyDB dataset. Dataset.addPrefixes(prefixes: object) \u00b6 Adds IRI prefix declarations to the dataset. The prefixes argument is a dictionary object whose keys are aliases and whose values are IRI prefixes. Examples \u00b6 The following snippet adds prefix declarations for aliases id and def to the Iris dataset: const organization = await triply.getOrganization('Triply') const dataset = await organization.getDataset(iris) await dataset.addPrefixes({ def: 'https://triplydb.com/Triply/iris/def/', id: 'https://triplydb.com/Triply/iris/id/', }) Dataset.ensureService(name: string, metadata?: object) \u00b6 Ensures the existence of a service with the given name and with the specified metadata if given. Calling this method ensures that the necessary changes (if any) are made in the connected-to TriplyDB server that result in an end state in which a service with the given name and metadata exists. This method is useful in practice, because it removes the burden on the programmer to have to write custom code for checking for the existence of a service, and conditionally create a new service or make metadata changes to an existing service. The changes made as a result of calling this method depend on the current state of the connected-to TriplyDB server: If this dataset does not yet have a service with the given name , then the behavior is identical to calling Dataset.addService(name: string, metadata?: object) with the same arguments. If this dataset already has a service with the given name , but with different metadata specified for it, then the behavior is identical to calling Account.getDataset(name: string) and Dataset.update(metadata: object) . If this dataset already has a service with the given name and with the same metadata , then this method returns that service. Required \u00b6 name Accepts a string value which is the name of the service to ensure. Optional: metadata \u00b6 serviceMetadata = { type: 'elasticsearch' | 'virtuoso' | 'jena' ; config?: { reasoner?: 'OWL' | 'RDFS' | 'None'; }; }; type Accepts a string value of one of the following: 'virtuoso' , 'elasticsearch' , 'jena' . config Config is an optional property. It accepts an object with a reasoner property. reasoner The reasoner property accepts a string value of either 'OWL' , 'RDFS' , or 'None' . Note: If no options are specified the default service is of type: virtuoso . Note that the config.reasoner will only accept a value when type is: 'jena' Examples Example 1: Ensure a service with no arguments. If not found it's type defaults to virtuoso . await someDataset.ensureService('someServiceName') Example 2: Ensure a service of type jena . await someDataset.ensureService('someServiceName', { type: 'jena' }) Dataset.addService(name: string, metadata?: object) \u00b6 Creates a new service for this dataset. Arguments \u00b6 Required \u00b6 name The URL-friendly name of the service. The name must only contain alphanumeric characters and hyphens (`[A-Za-z0-9\\-]`). Optional \u00b6 The service type is specified with the type parameter. If no type is given, a default of 'virtuoso' is used. It supports the following values: 'virtuoso' Starts a SPARQL service. A SPARQL 1.1 compliant service is very scalable and performance, but without advanced reasoning capabilities. 'jena' Starts a SPARQL JENA service. A SPARQL 1.1 compliant service that is less scalable and less performant, but allows reasoning (RDFS or OWL) to be enabled. 'elasticSearch' Starts an Elasticsearch service. A text search engine that can be used to power a search bar or similar textual search API. The name argument can be used to distinguish between different endpoints over the same dataset that are used for different tasks. Examples \u00b6 The following snippet starts two SPARQL endpoints over a specific dataset. One endpoint will be used in the acceptance environment while the other endpoint will be used in the production system. const account = await triply.getAccount() const dataset = await account.getDataset('my-dataset') const acceptance = await dataset.addService('acceptance') const production = await dataset.addService('production', { type: 'elasticsearch', }) const reasoning = await dataset.addService('reasoning', { type: 'jena', config: { reasoner: 'OWL' }, }) See also \u00b6 See class Service for an overview of the methods that can be used with service objects. Dataset.clear(...resourceType: string) \u00b6 Removes one or more resource types from the current dataset. Arguments \u00b6 The resources are specified by the rest parameter resourceType , which supports the following values : 'assets' Removes all assets in the dataset. 'graphs' Removes all graphs in the dataset. 'services' Removes all services in the dataset. Examples \u00b6 The following example code removes all graphs and services for a specific dataset: const account = await triply.getAccount() const dataset = await account.getDataset('my-dataset') await dataset.clear('graphs', 'services') Dataset.copy(account: string, dataset: string) \u00b6 Creates a copy of the current dataset. The owner (user or organization) of the copy is specified with parameter account . The name of the copy is specified with parameter dataset . This operation does not overwrite existing datasets: if the copied-to dataset already exists, a new dataset with suffix -1 will be created. Examples \u00b6 const account = await triply.getAccount() const dataset = await account.getDataset('my-dataset') console.log(await dataset.copy('account name', 'copy dataset name')) Dataset.delete() \u00b6 Deletes the dataset. This includes deleting the dataset metadata, all of its graphs, all of its services, and all of its assets. Examples \u00b6 The following snippet deletes a specific dataset that is part of the account associated with the current API token: const account = await triply.getAccount() const dataset = await account.getDataset('my-dataset') await dataset.delete() See also \u00b6 Sometimes it is more useful to only delete the graphs that belong to a dataset, but leave the dataset metadata, services, and assets in place. The following methods can be used for this purpose: Dataset.deleteGraph(graphName: string) Dataset.removeAllGraphs() Dataset.deleteGraph(name: string) \u00b6 Deletes the graph with the given name from this dataset. Graph names are IRIs. Examples \u00b6 The following snippet deletes a specific graph from a specified dataset: const account = await triply.getAccount() const dataset = await account.getDataset('my-dataset') await dataset.deleteGraph('https://example.org/some-graph') Dataset.describe(iri: string|NamedNode) \u00b6 Each dataset is a collection of triples that describe objects in linked data. Each object is defined with an IRI, an identifier for that object. An object often has incoming and outgoing connections. The Dataset.describe() call can retrieve the incoming and outgoing triples per object. The function returns for a given iri a list of quads where the iri is either in the subject or the object position. Examples \u00b6 The following snippet returns all triples that have https://example.org/id/some-instance in the subject or the object position: const user = await triply.getUser() const dataset = await account.getDataset('my-dataset') console.log(await dataset.describe('https://example.org/id/some-instance')) Dataset.getAsset(name: string, version?: number) \u00b6 Returns the asset with the given name for this dataset. Optionally allows the version number ( version ) of the asset to be specified. If the version number is absent, the latest version of the assert with the given name is returned. Examples \u00b6 The following snippet returns the original version of an image of a dog from the animals dataset: const user = await triply.getUser() const dataset = user.getDataset('my-dataset') const asset = await dataset.getAsset('file.png', 1) Dataset.getAssets() \u00b6 Returns an async iterator over the assets that belong to this dataset. Assets are binary files that are stored together with data graphs. Common examples include documents, images and videos. Examples \u00b6 The following snippet prints the assets for a specific dataset: const user = await triply.getUser() const dataset = user.getDataset('my-dataset') for await (const asset of dataset.getAssets()) { console.log(asset) } The following snippet prints the list of assets for a specific dataset: const account = await triply.getAccount() const dataset = await account.getDataset('my-dataset') console.log(await dataset.getAssets().toArray()) Dataset.getGraph(name: string) \u00b6 Each dataset with data consists out of one or more named graphs. All graphs together are thus the collection of triples of the dataset. Often the graph is used to denote a part of the dataset. For example the data model of the dataset or the metadata of the dataset. Instead of searching over the complete dataset where you want to scope it to a certain graph you can use the getGraph() function to specify the graph. Dataset.getGraph(name: string) returns the graph with the given name that belongs to this dataset. The name is the string representation of the graph IRI. The Dataset.getGraph returns a graph object. Examples \u00b6 The following snippet returns the graph about cats from the dataset about animals: const user = await triply.getUser() const dataset = await user.getDataset('animals') const graph = dataset.getGraph('https://example.com/cats') Dataset.getGraphs() \u00b6 Returns an async iterator over graphs that belong to this dataset. Examples \u00b6 The following snippet retrieves the graphs for a specific dataset: const account = await triply.getAccount() const dataset = await account.getDataset('my-dataset') console.log(await dataset.getGraphs().toArray()) Dataset.getInfo() \u00b6 Returns information about this dataset. Information is returned in a dictionary object. Individual keys can be accessed for specific information values. Examples \u00b6 The following snippet prints the information from the specified dataset of the current user: const user = await triply.getUser() const dataset = await user.getDataset('my-dataset') console.log(await dataset.getInfo()) Dataset.getPrefixes() \u00b6 Returns the prefixes that are defined for this dataset. This contains prefix declarations that are generic and configured for this TriplyDB server, and prefix declarations that are defined for this specific dataset. Examples \u00b6 The following snippet prints the prefix declarations that hold for my-dataset : const user = await triply.getUser() const dataset = user.getDataset('my-dataset') for await (const prefix of dataset.getPrefixes()) { console.log(prefix) } Dataset.getService(name: string) \u00b6 Returns the service with the given name for this dataset. Examples \u00b6 The following snippet retrieves the acceptance service for the product catalog of an imaginary company: const organization = triply.getOrganization('some-company') const dataset = organization.getDataset('product-catalog') const service = dataset.getService('acceptance') Dataset.getServices() \u00b6 Returns an async iterator over TriplyDB services under a dataset. See class Service for an overview of the methods for service objects. Examples \u00b6 The following snippet emits the services that are enabled for a specific dataset: const account = await triply.getAccount() const dataset = await account.getDataset('my-dataset') for await (const service of dataset.getServices()) { console.log(service) } If you do not want to iterate over the services with an async iterator, but instead want to get an array of services use the .toArray() call instead: const account = await triply.getAccount() const dataset = await account.getDataset('my-dataset') console.log(await dataset.getServices().toArray()) Dataset.getStatements({subject?: string, predicate?: string, object?: string, graph?: string}) \u00b6 Returns an async iterator with statements (quadruples) that fit the specified pattern. Arguments \u00b6 subject , if specified, is the subject term that should be matched. predicate , if specified, is the predicate term that should be matched. object , if specified, is the object term that should be matched. graph , if specified, is the graph name that should be matched. Example \u00b6 The following prints all statements in the dataset: const user = triply.getUser() const dataset = await user.getDataset('my-dataset') for await (const statement of dataset.getStatements()) { console.log(statement) } The following prints the description of the Amsterdam resource in the DBpedia dataset: const association = triply.getOrganization('DBpedia-association') const dbpedia = association.getDataset('dbpedia') for await (const statement of dbpedia.getStatements({subject: 'http://dbpedia.org/resource/Amsterdam'})) { console.log(statement) } Get the data locally \u00b6 Most of the time you do not need to download the entire dataset locally as TriplyDB supports a variety of methods to use linked data directly. But if you want to use the entire graph locally that is possible with TriplyDB.js . There are three methods to retrieve linked data from TriplyDB. graphsToFile() , graphsToStore() and graphsToStream() . Dataset.graphsToFile(destinationPath: string, arguments?: object) \u00b6 The first method downloads the linked data graphs directly and writes the data to the location of the destinationPath . The extension on the destinationPath defines the linked data type that is downloaded. The extensions that are supported are: nt , nq , trig , ttl , jsonld , json . If no extension is set or the extension is not recognized the function will throw an error. Optional \u00b6 The optional properties accepted as arguments for graphsToFile Compressed Argument compressed optionally is a boolean defining if a graph is compressed with GNU zip (gzip) compression algorithm and will end with a `.gz` extension. Graph Argument Graph optionally is an specific graph that you want to write to file. These graph is an instance of a 'Graph' class Examples \u00b6 The following example downloads the dataset to file: const triply = App.get({ token: process.env.TOKEN }) const user = await triply.getAccount() const dataset = await user.getDataset('pokemon') await dataset.graphsToFile('my-filename.ttl', {compressed: true}) Dataset.graphsToStore(graph?: Graph) \u00b6 The second method is to download the file into a N3.store . The n3 library is one of the most complete libraries for handling linked data in memory. The N3.js library is an implementation of the RDF.js low-level specification that lets you handle RDF in JavaScript easily, with an asynchronous, streaming approach. To reduce the overhead of downloading your data to file and then insert it in the N3 Store. TriplyDB.js has a graphsToStore() where a N3 store is returned as a result of the graphsToStore() function. Optional \u00b6 The optional argument for graphsToStore is Graph . With Graph you can optionally define a specific graph that you want to write to file. These graph is an instance of a 'Graph' class. Examples \u00b6 The following example downloads the dataset as N3.store : const triply = App.get({ token: process.env.TOKEN }) const user = await triply.getAccount() const dataset = await user.getDataset('pokemon') const store = await dataset.graphsToStore() Dataset.graphsToStream(type: 'compressed' | 'rdf-js', arguments?: object) \u00b6 The final method to download linked data to a local source is the graphsToStream this function returns a stream of quads that can directly be iterated over. The Stream is either of the type compressed which returns a gzipped stream of linked data, or type rdf-js which returns a stream of quads parsed according to the rdf-js standard . Optional \u00b6 The following arguments can be defined in the optional arguments object. Extension Argument Extension optionally defines the linked data type that is streamed. The extensions that are supported are: `nt`, `nq`, `trig`, `ttl`, `jsonld`, `json`. Graph Argument Graph optionally is an specific graph that you want to write to file. This graph is an instance of a 'Graph' class Examples \u00b6 The following example streams through the dataset as rdf-js quad objects and prints the quad to the screen. Notice that the stream is an async iterator. Example 1 const triply = App.get({ token: process.env.TOKEN }) const user = await triply.getAccount() const dataset = await user.getDataset('pokemon') const stream = await dataset.graphsToStream('rdf-js', {extension: '.nq'}) for await(const quad of stream){ console.log(quad) } The following example streams through the dataset as chunks of ttl. and prints the buffer to the screen. Example 2 const triply = App.get({ token: process.env.TOKEN }) const user = await triply.getAccount() const dataset = await user.getDataset('pokemon') const stream = await dataset.graphsToStream('compressed', {extension: '.ttl'}) for await(const quad of stream.pipe(zlib.createGunzip())){ console.log((quad as Buffer).toString()) } Dataset.importFromDataset(fromDataset: Dataset, arguments?: object) \u00b6 Imports one or more named graphs from a different dataset into this dataset. Data reuse is an important principle in linked data. This functionality makes it very easy to pull in vocabularies and datasets from other places. Changes in the fromDataset dataset are not automatically reflected in this dataset. If you want to synchronize with changes made in the imported-from dataset, the graphs must be explicitly imported. This protects this dataset against unanticipated changes in the imported-from dataset, while still being able to stay in sync with the imported-from dataset if this is explicitly requested. Required \u00b6 Argument fromDataset is the dataset object from which one or more graphs are imported over to this dataset. Optional \u00b6 The optional properties accepted as arguments for importFromDataset graphMap Argument ` graphMap ` optionally is an object with keys and values that implements a mapping from existing graph names (keys) to newly created graph names (values). Each key must be an existing graph name in the `from` dataset. Each value must be the corresponding graph name in this dataset. If this argument is not specified, then graph names in the `from` dataset are identical to graph names in this dataset. Note that either graphNames or graphMap can be given as optional argument and not both. graphNames Argument ` graphNames ` optionally is an array of graph names. These names can be one of three types: 'string', instances of a 'Graph' class, or instances of 'NamedNodes'. Note that either graphNames or graphMap can be given as optional argument and not both. overwrite Accepts a Boolean value. An optional property that determines whether existing graph names in this dataset are allowed to be silently overwritten. If this argument is not specified, then `false` is used as the default value. Examples \u00b6 The following snippet creates a new dataset ( newDataset ) and imports one graph from an existing dataset ( existingDataset ). Notice that the graph can be renamed as part of the import. Example 1 Imports the complete 'existingDataset' dataset to the 'newDataset' . const account = await triply.getAccount() const existingDataset = await account.getDataset('existingDataset') const newDataset = await account.addDataset('newDataset') await newDataset.importFromDataset(existingDataset) Example 2 Imports 'anotherDataset' dataset to a 'newDataset' Where a graph from the existing dataset is renamed to the a graphname in the new dataset. Only the graphs from the graphMap are imported. const account = await triply.getAccount() const anotherDataset = await account.getDataset('anotherDataset') const newDataset = await account.addDataset('newDataset') await newDataset.importFromDataset(anotherDataset, { graphMap: { 'https://example.org/existingDataset/graph': 'https://example.org/newDataset/graph'} }) Example 3 Import 'oneMoreDataset' dataset to the 'newDataset' Where a graph specific graph from the existing dataset is added to the new dataset. If the graph name already occurs in the 'newDataset' it will get overwritten. const account = await triply.getAccount() const oneMoreDataset = await account.getDataset('oneMoreDataset') const newDataset = await account.addDataset('newDataset') await newDataset.importFromDataset(oneMoreDataset, { graphNames: ['https://example.org/existingDataset/graph'], overwrite: true, }) Dataset.importFromFiles(files: list(string || File), defaultsConfig?: object) \u00b6 Required \u00b6 Imports one or more files into this dataset. The files must contain RDF data. Optional: defaultsConfig: object defaultGraphName Accepts a string value that is set as the default graph name for each imported file baseIRI Accepts a string value that is set as the default baseIRI for each imported file overwriteAll Accepts a boolean value that overwrites previously added graph names or baseIRIs (regardless of whether they came from a URL or a file) mergeGraphs Accepts a Boolean value. An optional property that determines whether existing graph in this dataset are merged with the imported graphs. If this argument is not specified, then `false` is used as the default value. Supported files \u00b6 The files must contain RDF data and must be encoded in one of the following standardized RDF serialization formats: N-Quads, N-Triples, TriG, Turtle. Examples \u00b6 Example 1 const account = await triply.getAccount('Triply') const dataset = await account.getDataset(iris) await dataset.importFromFiles('test.nt') await dataset.importFromFiles(['file.nq', 'file.tar.gz']) Example 2 const account = await triply.getAccount('Triply') const dataset = await account.getDataset(iris) await dataset.importFromFiles('test.nt') await dataset.importFromFiles(['file.nq', 'file.tar.gz'], { defaultGraphName: 'https://triplydb.com/Triply/example/graph/default', overwriteAll: true, }) Dataset.importFromStore(store: n3.Store, defaultsConfig?: object) \u00b6 One of the most complete libraries for handling linked data in memory is the n3 library . The N3.js library is an implementation of the RDF.js low-level specification that lets you handle RDF in JavaScript easily, with an asynchronous, streaming approach. To reduce the overhead of converting your data from the N3 Store to a file and uploading to TriplyDB. TriplyDB.js has a importFromStore() where a N3 store is given as first argument and uploaded directly to triplyDB. Examples \u00b6 const store = new Store() store.addQuad(DataFactory.namedNode('https://triplydb.com/id/me'),DataFactory.namedNode('http://www.w3.org/2000/01/rdf-schema#label'),DataFactory.literal('me'),DataFactory.namedNode('https://triplydb.com/Triply/example/graph/default')) const triply = App.get({ token: process.env.TOKEN }) const user = await triply.getUser() const dataset = (await user.getDatasets().toArray())[0] dataset.importFromStore(store) Dataset.importFromUrls(urls: list(string), defaultsConfig?: object) \u00b6 Required \u00b6 Imports one or more URLs into this dataset. The URLs must provide access to RDF data. Optional: defaultsConfig: object defaultGraphName Accepts a string value that is set as the default graph name for each imported URL baseIRI Accepts a string value that is set as the default baseIRI for each imported URL overwriteAll Accepts a boolean value that overwrites previously added graph names or baseIRIs (regardless of whether they came from a URL or a file) Examples \u00b6 dataset1.importFromUrls(['url1', 'url2', 'url3']) Dataset.removeAllGraphs() \u00b6 Removes all graphs from this dataset. Examples \u00b6 The following snippet removed all graphs from a specific dataset: const user = await triply.getUser() const dataset = await user.getDataset('my-dataset') await dataset.removeAllGraphs() Dataset.removePrefixes(prefixes: string[]) \u00b6 Removes IRI prefixes from this dataset. The prefixes argument is a string array, containing the prefix labels to be removed. Examples \u00b6 The following snippet removes the def and id prefixes from the specified dataset. const account = await triply.getAccount() const dataset = await account.getDataset('my-dataset') await dataset.removePrefixes(['def', 'id']) Dataset.renameGraph(from: string, to: string) \u00b6 Renames a graph of this dataset, where from is the current graph name and to is the new graph name. The string arguments for from and to must be valid IRIs. Examples \u00b6 The following snippet renames a specific graph of a specific dataset: const account = await triply.getAccount() const dataset = await account.getDataset('my-dataset') await dataset.renameGraph( 'https://example.org/old-graph', 'https://example.org/new-graph' ) Dataset.update(metadata: object) \u00b6 Updates the metadata for this dataset. Arguments \u00b6 The metadata argument takes a dictionary object with the following optional keys: Required: accessLevel The access level of the dataset. The following values are supported: 'private' The dataset can only be accessed by the Account object for which it is created. 'internal' The dataset can only be accessed by people who are logged into the TriplyDB server. 'public' The dataset can be accessed by everybody. Optional: description The description of the dataset. This description can make use of Markdown. displayName The human-readable name of the dataset. This name may contain spaces and other characters that are not allowed in the URL-friendly name. license The license of the dataset. The following license strings are currently supported: 'CC-BY-SA' 'CC0 1.0' 'GFDL' 'ODC-By' 'ODC-ODbL' 'PDDL' Example \u00b6 The following snippet updates the dataset's access level, description, display name and license: const account = await triply.getAccount() const dataset = await account.getDataset('my-dataset') dataset.update({ accessLevel: 'private', description: 'desc', displayName: 'disp', license: 'PDDL', }) Dataset.uploadAsset( filePath: string, opts?: {mode?: 'throw-if-exists'| 'replace-if-exists'| 'append-version', name?: string}) \u00b6 Uploads a file that does not contain RDF data as an asset. You can specify the name on the asset and what to do if the asset already exists (throws an error by default). User cases \u00b6 There are several use cases for assets: Source data that will be used as input files to an ETL process. Documentation files that describe the dataset. Media files (audio/image/video) that are described in the RDF graph. Examples \u00b6 The following snippet uploads a source CSV data file and a PDF documentation file: const account = await triply.getAccount() const dataset = await account.getDataset('my-dataset') await dataset.uploadAsset('my-source-data', {name: 'source.csv.gz'}) await dataset.uploadAsset('my-documentation', {name: 'documentation.pdf'}) await dataset.uploadAsset('my-documentation', {mode:'append-version', name: 'documentation.pdf'})","title":"Dataset"},{"location":"triplydb-js/dataset/#dataset","text":"The Dataset class represents a TriplyDB dataset.","title":"Dataset"},{"location":"triplydb-js/dataset/#datasetaddprefixesprefixes-object","text":"Adds IRI prefix declarations to the dataset. The prefixes argument is a dictionary object whose keys are aliases and whose values are IRI prefixes.","title":"Dataset.addPrefixes(prefixes: object)"},{"location":"triplydb-js/dataset/#examples","text":"The following snippet adds prefix declarations for aliases id and def to the Iris dataset: const organization = await triply.getOrganization('Triply') const dataset = await organization.getDataset(iris) await dataset.addPrefixes({ def: 'https://triplydb.com/Triply/iris/def/', id: 'https://triplydb.com/Triply/iris/id/', })","title":"Examples"},{"location":"triplydb-js/dataset/#datasetensureservicename-string-metadata-object","text":"Ensures the existence of a service with the given name and with the specified metadata if given. Calling this method ensures that the necessary changes (if any) are made in the connected-to TriplyDB server that result in an end state in which a service with the given name and metadata exists. This method is useful in practice, because it removes the burden on the programmer to have to write custom code for checking for the existence of a service, and conditionally create a new service or make metadata changes to an existing service. The changes made as a result of calling this method depend on the current state of the connected-to TriplyDB server: If this dataset does not yet have a service with the given name , then the behavior is identical to calling Dataset.addService(name: string, metadata?: object) with the same arguments. If this dataset already has a service with the given name , but with different metadata specified for it, then the behavior is identical to calling Account.getDataset(name: string) and Dataset.update(metadata: object) . If this dataset already has a service with the given name and with the same metadata , then this method returns that service.","title":"Dataset.ensureService(name: string, metadata?: object)"},{"location":"triplydb-js/dataset/#required","text":"name Accepts a string value which is the name of the service to ensure.","title":"Required"},{"location":"triplydb-js/dataset/#optional-metadata","text":"serviceMetadata = { type: 'elasticsearch' | 'virtuoso' | 'jena' ; config?: { reasoner?: 'OWL' | 'RDFS' | 'None'; }; }; type Accepts a string value of one of the following: 'virtuoso' , 'elasticsearch' , 'jena' . config Config is an optional property. It accepts an object with a reasoner property. reasoner The reasoner property accepts a string value of either 'OWL' , 'RDFS' , or 'None' . Note: If no options are specified the default service is of type: virtuoso . Note that the config.reasoner will only accept a value when type is: 'jena' Examples Example 1: Ensure a service with no arguments. If not found it's type defaults to virtuoso . await someDataset.ensureService('someServiceName') Example 2: Ensure a service of type jena . await someDataset.ensureService('someServiceName', { type: 'jena' })","title":"Optional: metadata"},{"location":"triplydb-js/dataset/#datasetaddservicename-string-metadata-object","text":"Creates a new service for this dataset.","title":"Dataset.addService(name: string, metadata?: object)"},{"location":"triplydb-js/dataset/#arguments","text":"","title":"Arguments"},{"location":"triplydb-js/dataset/#required_1","text":"name The URL-friendly name of the service. The name must only contain alphanumeric characters and hyphens (`[A-Za-z0-9\\-]`).","title":"Required"},{"location":"triplydb-js/dataset/#optional","text":"The service type is specified with the type parameter. If no type is given, a default of 'virtuoso' is used. It supports the following values: 'virtuoso' Starts a SPARQL service. A SPARQL 1.1 compliant service is very scalable and performance, but without advanced reasoning capabilities. 'jena' Starts a SPARQL JENA service. A SPARQL 1.1 compliant service that is less scalable and less performant, but allows reasoning (RDFS or OWL) to be enabled. 'elasticSearch' Starts an Elasticsearch service. A text search engine that can be used to power a search bar or similar textual search API. The name argument can be used to distinguish between different endpoints over the same dataset that are used for different tasks.","title":"Optional"},{"location":"triplydb-js/dataset/#examples_1","text":"The following snippet starts two SPARQL endpoints over a specific dataset. One endpoint will be used in the acceptance environment while the other endpoint will be used in the production system. const account = await triply.getAccount() const dataset = await account.getDataset('my-dataset') const acceptance = await dataset.addService('acceptance') const production = await dataset.addService('production', { type: 'elasticsearch', }) const reasoning = await dataset.addService('reasoning', { type: 'jena', config: { reasoner: 'OWL' }, })","title":"Examples"},{"location":"triplydb-js/dataset/#see-also","text":"See class Service for an overview of the methods that can be used with service objects.","title":"See also"},{"location":"triplydb-js/dataset/#datasetclearresourcetype-string","text":"Removes one or more resource types from the current dataset.","title":"Dataset.clear(...resourceType: string)"},{"location":"triplydb-js/dataset/#arguments_1","text":"The resources are specified by the rest parameter resourceType , which supports the following values : 'assets' Removes all assets in the dataset. 'graphs' Removes all graphs in the dataset. 'services' Removes all services in the dataset.","title":"Arguments"},{"location":"triplydb-js/dataset/#examples_2","text":"The following example code removes all graphs and services for a specific dataset: const account = await triply.getAccount() const dataset = await account.getDataset('my-dataset') await dataset.clear('graphs', 'services')","title":"Examples"},{"location":"triplydb-js/dataset/#datasetcopyaccount-string-dataset-string","text":"Creates a copy of the current dataset. The owner (user or organization) of the copy is specified with parameter account . The name of the copy is specified with parameter dataset . This operation does not overwrite existing datasets: if the copied-to dataset already exists, a new dataset with suffix -1 will be created.","title":"Dataset.copy(account: string, dataset: string)"},{"location":"triplydb-js/dataset/#examples_3","text":"const account = await triply.getAccount() const dataset = await account.getDataset('my-dataset') console.log(await dataset.copy('account name', 'copy dataset name'))","title":"Examples"},{"location":"triplydb-js/dataset/#datasetdelete","text":"Deletes the dataset. This includes deleting the dataset metadata, all of its graphs, all of its services, and all of its assets.","title":"Dataset.delete()"},{"location":"triplydb-js/dataset/#examples_4","text":"The following snippet deletes a specific dataset that is part of the account associated with the current API token: const account = await triply.getAccount() const dataset = await account.getDataset('my-dataset') await dataset.delete()","title":"Examples"},{"location":"triplydb-js/dataset/#see-also_1","text":"Sometimes it is more useful to only delete the graphs that belong to a dataset, but leave the dataset metadata, services, and assets in place. The following methods can be used for this purpose: Dataset.deleteGraph(graphName: string) Dataset.removeAllGraphs()","title":"See also"},{"location":"triplydb-js/dataset/#datasetdeletegraphname-string","text":"Deletes the graph with the given name from this dataset. Graph names are IRIs.","title":"Dataset.deleteGraph(name: string)"},{"location":"triplydb-js/dataset/#examples_5","text":"The following snippet deletes a specific graph from a specified dataset: const account = await triply.getAccount() const dataset = await account.getDataset('my-dataset') await dataset.deleteGraph('https://example.org/some-graph')","title":"Examples"},{"location":"triplydb-js/dataset/#datasetdescribeiri-stringnamednode","text":"Each dataset is a collection of triples that describe objects in linked data. Each object is defined with an IRI, an identifier for that object. An object often has incoming and outgoing connections. The Dataset.describe() call can retrieve the incoming and outgoing triples per object. The function returns for a given iri a list of quads where the iri is either in the subject or the object position.","title":"Dataset.describe(iri: string|NamedNode)"},{"location":"triplydb-js/dataset/#examples_6","text":"The following snippet returns all triples that have https://example.org/id/some-instance in the subject or the object position: const user = await triply.getUser() const dataset = await account.getDataset('my-dataset') console.log(await dataset.describe('https://example.org/id/some-instance'))","title":"Examples"},{"location":"triplydb-js/dataset/#datasetgetassetname-string-version-number","text":"Returns the asset with the given name for this dataset. Optionally allows the version number ( version ) of the asset to be specified. If the version number is absent, the latest version of the assert with the given name is returned.","title":"Dataset.getAsset(name: string, version?: number)"},{"location":"triplydb-js/dataset/#examples_7","text":"The following snippet returns the original version of an image of a dog from the animals dataset: const user = await triply.getUser() const dataset = user.getDataset('my-dataset') const asset = await dataset.getAsset('file.png', 1)","title":"Examples"},{"location":"triplydb-js/dataset/#datasetgetassets","text":"Returns an async iterator over the assets that belong to this dataset. Assets are binary files that are stored together with data graphs. Common examples include documents, images and videos.","title":"Dataset.getAssets()"},{"location":"triplydb-js/dataset/#examples_8","text":"The following snippet prints the assets for a specific dataset: const user = await triply.getUser() const dataset = user.getDataset('my-dataset') for await (const asset of dataset.getAssets()) { console.log(asset) } The following snippet prints the list of assets for a specific dataset: const account = await triply.getAccount() const dataset = await account.getDataset('my-dataset') console.log(await dataset.getAssets().toArray())","title":"Examples"},{"location":"triplydb-js/dataset/#datasetgetgraphname-string","text":"Each dataset with data consists out of one or more named graphs. All graphs together are thus the collection of triples of the dataset. Often the graph is used to denote a part of the dataset. For example the data model of the dataset or the metadata of the dataset. Instead of searching over the complete dataset where you want to scope it to a certain graph you can use the getGraph() function to specify the graph. Dataset.getGraph(name: string) returns the graph with the given name that belongs to this dataset. The name is the string representation of the graph IRI. The Dataset.getGraph returns a graph object.","title":"Dataset.getGraph(name: string)"},{"location":"triplydb-js/dataset/#examples_9","text":"The following snippet returns the graph about cats from the dataset about animals: const user = await triply.getUser() const dataset = await user.getDataset('animals') const graph = dataset.getGraph('https://example.com/cats')","title":"Examples"},{"location":"triplydb-js/dataset/#datasetgetgraphs","text":"Returns an async iterator over graphs that belong to this dataset.","title":"Dataset.getGraphs()"},{"location":"triplydb-js/dataset/#examples_10","text":"The following snippet retrieves the graphs for a specific dataset: const account = await triply.getAccount() const dataset = await account.getDataset('my-dataset') console.log(await dataset.getGraphs().toArray())","title":"Examples"},{"location":"triplydb-js/dataset/#datasetgetinfo","text":"Returns information about this dataset. Information is returned in a dictionary object. Individual keys can be accessed for specific information values.","title":"Dataset.getInfo()"},{"location":"triplydb-js/dataset/#examples_11","text":"The following snippet prints the information from the specified dataset of the current user: const user = await triply.getUser() const dataset = await user.getDataset('my-dataset') console.log(await dataset.getInfo())","title":"Examples"},{"location":"triplydb-js/dataset/#datasetgetprefixes","text":"Returns the prefixes that are defined for this dataset. This contains prefix declarations that are generic and configured for this TriplyDB server, and prefix declarations that are defined for this specific dataset.","title":"Dataset.getPrefixes()"},{"location":"triplydb-js/dataset/#examples_12","text":"The following snippet prints the prefix declarations that hold for my-dataset : const user = await triply.getUser() const dataset = user.getDataset('my-dataset') for await (const prefix of dataset.getPrefixes()) { console.log(prefix) }","title":"Examples"},{"location":"triplydb-js/dataset/#datasetgetservicename-string","text":"Returns the service with the given name for this dataset.","title":"Dataset.getService(name: string)"},{"location":"triplydb-js/dataset/#examples_13","text":"The following snippet retrieves the acceptance service for the product catalog of an imaginary company: const organization = triply.getOrganization('some-company') const dataset = organization.getDataset('product-catalog') const service = dataset.getService('acceptance')","title":"Examples"},{"location":"triplydb-js/dataset/#datasetgetservices","text":"Returns an async iterator over TriplyDB services under a dataset. See class Service for an overview of the methods for service objects.","title":"Dataset.getServices()"},{"location":"triplydb-js/dataset/#examples_14","text":"The following snippet emits the services that are enabled for a specific dataset: const account = await triply.getAccount() const dataset = await account.getDataset('my-dataset') for await (const service of dataset.getServices()) { console.log(service) } If you do not want to iterate over the services with an async iterator, but instead want to get an array of services use the .toArray() call instead: const account = await triply.getAccount() const dataset = await account.getDataset('my-dataset') console.log(await dataset.getServices().toArray())","title":"Examples"},{"location":"triplydb-js/dataset/#datasetgetstatementssubject-string-predicate-string-object-string-graph-string","text":"Returns an async iterator with statements (quadruples) that fit the specified pattern.","title":"Dataset.getStatements({subject?: string, predicate?: string, object?: string, graph?: string})"},{"location":"triplydb-js/dataset/#arguments_2","text":"subject , if specified, is the subject term that should be matched. predicate , if specified, is the predicate term that should be matched. object , if specified, is the object term that should be matched. graph , if specified, is the graph name that should be matched.","title":"Arguments"},{"location":"triplydb-js/dataset/#example","text":"The following prints all statements in the dataset: const user = triply.getUser() const dataset = await user.getDataset('my-dataset') for await (const statement of dataset.getStatements()) { console.log(statement) } The following prints the description of the Amsterdam resource in the DBpedia dataset: const association = triply.getOrganization('DBpedia-association') const dbpedia = association.getDataset('dbpedia') for await (const statement of dbpedia.getStatements({subject: 'http://dbpedia.org/resource/Amsterdam'})) { console.log(statement) }","title":"Example"},{"location":"triplydb-js/dataset/#get-the-data-locally","text":"Most of the time you do not need to download the entire dataset locally as TriplyDB supports a variety of methods to use linked data directly. But if you want to use the entire graph locally that is possible with TriplyDB.js . There are three methods to retrieve linked data from TriplyDB. graphsToFile() , graphsToStore() and graphsToStream() .","title":"Get the data locally"},{"location":"triplydb-js/dataset/#datasetgraphstofiledestinationpath-string-arguments-object","text":"The first method downloads the linked data graphs directly and writes the data to the location of the destinationPath . The extension on the destinationPath defines the linked data type that is downloaded. The extensions that are supported are: nt , nq , trig , ttl , jsonld , json . If no extension is set or the extension is not recognized the function will throw an error.","title":"Dataset.graphsToFile(destinationPath: string, arguments?: object)"},{"location":"triplydb-js/dataset/#optional_1","text":"The optional properties accepted as arguments for graphsToFile Compressed Argument compressed optionally is a boolean defining if a graph is compressed with GNU zip (gzip) compression algorithm and will end with a `.gz` extension. Graph Argument Graph optionally is an specific graph that you want to write to file. These graph is an instance of a 'Graph' class","title":"Optional"},{"location":"triplydb-js/dataset/#examples_15","text":"The following example downloads the dataset to file: const triply = App.get({ token: process.env.TOKEN }) const user = await triply.getAccount() const dataset = await user.getDataset('pokemon') await dataset.graphsToFile('my-filename.ttl', {compressed: true})","title":"Examples"},{"location":"triplydb-js/dataset/#datasetgraphstostoregraph-graph","text":"The second method is to download the file into a N3.store . The n3 library is one of the most complete libraries for handling linked data in memory. The N3.js library is an implementation of the RDF.js low-level specification that lets you handle RDF in JavaScript easily, with an asynchronous, streaming approach. To reduce the overhead of downloading your data to file and then insert it in the N3 Store. TriplyDB.js has a graphsToStore() where a N3 store is returned as a result of the graphsToStore() function.","title":"Dataset.graphsToStore(graph?: Graph)"},{"location":"triplydb-js/dataset/#optional_2","text":"The optional argument for graphsToStore is Graph . With Graph you can optionally define a specific graph that you want to write to file. These graph is an instance of a 'Graph' class.","title":"Optional"},{"location":"triplydb-js/dataset/#examples_16","text":"The following example downloads the dataset as N3.store : const triply = App.get({ token: process.env.TOKEN }) const user = await triply.getAccount() const dataset = await user.getDataset('pokemon') const store = await dataset.graphsToStore()","title":"Examples"},{"location":"triplydb-js/dataset/#datasetgraphstostreamtype-compressed-rdf-js-arguments-object","text":"The final method to download linked data to a local source is the graphsToStream this function returns a stream of quads that can directly be iterated over. The Stream is either of the type compressed which returns a gzipped stream of linked data, or type rdf-js which returns a stream of quads parsed according to the rdf-js standard .","title":"Dataset.graphsToStream(type: 'compressed' | 'rdf-js', arguments?: object)"},{"location":"triplydb-js/dataset/#optional_3","text":"The following arguments can be defined in the optional arguments object. Extension Argument Extension optionally defines the linked data type that is streamed. The extensions that are supported are: `nt`, `nq`, `trig`, `ttl`, `jsonld`, `json`. Graph Argument Graph optionally is an specific graph that you want to write to file. This graph is an instance of a 'Graph' class","title":"Optional"},{"location":"triplydb-js/dataset/#examples_17","text":"The following example streams through the dataset as rdf-js quad objects and prints the quad to the screen. Notice that the stream is an async iterator. Example 1 const triply = App.get({ token: process.env.TOKEN }) const user = await triply.getAccount() const dataset = await user.getDataset('pokemon') const stream = await dataset.graphsToStream('rdf-js', {extension: '.nq'}) for await(const quad of stream){ console.log(quad) } The following example streams through the dataset as chunks of ttl. and prints the buffer to the screen. Example 2 const triply = App.get({ token: process.env.TOKEN }) const user = await triply.getAccount() const dataset = await user.getDataset('pokemon') const stream = await dataset.graphsToStream('compressed', {extension: '.ttl'}) for await(const quad of stream.pipe(zlib.createGunzip())){ console.log((quad as Buffer).toString()) }","title":"Examples"},{"location":"triplydb-js/dataset/#datasetimportfromdatasetfromdataset-dataset-arguments-object","text":"Imports one or more named graphs from a different dataset into this dataset. Data reuse is an important principle in linked data. This functionality makes it very easy to pull in vocabularies and datasets from other places. Changes in the fromDataset dataset are not automatically reflected in this dataset. If you want to synchronize with changes made in the imported-from dataset, the graphs must be explicitly imported. This protects this dataset against unanticipated changes in the imported-from dataset, while still being able to stay in sync with the imported-from dataset if this is explicitly requested.","title":"Dataset.importFromDataset(fromDataset: Dataset, arguments?: object)"},{"location":"triplydb-js/dataset/#required_2","text":"Argument fromDataset is the dataset object from which one or more graphs are imported over to this dataset.","title":"Required"},{"location":"triplydb-js/dataset/#optional_4","text":"The optional properties accepted as arguments for importFromDataset graphMap Argument ` graphMap ` optionally is an object with keys and values that implements a mapping from existing graph names (keys) to newly created graph names (values). Each key must be an existing graph name in the `from` dataset. Each value must be the corresponding graph name in this dataset. If this argument is not specified, then graph names in the `from` dataset are identical to graph names in this dataset. Note that either graphNames or graphMap can be given as optional argument and not both. graphNames Argument ` graphNames ` optionally is an array of graph names. These names can be one of three types: 'string', instances of a 'Graph' class, or instances of 'NamedNodes'. Note that either graphNames or graphMap can be given as optional argument and not both. overwrite Accepts a Boolean value. An optional property that determines whether existing graph names in this dataset are allowed to be silently overwritten. If this argument is not specified, then `false` is used as the default value.","title":"Optional"},{"location":"triplydb-js/dataset/#examples_18","text":"The following snippet creates a new dataset ( newDataset ) and imports one graph from an existing dataset ( existingDataset ). Notice that the graph can be renamed as part of the import. Example 1 Imports the complete 'existingDataset' dataset to the 'newDataset' . const account = await triply.getAccount() const existingDataset = await account.getDataset('existingDataset') const newDataset = await account.addDataset('newDataset') await newDataset.importFromDataset(existingDataset) Example 2 Imports 'anotherDataset' dataset to a 'newDataset' Where a graph from the existing dataset is renamed to the a graphname in the new dataset. Only the graphs from the graphMap are imported. const account = await triply.getAccount() const anotherDataset = await account.getDataset('anotherDataset') const newDataset = await account.addDataset('newDataset') await newDataset.importFromDataset(anotherDataset, { graphMap: { 'https://example.org/existingDataset/graph': 'https://example.org/newDataset/graph'} }) Example 3 Import 'oneMoreDataset' dataset to the 'newDataset' Where a graph specific graph from the existing dataset is added to the new dataset. If the graph name already occurs in the 'newDataset' it will get overwritten. const account = await triply.getAccount() const oneMoreDataset = await account.getDataset('oneMoreDataset') const newDataset = await account.addDataset('newDataset') await newDataset.importFromDataset(oneMoreDataset, { graphNames: ['https://example.org/existingDataset/graph'], overwrite: true, })","title":"Examples"},{"location":"triplydb-js/dataset/#datasetimportfromfilesfiles-liststring-file-defaultsconfig-object","text":"","title":"Dataset.importFromFiles(files: list(string || File), defaultsConfig?: object)"},{"location":"triplydb-js/dataset/#required_3","text":"Imports one or more files into this dataset. The files must contain RDF data. Optional: defaultsConfig: object defaultGraphName Accepts a string value that is set as the default graph name for each imported file baseIRI Accepts a string value that is set as the default baseIRI for each imported file overwriteAll Accepts a boolean value that overwrites previously added graph names or baseIRIs (regardless of whether they came from a URL or a file) mergeGraphs Accepts a Boolean value. An optional property that determines whether existing graph in this dataset are merged with the imported graphs. If this argument is not specified, then `false` is used as the default value.","title":"Required"},{"location":"triplydb-js/dataset/#supported-files","text":"The files must contain RDF data and must be encoded in one of the following standardized RDF serialization formats: N-Quads, N-Triples, TriG, Turtle.","title":"Supported files"},{"location":"triplydb-js/dataset/#examples_19","text":"Example 1 const account = await triply.getAccount('Triply') const dataset = await account.getDataset(iris) await dataset.importFromFiles('test.nt') await dataset.importFromFiles(['file.nq', 'file.tar.gz']) Example 2 const account = await triply.getAccount('Triply') const dataset = await account.getDataset(iris) await dataset.importFromFiles('test.nt') await dataset.importFromFiles(['file.nq', 'file.tar.gz'], { defaultGraphName: 'https://triplydb.com/Triply/example/graph/default', overwriteAll: true, })","title":"Examples"},{"location":"triplydb-js/dataset/#datasetimportfromstorestore-n3store-defaultsconfig-object","text":"One of the most complete libraries for handling linked data in memory is the n3 library . The N3.js library is an implementation of the RDF.js low-level specification that lets you handle RDF in JavaScript easily, with an asynchronous, streaming approach. To reduce the overhead of converting your data from the N3 Store to a file and uploading to TriplyDB. TriplyDB.js has a importFromStore() where a N3 store is given as first argument and uploaded directly to triplyDB.","title":"Dataset.importFromStore(store: n3.Store, defaultsConfig?: object)"},{"location":"triplydb-js/dataset/#examples_20","text":"const store = new Store() store.addQuad(DataFactory.namedNode('https://triplydb.com/id/me'),DataFactory.namedNode('http://www.w3.org/2000/01/rdf-schema#label'),DataFactory.literal('me'),DataFactory.namedNode('https://triplydb.com/Triply/example/graph/default')) const triply = App.get({ token: process.env.TOKEN }) const user = await triply.getUser() const dataset = (await user.getDatasets().toArray())[0] dataset.importFromStore(store)","title":"Examples"},{"location":"triplydb-js/dataset/#datasetimportfromurlsurls-liststring-defaultsconfig-object","text":"","title":"Dataset.importFromUrls(urls: list(string), defaultsConfig?: object)"},{"location":"triplydb-js/dataset/#required_4","text":"Imports one or more URLs into this dataset. The URLs must provide access to RDF data. Optional: defaultsConfig: object defaultGraphName Accepts a string value that is set as the default graph name for each imported URL baseIRI Accepts a string value that is set as the default baseIRI for each imported URL overwriteAll Accepts a boolean value that overwrites previously added graph names or baseIRIs (regardless of whether they came from a URL or a file)","title":"Required"},{"location":"triplydb-js/dataset/#examples_21","text":"dataset1.importFromUrls(['url1', 'url2', 'url3'])","title":"Examples"},{"location":"triplydb-js/dataset/#datasetremoveallgraphs","text":"Removes all graphs from this dataset.","title":"Dataset.removeAllGraphs()"},{"location":"triplydb-js/dataset/#examples_22","text":"The following snippet removed all graphs from a specific dataset: const user = await triply.getUser() const dataset = await user.getDataset('my-dataset') await dataset.removeAllGraphs()","title":"Examples"},{"location":"triplydb-js/dataset/#datasetremoveprefixesprefixes-string","text":"Removes IRI prefixes from this dataset. The prefixes argument is a string array, containing the prefix labels to be removed.","title":"Dataset.removePrefixes(prefixes: string[])"},{"location":"triplydb-js/dataset/#examples_23","text":"The following snippet removes the def and id prefixes from the specified dataset. const account = await triply.getAccount() const dataset = await account.getDataset('my-dataset') await dataset.removePrefixes(['def', 'id'])","title":"Examples"},{"location":"triplydb-js/dataset/#datasetrenamegraphfrom-string-to-string","text":"Renames a graph of this dataset, where from is the current graph name and to is the new graph name. The string arguments for from and to must be valid IRIs.","title":"Dataset.renameGraph(from: string, to: string)"},{"location":"triplydb-js/dataset/#examples_24","text":"The following snippet renames a specific graph of a specific dataset: const account = await triply.getAccount() const dataset = await account.getDataset('my-dataset') await dataset.renameGraph( 'https://example.org/old-graph', 'https://example.org/new-graph' )","title":"Examples"},{"location":"triplydb-js/dataset/#datasetupdatemetadata-object","text":"Updates the metadata for this dataset.","title":"Dataset.update(metadata: object)"},{"location":"triplydb-js/dataset/#arguments_3","text":"The metadata argument takes a dictionary object with the following optional keys: Required: accessLevel The access level of the dataset. The following values are supported: 'private' The dataset can only be accessed by the Account object for which it is created. 'internal' The dataset can only be accessed by people who are logged into the TriplyDB server. 'public' The dataset can be accessed by everybody. Optional: description The description of the dataset. This description can make use of Markdown. displayName The human-readable name of the dataset. This name may contain spaces and other characters that are not allowed in the URL-friendly name. license The license of the dataset. The following license strings are currently supported: 'CC-BY-SA' 'CC0 1.0' 'GFDL' 'ODC-By' 'ODC-ODbL' 'PDDL'","title":"Arguments"},{"location":"triplydb-js/dataset/#example_1","text":"The following snippet updates the dataset's access level, description, display name and license: const account = await triply.getAccount() const dataset = await account.getDataset('my-dataset') dataset.update({ accessLevel: 'private', description: 'desc', displayName: 'disp', license: 'PDDL', })","title":"Example"},{"location":"triplydb-js/dataset/#datasetuploadasset-filepath-string-opts-mode-throw-if-exists-replace-if-exists-append-version-name-string","text":"Uploads a file that does not contain RDF data as an asset. You can specify the name on the asset and what to do if the asset already exists (throws an error by default).","title":"Dataset.uploadAsset( filePath: string, opts?: {mode?: 'throw-if-exists'| 'replace-if-exists'| 'append-version', name?: string})"},{"location":"triplydb-js/dataset/#user-cases","text":"There are several use cases for assets: Source data that will be used as input files to an ETL process. Documentation files that describe the dataset. Media files (audio/image/video) that are described in the RDF graph.","title":"User cases"},{"location":"triplydb-js/dataset/#examples_25","text":"The following snippet uploads a source CSV data file and a PDF documentation file: const account = await triply.getAccount() const dataset = await account.getDataset('my-dataset') await dataset.uploadAsset('my-source-data', {name: 'source.csv.gz'}) await dataset.uploadAsset('my-documentation', {name: 'documentation.pdf'}) await dataset.uploadAsset('my-documentation', {mode:'append-version', name: 'documentation.pdf'})","title":"Examples"},{"location":"triplydb-js/faq/","text":"On this page: FAQ How to perform a SPARQL query? What is the latest version of TriplyDB.js? What to do when the \u201cError: Unauthorized\u201d appears? How do I get the results of a saved query using TriplyDB.js? What is an async iterator? FAQ \u00b6 This section includes answers to frequently asked questions. Please contact info@triply.cc if you have a question that does not appear in this list. How to perform a SPARQL query? \u00b6 The SPARQL 1.1 Protocol standard specifies a native HTTP API for performing SPARQL requests. Such requests can be performed with regular HTTP libraries. Here we give an example indicating how such an HTTP library can be used: import SuperAgent from 'superagent'; const reply = await SuperAgent.post('SPARQL_ENDPOINT') .set('Accept', 'application/sparql-results+json') .set('Authorization', 'Bearer ' + process.env.TOKEN) .buffer(true) .send({ query: 'select * { WHERE_CLAUSE } offset 0 limit 10000' }) // break condition when the result set is empty. // downsides: caching, string manipulation What is the latest version of TriplyDB.js? \u00b6 The latest version of TriplyDB.js can be found in the NPM repository . What to do when the \u201cError: Unauthorized\u201d appears? \u00b6 This error appears whenever an operation is performed for which the user denoted by the current API token is not authorized. One common appearance of this error is when the environment variable TOKEN is not set to an API token. The current value of the environment variable can be tested by running the following command in the terminal: echo $TOKEN How do I get the results of a saved query using TriplyDB.js? \u00b6 To reliably retrieve a large number of results as the output of a construct or select query, follow these steps: 1. Import the triplydb library. import App from '@triply/triplydb'; 2. Set your parameters, regarding the TriplyDB server and the account in which you have saved the query as well as the name of the query. const triply = App.get({ url: 'https://api.triplydb.com' }) const account = await triply.getAccount('account-name') const query = await account.getQuery('name-of-some-query') If the query is not public, you should set your API token rather than the URL. const triply = App.get({ token: process.env.TOKEN }) 3. Do not forget that we perform TriplyDB.js requests within an async context. That is: async function run() { // Your code goes here. } run() 4. Get the results of a query by setting a results variable. More specifically, for construct queries: const results = query.results().statements() For select queries: const results = query.results().bindings() Note that for SPARQL construct queries, we use method .statements() , while for SPARQL select queries, we use method .bindings() . Additionally, saved queries can have API variables that allow you to specify variables that are used in the query. Thus, if you have query parameters, pass their values as the first argument to results as follows: // For SPARQL construct queries. const results = query .results({ someVariable: 'value of someVariable', anotherVariable: 'value of anotherVariable', }) .statements() // For SPARQL select queries. const results = query .results({ someVariable: 'value of someVariable', anotherVariable: 'value of anotherVariable', }) .bindings() 5. To read the results you have three options: 5a. Iterate through the results per row in a for -loop: // Iterating over the results per row for await (const row of results) { // execute something } 5b. Save the results to a file. For saving SPARQL construct queries: // Saving the results of a SPARQL construct query to a file. await results.toFile('my-file.nt') For saving SPARQL select queries. Currently we only support saving the file to a .tsv format: // Saving the results of a SPARQL select query to a file. await results.toFile('my-file.tsv') 5c. Load all results into memory. Note that this is almost never used. If you want to process results, then option 5a is better; if you want to persist results, then option 5b is better. // Loading results for a SPARQL construct or SPARQL select query into memory. const array = await results.toArray() What is an async iterator? \u00b6 TriplyDB.js makes use of async iterators for retrieving lists of objects. Async iterators are a method of fetching and iterating through large lists, without having to first fetch the whole set. An example of an async iterator in TriplyDB.js is App.getAccounts() . The following code illustrates how it can be used. for await (const account of triply.getAccounts()) { console.log(account) } For cases where you want the complete list, you can use the toArray function of the iterator. const accounts = await triply.getAccounts().toArray() TriplyDB.js returns async iterators from the following methods: App.getAccounts() Account.getDatasets() Account.getQueries() Account.getStories() Dataset.getServices() Dataset.getAssets() Dataset.getGraphs() Dataset.getStatements() Query.results().statements() for SPARQL construct and describe queries Query.results().bindings() for SPARQL select queries","title":"FAQ"},{"location":"triplydb-js/faq/#faq","text":"This section includes answers to frequently asked questions. Please contact info@triply.cc if you have a question that does not appear in this list.","title":"FAQ"},{"location":"triplydb-js/faq/#how-to-perform-a-sparql-query","text":"The SPARQL 1.1 Protocol standard specifies a native HTTP API for performing SPARQL requests. Such requests can be performed with regular HTTP libraries. Here we give an example indicating how such an HTTP library can be used: import SuperAgent from 'superagent'; const reply = await SuperAgent.post('SPARQL_ENDPOINT') .set('Accept', 'application/sparql-results+json') .set('Authorization', 'Bearer ' + process.env.TOKEN) .buffer(true) .send({ query: 'select * { WHERE_CLAUSE } offset 0 limit 10000' }) // break condition when the result set is empty. // downsides: caching, string manipulation","title":"How to perform a SPARQL query?"},{"location":"triplydb-js/faq/#what-is-the-latest-version-of-triplydbjs","text":"The latest version of TriplyDB.js can be found in the NPM repository .","title":"What is the latest version of TriplyDB.js?"},{"location":"triplydb-js/faq/#what-to-do-when-the-error-unauthorized-appears","text":"This error appears whenever an operation is performed for which the user denoted by the current API token is not authorized. One common appearance of this error is when the environment variable TOKEN is not set to an API token. The current value of the environment variable can be tested by running the following command in the terminal: echo $TOKEN","title":"What to do when the \u201cError: Unauthorized\u201d appears?"},{"location":"triplydb-js/faq/#how-do-i-get-the-results-of-a-saved-query-using-triplydbjs","text":"To reliably retrieve a large number of results as the output of a construct or select query, follow these steps: 1. Import the triplydb library. import App from '@triply/triplydb'; 2. Set your parameters, regarding the TriplyDB server and the account in which you have saved the query as well as the name of the query. const triply = App.get({ url: 'https://api.triplydb.com' }) const account = await triply.getAccount('account-name') const query = await account.getQuery('name-of-some-query') If the query is not public, you should set your API token rather than the URL. const triply = App.get({ token: process.env.TOKEN }) 3. Do not forget that we perform TriplyDB.js requests within an async context. That is: async function run() { // Your code goes here. } run() 4. Get the results of a query by setting a results variable. More specifically, for construct queries: const results = query.results().statements() For select queries: const results = query.results().bindings() Note that for SPARQL construct queries, we use method .statements() , while for SPARQL select queries, we use method .bindings() . Additionally, saved queries can have API variables that allow you to specify variables that are used in the query. Thus, if you have query parameters, pass their values as the first argument to results as follows: // For SPARQL construct queries. const results = query .results({ someVariable: 'value of someVariable', anotherVariable: 'value of anotherVariable', }) .statements() // For SPARQL select queries. const results = query .results({ someVariable: 'value of someVariable', anotherVariable: 'value of anotherVariable', }) .bindings() 5. To read the results you have three options: 5a. Iterate through the results per row in a for -loop: // Iterating over the results per row for await (const row of results) { // execute something } 5b. Save the results to a file. For saving SPARQL construct queries: // Saving the results of a SPARQL construct query to a file. await results.toFile('my-file.nt') For saving SPARQL select queries. Currently we only support saving the file to a .tsv format: // Saving the results of a SPARQL select query to a file. await results.toFile('my-file.tsv') 5c. Load all results into memory. Note that this is almost never used. If you want to process results, then option 5a is better; if you want to persist results, then option 5b is better. // Loading results for a SPARQL construct or SPARQL select query into memory. const array = await results.toArray()","title":"How do I get the results of a saved query using TriplyDB.js?"},{"location":"triplydb-js/faq/#what-is-an-async-iterator","text":"TriplyDB.js makes use of async iterators for retrieving lists of objects. Async iterators are a method of fetching and iterating through large lists, without having to first fetch the whole set. An example of an async iterator in TriplyDB.js is App.getAccounts() . The following code illustrates how it can be used. for await (const account of triply.getAccounts()) { console.log(account) } For cases where you want the complete list, you can use the toArray function of the iterator. const accounts = await triply.getAccounts().toArray() TriplyDB.js returns async iterators from the following methods: App.getAccounts() Account.getDatasets() Account.getQueries() Account.getStories() Dataset.getServices() Dataset.getAssets() Dataset.getGraphs() Dataset.getStatements() Query.results().statements() for SPARQL construct and describe queries Query.results().bindings() for SPARQL select queries","title":"What is an async iterator?"},{"location":"triplydb-js/graph/","text":"On this page: Graph Examples Graph.delete() Examples Graph.getInfo() Examples Graph.rename(name: string) Examples Get the data locally Graph.toFile(destinationPath: string, arguments?: object) Optional Examples Graph.toStore(graph?: Graph) Examples Graph.toStream(type: 'compressed' | 'rdf-js', arguments?: object) Optional Examples Graph \u00b6 Each dataset with data consists out of one or more named graphs. All graphs together is thus the collection of triples of the dataset. Often the graph is used to denote a part of the dataset. For example the data model of the dataset or the metadata of the dataset. A graph has as advantage that is can partition data while at the same time keep the data in the same dataset. Reducing the overhead of having to move between datasets to traverse a graph. You can retrieve either retrieve all graphs from a dataset in the form of an async iterator. Or retrieve a specific graph from a dataset. Examples \u00b6 The following snippet retrieves the graph 'https://example.com/my-graph' for a specific dataset: const user = await triply.getUser() const dataset = await user.getDataset('my-dataset') const graph = await dataset.getGraph('https://example.com/my-graph') The following snippet retrieves all the graphs for a specific dataset: const account = await triply.getAccount() const dataset = await account.getDataset('my-dataset') const graphs = dataset.getGraphs() The Graph is the smallest object that can be individually deleted or modified. Graph.delete() \u00b6 Deletes the graph of this dataset. Any copies of the graph will not be deleted. All services containing this graph will still contain the graph until the service is synced again. Examples \u00b6 The following snippet deletes a specific graph that is part of the account associated with the current API token: const account = await triply.getAccount() const dataset = await account.getDataset('my-dataset') const graph = await dataset.getGraph('https://example.com/my-graph') await graph.delete() Graph.getInfo() \u00b6 Returns information about this graph. Information is returned in a dictionary object. Individual keys can be accessed for specific information values. The following keys and values are returned for graph.getInfo() id A hexadecimal hash of the graph to identify the graph for internal identification. graphName The URL-friendly name of the graphName that is used as identifier and name. numberOfStatements The number of statements in the graph. uploadedAt (Optional) The date/time at which the graph was uploaded to TriplyDB. importedAt (Optional) The date/time at which the query was imported from another dataset. importedFrom (Optional) graphName The graphname of the graph from the dataset from which the graph was imported. dataset The dataset from which the graph was imported. Examples \u00b6 The following snippet prints the information from the specified graph of the specified dataset of the current user: const user = await triply.getUser() const dataset = await user.getDataset('my-dataset') const graph = await dataset.getGraph('https://example.com/my-graph') console.log(await graph.getInfo()) Graph.rename(name: string) \u00b6 Renames the graph, the argument name is the new graph name. The string argument for name must be a valid IRI. Examples \u00b6 The following snippet renames a specific graph of a specific dataset: const account = await triply.getAccount() const dataset = await account.getDataset('my-dataset') const graph = await dataset.getGraph('https://example.com/my-graph') await dataset.rename('https://example.org/new-graph') Get the data locally \u00b6 Most of the time you do not need to download a graph locally as TriplyDB supports a variety of methods to use linked data directly. But if you want to use a graph locally that is possible with TriplyDB.js . There are three methods to retrieve linked data from a graph. toFile() , toStore() and toStream() . Graph.toFile(destinationPath: string, arguments?: object) \u00b6 The first method downloads the linked data graphs directly and writes the data to the location of the destinationPath . The extension on the destinationPath defines the linked data type that is downloaded. The extensions that are supported are: nt , nq , trig , ttl , jsonld , json . If no extension is set or the extension is not recognized the function will throw an error. Optional \u00b6 The optional properties accepted as arguments for toFile Compressed Argument compressed optionally is an boolean defining if a graph is compresssed with GNU zip (gzip) compression algorithm and will end with a `.gz` extension. Examples \u00b6 The following example downloads the graph to file: const triply = App.get({ token: process.env.TOKEN }) const user = await triply.getAccount() const dataset = await user.getDataset('pokemon') const graph = await dataset.getGraph('https://example.com/my-graph') await graph.toFile('my-filename.ttl', {compressed: true}) Graph.toStore(graph?: Graph) \u00b6 The second method is to download the file into a N3.store . The n3 library is one of the most complete libraries for handling linked data in memory. The N3.js library is an implementation of the RDF.js low-level specification that lets you handle RDF in JavaScript easily, with an asynchronous, streaming approach. To reduce the overhead of downloading your data to file and then insert it in the N3 Store. TriplyDB.js has a toStore() where a N3 store is returned as a result of the the toStore() function. Examples \u00b6 The following example downloads the graph as N3.store : const triply = App.get({ token: process.env.TOKEN }) const user = await triply.getAccount() const dataset = await user.getDataset('pokemon') const graph = await dataset.getGraph('https://example.com/my-graph') const store = await graph.toStore() Graph.toStream(type: 'compressed' | 'rdf-js', arguments?: object) \u00b6 The final method to download linked data to a local source is the toStream this function returns a stream of quads that can directly be iterated over. The Stream is either of the type compressed which returns a gzipped stream of linked data, or type rdf-js which returns a stream of quads parsed according to the rdf-js standard . Optional \u00b6 The following arguments can be defined in the optional arguments object. Extension Argument Extension optionally defines the linked data type that is streamed. The extensions that are supported are: `nt`, `nq`, `trig`, `ttl`, `jsonld`, `json`. Examples \u00b6 The following example streams through the graph as rdf-js quad objects. and prints the quad to the screen. notice that the stream is an async iterator. Example 1 const triply = App.get({ token: process.env.TOKEN }) const user = await triply.getAccount() const dataset = await user.getDataset('pokemon') const graph = await dataset.getGraph('https://example.com/my-graph') const stream = await graph.toStream('rdf-js', {extension: '.nq'}) for await(const quad of stream){ console.log(quad) } The following example streams through the graph as chunks of ttl. and prints the buffer to the screen. Example 2 const triply = App.get({ token: process.env.TOKEN }) const user = await triply.getAccount() const dataset = await user.getDataset('pokemon') const graph = await dataset.getGraph('https://example.com/my-graph') const stream = await graph.toStream('compressed', {extension: '.ttl'}) for await(const quad of stream.pipe(zlib.createGunzip())){ console.log((quad as Buffer).toString()) }","title":"Graph"},{"location":"triplydb-js/graph/#graph","text":"Each dataset with data consists out of one or more named graphs. All graphs together is thus the collection of triples of the dataset. Often the graph is used to denote a part of the dataset. For example the data model of the dataset or the metadata of the dataset. A graph has as advantage that is can partition data while at the same time keep the data in the same dataset. Reducing the overhead of having to move between datasets to traverse a graph. You can retrieve either retrieve all graphs from a dataset in the form of an async iterator. Or retrieve a specific graph from a dataset.","title":"Graph"},{"location":"triplydb-js/graph/#examples","text":"The following snippet retrieves the graph 'https://example.com/my-graph' for a specific dataset: const user = await triply.getUser() const dataset = await user.getDataset('my-dataset') const graph = await dataset.getGraph('https://example.com/my-graph') The following snippet retrieves all the graphs for a specific dataset: const account = await triply.getAccount() const dataset = await account.getDataset('my-dataset') const graphs = dataset.getGraphs() The Graph is the smallest object that can be individually deleted or modified.","title":"Examples"},{"location":"triplydb-js/graph/#graphdelete","text":"Deletes the graph of this dataset. Any copies of the graph will not be deleted. All services containing this graph will still contain the graph until the service is synced again.","title":"Graph.delete()"},{"location":"triplydb-js/graph/#examples_1","text":"The following snippet deletes a specific graph that is part of the account associated with the current API token: const account = await triply.getAccount() const dataset = await account.getDataset('my-dataset') const graph = await dataset.getGraph('https://example.com/my-graph') await graph.delete()","title":"Examples"},{"location":"triplydb-js/graph/#graphgetinfo","text":"Returns information about this graph. Information is returned in a dictionary object. Individual keys can be accessed for specific information values. The following keys and values are returned for graph.getInfo() id A hexadecimal hash of the graph to identify the graph for internal identification. graphName The URL-friendly name of the graphName that is used as identifier and name. numberOfStatements The number of statements in the graph. uploadedAt (Optional) The date/time at which the graph was uploaded to TriplyDB. importedAt (Optional) The date/time at which the query was imported from another dataset. importedFrom (Optional) graphName The graphname of the graph from the dataset from which the graph was imported. dataset The dataset from which the graph was imported.","title":"Graph.getInfo()"},{"location":"triplydb-js/graph/#examples_2","text":"The following snippet prints the information from the specified graph of the specified dataset of the current user: const user = await triply.getUser() const dataset = await user.getDataset('my-dataset') const graph = await dataset.getGraph('https://example.com/my-graph') console.log(await graph.getInfo())","title":"Examples"},{"location":"triplydb-js/graph/#graphrenamename-string","text":"Renames the graph, the argument name is the new graph name. The string argument for name must be a valid IRI.","title":"Graph.rename(name: string)"},{"location":"triplydb-js/graph/#examples_3","text":"The following snippet renames a specific graph of a specific dataset: const account = await triply.getAccount() const dataset = await account.getDataset('my-dataset') const graph = await dataset.getGraph('https://example.com/my-graph') await dataset.rename('https://example.org/new-graph')","title":"Examples"},{"location":"triplydb-js/graph/#get-the-data-locally","text":"Most of the time you do not need to download a graph locally as TriplyDB supports a variety of methods to use linked data directly. But if you want to use a graph locally that is possible with TriplyDB.js . There are three methods to retrieve linked data from a graph. toFile() , toStore() and toStream() .","title":"Get the data locally"},{"location":"triplydb-js/graph/#graphtofiledestinationpath-string-arguments-object","text":"The first method downloads the linked data graphs directly and writes the data to the location of the destinationPath . The extension on the destinationPath defines the linked data type that is downloaded. The extensions that are supported are: nt , nq , trig , ttl , jsonld , json . If no extension is set or the extension is not recognized the function will throw an error.","title":"Graph.toFile(destinationPath: string, arguments?: object)"},{"location":"triplydb-js/graph/#optional","text":"The optional properties accepted as arguments for toFile Compressed Argument compressed optionally is an boolean defining if a graph is compresssed with GNU zip (gzip) compression algorithm and will end with a `.gz` extension.","title":"Optional"},{"location":"triplydb-js/graph/#examples_4","text":"The following example downloads the graph to file: const triply = App.get({ token: process.env.TOKEN }) const user = await triply.getAccount() const dataset = await user.getDataset('pokemon') const graph = await dataset.getGraph('https://example.com/my-graph') await graph.toFile('my-filename.ttl', {compressed: true})","title":"Examples"},{"location":"triplydb-js/graph/#graphtostoregraph-graph","text":"The second method is to download the file into a N3.store . The n3 library is one of the most complete libraries for handling linked data in memory. The N3.js library is an implementation of the RDF.js low-level specification that lets you handle RDF in JavaScript easily, with an asynchronous, streaming approach. To reduce the overhead of downloading your data to file and then insert it in the N3 Store. TriplyDB.js has a toStore() where a N3 store is returned as a result of the the toStore() function.","title":"Graph.toStore(graph?: Graph)"},{"location":"triplydb-js/graph/#examples_5","text":"The following example downloads the graph as N3.store : const triply = App.get({ token: process.env.TOKEN }) const user = await triply.getAccount() const dataset = await user.getDataset('pokemon') const graph = await dataset.getGraph('https://example.com/my-graph') const store = await graph.toStore()","title":"Examples"},{"location":"triplydb-js/graph/#graphtostreamtype-compressed-rdf-js-arguments-object","text":"The final method to download linked data to a local source is the toStream this function returns a stream of quads that can directly be iterated over. The Stream is either of the type compressed which returns a gzipped stream of linked data, or type rdf-js which returns a stream of quads parsed according to the rdf-js standard .","title":"Graph.toStream(type: 'compressed' | 'rdf-js', arguments?: object)"},{"location":"triplydb-js/graph/#optional_1","text":"The following arguments can be defined in the optional arguments object. Extension Argument Extension optionally defines the linked data type that is streamed. The extensions that are supported are: `nt`, `nq`, `trig`, `ttl`, `jsonld`, `json`.","title":"Optional"},{"location":"triplydb-js/graph/#examples_6","text":"The following example streams through the graph as rdf-js quad objects. and prints the quad to the screen. notice that the stream is an async iterator. Example 1 const triply = App.get({ token: process.env.TOKEN }) const user = await triply.getAccount() const dataset = await user.getDataset('pokemon') const graph = await dataset.getGraph('https://example.com/my-graph') const stream = await graph.toStream('rdf-js', {extension: '.nq'}) for await(const quad of stream){ console.log(quad) } The following example streams through the graph as chunks of ttl. and prints the buffer to the screen. Example 2 const triply = App.get({ token: process.env.TOKEN }) const user = await triply.getAccount() const dataset = await user.getDataset('pokemon') const graph = await dataset.getGraph('https://example.com/my-graph') const stream = await graph.toStream('compressed', {extension: '.ttl'}) for await(const quad of stream.pipe(zlib.createGunzip())){ console.log((quad as Buffer).toString()) }","title":"Examples"},{"location":"triplydb-js/organization/","text":"On this page: Organization Obtaining instances Inheritance Organization.addDataset(name: string, metadata?: object) Organization.addMember(user: User, role?: Role) Arguments Examples Organization.removeMember(user: User) Organization.addQuery(name: string, metadata: object) Organization.ensureStory(name: string, metadata: object) Organization.addStory(name: string, metadata?: object) Organization.delete() Examples Organization.ensureDataset(name: string, metadata?: object) Organization.getDataset(name: string) Organization.getDatasets() Organization.getMembers() Return type Examples See also Organization.getPinnedItems() Organization.removeMember(user: User) Arguments Existence considerations Examples Organization.setAvatar(file: string) Organization.update(metadata: object) Organization \u00b6 Instances of class Organization denote organizations in TriplyDB. Obtaining instances \u00b6 Organizations are obtained with method App.getOrganization(name: string) : const organization = await triply.getOrganization('Triply') Alternatively, organizations are obtained by first obtaining an account ( App.getAccount(name?: string) ) and then casting it to an organization ( Account.asOrganization() ): const account = await triply.getAccount('Triply') const organization = account.asOrganization() Inheritance \u00b6 Organization is a subclass of Account , from which it inherits most of its methods. Organization.addDataset(name: string, metadata?: object) \u00b6 Adds a new TriplyDB dataset with the given name to the current organization. Inherited from Account.addDataset(name: string, metadata?: object) . Organization.addMember(user: User, role?: Role) \u00b6 Adds a member to the given Organization , with the given role of either member or owner. Arguments \u00b6 The user argument has to be a user object of the user which should be added to the organization. The role argument can be either 'member' or 'owner' . If this argument is not specified, then 'member' is used as the default. 'member' A regular member that is allowed to read and write the datasets that are published under the organization. 'owner' An owner of the organization. Owners have all the rights of regular users, plus the ability to add/remove users to/from the organization, the ability to change the roles of existing users, and the ability to delete the organization. Examples \u00b6 user The following snippet adds user John Doe to the Triply organization as a regular member. const organization = await triply.getOrganization('Triply') const johnDoe = await app.getUser('john-doe') await organization.addMember(johnDoe) Organization.removeMember(user: User) \u00b6 Removes a member from the given Organization . Organization.addQuery(name: string, metadata: object) \u00b6 Adds a new TriplyDB query to the current organization. Inherited from Account.addQuery(name: string, metadata: object) . Organization.ensureStory(name: string, metadata: object) \u00b6 Ensures the existence of a story with the given name and with the specified metadata . Inherited from Account.ensureStory(name: string, metadata: object) . Organization.addStory(name: string, metadata?: object) \u00b6 Adds a new TriplyDB story with the given name to the current organization. Inherited from Account.addStory(name: string, metadata?: object) . Organization.delete() \u00b6 Deletes this account. This also deletes all datasets, stories and queries that belong to this organization. Examples \u00b6 The following code example deletes the specified organization: const organization = await triply.getOrganization('Neo4j') await organization.delete() Organization.ensureDataset(name: string, metadata?: object) \u00b6 Ensures the existence of a dataset with the given name and with the specified metadata . Inherited from Account.ensureDataset(name: string, metadata?: object) . Organization.getDataset(name: string) \u00b6 Returns the dataset with the given name that is published by this organization. Inherited from Account.getDataset(name: string) . Organization.getDatasets() \u00b6 Returns an async iterator over the accessible datasets that belong to this organization. Inherited from Account.getDatasets() . Organization.getMembers() \u00b6 Returns the list of memberships for the given organization. Return type \u00b6 A membership contains the following components: role The role of the membership ( OrgRole ): either 'owner' for owners of the organization, or 'member' for regular members. The difference between owners and regular members is that owners can perform user management for the organization (add/remove/change memberships). user An instance of class User . createdAt A date/time string. updatedAt A date/time string. Examples \u00b6 const org = await triply.getOrganization('acme') for (const membership of await org.getMembers()) { console.log(user) } See also \u00b6 Memberships of organization are TriplyDB users . Organization.getPinnedItems() \u00b6 Returns the list of datasets, stories and queries that are pinned for the current organization. Inherited from Account.getPinnedItems() . Organization.removeMember(user: User) \u00b6 Removes the specified user from this organization. Arguments \u00b6 The user argument has to be a User object of a user. Existence considerations \u00b6 The user must be a current member of the organization for this method to succeed. If the user is not a current member of the organization, an error is thrown. Examples \u00b6 The following snippet removes John Doe from the Triply organization, using a string argument: const organization = await triply.getOrganization('Triply') const johnDoe = await app.getUser('john-doe') await organization.removeMember(johnDoe) The following snippet removes John Doe from the Triply organization, using a User object: const organization = await triply.getOrganization('Triply') const user = await triply.getUser('john-doe') await organization.removeMember(user) Organization.setAvatar(file: string) \u00b6 Sets a new image that characterized this organization. Inherited from Account.setAvatar(file: string) . Organization.update(metadata: object) \u00b6 Updates the metadata for this account. Inherited from Account.update(metadata: object) .","title":"Organization"},{"location":"triplydb-js/organization/#organization","text":"Instances of class Organization denote organizations in TriplyDB.","title":"Organization"},{"location":"triplydb-js/organization/#obtaining-instances","text":"Organizations are obtained with method App.getOrganization(name: string) : const organization = await triply.getOrganization('Triply') Alternatively, organizations are obtained by first obtaining an account ( App.getAccount(name?: string) ) and then casting it to an organization ( Account.asOrganization() ): const account = await triply.getAccount('Triply') const organization = account.asOrganization()","title":"Obtaining instances"},{"location":"triplydb-js/organization/#inheritance","text":"Organization is a subclass of Account , from which it inherits most of its methods.","title":"Inheritance"},{"location":"triplydb-js/organization/#organizationadddatasetname-string-metadata-object","text":"Adds a new TriplyDB dataset with the given name to the current organization. Inherited from Account.addDataset(name: string, metadata?: object) .","title":"Organization.addDataset(name: string, metadata?: object)"},{"location":"triplydb-js/organization/#organizationaddmemberuser-user-role-role","text":"Adds a member to the given Organization , with the given role of either member or owner.","title":"Organization.addMember(user: User, role?: Role)"},{"location":"triplydb-js/organization/#arguments","text":"The user argument has to be a user object of the user which should be added to the organization. The role argument can be either 'member' or 'owner' . If this argument is not specified, then 'member' is used as the default. 'member' A regular member that is allowed to read and write the datasets that are published under the organization. 'owner' An owner of the organization. Owners have all the rights of regular users, plus the ability to add/remove users to/from the organization, the ability to change the roles of existing users, and the ability to delete the organization.","title":"Arguments"},{"location":"triplydb-js/organization/#examples","text":"user The following snippet adds user John Doe to the Triply organization as a regular member. const organization = await triply.getOrganization('Triply') const johnDoe = await app.getUser('john-doe') await organization.addMember(johnDoe)","title":"Examples"},{"location":"triplydb-js/organization/#organizationremovememberuser-user","text":"Removes a member from the given Organization .","title":"Organization.removeMember(user: User)"},{"location":"triplydb-js/organization/#organizationaddqueryname-string-metadata-object","text":"Adds a new TriplyDB query to the current organization. Inherited from Account.addQuery(name: string, metadata: object) .","title":"Organization.addQuery(name: string, metadata: object)"},{"location":"triplydb-js/organization/#organizationensurestoryname-string-metadata-object","text":"Ensures the existence of a story with the given name and with the specified metadata . Inherited from Account.ensureStory(name: string, metadata: object) .","title":"Organization.ensureStory(name: string, metadata: object)"},{"location":"triplydb-js/organization/#organizationaddstoryname-string-metadata-object","text":"Adds a new TriplyDB story with the given name to the current organization. Inherited from Account.addStory(name: string, metadata?: object) .","title":"Organization.addStory(name: string, metadata?: object)"},{"location":"triplydb-js/organization/#organizationdelete","text":"Deletes this account. This also deletes all datasets, stories and queries that belong to this organization.","title":"Organization.delete()"},{"location":"triplydb-js/organization/#examples_1","text":"The following code example deletes the specified organization: const organization = await triply.getOrganization('Neo4j') await organization.delete()","title":"Examples"},{"location":"triplydb-js/organization/#organizationensuredatasetname-string-metadata-object","text":"Ensures the existence of a dataset with the given name and with the specified metadata . Inherited from Account.ensureDataset(name: string, metadata?: object) .","title":"Organization.ensureDataset(name: string, metadata?: object)"},{"location":"triplydb-js/organization/#organizationgetdatasetname-string","text":"Returns the dataset with the given name that is published by this organization. Inherited from Account.getDataset(name: string) .","title":"Organization.getDataset(name: string)"},{"location":"triplydb-js/organization/#organizationgetdatasets","text":"Returns an async iterator over the accessible datasets that belong to this organization. Inherited from Account.getDatasets() .","title":"Organization.getDatasets()"},{"location":"triplydb-js/organization/#organizationgetmembers","text":"Returns the list of memberships for the given organization.","title":"Organization.getMembers()"},{"location":"triplydb-js/organization/#return-type","text":"A membership contains the following components: role The role of the membership ( OrgRole ): either 'owner' for owners of the organization, or 'member' for regular members. The difference between owners and regular members is that owners can perform user management for the organization (add/remove/change memberships). user An instance of class User . createdAt A date/time string. updatedAt A date/time string.","title":"Return type"},{"location":"triplydb-js/organization/#examples_2","text":"const org = await triply.getOrganization('acme') for (const membership of await org.getMembers()) { console.log(user) }","title":"Examples"},{"location":"triplydb-js/organization/#see-also","text":"Memberships of organization are TriplyDB users .","title":"See also"},{"location":"triplydb-js/organization/#organizationgetpinneditems","text":"Returns the list of datasets, stories and queries that are pinned for the current organization. Inherited from Account.getPinnedItems() .","title":"Organization.getPinnedItems()"},{"location":"triplydb-js/organization/#organizationremovememberuser-user_1","text":"Removes the specified user from this organization.","title":"Organization.removeMember(user: User)"},{"location":"triplydb-js/organization/#arguments_1","text":"The user argument has to be a User object of a user.","title":"Arguments"},{"location":"triplydb-js/organization/#existence-considerations","text":"The user must be a current member of the organization for this method to succeed. If the user is not a current member of the organization, an error is thrown.","title":"Existence considerations"},{"location":"triplydb-js/organization/#examples_3","text":"The following snippet removes John Doe from the Triply organization, using a string argument: const organization = await triply.getOrganization('Triply') const johnDoe = await app.getUser('john-doe') await organization.removeMember(johnDoe) The following snippet removes John Doe from the Triply organization, using a User object: const organization = await triply.getOrganization('Triply') const user = await triply.getUser('john-doe') await organization.removeMember(user)","title":"Examples"},{"location":"triplydb-js/organization/#organizationsetavatarfile-string","text":"Sets a new image that characterized this organization. Inherited from Account.setAvatar(file: string) .","title":"Organization.setAvatar(file: string)"},{"location":"triplydb-js/organization/#organizationupdatemetadata-object","text":"Updates the metadata for this account. Inherited from Account.update(metadata: object) .","title":"Organization.update(metadata: object)"},{"location":"triplydb-js/query/","text":"On this page: Query Query.delete() Query.getInfo() Query.getString(apiVariables?: object) Examples Query.addVersion(metadata: object) Arguments Query.getRunLink() Query.results(apiVariables?: object, options?: object) Examples Query.update(metadata: object) Arguments Query.useVersion(version: number|'latest') Example Query.copy(queryName?: string, account?:object, metadataToReplace?: object) Arguments Example Query \u00b6 A Saved Query is a versioned SPARQL query with its own URL. Using this URL, users are able to view any version of the query and its results. It can also be used to run the query and retrieve the results from a browser or a program, removing the hassle of figuring out how to run a SPARQL query. Saved queries come with a RESTful API that can be configured with the use a SPARQL API variables. Query.delete() \u00b6 Permanently deletes this query and all of its versions. Query.getInfo() \u00b6 The returned dictionary object includes the following keys: accessLevel The access level of the query. The following values are possible: 'private' (default) The dataset can only be accessed by organization members. 'internal' The dataset can only be accessed by users that are logged into the TriplyDB server. 'public' The dataset can be accessed by everybody. autoselectService Whether the SPARQL service is automatically chosen ( true ), or whether a specific SPARQL service is configured ( false ). createdAt The date/time at which the query was created. dataset A dictionary object representing the dataset against which the query is evaluated. description The human-readable description of the query. This typically explains what the query does in natural language. displayName The human-readable name of the query. This name may include spaces and other characters that are not allowed in the URL-friendly name. name The URL-friendly name of the query that is used in URL paths. This name can only include ASCII letters and hyphens. numberOfVersions The number of currently stored versions of this query. owner A dictionary object representing the account (organization or user) to which the query belongs. \ud83d\udea7 link Stores part of the URL to run the query. Please use Query.getRunLink() to obtain the full URL to run the query. service The location of the SPARQL endpoint that is used to run the query. updatedAt The date/time at which the query was last modified. Query.getString(apiVariables?: object) \u00b6 Returns the query string of the current version of this query. Optionally, arguments can be specified for the API variables to this query. Examples \u00b6 The following code stores the SPARQL query string for the query object: const queryString = await query.getString() Query.addVersion(metadata: object) \u00b6 Adds a new version to the query used. It requires similar options to that of Query.addQuery . Arguments \u00b6 At least one of the following arguments is required to create a new version. Any argument not given will be copied from the previous version of that query. queryString: string the SPARQL compliant query as a string value output: string The visualization plugin that is used to display the result set. If none is set it defaults to 'table' . Other options may include: 'response' , 'geo' , 'gallery' , 'markup' , etc. Output will take priority over ldFrame ldFrame: object JSON LD frame object used to transform plain JSON LD into a framed JSON. Will be used only if an output is not provided. variables: Variable[] A list of objects with the following keys: IRI variable An object of the form `Variable` (see Account.addQuery() ) You can see how many versions exist on a query accessing Query.getInfo().numOfVersions You can use a specified version of a query accessing Query.useVersion(x: number) Query.getRunLink() \u00b6 Returns the URL link to run the query. It currently does not support the use of variables. Query.results(apiVariables?: object, options?: object) \u00b6 Query.results() function will automatically return all the results from a saved query. You can retrieve both results from a select or ask query and a construct or describe query. The results are returned as an async iterator . If there are more than 10 000 query results, they could be retrieved using pagination with TriplyDB.js . Examples \u00b6 Get the results of a query by setting a results variable. More specifically, for construct queries you use the statements() call: const triply = App.get({token: process.env.TOKEN}) const account = await triply.getAccount('account-name') const query = await account.getQuery('name-of-some-query') // For select queries you use the `statements()` call: const results = query.results().statements() // For select queries you use the `bindings()` call: const results = query.results().bindings() Additionally, saved queries can have 'API variables' that allow you to specify variables that are used in the query. Thus, if you have query parameters, pass their values as the first argument to results as follows: const triply = App.get({token: process.env.TOKEN}) const account = await triply.getAccount('account-name') const query = await account.getQuery('name-of-some-query') // For SPARQL construct queries. const results = query.results({ someVariable: 'value of someVariable', anotherVariable: 'value of anotherVariable' }).statements() // For SPARQL select queries. const results = query.results({ someVariable: 'value of someVariable', anotherVariable: 'value of anotherVariable' }).bindings() Query.update(metadata: object) \u00b6 Updates the metadata for the saved query. This does not result in a new query version. It requires similar options to that of Query.addQuery . Arguments \u00b6 At least one of the following arguments is required to update the metadata. Any argument given will be copied from the previous version of that query. accessLevel The access level of the query. The following values are possible: 'private' (default) The dataset can only be accessed by organization members. 'internal' The dataset can only be accessed by users that are logged into the TriplyDB server. 'public' The dataset can be accessed by everybody. autoselectService Whether the SPARQL service is automatically chosen ( true ), or whether a specific SPARQL service is configured ( false ). dataset The Dataset object against which the query is evaluated. description The human-readable description of the query. This typically explains what the query does in natural language. displayName The human-readable name of the query. This name may include spaces and other characters that are not allowed in the URL-friendly name. name The URL-friendly name of the query that is used in URL paths. This name can only include ASCII letters and hyphens. preferredService If the autoselectService is not selected the user can set the preferred service. Query.useVersion(version: number|'latest') \u00b6 A saved query is saved with a version number. Each time the query or the visualization changes the version number is incremented with one. When you want to retrieve a saved query with a particular version you need the useVersion function. The function returns the query object corresponding to that version of the query. If you want to use the latest version of the query you need to set the version argument to 'latest' . Example \u00b6 const user = await triply.getAccount('my-account') const query = await user.getQuery('my-query') const query_1 = await query.useVersion(1) Query.copy(queryName?: string, account?:object, metadataToReplace?: object) \u00b6 Copies a query using either the same name or a new name (if queryName is provided) to the current account or a new account (if accountName is provided) with the same metadata or overwritten metadata (if metadataToReplace is provided) Arguments \u00b6 queryName An optional parameter. The new URL-friendly name given to the duplicated query that is used in URL paths. This name can only include ASCII letters and hyphens. Defaults to the original query name. account An optional parameter.Expected to be either an User or an Organization object if provided. The new account to which the query will be copied to. Defaults to the current account metadataToReplace An optional metadata object with optionl properties that can be provided to override any of the existing metadata of the duplicated query if required accessLevel The access level of the query. The following values are possible: 'private' (default) The dataset can only be accessed by organization members. 'internal' The dataset can only be accessed by users that are logged into the TriplyDB server. 'public' The dataset can be accessed by everybody. queryString: string the SPARQL compliant query as a string value output: string The visualization plugin that is used to display the result set. If none is set it defaults to 'table' . Other options may include: 'response' , 'geo' , 'gallery' , 'markup' , etc dataset: Dataset The Dataset object against which the query is evaluated. description: string The human-readable description of the query. This typically explains what the query does in natural language. displayName: string The human-readable name of the query. This name may include spaces and other characters that are not allowed in the URL-friendly name. variables: Variable[] A list of objects with the following keys: IRI variable An object of the form `Variable` (see Account.addQuery() ) serviceType: string (\"speedy\" | \"virtuoso\" | \"jena\" | \"blazegraph\" | \"prolog\") The SPARQL service type the duplicated query needs to be configured to Example \u00b6 const user = await triply.getAccount('my-account') const query = await user.getQuery('my-query') const query_1 = await query.useVersion(1) const orgAccount = await triply.getAccount('org-account'); // Within the same account under a new name const duplicatedQuery_1 = await query.copy('newDuplicateQuery') // To a new account with some metadata overwritten using the same query name const duplicatedQuery_2 = await query.copy(undefined, orgAccount , { description: 'newDescription', displayName: 'newDisplayName' })","title":"Query"},{"location":"triplydb-js/query/#query","text":"A Saved Query is a versioned SPARQL query with its own URL. Using this URL, users are able to view any version of the query and its results. It can also be used to run the query and retrieve the results from a browser or a program, removing the hassle of figuring out how to run a SPARQL query. Saved queries come with a RESTful API that can be configured with the use a SPARQL API variables.","title":"Query"},{"location":"triplydb-js/query/#querydelete","text":"Permanently deletes this query and all of its versions.","title":"Query.delete()"},{"location":"triplydb-js/query/#querygetinfo","text":"The returned dictionary object includes the following keys: accessLevel The access level of the query. The following values are possible: 'private' (default) The dataset can only be accessed by organization members. 'internal' The dataset can only be accessed by users that are logged into the TriplyDB server. 'public' The dataset can be accessed by everybody. autoselectService Whether the SPARQL service is automatically chosen ( true ), or whether a specific SPARQL service is configured ( false ). createdAt The date/time at which the query was created. dataset A dictionary object representing the dataset against which the query is evaluated. description The human-readable description of the query. This typically explains what the query does in natural language. displayName The human-readable name of the query. This name may include spaces and other characters that are not allowed in the URL-friendly name. name The URL-friendly name of the query that is used in URL paths. This name can only include ASCII letters and hyphens. numberOfVersions The number of currently stored versions of this query. owner A dictionary object representing the account (organization or user) to which the query belongs. \ud83d\udea7 link Stores part of the URL to run the query. Please use Query.getRunLink() to obtain the full URL to run the query. service The location of the SPARQL endpoint that is used to run the query. updatedAt The date/time at which the query was last modified.","title":"Query.getInfo()"},{"location":"triplydb-js/query/#querygetstringapivariables-object","text":"Returns the query string of the current version of this query. Optionally, arguments can be specified for the API variables to this query.","title":"Query.getString(apiVariables?: object)"},{"location":"triplydb-js/query/#examples","text":"The following code stores the SPARQL query string for the query object: const queryString = await query.getString()","title":"Examples"},{"location":"triplydb-js/query/#queryaddversionmetadata-object","text":"Adds a new version to the query used. It requires similar options to that of Query.addQuery .","title":"Query.addVersion(metadata: object)"},{"location":"triplydb-js/query/#arguments","text":"At least one of the following arguments is required to create a new version. Any argument not given will be copied from the previous version of that query. queryString: string the SPARQL compliant query as a string value output: string The visualization plugin that is used to display the result set. If none is set it defaults to 'table' . Other options may include: 'response' , 'geo' , 'gallery' , 'markup' , etc. Output will take priority over ldFrame ldFrame: object JSON LD frame object used to transform plain JSON LD into a framed JSON. Will be used only if an output is not provided. variables: Variable[] A list of objects with the following keys: IRI variable An object of the form `Variable` (see Account.addQuery() ) You can see how many versions exist on a query accessing Query.getInfo().numOfVersions You can use a specified version of a query accessing Query.useVersion(x: number)","title":"Arguments"},{"location":"triplydb-js/query/#querygetrunlink","text":"Returns the URL link to run the query. It currently does not support the use of variables.","title":"Query.getRunLink()"},{"location":"triplydb-js/query/#queryresultsapivariables-object-options-object","text":"Query.results() function will automatically return all the results from a saved query. You can retrieve both results from a select or ask query and a construct or describe query. The results are returned as an async iterator . If there are more than 10 000 query results, they could be retrieved using pagination with TriplyDB.js .","title":"Query.results(apiVariables?: object, options?: object)"},{"location":"triplydb-js/query/#examples_1","text":"Get the results of a query by setting a results variable. More specifically, for construct queries you use the statements() call: const triply = App.get({token: process.env.TOKEN}) const account = await triply.getAccount('account-name') const query = await account.getQuery('name-of-some-query') // For select queries you use the `statements()` call: const results = query.results().statements() // For select queries you use the `bindings()` call: const results = query.results().bindings() Additionally, saved queries can have 'API variables' that allow you to specify variables that are used in the query. Thus, if you have query parameters, pass their values as the first argument to results as follows: const triply = App.get({token: process.env.TOKEN}) const account = await triply.getAccount('account-name') const query = await account.getQuery('name-of-some-query') // For SPARQL construct queries. const results = query.results({ someVariable: 'value of someVariable', anotherVariable: 'value of anotherVariable' }).statements() // For SPARQL select queries. const results = query.results({ someVariable: 'value of someVariable', anotherVariable: 'value of anotherVariable' }).bindings()","title":"Examples"},{"location":"triplydb-js/query/#queryupdatemetadata-object","text":"Updates the metadata for the saved query. This does not result in a new query version. It requires similar options to that of Query.addQuery .","title":"Query.update(metadata: object)"},{"location":"triplydb-js/query/#arguments_1","text":"At least one of the following arguments is required to update the metadata. Any argument given will be copied from the previous version of that query. accessLevel The access level of the query. The following values are possible: 'private' (default) The dataset can only be accessed by organization members. 'internal' The dataset can only be accessed by users that are logged into the TriplyDB server. 'public' The dataset can be accessed by everybody. autoselectService Whether the SPARQL service is automatically chosen ( true ), or whether a specific SPARQL service is configured ( false ). dataset The Dataset object against which the query is evaluated. description The human-readable description of the query. This typically explains what the query does in natural language. displayName The human-readable name of the query. This name may include spaces and other characters that are not allowed in the URL-friendly name. name The URL-friendly name of the query that is used in URL paths. This name can only include ASCII letters and hyphens. preferredService If the autoselectService is not selected the user can set the preferred service.","title":"Arguments"},{"location":"triplydb-js/query/#queryuseversionversion-numberlatest","text":"A saved query is saved with a version number. Each time the query or the visualization changes the version number is incremented with one. When you want to retrieve a saved query with a particular version you need the useVersion function. The function returns the query object corresponding to that version of the query. If you want to use the latest version of the query you need to set the version argument to 'latest' .","title":"Query.useVersion(version: number|'latest')"},{"location":"triplydb-js/query/#example","text":"const user = await triply.getAccount('my-account') const query = await user.getQuery('my-query') const query_1 = await query.useVersion(1)","title":"Example"},{"location":"triplydb-js/query/#querycopyqueryname-string-accountobject-metadatatoreplace-object","text":"Copies a query using either the same name or a new name (if queryName is provided) to the current account or a new account (if accountName is provided) with the same metadata or overwritten metadata (if metadataToReplace is provided)","title":"Query.copy(queryName?: string, account?:object, metadataToReplace?: object)"},{"location":"triplydb-js/query/#arguments_2","text":"queryName An optional parameter. The new URL-friendly name given to the duplicated query that is used in URL paths. This name can only include ASCII letters and hyphens. Defaults to the original query name. account An optional parameter.Expected to be either an User or an Organization object if provided. The new account to which the query will be copied to. Defaults to the current account metadataToReplace An optional metadata object with optionl properties that can be provided to override any of the existing metadata of the duplicated query if required accessLevel The access level of the query. The following values are possible: 'private' (default) The dataset can only be accessed by organization members. 'internal' The dataset can only be accessed by users that are logged into the TriplyDB server. 'public' The dataset can be accessed by everybody. queryString: string the SPARQL compliant query as a string value output: string The visualization plugin that is used to display the result set. If none is set it defaults to 'table' . Other options may include: 'response' , 'geo' , 'gallery' , 'markup' , etc dataset: Dataset The Dataset object against which the query is evaluated. description: string The human-readable description of the query. This typically explains what the query does in natural language. displayName: string The human-readable name of the query. This name may include spaces and other characters that are not allowed in the URL-friendly name. variables: Variable[] A list of objects with the following keys: IRI variable An object of the form `Variable` (see Account.addQuery() ) serviceType: string (\"speedy\" | \"virtuoso\" | \"jena\" | \"blazegraph\" | \"prolog\") The SPARQL service type the duplicated query needs to be configured to","title":"Arguments"},{"location":"triplydb-js/query/#example_1","text":"const user = await triply.getAccount('my-account') const query = await user.getQuery('my-query') const query_1 = await query.useVersion(1) const orgAccount = await triply.getAccount('org-account'); // Within the same account under a new name const duplicatedQuery_1 = await query.copy('newDuplicateQuery') // To a new account with some metadata overwritten using the same query name const duplicatedQuery_2 = await query.copy(undefined, orgAccount , { description: 'newDescription', displayName: 'newDisplayName' })","title":"Example"},{"location":"triplydb-js/service/","text":"On this page: Service Service.delete() Examples Service.getInfo() Examples Service.isUpToDate() Synchronization Examples Service.update( opts?) Examples Service.waitUntilRunning() Example Setting up index templates for ElasticSearch service Index templates Component templates Service \u00b6 Service objects describe specific functionalities that can be created over datasets in TriplyDB. Service objects are obtained through the the following methods: Dataset.addService Dataset.getServices A service always has one of the following statuses: Removing The service is being removed. Running The service is running normally. Starting The service is starting up. Stopped The services was stopped in the past. It cannot be used at the moment, but it can be enable again if needed. Stopping The service is currently being stopped. Service.delete() \u00b6 Permanently deletes this service. Examples \u00b6 const user = await triply.getAccount('my-account') const dataset = await user.getDataset('my-dataset') const service = await dataset.addService('my-service') await service.delete() Service.getInfo() \u00b6 Returns information about this service. Information is returned in a dictionary object. Individual keys can be accessed for specific information values. Examples \u00b6 The following snippet prints information about the newly created service (named my-service ): const account = await triply.getAccount() const dataset = await account.getDataset('my-dataset') const service = await dataset.addService('my-service') console.log(await service.getInfo()) Service.isUpToDate() \u00b6 Returns whether this service is synchronized with the dataset contents. Synchronization \u00b6 Because services must be explicitly synchronized in TriplyDB, it is possible to have services that expose an older version of the dataset and services that expose a newer version of the dataset running next to one another. There are two very common use cases for this: The production version of an application or website runs on an older service. The data does not change, so the application keeps working. The acceptance version of the same application or website runs on a newer service. Once the acceptance version is finished, it becomes the production version and a new service for the new acceptance version is created, etc. An old service is used by legacy software. New users are using the newer endpoint over the current version of the data, but a limited number of older users want to use the legacy version. Examples \u00b6 The following code checks whether a specific service is synchronized: const account = await triply.getAccount() const dataset = await account.getDataset('my-dataset') const service = await dataset.ensureService('my-service', {type: 'sparql'}) console.log(await service.isUpToDate()) Service.update( opts?) \u00b6 Synchronizes the service. Synchronization means that the data that is used in the service is made consistent with the data that is present in the graphs of the dataset. When one or more graphs are added or deleted, existing services keep exposing the old state of the data. The changes in the data are only exposed in the services after synchronization is performed. You can choose to perform a rolling update, during which a new service is replacing the old one, in order to eliminate downtime. For a rolling update you should use the object {rollingUpdate:true} as opts . In case you want to be able to track the progress of the rolling update, you can use a logging function in the second argument of opts , called onProgress . However, be aware that when using a rolling update, depending on your dataset statements, you might reach your instance's limit of statements' number for services. In this case, you will not be able to do a successful rolling update and you should switch to a regular update. Examples \u00b6 When there are multiple services, it is common to synchronize them all in sequence . This ensures that there are always one or more services available. This allows applications to use such services as their backend without any downtime during data changes. The following code synchronizes all services of a dataset in sequence: for (const service of await dataset.getServices()) { service.update() } // For a rolling update for (const service of await dataset.getServices()) { service.update({ rollingUpdate: true }) } Although less common, it is also possible to synchronize all services of a dataset in parallel . This is typically not used in production systems, where data changes must not result in any downtime. Still, parallel synchronization can be useful in development and/or acceptance environments. The following code synchronizes all services of a dataset in parallel: await Promise.all(dataset.getServices().map(service => service.update())) Service.waitUntilRunning() \u00b6 A service can be stopped or updated. The use of asynchronous code means that when a start command is given it takes a while before the service is ready for use. To make sure a service is available for querying you can user the function waitUntilRunning() to make sure that the script will wait until the service is ready for use. Example \u00b6 An example of a service being updated and afterwards a query needs to be executed: const triply = App.get({ token: process.env.TOKEN }) const user = await triply.getAccount() const dataset = await user.getDataset('some-dataset') const service = await dataset.getService('some-service') // starting a service but does not wait until it is started await service.start() // Function that checks if a service is available await service.waitUntilRunning() Setting up index templates for ElasticSearch service \u00b6 TriplyDB allows you to configure a custom mapping for Elasticsearch services in TriplyDB using index templates. Index templates \u00b6 Index templates make it possible to create indices with user defined configuration, which an index can then pull from. A template will be defined with a name pattern and some configuration in it. If the name of the index matches the template\u2019s naming pattern, the new index will be created with the configuration defined in the template. Official documentation from ElasticSearch on how to use Index templates can be found here . Index templates on TriplyDB can be configured through either TriplyDB API or TriplyDB.js. When creating a new service for the dataset, we add the config object to the metadata: Dataset.addService(\"SERVICE_NAME\", { type: \"elasticSearch\", config: { indexTemplates: [ { \"index_patterns\": \"index\", \"name\": \"TEMPLATE_NAME\", ... } ] } }) index_patterns and name are obligatory fields to include in the body of index template. It's important that every index template has the field index_patterns equal index ! Below is an example of creating an index template in TriplyDB-JS: import App from '@triply/triplydb/App.js' import dotenv from 'dotenv' dotenv.config() const app = App.get({ token: process.env.TRIPLYDB_TOKEN }) const account = await app.getAccount('ACCOUNT') const dataset = await account.getDataset('DATASET') await dataset.addService('SERVICE_NAME', { \"type\": \"elasticSearch\", \"config\": { \"indexTemplates\": [ { \"name\": \"TEMPLATE_NAME\", \"index_patterns\": \"index\" } ] } }) Component templates \u00b6 Component templates are building blocks for constructing index templates that specify index mappings, settings, and aliases. You can find the official documentation on their use in ElasticSearch here . They can be configured through either TriplyDB API or TriplyDB-JS . When creating a new service for the dataset, we add the config object to the metadata: Dataset.addService(\"SERVICE_NAME\", { type: \"elasticSearch\", config: { componentTemplates: [ { \"name\": \"TEMPLATE_NAME\", \"template\": { \"mappings\": { \"properties\": { ... } } }, ... } ] } }) name and template are obligatory fields to include in the body of component template. Component template can only be created together with an index template. In this case Index template needs to contain the field composed_of with the name of the component template. Below is an example of creating a component template for the property https://schema.org/dateCreated to be of type date . import App from '@triply/triplydb/App.js' import dotenv from 'dotenv' dotenv.config() const app = App.get({ token: process.env.TRIPLYDB_TOKEN }) const account = await app.getAccount('ACCOUNT') const dataset = await account.getDataset('DATASET') await dataset.addService('SERVICE_NAME', { \"type\": \"elasticSearch\", \"config\": { \"indexTemplates\": [ { \"name\": \"TEMPLATE_NAME\", \"index_patterns\": \"index\", \"composed_of\": [\"COMPONENT_TEMPLATE_NAME\"], } ], \"componentTemplates\": [ { \"name\": \"COMPONENT_TEMPLATE_NAME\", \"template\": { \"mappings\": { \"properties\": { \"https://schema org/dateCreated\": { \"type\": \"date\" } } } } } ] } })","title":"Service"},{"location":"triplydb-js/service/#service","text":"Service objects describe specific functionalities that can be created over datasets in TriplyDB. Service objects are obtained through the the following methods: Dataset.addService Dataset.getServices A service always has one of the following statuses: Removing The service is being removed. Running The service is running normally. Starting The service is starting up. Stopped The services was stopped in the past. It cannot be used at the moment, but it can be enable again if needed. Stopping The service is currently being stopped.","title":"Service"},{"location":"triplydb-js/service/#servicedelete","text":"Permanently deletes this service.","title":"Service.delete()"},{"location":"triplydb-js/service/#examples","text":"const user = await triply.getAccount('my-account') const dataset = await user.getDataset('my-dataset') const service = await dataset.addService('my-service') await service.delete()","title":"Examples"},{"location":"triplydb-js/service/#servicegetinfo","text":"Returns information about this service. Information is returned in a dictionary object. Individual keys can be accessed for specific information values.","title":"Service.getInfo()"},{"location":"triplydb-js/service/#examples_1","text":"The following snippet prints information about the newly created service (named my-service ): const account = await triply.getAccount() const dataset = await account.getDataset('my-dataset') const service = await dataset.addService('my-service') console.log(await service.getInfo())","title":"Examples"},{"location":"triplydb-js/service/#serviceisuptodate","text":"Returns whether this service is synchronized with the dataset contents.","title":"Service.isUpToDate()"},{"location":"triplydb-js/service/#synchronization","text":"Because services must be explicitly synchronized in TriplyDB, it is possible to have services that expose an older version of the dataset and services that expose a newer version of the dataset running next to one another. There are two very common use cases for this: The production version of an application or website runs on an older service. The data does not change, so the application keeps working. The acceptance version of the same application or website runs on a newer service. Once the acceptance version is finished, it becomes the production version and a new service for the new acceptance version is created, etc. An old service is used by legacy software. New users are using the newer endpoint over the current version of the data, but a limited number of older users want to use the legacy version.","title":"Synchronization"},{"location":"triplydb-js/service/#examples_2","text":"The following code checks whether a specific service is synchronized: const account = await triply.getAccount() const dataset = await account.getDataset('my-dataset') const service = await dataset.ensureService('my-service', {type: 'sparql'}) console.log(await service.isUpToDate())","title":"Examples"},{"location":"triplydb-js/service/#serviceupdate-opts","text":"Synchronizes the service. Synchronization means that the data that is used in the service is made consistent with the data that is present in the graphs of the dataset. When one or more graphs are added or deleted, existing services keep exposing the old state of the data. The changes in the data are only exposed in the services after synchronization is performed. You can choose to perform a rolling update, during which a new service is replacing the old one, in order to eliminate downtime. For a rolling update you should use the object {rollingUpdate:true} as opts . In case you want to be able to track the progress of the rolling update, you can use a logging function in the second argument of opts , called onProgress . However, be aware that when using a rolling update, depending on your dataset statements, you might reach your instance's limit of statements' number for services. In this case, you will not be able to do a successful rolling update and you should switch to a regular update.","title":"Service.update( opts?)"},{"location":"triplydb-js/service/#examples_3","text":"When there are multiple services, it is common to synchronize them all in sequence . This ensures that there are always one or more services available. This allows applications to use such services as their backend without any downtime during data changes. The following code synchronizes all services of a dataset in sequence: for (const service of await dataset.getServices()) { service.update() } // For a rolling update for (const service of await dataset.getServices()) { service.update({ rollingUpdate: true }) } Although less common, it is also possible to synchronize all services of a dataset in parallel . This is typically not used in production systems, where data changes must not result in any downtime. Still, parallel synchronization can be useful in development and/or acceptance environments. The following code synchronizes all services of a dataset in parallel: await Promise.all(dataset.getServices().map(service => service.update()))","title":"Examples"},{"location":"triplydb-js/service/#servicewaituntilrunning","text":"A service can be stopped or updated. The use of asynchronous code means that when a start command is given it takes a while before the service is ready for use. To make sure a service is available for querying you can user the function waitUntilRunning() to make sure that the script will wait until the service is ready for use.","title":"Service.waitUntilRunning()"},{"location":"triplydb-js/service/#example","text":"An example of a service being updated and afterwards a query needs to be executed: const triply = App.get({ token: process.env.TOKEN }) const user = await triply.getAccount() const dataset = await user.getDataset('some-dataset') const service = await dataset.getService('some-service') // starting a service but does not wait until it is started await service.start() // Function that checks if a service is available await service.waitUntilRunning()","title":"Example"},{"location":"triplydb-js/service/#setting-up-index-templates-for-elasticsearch-service","text":"TriplyDB allows you to configure a custom mapping for Elasticsearch services in TriplyDB using index templates.","title":"Setting up index templates for ElasticSearch service"},{"location":"triplydb-js/service/#index-templates","text":"Index templates make it possible to create indices with user defined configuration, which an index can then pull from. A template will be defined with a name pattern and some configuration in it. If the name of the index matches the template\u2019s naming pattern, the new index will be created with the configuration defined in the template. Official documentation from ElasticSearch on how to use Index templates can be found here . Index templates on TriplyDB can be configured through either TriplyDB API or TriplyDB.js. When creating a new service for the dataset, we add the config object to the metadata: Dataset.addService(\"SERVICE_NAME\", { type: \"elasticSearch\", config: { indexTemplates: [ { \"index_patterns\": \"index\", \"name\": \"TEMPLATE_NAME\", ... } ] } }) index_patterns and name are obligatory fields to include in the body of index template. It's important that every index template has the field index_patterns equal index ! Below is an example of creating an index template in TriplyDB-JS: import App from '@triply/triplydb/App.js' import dotenv from 'dotenv' dotenv.config() const app = App.get({ token: process.env.TRIPLYDB_TOKEN }) const account = await app.getAccount('ACCOUNT') const dataset = await account.getDataset('DATASET') await dataset.addService('SERVICE_NAME', { \"type\": \"elasticSearch\", \"config\": { \"indexTemplates\": [ { \"name\": \"TEMPLATE_NAME\", \"index_patterns\": \"index\" } ] } })","title":"Index templates"},{"location":"triplydb-js/service/#component-templates","text":"Component templates are building blocks for constructing index templates that specify index mappings, settings, and aliases. You can find the official documentation on their use in ElasticSearch here . They can be configured through either TriplyDB API or TriplyDB-JS . When creating a new service for the dataset, we add the config object to the metadata: Dataset.addService(\"SERVICE_NAME\", { type: \"elasticSearch\", config: { componentTemplates: [ { \"name\": \"TEMPLATE_NAME\", \"template\": { \"mappings\": { \"properties\": { ... } } }, ... } ] } }) name and template are obligatory fields to include in the body of component template. Component template can only be created together with an index template. In this case Index template needs to contain the field composed_of with the name of the component template. Below is an example of creating a component template for the property https://schema.org/dateCreated to be of type date . import App from '@triply/triplydb/App.js' import dotenv from 'dotenv' dotenv.config() const app = App.get({ token: process.env.TRIPLYDB_TOKEN }) const account = await app.getAccount('ACCOUNT') const dataset = await account.getDataset('DATASET') await dataset.addService('SERVICE_NAME', { \"type\": \"elasticSearch\", \"config\": { \"indexTemplates\": [ { \"name\": \"TEMPLATE_NAME\", \"index_patterns\": \"index\", \"composed_of\": [\"COMPONENT_TEMPLATE_NAME\"], } ], \"componentTemplates\": [ { \"name\": \"COMPONENT_TEMPLATE_NAME\", \"template\": { \"mappings\": { \"properties\": { \"https://schema org/dateCreated\": { \"type\": \"date\" } } } } } ] } })","title":"Component templates"},{"location":"triplydb-js/story/","text":"On this page: Story Story.delete() Examples Story.getInfo() Examples Story.setBanner(file: string) Examples Story \u00b6 A TriplyDB data story is a way of communicating information about your linked data along with explanatory text while also being able to integrate query results. To create Data stories with TriplyDB.js You can use the User.ensureStory or User.addStory functions to create. If you want to retrieve an already created data story you can use the functions User.getStories to iterate over all stories, or retrieve a particular one with User.getStory . Story objects are obtained through the the following methods: User.addStory User.ensureStory User.getStories User.getStory Story.delete() \u00b6 Deletes this story. This deletes all paragraphs that belong to this story. This does not delete the queries that are linked into this story. If you also want to delete the queries, then this must be done with distinct calls of Query.delete() . Examples \u00b6 The following code example deletes a story called 'example-story' under the current user's account: const user = await triply.getUser() const story = await user.getStory('example-story') await story.delete() Story.getInfo() \u00b6 Returns information about this data story. Information is returned in a dictionary object. Individual keys can be accessed for specific information values. Examples \u00b6 The following snippet prints the paragraphs that appear in a data story: for (const element of (await story.getInfo()).content) { if ((element.type = 'paragraph')) { console.log(element.paragraph) } } Story.setBanner(file: string) \u00b6 Sets a new banner for the story. Examples \u00b6 The following snippet uploads the local image in file banner.webp and sets it as the banner image for the story: const user = await triply.getUser() const story = await user.getStory('example-story') await story.setBanner(\"banner.webp\");","title":"Story"},{"location":"triplydb-js/story/#story","text":"A TriplyDB data story is a way of communicating information about your linked data along with explanatory text while also being able to integrate query results. To create Data stories with TriplyDB.js You can use the User.ensureStory or User.addStory functions to create. If you want to retrieve an already created data story you can use the functions User.getStories to iterate over all stories, or retrieve a particular one with User.getStory . Story objects are obtained through the the following methods: User.addStory User.ensureStory User.getStories User.getStory","title":"Story"},{"location":"triplydb-js/story/#storydelete","text":"Deletes this story. This deletes all paragraphs that belong to this story. This does not delete the queries that are linked into this story. If you also want to delete the queries, then this must be done with distinct calls of Query.delete() .","title":"Story.delete()"},{"location":"triplydb-js/story/#examples","text":"The following code example deletes a story called 'example-story' under the current user's account: const user = await triply.getUser() const story = await user.getStory('example-story') await story.delete()","title":"Examples"},{"location":"triplydb-js/story/#storygetinfo","text":"Returns information about this data story. Information is returned in a dictionary object. Individual keys can be accessed for specific information values.","title":"Story.getInfo()"},{"location":"triplydb-js/story/#examples_1","text":"The following snippet prints the paragraphs that appear in a data story: for (const element of (await story.getInfo()).content) { if ((element.type = 'paragraph')) { console.log(element.paragraph) } }","title":"Examples"},{"location":"triplydb-js/story/#storysetbannerfile-string","text":"Sets a new banner for the story.","title":"Story.setBanner(file: string)"},{"location":"triplydb-js/story/#examples_2","text":"The following snippet uploads the local image in file banner.webp and sets it as the banner image for the story: const user = await triply.getUser() const story = await user.getStory('example-story') await story.setBanner(\"banner.webp\");","title":"Examples"},{"location":"triplydb-js/user/","text":"On this page: User Obtaining instances Inheritance Limitations User.addDataset(name: string, metadata?: object) User.addQuery(metadata: object) User.ensureStory(name: string, metadata: object) User.addStory(name: string, metadata?: object) User.createOrganization(name: string, metadata?: object) Access restrictions Arguments Examples User.ensureDataset(name: string, metadata?: object) User.getDataset(name: string) User.getDatasets() User.getInfo() Examples User.getOrganizations() Order considerations Examples See also User.getPinnedItems() User.setAvatar(file: string) User.update(metadata: object) User \u00b6 Instances of class User denote users in TriplyDB. Obtaining instances \u00b6 Users are obtained with method App.getUser(name?: string) : const user = triply.getUser('john-doe') const user = triply.getUser() Alternatively, users are obtained by first obtaining an account ( App.getAccount(name?: string) ) and then casting it to a use ( Account.asUser() ): const account = await triply.getAccount('john-doe') const user = account.asUser() Inheritance \u00b6 User is a subclass of Account , from which it inherits most of its methods. Limitations \u00b6 Users cannot be created or deleted through the TriplyDB.js library. See the Triply Console documentation for how to create and delete users through the web-based GUI. User.addDataset(name: string, metadata?: object) \u00b6 Adds a new TriplyDB dataset with the given name to the current account. Inherited from Account.addDataset(name: string, metadata?: object) . User.addQuery(metadata: object) \u00b6 Adds a new TriplyDB query to the current user. Inherited from Account.addQuery(name:string, metadata: object) . User.ensureStory(name: string, metadata: object) \u00b6 Ensures the existence of a story with the given name and with the specified metadata . Inherited from Account.ensureStory(name: string, metadata: object) . User.addStory(name: string, metadata?: object) \u00b6 Adds a new TriplyDB story with the given name to the current user. Inherited from Account.addStory(name: string, metadata?: object) . User.createOrganization(name: string, metadata?: object) \u00b6 Creates a new organization for which this user will be the owner. Access restrictions \u00b6 This method requires an API token with write access for this user. Arguments \u00b6 Argument name is the URL-friendly name of the new organization. This name can only contain alphanumeric characters and hyphens ( [A-Za-z0-9\\-] ). The optional metadata argument can be used to specify additional metadata. This is a dictionary object with the following optional keys: description The description of the organization. This description can make use of Markdown. email The email address at which the organization can be reached. name The human-readable name of the organization. This name may contain spaces and other non-alphanumeric characters. Examples \u00b6 The following snippet creates a new organization for which John Doe will be the owner. Notice that both a required URL-friendly name ( 'my-organization' ) and an optional display name ( 'My Organization' ) are specified. const user = await triply.getUser('john-doe') await user.createOrganization(my-organization, {name: 'My Organization'})) User.ensureDataset(name: string, metadata?: object) \u00b6 Ensures the existence of a dataset with the given name and with the specified metadata . Inherited from Account.ensureDataset(name: string, metadata?: object) . User.getDataset(name: string) \u00b6 Returns the TriplyDB dataset with the given name that is published by this user. Inherited from Account.getDataset(name: string) . User.getDatasets() \u00b6 Returns an async iterator over the accessible datasets for the current user. Inherited from Account.getDatasets() . User.getInfo() \u00b6 Returns information about this user. Information is returned in a dictionary object. Individual keys can be accessed for specific information values. The information object for users includes the following keys: avatarUrl A URL to the user image. accountName The URL-friendly name of the user. name The human-readable display name of the user description The human-readable description of the user. createdAt The date and time on which the user was created. datasetCount The number of datasets for the user. queryCount The number of queries for the user. storyCount The number of stories for the user pinnedItems An array containing the pinned items (datasets, stories and queries) for the user. role The role of the user. Either 'light', 'regular' or 'siteAdmin'. orgs An array of organizations of which the user is a member. Email address The email address of the user. updatedAt The date and time on which the user was last updated. lastActivity The date and time on which the user was last online on TriplyDB. Examples \u00b6 The following snippet prints an overview of account that is associated with the used API token: const user = await triply.getUser() console.log(await user.getInfo()) User.getOrganizations() \u00b6 Returns an async iterator over the organizations that this user is a member of. Order considerations \u00b6 The order in the list reflects the order in which the organizations appear on the user page in the Triply GUI. Examples \u00b6 The following snippet prints the list of organizations that John Doe is a member of: const user = await triply.getUser('john-doe') for await (const organization of await user.getOrganizations()) { console.log((await organization.getInfo()).name) } See also \u00b6 The async iterator contains organization objects. See the section about the Organization class for methods that can be used on such objects. User.getPinnedItems() \u00b6 Returns the list of datasets, stories and queries that are pinned for the current user. Inherited from Account.getPinnedItems() . User.setAvatar(file: string) \u00b6 Sets a new image that characterized this user. Inherited from Account.setAvatar(file: string) . User.update(metadata: object) \u00b6 Updates the metadata for this user. Inherited from Account.update(metadata: object) .","title":"User"},{"location":"triplydb-js/user/#user","text":"Instances of class User denote users in TriplyDB.","title":"User"},{"location":"triplydb-js/user/#obtaining-instances","text":"Users are obtained with method App.getUser(name?: string) : const user = triply.getUser('john-doe') const user = triply.getUser() Alternatively, users are obtained by first obtaining an account ( App.getAccount(name?: string) ) and then casting it to a use ( Account.asUser() ): const account = await triply.getAccount('john-doe') const user = account.asUser()","title":"Obtaining instances"},{"location":"triplydb-js/user/#inheritance","text":"User is a subclass of Account , from which it inherits most of its methods.","title":"Inheritance"},{"location":"triplydb-js/user/#limitations","text":"Users cannot be created or deleted through the TriplyDB.js library. See the Triply Console documentation for how to create and delete users through the web-based GUI.","title":"Limitations"},{"location":"triplydb-js/user/#useradddatasetname-string-metadata-object","text":"Adds a new TriplyDB dataset with the given name to the current account. Inherited from Account.addDataset(name: string, metadata?: object) .","title":"User.addDataset(name: string, metadata?: object)"},{"location":"triplydb-js/user/#useraddquerymetadata-object","text":"Adds a new TriplyDB query to the current user. Inherited from Account.addQuery(name:string, metadata: object) .","title":"User.addQuery(metadata: object)"},{"location":"triplydb-js/user/#userensurestoryname-string-metadata-object","text":"Ensures the existence of a story with the given name and with the specified metadata . Inherited from Account.ensureStory(name: string, metadata: object) .","title":"User.ensureStory(name: string, metadata: object)"},{"location":"triplydb-js/user/#useraddstoryname-string-metadata-object","text":"Adds a new TriplyDB story with the given name to the current user. Inherited from Account.addStory(name: string, metadata?: object) .","title":"User.addStory(name: string, metadata?: object)"},{"location":"triplydb-js/user/#usercreateorganizationname-string-metadata-object","text":"Creates a new organization for which this user will be the owner.","title":"User.createOrganization(name: string, metadata?: object)"},{"location":"triplydb-js/user/#access-restrictions","text":"This method requires an API token with write access for this user.","title":"Access restrictions"},{"location":"triplydb-js/user/#arguments","text":"Argument name is the URL-friendly name of the new organization. This name can only contain alphanumeric characters and hyphens ( [A-Za-z0-9\\-] ). The optional metadata argument can be used to specify additional metadata. This is a dictionary object with the following optional keys: description The description of the organization. This description can make use of Markdown. email The email address at which the organization can be reached. name The human-readable name of the organization. This name may contain spaces and other non-alphanumeric characters.","title":"Arguments"},{"location":"triplydb-js/user/#examples","text":"The following snippet creates a new organization for which John Doe will be the owner. Notice that both a required URL-friendly name ( 'my-organization' ) and an optional display name ( 'My Organization' ) are specified. const user = await triply.getUser('john-doe') await user.createOrganization(my-organization, {name: 'My Organization'}))","title":"Examples"},{"location":"triplydb-js/user/#userensuredatasetname-string-metadata-object","text":"Ensures the existence of a dataset with the given name and with the specified metadata . Inherited from Account.ensureDataset(name: string, metadata?: object) .","title":"User.ensureDataset(name: string, metadata?: object)"},{"location":"triplydb-js/user/#usergetdatasetname-string","text":"Returns the TriplyDB dataset with the given name that is published by this user. Inherited from Account.getDataset(name: string) .","title":"User.getDataset(name: string)"},{"location":"triplydb-js/user/#usergetdatasets","text":"Returns an async iterator over the accessible datasets for the current user. Inherited from Account.getDatasets() .","title":"User.getDatasets()"},{"location":"triplydb-js/user/#usergetinfo","text":"Returns information about this user. Information is returned in a dictionary object. Individual keys can be accessed for specific information values. The information object for users includes the following keys: avatarUrl A URL to the user image. accountName The URL-friendly name of the user. name The human-readable display name of the user description The human-readable description of the user. createdAt The date and time on which the user was created. datasetCount The number of datasets for the user. queryCount The number of queries for the user. storyCount The number of stories for the user pinnedItems An array containing the pinned items (datasets, stories and queries) for the user. role The role of the user. Either 'light', 'regular' or 'siteAdmin'. orgs An array of organizations of which the user is a member. Email address The email address of the user. updatedAt The date and time on which the user was last updated. lastActivity The date and time on which the user was last online on TriplyDB.","title":"User.getInfo()"},{"location":"triplydb-js/user/#examples_1","text":"The following snippet prints an overview of account that is associated with the used API token: const user = await triply.getUser() console.log(await user.getInfo())","title":"Examples"},{"location":"triplydb-js/user/#usergetorganizations","text":"Returns an async iterator over the organizations that this user is a member of.","title":"User.getOrganizations()"},{"location":"triplydb-js/user/#order-considerations","text":"The order in the list reflects the order in which the organizations appear on the user page in the Triply GUI.","title":"Order considerations"},{"location":"triplydb-js/user/#examples_2","text":"The following snippet prints the list of organizations that John Doe is a member of: const user = await triply.getUser('john-doe') for await (const organization of await user.getOrganizations()) { console.log((await organization.getInfo()).name) }","title":"Examples"},{"location":"triplydb-js/user/#see-also","text":"The async iterator contains organization objects. See the section about the Organization class for methods that can be used on such objects.","title":"See also"},{"location":"triplydb-js/user/#usergetpinneditems","text":"Returns the list of datasets, stories and queries that are pinned for the current user. Inherited from Account.getPinnedItems() .","title":"User.getPinnedItems()"},{"location":"triplydb-js/user/#usersetavatarfile-string","text":"Sets a new image that characterized this user. Inherited from Account.setAvatar(file: string) .","title":"User.setAvatar(file: string)"},{"location":"triplydb-js/user/#userupdatemetadata-object","text":"Updates the metadata for this user. Inherited from Account.update(metadata: object) .","title":"User.update(metadata: object)"},{"location":"triplydb-technical-changelog/","text":"On this page: 26.2.200 26.2.100 26.1.200 26.1.100 25.12.100 25.11.200 25.11.100 25.10.300 25.10.200 25.10.100 25.9.100 25.8.200 25.08.100 25.7.100 25.6.200 25.6.100 25.5.100 25.4.300 25.4.200 25.4.100 25.3.200 25.3.100 25.2.200 25.2.100 25.1.200 25.1.100 24.12.200 24.12.104 24.11.200 24.11.100 24.10.200 24.10.100 24.9.200 24.9.100 24.08.1 24.08.0 24.07.1 24.07.0 SAML 24.06.1 24.06.0 Version tags This changelog covers technical changes related to TriplyDB on-premise deployments. See here for the TriplyDB changelog that is user facing. 26.2.200 \u00b6 Release date: 2026-02-18 Breaking The ingress.proxyCacheName values is unused and has been removed. No migration needed, except for removing this property if it's set. Next to Nginx, we now include full support for HAProxy ingress controllers as well. Ingress templates have been refactored to support both nginx and HAProxy ingress controllers. A new ingress.controller property (default: \"nginx\" ) controls which controller-specific annotations are generated. HAProxy support covers both the HAProxy Kubernetes Ingress Controller ( haproxy.org/* ) and OpenShift HAProxy Router ( haproxy.router.openshift.io/* ). Fixed the console-api-proxy ingress rewrite rule for nginx. The path type is now ImplementationSpecific with regex pattern /_api/(.*) instead of a plain Prefix match, ensuring the rewrite-target correctly strips the /_api/ prefix. 26.2.100 \u00b6 Release date: 2026-02-04 None 26.1.200 \u00b6 Release date: 2026-01-21 None 26.1.100 \u00b6 Release date: 2026-01-07 API network policy updated to allow inter-API pod communication on the private port. API pods can now send requests to other API pods within the same namespace. This change supports distributed coordination features. Kubernetes lease manager watch restart delay reduced from 5-10 seconds to 1 second, improving leader election responsiveness and reducing failover time for distributed coordination tasks. 25.12.100 \u00b6 Release date: 2025-12-10 None 25.11.200 \u00b6 Release date: 2025-11-27 Added ServiceMonitor support for environments with a Prometheus Operator deployement. Configure via api.metrics.serviceMonitor.enabled (default: false ). When enabled, a ServiceMonitor resource is created with configurable scrape interval, timeout, and labels, allowing automatic scraping of TriplyDB metrics. API network policy updated to allow Prometheus ingress when ServiceMonitor is enabled. The network policy configuration is specified via api.metrics.serviceMonitor.networkPolicy.prometheusIngressSelector to specify which namespace/pod combination is allowed to scrape the metrics endpoint. 25.11.100 \u00b6 Release date: 2025-11-12 API service account RBAC permissions expanded: added deployments resource permissions ( get , list ) for apps API group better monitoring and management. PersistentVolumeClaim and Service creation now support owner references for proper garbage collection. This ensures Kubernetes automatically cleans up related resources when parent resources are deleted. 25.10.300 \u00b6 Release date: 2025-10-29 SAML authentication attribute mapping configuration changed. The auth.saml[*].attributeMappings structure now supports additional properties for attribute updates. Added allowOverwrite boolean property to control whether attributes can be updated on subsequent logins (default: false ). Added expiresAt and displayedName attribute mappings for temporary user accounts. Existing SAML configurations will continue to work as is Added indexJobs.maxJsonLdFileSizeMb configuration property (default: 50 ). This limits the maximum size of JSON-LD files during index job processing to prevent memory issues with very large files. The environment API variable TRIPLY__JSONLD_MAX_BYTES_SIZE is not allowed anymore. Removed speedy.maxNumberOfStatementsInMemory configuration property. Memory handling for SPARQL operations is now managed automatically without a fixed statement limit, improving support for large result sets. 25.10.200 \u00b6 Release date: 2025-10-16 Node.js upgraded from version 22 to version 24. All TriplyDB container images now use Node.js 24 as the base runtime. This affects all pods including API, console, orchestrator, and job runners. Redis upgraded from version 7.4.2 to 8.2.2 to address security vulnerabilities. Note that the security vulnerability in question could not have been abused in the context of the TriplyDB deployment. Added indexJobs.legacyDelimitedFormat configuration property (default: true ). This controls CSV/TSV import behavior. When set to true , uses the legacy simple format. When set to false , uses the new format which supports more complex table structures but produces more verbose RDF output. 25.10.100 \u00b6 Release date: 2025-10-02 API service account RBAC permissions changed: added leases resource permissions ( get , list , watch , create , update , patch ) for coordination.k8s.io API group. This enables proper Kubernetes leader election coordination for watching job events. Orchestrator service account removed: orchestrator pods now use default namespace permissions instead of dedicated service account. The orchestrator deployment no longer specifies a serviceAccountName . 25.9.100 \u00b6 Release date: 2025-09-18 None 25.8.200 \u00b6 Release date: 2025-08-28 None 25.08.100 \u00b6 Release date: 2025-08-06 None 25.7.100 \u00b6 Release date: 2025-07-16 None 25.6.200 \u00b6 Release date: 2025-06-26 None 25.6.100 \u00b6 Release date: 2025-06-05 None 25.5.100 \u00b6 Release date: 2025-05-14 None 25.4.300 \u00b6 Release date: 2025-04-28 None 25.4.200 \u00b6 Release date: 2025-04-16 None 25.4.100 \u00b6 Release date: 2025-04-03 None 25.3.200 \u00b6 Release date: 2025-04-20 None 25.3.100 \u00b6 Release date: 2025-03-06 Features SAML can now be configured through native helm values, instead of using environment variables for the API pod. I.e., A configuration like this: api: env: TRIPLY__SAML__0__ENTRY_POINT: some_entrypoint_url TRIPLY__SAML__0__IDP: some_idp_id TRIPLY__SAML__0__IDP_ATTRIBUTE_MAPPINGS__EMAIL__KEY: http://schemas.xmlsoap.org/ws/2005/05/identity/claims/emailaddress TRIPLY__SAML__0__IDP_ATTRIBUTE_MAPPINGS__NAME__KEY: http://schemas.xmlsoap.org/ws/2005/05/identity/claims/nameidentifier TRIPLY__SAML__0__ISSUER: some_issuer_id TRIPLY__SAML__0__LABEL: Login with SAML TRIPLY__SAML__0__SIGNATURE_ALGORITHM: RSA_SHA256 TRIPLY__SAML__0__SUPPORT_IDP_LOGOUT: \"false\" TRIPLY__SAML__0__WANT_AUTHN_RESPONSE_SIGNED: \"false\" TRIPLY__SAML__0__IDP_CERT: valueFrom: secretKeyRef: name: saml-credentials-0 key: CERT TRIPLY__SAML__0__PRIVATE_KEY: valueFrom: secretKeyRef: name: saml-credentials-0 key: PRIVATE_KEY TRIPLY__SAML__0__PUBLIC_CERT: valueFrom: secretKeyRef: name: saml-credentials-0 key: SIGNING_CERT TRIPLY__SAML__0__DECRYPTION_CERT: valueFrom: secretKeyRef: name: saml-credentials-0 key: DECRYPTION_CERT TRIPLY__SAML__0__DECRYPTION_PVK: valueFrom: secretKeyRef: name: saml-credentials-0 key: DECRYPTION_PVK will now be set like this: auth: saml: some_idp_id: entryPoint: some_entrypoint_url issuer: some_issuer_id label: Login with SAML signatureAlgorithm: RSA_SHA256 idpAttributeMappings: email: key: http://schemas.xmlsoap.org/ws/2005/05/identity/claims/emailaddress name: key: http://schemas.xmlsoap.org/ws/2005/05/identity/claims/nameidentifier additionalConfig: wantAuthnResponseSigned: false supportIdpLogout: false idpCertSecret: name: saml-credentials-0 key: CERT spCertSecret: name: saml-sp-certs Note that: The IDP ID is now the key of the helm values object. Certificates can only be set through secrets. For the IDP certificates this can be any secret with any field. For the SP certificate we expect a secret of type TLS , containing the private key and the public certificate. 25.2.200 \u00b6 Release date: 2025-02-20 None 25.2.100 \u00b6 Release date: 2025-02-06 The type tdbServices.customLabels changed from an array to an object. I.e., change this: tdbServices: customLabels: - VAL=KEY to: tdbServices: customLabels: VAL: KEY 25.1.200 \u00b6 Release date: 2025-01-23 disablePreUpgradeHook setting is no longer needed. If you have set this, you can safely remove it 25.1.100 \u00b6 Release date: 2025-01-09 indexJobs.storageClassName and queryJobs.storageClassName are both required properties now. tdbServices.podNodeAffinity and tdbServices.podTolerations now take a yaml string as argument, instead of a yaml object. indexJobs.chunkSize must now be a number and not a string Some properties from the values file are now propagated to the console and API via a configmap. This change only affects on-premise deployments where the helm manifests are post-processed and manipulated. 24.12.200 \u00b6 Release date: 2024-12-18 None 24.12.104 \u00b6 Release date: 2024-12-06 The .disableNetworkPolicies property is removed. Instead, use the networkPolicies.enabled property. The API and console apply stricter network policies. As a consequence, you will need to specify a source selector that references your ingress. See the kubernetes documentation for more info on such selectors. Specify this selector in networkPolicies.ingressSelector . An example definition is the following: networkPolicies: ingressSelector: namespaceSelector: matchLabels: kubernetes.io/metadata.name: ingress-nginx 24.11.200 \u00b6 Release date: 2024-11-22 None 24.11.100 \u00b6 Release date: 2024-11-08 None 24.10.200 \u00b6 Release date: 2024-10-25 The .defaultImageRegistry and triplydbImageRegistry fields are now removed. If you used these fields, then you should instead reference the full image path (without the tag) for the images. If you used .defaultImageRegistry with a custom registry, then set the full image tags for these keys: mongodb.image redis.image kubernetesWaitFor.image If you used .triplydbImageRegistry with a custom registry, then set the full image tags for these keys: api.image console.image orchestrator.image indexJobs.image queryJobs.image tdbServices.virtuoso.image tdbServices.jena.image tdbServices.elastic.image tdbServices.blazegraph.image 24.10.100 \u00b6 Release date: 2024-10-11 None 24.9.200 \u00b6 Release date: 2024-09-27 None 24.9.100 \u00b6 The version scheme for the TriplyDB helm charts changed. This is now reflected in this technical changelog. indexingChunkSize (if present) should move to indexJobs.chunkSize queryJobs.nodeMemoryLimitInGb is renamed to queryJobs.nodejsMemoryLimitInGb 24.08.1 \u00b6 None 24.08.0 \u00b6 The service-orchestrator key is renamed to orchestrator 24.07.1 \u00b6 None 24.07.0 \u00b6 SAML \u00b6 SAML configurations changed as follows: api.env.TRIPLY__SAML__0__SIGNING_CERT should be renamed to TRIPLY__SAML__0__PUBLIC_CERT api.env.TRIPLY__SAML__0__CERT should be renamed to TRIPLY__SAML__0__IDP_CERT 24.06.1 \u00b6 None 24.06.0 \u00b6 Version tags \u00b6 Version tags changed. If you have hardcoded versions in your values file (e.g. api.version or console.version ), then remove the -k8 postfix. E.g., change 24.05.1-k8 to 24.05.1 The tdbServices.[service-name].tag property changed to tdbServices.[service-name].version","title":"TriplyDB technical changelog"},{"location":"triplydb-technical-changelog/#26.2.200","text":"Release date: 2026-02-18 Breaking The ingress.proxyCacheName values is unused and has been removed. No migration needed, except for removing this property if it's set. Next to Nginx, we now include full support for HAProxy ingress controllers as well. Ingress templates have been refactored to support both nginx and HAProxy ingress controllers. A new ingress.controller property (default: \"nginx\" ) controls which controller-specific annotations are generated. HAProxy support covers both the HAProxy Kubernetes Ingress Controller ( haproxy.org/* ) and OpenShift HAProxy Router ( haproxy.router.openshift.io/* ). Fixed the console-api-proxy ingress rewrite rule for nginx. The path type is now ImplementationSpecific with regex pattern /_api/(.*) instead of a plain Prefix match, ensuring the rewrite-target correctly strips the /_api/ prefix.","title":"26.2.200"},{"location":"triplydb-technical-changelog/#26.2.100","text":"Release date: 2026-02-04 None","title":"26.2.100"},{"location":"triplydb-technical-changelog/#26.1.200","text":"Release date: 2026-01-21 None","title":"26.1.200"},{"location":"triplydb-technical-changelog/#26.1.100","text":"Release date: 2026-01-07 API network policy updated to allow inter-API pod communication on the private port. API pods can now send requests to other API pods within the same namespace. This change supports distributed coordination features. Kubernetes lease manager watch restart delay reduced from 5-10 seconds to 1 second, improving leader election responsiveness and reducing failover time for distributed coordination tasks.","title":"26.1.100"},{"location":"triplydb-technical-changelog/#25.12.100","text":"Release date: 2025-12-10 None","title":"25.12.100"},{"location":"triplydb-technical-changelog/#25.11.200","text":"Release date: 2025-11-27 Added ServiceMonitor support for environments with a Prometheus Operator deployement. Configure via api.metrics.serviceMonitor.enabled (default: false ). When enabled, a ServiceMonitor resource is created with configurable scrape interval, timeout, and labels, allowing automatic scraping of TriplyDB metrics. API network policy updated to allow Prometheus ingress when ServiceMonitor is enabled. The network policy configuration is specified via api.metrics.serviceMonitor.networkPolicy.prometheusIngressSelector to specify which namespace/pod combination is allowed to scrape the metrics endpoint.","title":"25.11.200"},{"location":"triplydb-technical-changelog/#25.11.100","text":"Release date: 2025-11-12 API service account RBAC permissions expanded: added deployments resource permissions ( get , list ) for apps API group better monitoring and management. PersistentVolumeClaim and Service creation now support owner references for proper garbage collection. This ensures Kubernetes automatically cleans up related resources when parent resources are deleted.","title":"25.11.100"},{"location":"triplydb-technical-changelog/#25.10.300","text":"Release date: 2025-10-29 SAML authentication attribute mapping configuration changed. The auth.saml[*].attributeMappings structure now supports additional properties for attribute updates. Added allowOverwrite boolean property to control whether attributes can be updated on subsequent logins (default: false ). Added expiresAt and displayedName attribute mappings for temporary user accounts. Existing SAML configurations will continue to work as is Added indexJobs.maxJsonLdFileSizeMb configuration property (default: 50 ). This limits the maximum size of JSON-LD files during index job processing to prevent memory issues with very large files. The environment API variable TRIPLY__JSONLD_MAX_BYTES_SIZE is not allowed anymore. Removed speedy.maxNumberOfStatementsInMemory configuration property. Memory handling for SPARQL operations is now managed automatically without a fixed statement limit, improving support for large result sets.","title":"25.10.300"},{"location":"triplydb-technical-changelog/#25.10.200","text":"Release date: 2025-10-16 Node.js upgraded from version 22 to version 24. All TriplyDB container images now use Node.js 24 as the base runtime. This affects all pods including API, console, orchestrator, and job runners. Redis upgraded from version 7.4.2 to 8.2.2 to address security vulnerabilities. Note that the security vulnerability in question could not have been abused in the context of the TriplyDB deployment. Added indexJobs.legacyDelimitedFormat configuration property (default: true ). This controls CSV/TSV import behavior. When set to true , uses the legacy simple format. When set to false , uses the new format which supports more complex table structures but produces more verbose RDF output.","title":"25.10.200"},{"location":"triplydb-technical-changelog/#25.10.100","text":"Release date: 2025-10-02 API service account RBAC permissions changed: added leases resource permissions ( get , list , watch , create , update , patch ) for coordination.k8s.io API group. This enables proper Kubernetes leader election coordination for watching job events. Orchestrator service account removed: orchestrator pods now use default namespace permissions instead of dedicated service account. The orchestrator deployment no longer specifies a serviceAccountName .","title":"25.10.100"},{"location":"triplydb-technical-changelog/#25.9.100","text":"Release date: 2025-09-18 None","title":"25.9.100"},{"location":"triplydb-technical-changelog/#25.8.200","text":"Release date: 2025-08-28 None","title":"25.8.200"},{"location":"triplydb-technical-changelog/#25.08.100","text":"Release date: 2025-08-06 None","title":"25.08.100"},{"location":"triplydb-technical-changelog/#25.7.100","text":"Release date: 2025-07-16 None","title":"25.7.100"},{"location":"triplydb-technical-changelog/#25.6.200","text":"Release date: 2025-06-26 None","title":"25.6.200"},{"location":"triplydb-technical-changelog/#25.6.100","text":"Release date: 2025-06-05 None","title":"25.6.100"},{"location":"triplydb-technical-changelog/#25.5.100","text":"Release date: 2025-05-14 None","title":"25.5.100"},{"location":"triplydb-technical-changelog/#25.4.300","text":"Release date: 2025-04-28 None","title":"25.4.300"},{"location":"triplydb-technical-changelog/#25.4.200","text":"Release date: 2025-04-16 None","title":"25.4.200"},{"location":"triplydb-technical-changelog/#25.4.100","text":"Release date: 2025-04-03 None","title":"25.4.100"},{"location":"triplydb-technical-changelog/#25.3.200","text":"Release date: 2025-04-20 None","title":"25.3.200"},{"location":"triplydb-technical-changelog/#25.3.100","text":"Release date: 2025-03-06 Features SAML can now be configured through native helm values, instead of using environment variables for the API pod. I.e., A configuration like this: api: env: TRIPLY__SAML__0__ENTRY_POINT: some_entrypoint_url TRIPLY__SAML__0__IDP: some_idp_id TRIPLY__SAML__0__IDP_ATTRIBUTE_MAPPINGS__EMAIL__KEY: http://schemas.xmlsoap.org/ws/2005/05/identity/claims/emailaddress TRIPLY__SAML__0__IDP_ATTRIBUTE_MAPPINGS__NAME__KEY: http://schemas.xmlsoap.org/ws/2005/05/identity/claims/nameidentifier TRIPLY__SAML__0__ISSUER: some_issuer_id TRIPLY__SAML__0__LABEL: Login with SAML TRIPLY__SAML__0__SIGNATURE_ALGORITHM: RSA_SHA256 TRIPLY__SAML__0__SUPPORT_IDP_LOGOUT: \"false\" TRIPLY__SAML__0__WANT_AUTHN_RESPONSE_SIGNED: \"false\" TRIPLY__SAML__0__IDP_CERT: valueFrom: secretKeyRef: name: saml-credentials-0 key: CERT TRIPLY__SAML__0__PRIVATE_KEY: valueFrom: secretKeyRef: name: saml-credentials-0 key: PRIVATE_KEY TRIPLY__SAML__0__PUBLIC_CERT: valueFrom: secretKeyRef: name: saml-credentials-0 key: SIGNING_CERT TRIPLY__SAML__0__DECRYPTION_CERT: valueFrom: secretKeyRef: name: saml-credentials-0 key: DECRYPTION_CERT TRIPLY__SAML__0__DECRYPTION_PVK: valueFrom: secretKeyRef: name: saml-credentials-0 key: DECRYPTION_PVK will now be set like this: auth: saml: some_idp_id: entryPoint: some_entrypoint_url issuer: some_issuer_id label: Login with SAML signatureAlgorithm: RSA_SHA256 idpAttributeMappings: email: key: http://schemas.xmlsoap.org/ws/2005/05/identity/claims/emailaddress name: key: http://schemas.xmlsoap.org/ws/2005/05/identity/claims/nameidentifier additionalConfig: wantAuthnResponseSigned: false supportIdpLogout: false idpCertSecret: name: saml-credentials-0 key: CERT spCertSecret: name: saml-sp-certs Note that: The IDP ID is now the key of the helm values object. Certificates can only be set through secrets. For the IDP certificates this can be any secret with any field. For the SP certificate we expect a secret of type TLS , containing the private key and the public certificate.","title":"25.3.100"},{"location":"triplydb-technical-changelog/#25.2.200","text":"Release date: 2025-02-20 None","title":"25.2.200"},{"location":"triplydb-technical-changelog/#25.2.100","text":"Release date: 2025-02-06 The type tdbServices.customLabels changed from an array to an object. I.e., change this: tdbServices: customLabels: - VAL=KEY to: tdbServices: customLabels: VAL: KEY","title":"25.2.100"},{"location":"triplydb-technical-changelog/#25.1.200","text":"Release date: 2025-01-23 disablePreUpgradeHook setting is no longer needed. If you have set this, you can safely remove it","title":"25.1.200"},{"location":"triplydb-technical-changelog/#25.1.100","text":"Release date: 2025-01-09 indexJobs.storageClassName and queryJobs.storageClassName are both required properties now. tdbServices.podNodeAffinity and tdbServices.podTolerations now take a yaml string as argument, instead of a yaml object. indexJobs.chunkSize must now be a number and not a string Some properties from the values file are now propagated to the console and API via a configmap. This change only affects on-premise deployments where the helm manifests are post-processed and manipulated.","title":"25.1.100"},{"location":"triplydb-technical-changelog/#24.12.200","text":"Release date: 2024-12-18 None","title":"24.12.200"},{"location":"triplydb-technical-changelog/#24.12.104","text":"Release date: 2024-12-06 The .disableNetworkPolicies property is removed. Instead, use the networkPolicies.enabled property. The API and console apply stricter network policies. As a consequence, you will need to specify a source selector that references your ingress. See the kubernetes documentation for more info on such selectors. Specify this selector in networkPolicies.ingressSelector . An example definition is the following: networkPolicies: ingressSelector: namespaceSelector: matchLabels: kubernetes.io/metadata.name: ingress-nginx","title":"24.12.104"},{"location":"triplydb-technical-changelog/#24.11.200","text":"Release date: 2024-11-22 None","title":"24.11.200"},{"location":"triplydb-technical-changelog/#24.11.100","text":"Release date: 2024-11-08 None","title":"24.11.100"},{"location":"triplydb-technical-changelog/#24.10.200","text":"Release date: 2024-10-25 The .defaultImageRegistry and triplydbImageRegistry fields are now removed. If you used these fields, then you should instead reference the full image path (without the tag) for the images. If you used .defaultImageRegistry with a custom registry, then set the full image tags for these keys: mongodb.image redis.image kubernetesWaitFor.image If you used .triplydbImageRegistry with a custom registry, then set the full image tags for these keys: api.image console.image orchestrator.image indexJobs.image queryJobs.image tdbServices.virtuoso.image tdbServices.jena.image tdbServices.elastic.image tdbServices.blazegraph.image","title":"24.10.200"},{"location":"triplydb-technical-changelog/#24.10.100","text":"Release date: 2024-10-11 None","title":"24.10.100"},{"location":"triplydb-technical-changelog/#24.9.200","text":"Release date: 2024-09-27 None","title":"24.9.200"},{"location":"triplydb-technical-changelog/#24.9.100","text":"The version scheme for the TriplyDB helm charts changed. This is now reflected in this technical changelog. indexingChunkSize (if present) should move to indexJobs.chunkSize queryJobs.nodeMemoryLimitInGb is renamed to queryJobs.nodejsMemoryLimitInGb","title":"24.9.100"},{"location":"triplydb-technical-changelog/#24.08.1","text":"None","title":"24.08.1"},{"location":"triplydb-technical-changelog/#24.08.0","text":"The service-orchestrator key is renamed to orchestrator","title":"24.08.0"},{"location":"triplydb-technical-changelog/#24.07.1","text":"None","title":"24.07.1"},{"location":"triplydb-technical-changelog/#24.07.0","text":"","title":"24.07.0"},{"location":"triplydb-technical-changelog/#saml","text":"SAML configurations changed as follows: api.env.TRIPLY__SAML__0__SIGNING_CERT should be renamed to TRIPLY__SAML__0__PUBLIC_CERT api.env.TRIPLY__SAML__0__CERT should be renamed to TRIPLY__SAML__0__IDP_CERT","title":"SAML"},{"location":"triplydb-technical-changelog/#24.06.1","text":"None","title":"24.06.1"},{"location":"triplydb-technical-changelog/#24.06.0","text":"","title":"24.06.0"},{"location":"triplydb-technical-changelog/#version-tags","text":"Version tags changed. If you have hardcoded versions in your values file (e.g. api.version or console.version ), then remove the -k8 postfix. E.g., change 24.05.1-k8 to 24.05.1 The tdbServices.[service-name].tag property changed to tdbServices.[service-name].version","title":"Version tags"},{"location":"tutorial/building-a-restful-api/","text":"Building a RESTful API \u00b6 Write the Iterator \u00b6 The iterator is a SPARQL Select query that returns a sequence of bindings that adheres to the query parameters supported for the REST API path. For example, a REST API path for occupations returns bindings for individual occupations, and has the following query parameters: The name of the occupation (variable ?name ) Configuration in case no query parameter is specified \u00b6 The first part of the query string returns the sequence of bindings in case no query parameter is specified. It includes the following things: - Recommended: a graph-clause that scopes to the instance data graph. - The class that corresponds to the REST API path (example: cnluwv:Occupation ). prefix cnluwv: <https://linkeddata.uwv.nl/ns/competentnl_uwv#> select $this { $this a cnluwv:Occupation. } Configuration for query parameters that map onto required properties \u00b6 For every query parameter, determine whether it is mapped into a property that is required or optional. In our example, query parameter ?name maps onto property skosxl:prefLabel , which is required for every occupation. For each required property, add a Basic Graph Pattern: prefix cnluwv: <https://linkeddata.uwv.nl/ns/competentnl_uwv#> prefix skosxl: <http://www.w3.org/2008/05/skos-xl#> select $this { $this a cnluwv:Occupation; skosxl:prefLabel/skosxl:literalForm $name. } Configuration for query parameters that map onto optional properties \u00b6 For every query parameter that maps onto a property that is optional, and the following 3-line template: bind(?ApiVar as ?ApiVar1) optional { $this PROPERTY_PATH ?ApiVar2. } filter(!bound(?ApiVar1) || ?ApiVar1 = ?ApiVar2) We explain each line in detail: The Bind clause makes sure that ?ApiVar1 is only bound if the API variable is specified through a query parameter. The Optional clause makes sure that we match the optional property, if it is present. The Filter clause ensures that the query succeeds if either the query parameter was not set, or if the query parameter was set to the value of the current binfing for $this . prefix cnluwv: <https://linkeddata.uwv.nl/ns/competentnl_uwv#> prefix skosxl: <http://www.w3.org/2008/05/skos-xl#> select $this { $this a cnluwv:Occupation; skosxl:prefLabel/skosxl:literalForm $name. bind(?ApiVar as ?ApiVar1) optional { $this PROPERTY_PATH ?ApiVar2. } filter(!bound(?ApiVar1) || ?ApiVar1 = ?ApiVar2) } Full example \u00b6 The following query is an example of an Iterator: prefix cnluwv: <https://linkeddata.uwv.nl/ns/competentnl_uwv#> prefix skosxl: <http://www.w3.org/2008/05/skos-xl#> select $this { $this a cnluwv:Occupation; skosxl:prefLabel/skosxl:literalForm $name } Write the Generator \u00b6 The Generator is a SPARQL Construct query that returns one record for each binding of the $this variable in our Iterator. Since we are working with linked data graphs, there is no real notion of a 'record'. This requires us to configure which data items we want to include, and which we want to exclude. The basic Construct query \u00b6 Since our Generator will be a Construct query, we can start out with the following basic query, which return triples from our graph: construct { $this ?p1 ?o1. } where { $this ?p1 ?o1. } We have used the same variable name $this as in our Iterator, but this is merely a naming convention. We must do some extra work to connect the bindings from our Iterator to our Generator... Integrate the Iterator into the Generator \u00b6 In SPARQL, we can integrate any Select query into what is called a Sub-Select clause. This allows us to connect the binding for $this that come out of our Iterator , to the basic template in our Generator : prefix cnluwv: <https://linkeddata.uwv.nl/ns/competentnl_uwv#> prefix skosxl: <http://www.w3.org/2008/05/skos-xl#> construct { $this ?p1 ?o1. } where { { select $this { $this a cnluwv:Occupation; skosxl:prefLabel/skosxl:literalForm ?name } } $this ?p1 ?o1. } If we specify a query parameter for ?name , we get back the triples that describe the occupation with that name (Basic Triple Pattern $this ?p1 ?o1 ). Extend the Generator with nested information \u00b6 In the previous section , we only returned the triples that directly describe the bindings for $this . However, some relevant information may appear in triples that are further removed from $this . For example, SKOS-XL labels use an extra level of nesting, where the actual label content appears 2-hops away from $this . If we want to include such nested information into our Generate, we must specify this with additional Basic Triple Patterns. Since only some properties use nesting, we must typically enclose deeper hops inside an Optional clause, together with either a whitelist of properties we want to follow, or a blacklist of properties we do not want to follow. In the following example, we use a whitelist to include properties whose textual content is found one nesting level deeper: prefix cnluwv: <https://linkeddata.uwv.nl/ns/competentnl_uwv#> prefix skosxl: <http://www.w3.org/2008/05/skos-xl#> construct { ?this ?p1 ?o1. ?o1 ?p2 ?o2. } where { { select $this { $this a cnluwv:Occupation; skosxl:prefLabel/skosxl:literalForm $name. } } $this ?p1 ?o1. optional { ?o1 ?p2 ?o2. filter(?p1 in (cnluwv:hasContentStatus, cnluwv:hasDetailedDescription, skosxl:altLabel, skosxl:prefLabel)) } } Notice that the whitelist is implemented with operator in ; blacklists are similarly implemented with operator not in . Full example \u00b6 The following query is an example of a working Generator query: prefix cnluwv: <https://linkeddata.uwv.nl/ns/competentnl_uwv#> prefix skosxl: <http://www.w3.org/2008/05/skos-xl#> construct { ?this ?p1 ?o1. ?o1 ?p2 ?o2. } where { { select $this { $this a cnluwv:Occupation; skosxl:prefLabel/skosxl:literalForm $name. } } $this ?p1 ?o1. optional { ?o1 ?p2 ?o2. filter(?p1 in (cnluwv:hasContentStatus, cnluwv:hasDetailedDescription, skosxl:altLabel, skosxl:prefLabel)) } } Returning JSON \u00b6 From a content perspective, our Generator query functions like a REST API path: we can set zero or more query parameters, and we receive the information that conforms to our configuration. By default, the Generator returns various RDF serializations, such as JSON-LD. While the JSON body contains all the relevant information, the syntactic structure of the body looks quite messy. Adding a JSON-LD Frame \u00b6 In order to better structure the syntax of the returned JSON body, we make use of a JSON-LD Frame. We start out with the empty frame, and will build this out in subsequent steps: {} You can try this out by going to the following query: link In fact, it is best to keep this query open in a separate window or tab, and apply each of the following steps yourself, to see the live effects of changing the JSON-LD Frame configuration. Configure the type \u00b6 We want the JSON objects to describe information of a specific type. In our example, each object should describe an occupation. We can configure this in the JSON-LD Frame by using the \"@type\" key, together with the IRI of the occupation class: { \"@type\": \"https://linkeddata.uwv.nl/ns/competentnl_uwv#Occupation\" } We now see that the JSON data starts to form around the occupation node. Configure the context \u00b6 The JSON object that describes an occupation contains a lot of confusing syntax and lengthy IRIs. The JSON-LD standard allows us to clean this up through a piece of configuration called the Context. The context is typically the same for all objects that are returned by an API. The context is itself a JSON object, that is specified under the \"@context\" key. We start by including the following sub-keys: ' \"@base\" configures the IRI namespace of the instances. \"@version\" indicates which JSON-LD version we use. \"@vocab\" configures the IRI namespace of the main vocabulary that is used. We can now abbreviate the configuration for \"@type\" to \"Occupation\" . This will make use of the occupation class within the IRI namespace that is specified under \"@vocab\" : { \"@context\": { \"@base\": \"https://linkeddata.uwv.nl/id/\", \"@version\": 1.1, \"@vocab\": \"https://linkeddata.uwv.nl/ns/competentnl_uwv#\" }, \"@type\": \"Occupation\" } Add IRI prefix declarations \u00b6 We can add IRI prefix declarations to the Context. This results in shorted keys and values in the JSON objects: { \"@context\": { \"cnl\": \"https://linkeddata.uwv.nl/ns/competentnl_uwv#\", \"dct\": \"http://purl.org/dc/terms/\", \"rdfs\": \"http://www.w3.org/2000/01/rdf-schema#\", \"skos\": \"http://www.w3.org/2004/02/skos/core#\", \"skosxl\": \"http://www.w3.org/2008/05/skos-xl#\", \"xsd\": \"http://www.w3.org/2001/XMLSchema#\", ... } } Configure key names \u00b6 While adding IRI prefix declarations makes many keys and values shorter, we can go one step further and use completely different key names that map onto IRIs. This allows us to add keys in a different language (e.g. in Dutch), or it allows us to get rid of the IRI alias that was still included after adding IRI prefix declarations. Furthermore, we can introduce our own names for the somewhat awkward looking keys \"@id\" and \"@type\" . The following Context results in keys that consist in simple names, devoid of any (abbreviated or full) IRIs, and devoid of strange @-signs (except for the Context key, which cannot be renamed): { \"@context\": { \"@base\": \"https://linkeddata.uwv.nl/id/\", \"@version\": 1.1, \"@vocab\": \"https://linkeddata.uwv.nl/ns/competentnl_uwv#\", \"altLabel\": \"skosxl:altLabel\", \"cnl\": \"https://linkeddata.uwv.nl/ns/competentnl_uwv#\", \"broadMatch\": \"skos:broadMatch\", \"created\": \"dct:created\", \"dct\": \"http://purl.org/dc/terms/\", \"id\": \"@id\", \"inScheme\": \"skos:inScheme\", \"rdfs\": \"http://www.w3.org/2000/01/rdf-schema#\", \"relatedMatch\": \"skos:relatedMatch\", \"skos\": \"http://www.w3.org/2004/02/skos/core#\", \"skosxl\": \"http://www.w3.org/2008/05/skos-xl#\", \"type\": \"@type\", \"xsd\": \"http://www.w3.org/2001/XMLSchema#\" }, \"type\": \"Occupation\" } Configure datatypes \u00b6 We still have too much syntactic clutter for values with a datatype. For example, this is how a created date is shown: { \"@context\": { ... }, ..., \"created\": { \"type\": \"xsd:dateTime\", \"@value\": \"2024-12-09T00:00:00\" }, ... } Again, we can use the Context to hide unnecessary details from the JSON object. The following entry specifies that the datatype of 'created' values is XML Schema Datatypes (XSD) date/time: { \"@context\": { ..., \"created\": { \"@id\": \"dct:created\", \"@type\": \"xsd:dateTime\" }, ... } } Configure languages \u00b6 We still have too much syntactic clutter for values with a language tag. For example, this is how a literal form is shown: { \"@context\": { ... }, ..., \"literalForm\": { \"@language\": \"nl\", \"@value\": \"medewerker archief\" }, ... } Again, we can use the Context to hide unnecessary details from the JSON object. The following entry specifies that the language of 'literalForm' values is Dutch ('nl'): { \"@context\": { ..., \"literalForm\": { \"@id\": \"skosxl:literalForm\", \"@language\": \"nl\" }, ... } } Scoped contexts \u00b6 { \"@context\": { ..., \"altLabel\": { \"@id\": \"skosxl:altLabel\", \"@context\": { \"literalForm\": { \"@id\": \"skosxl:literalForm\", \"@language\": \"nl\" } } }, ... }, ... } Using the RESTful API \u00b6 Once a couple of SPARQL queries have been specified, it is possible to use the REST API through an OpenAPI Specification. This is done by the following these steps: Create an API Token in the Triply GUI. Go to an HTTPS program, and configure the API Token as an HTTPS Bearer Token. Specify the standard Accept header for YAML, the format used by the OpenAPI Specification: text/vnd.yaml Perform an HTTPS request against URL https://${host}/queries/$[account}/ , where you enter the host name of your Triply environment and the name of the account under which the queries are stored. This downloads the OpenAPI Specification that contains metadata about all queries under the specified account. If you want to retrieve the metadata for one specific query version, change the URL in item 4 to https://${host}/queries/$[account}/${query}/${version} Load the OpenAPI Specification YAML file into an HTTPS program. With the same API Token configured, you can now easily submit queries to the various REST paths.","title":"Building a RESTful API"},{"location":"tutorial/building-a-restful-api/#building-a-restful-api","text":"","title":"Building a RESTful API"},{"location":"tutorial/building-a-restful-api/#write-the-iterator","text":"The iterator is a SPARQL Select query that returns a sequence of bindings that adheres to the query parameters supported for the REST API path. For example, a REST API path for occupations returns bindings for individual occupations, and has the following query parameters: The name of the occupation (variable ?name )","title":"Write the Iterator"},{"location":"tutorial/building-a-restful-api/#configuration-in-case-no-query-parameter-is-specified","text":"The first part of the query string returns the sequence of bindings in case no query parameter is specified. It includes the following things: - Recommended: a graph-clause that scopes to the instance data graph. - The class that corresponds to the REST API path (example: cnluwv:Occupation ). prefix cnluwv: <https://linkeddata.uwv.nl/ns/competentnl_uwv#> select $this { $this a cnluwv:Occupation. }","title":"Configuration in case no query parameter is specified"},{"location":"tutorial/building-a-restful-api/#configuration-for-query-parameters-that-map-onto-required-properties","text":"For every query parameter, determine whether it is mapped into a property that is required or optional. In our example, query parameter ?name maps onto property skosxl:prefLabel , which is required for every occupation. For each required property, add a Basic Graph Pattern: prefix cnluwv: <https://linkeddata.uwv.nl/ns/competentnl_uwv#> prefix skosxl: <http://www.w3.org/2008/05/skos-xl#> select $this { $this a cnluwv:Occupation; skosxl:prefLabel/skosxl:literalForm $name. }","title":"Configuration for query parameters that map onto required properties"},{"location":"tutorial/building-a-restful-api/#configuration-for-query-parameters-that-map-onto-optional-properties","text":"For every query parameter that maps onto a property that is optional, and the following 3-line template: bind(?ApiVar as ?ApiVar1) optional { $this PROPERTY_PATH ?ApiVar2. } filter(!bound(?ApiVar1) || ?ApiVar1 = ?ApiVar2) We explain each line in detail: The Bind clause makes sure that ?ApiVar1 is only bound if the API variable is specified through a query parameter. The Optional clause makes sure that we match the optional property, if it is present. The Filter clause ensures that the query succeeds if either the query parameter was not set, or if the query parameter was set to the value of the current binfing for $this . prefix cnluwv: <https://linkeddata.uwv.nl/ns/competentnl_uwv#> prefix skosxl: <http://www.w3.org/2008/05/skos-xl#> select $this { $this a cnluwv:Occupation; skosxl:prefLabel/skosxl:literalForm $name. bind(?ApiVar as ?ApiVar1) optional { $this PROPERTY_PATH ?ApiVar2. } filter(!bound(?ApiVar1) || ?ApiVar1 = ?ApiVar2) }","title":"Configuration for query parameters that map onto optional properties"},{"location":"tutorial/building-a-restful-api/#full-example","text":"The following query is an example of an Iterator: prefix cnluwv: <https://linkeddata.uwv.nl/ns/competentnl_uwv#> prefix skosxl: <http://www.w3.org/2008/05/skos-xl#> select $this { $this a cnluwv:Occupation; skosxl:prefLabel/skosxl:literalForm $name }","title":"Full example"},{"location":"tutorial/building-a-restful-api/#write-the-generator","text":"The Generator is a SPARQL Construct query that returns one record for each binding of the $this variable in our Iterator. Since we are working with linked data graphs, there is no real notion of a 'record'. This requires us to configure which data items we want to include, and which we want to exclude.","title":"Write the Generator"},{"location":"tutorial/building-a-restful-api/#the-basic-construct-query","text":"Since our Generator will be a Construct query, we can start out with the following basic query, which return triples from our graph: construct { $this ?p1 ?o1. } where { $this ?p1 ?o1. } We have used the same variable name $this as in our Iterator, but this is merely a naming convention. We must do some extra work to connect the bindings from our Iterator to our Generator...","title":"The basic Construct query"},{"location":"tutorial/building-a-restful-api/#integrate-the-iterator-into-the-generator","text":"In SPARQL, we can integrate any Select query into what is called a Sub-Select clause. This allows us to connect the binding for $this that come out of our Iterator , to the basic template in our Generator : prefix cnluwv: <https://linkeddata.uwv.nl/ns/competentnl_uwv#> prefix skosxl: <http://www.w3.org/2008/05/skos-xl#> construct { $this ?p1 ?o1. } where { { select $this { $this a cnluwv:Occupation; skosxl:prefLabel/skosxl:literalForm ?name } } $this ?p1 ?o1. } If we specify a query parameter for ?name , we get back the triples that describe the occupation with that name (Basic Triple Pattern $this ?p1 ?o1 ).","title":"Integrate the Iterator into the Generator"},{"location":"tutorial/building-a-restful-api/#extend-the-generator-with-nested-information","text":"In the previous section , we only returned the triples that directly describe the bindings for $this . However, some relevant information may appear in triples that are further removed from $this . For example, SKOS-XL labels use an extra level of nesting, where the actual label content appears 2-hops away from $this . If we want to include such nested information into our Generate, we must specify this with additional Basic Triple Patterns. Since only some properties use nesting, we must typically enclose deeper hops inside an Optional clause, together with either a whitelist of properties we want to follow, or a blacklist of properties we do not want to follow. In the following example, we use a whitelist to include properties whose textual content is found one nesting level deeper: prefix cnluwv: <https://linkeddata.uwv.nl/ns/competentnl_uwv#> prefix skosxl: <http://www.w3.org/2008/05/skos-xl#> construct { ?this ?p1 ?o1. ?o1 ?p2 ?o2. } where { { select $this { $this a cnluwv:Occupation; skosxl:prefLabel/skosxl:literalForm $name. } } $this ?p1 ?o1. optional { ?o1 ?p2 ?o2. filter(?p1 in (cnluwv:hasContentStatus, cnluwv:hasDetailedDescription, skosxl:altLabel, skosxl:prefLabel)) } } Notice that the whitelist is implemented with operator in ; blacklists are similarly implemented with operator not in .","title":"Extend the Generator with nested information"},{"location":"tutorial/building-a-restful-api/#full-example_1","text":"The following query is an example of a working Generator query: prefix cnluwv: <https://linkeddata.uwv.nl/ns/competentnl_uwv#> prefix skosxl: <http://www.w3.org/2008/05/skos-xl#> construct { ?this ?p1 ?o1. ?o1 ?p2 ?o2. } where { { select $this { $this a cnluwv:Occupation; skosxl:prefLabel/skosxl:literalForm $name. } } $this ?p1 ?o1. optional { ?o1 ?p2 ?o2. filter(?p1 in (cnluwv:hasContentStatus, cnluwv:hasDetailedDescription, skosxl:altLabel, skosxl:prefLabel)) } }","title":"Full example"},{"location":"tutorial/building-a-restful-api/#returning-json","text":"From a content perspective, our Generator query functions like a REST API path: we can set zero or more query parameters, and we receive the information that conforms to our configuration. By default, the Generator returns various RDF serializations, such as JSON-LD. While the JSON body contains all the relevant information, the syntactic structure of the body looks quite messy.","title":"Returning JSON"},{"location":"tutorial/building-a-restful-api/#adding-a-json-ld-frame","text":"In order to better structure the syntax of the returned JSON body, we make use of a JSON-LD Frame. We start out with the empty frame, and will build this out in subsequent steps: {} You can try this out by going to the following query: link In fact, it is best to keep this query open in a separate window or tab, and apply each of the following steps yourself, to see the live effects of changing the JSON-LD Frame configuration.","title":"Adding a JSON-LD Frame"},{"location":"tutorial/building-a-restful-api/#configure-the-type","text":"We want the JSON objects to describe information of a specific type. In our example, each object should describe an occupation. We can configure this in the JSON-LD Frame by using the \"@type\" key, together with the IRI of the occupation class: { \"@type\": \"https://linkeddata.uwv.nl/ns/competentnl_uwv#Occupation\" } We now see that the JSON data starts to form around the occupation node.","title":"Configure the type"},{"location":"tutorial/building-a-restful-api/#configure-the-context","text":"The JSON object that describes an occupation contains a lot of confusing syntax and lengthy IRIs. The JSON-LD standard allows us to clean this up through a piece of configuration called the Context. The context is typically the same for all objects that are returned by an API. The context is itself a JSON object, that is specified under the \"@context\" key. We start by including the following sub-keys: ' \"@base\" configures the IRI namespace of the instances. \"@version\" indicates which JSON-LD version we use. \"@vocab\" configures the IRI namespace of the main vocabulary that is used. We can now abbreviate the configuration for \"@type\" to \"Occupation\" . This will make use of the occupation class within the IRI namespace that is specified under \"@vocab\" : { \"@context\": { \"@base\": \"https://linkeddata.uwv.nl/id/\", \"@version\": 1.1, \"@vocab\": \"https://linkeddata.uwv.nl/ns/competentnl_uwv#\" }, \"@type\": \"Occupation\" }","title":"Configure the context"},{"location":"tutorial/building-a-restful-api/#add-iri-prefix-declarations","text":"We can add IRI prefix declarations to the Context. This results in shorted keys and values in the JSON objects: { \"@context\": { \"cnl\": \"https://linkeddata.uwv.nl/ns/competentnl_uwv#\", \"dct\": \"http://purl.org/dc/terms/\", \"rdfs\": \"http://www.w3.org/2000/01/rdf-schema#\", \"skos\": \"http://www.w3.org/2004/02/skos/core#\", \"skosxl\": \"http://www.w3.org/2008/05/skos-xl#\", \"xsd\": \"http://www.w3.org/2001/XMLSchema#\", ... } }","title":"Add IRI prefix declarations"},{"location":"tutorial/building-a-restful-api/#configure-key-names","text":"While adding IRI prefix declarations makes many keys and values shorter, we can go one step further and use completely different key names that map onto IRIs. This allows us to add keys in a different language (e.g. in Dutch), or it allows us to get rid of the IRI alias that was still included after adding IRI prefix declarations. Furthermore, we can introduce our own names for the somewhat awkward looking keys \"@id\" and \"@type\" . The following Context results in keys that consist in simple names, devoid of any (abbreviated or full) IRIs, and devoid of strange @-signs (except for the Context key, which cannot be renamed): { \"@context\": { \"@base\": \"https://linkeddata.uwv.nl/id/\", \"@version\": 1.1, \"@vocab\": \"https://linkeddata.uwv.nl/ns/competentnl_uwv#\", \"altLabel\": \"skosxl:altLabel\", \"cnl\": \"https://linkeddata.uwv.nl/ns/competentnl_uwv#\", \"broadMatch\": \"skos:broadMatch\", \"created\": \"dct:created\", \"dct\": \"http://purl.org/dc/terms/\", \"id\": \"@id\", \"inScheme\": \"skos:inScheme\", \"rdfs\": \"http://www.w3.org/2000/01/rdf-schema#\", \"relatedMatch\": \"skos:relatedMatch\", \"skos\": \"http://www.w3.org/2004/02/skos/core#\", \"skosxl\": \"http://www.w3.org/2008/05/skos-xl#\", \"type\": \"@type\", \"xsd\": \"http://www.w3.org/2001/XMLSchema#\" }, \"type\": \"Occupation\" }","title":"Configure key names"},{"location":"tutorial/building-a-restful-api/#configure-datatypes","text":"We still have too much syntactic clutter for values with a datatype. For example, this is how a created date is shown: { \"@context\": { ... }, ..., \"created\": { \"type\": \"xsd:dateTime\", \"@value\": \"2024-12-09T00:00:00\" }, ... } Again, we can use the Context to hide unnecessary details from the JSON object. The following entry specifies that the datatype of 'created' values is XML Schema Datatypes (XSD) date/time: { \"@context\": { ..., \"created\": { \"@id\": \"dct:created\", \"@type\": \"xsd:dateTime\" }, ... } }","title":"Configure datatypes"},{"location":"tutorial/building-a-restful-api/#configure-languages","text":"We still have too much syntactic clutter for values with a language tag. For example, this is how a literal form is shown: { \"@context\": { ... }, ..., \"literalForm\": { \"@language\": \"nl\", \"@value\": \"medewerker archief\" }, ... } Again, we can use the Context to hide unnecessary details from the JSON object. The following entry specifies that the language of 'literalForm' values is Dutch ('nl'): { \"@context\": { ..., \"literalForm\": { \"@id\": \"skosxl:literalForm\", \"@language\": \"nl\" }, ... } }","title":"Configure languages"},{"location":"tutorial/building-a-restful-api/#scoped-contexts","text":"{ \"@context\": { ..., \"altLabel\": { \"@id\": \"skosxl:altLabel\", \"@context\": { \"literalForm\": { \"@id\": \"skosxl:literalForm\", \"@language\": \"nl\" } } }, ... }, ... }","title":"Scoped contexts"},{"location":"tutorial/building-a-restful-api/#using-the-restful-api","text":"Once a couple of SPARQL queries have been specified, it is possible to use the REST API through an OpenAPI Specification. This is done by the following these steps: Create an API Token in the Triply GUI. Go to an HTTPS program, and configure the API Token as an HTTPS Bearer Token. Specify the standard Accept header for YAML, the format used by the OpenAPI Specification: text/vnd.yaml Perform an HTTPS request against URL https://${host}/queries/$[account}/ , where you enter the host name of your Triply environment and the name of the account under which the queries are stored. This downloads the OpenAPI Specification that contains metadata about all queries under the specified account. If you want to retrieve the metadata for one specific query version, change the URL in item 4 to https://${host}/queries/$[account}/${query}/${version} Load the OpenAPI Specification YAML file into an HTTPS program. With the same API Token configured, you can now easily submit queries to the various REST paths.","title":"Using the RESTful API"},{"location":"yasgui/","text":"On this page: Yasgui SPARQL Editor Supported key combinations Templates SPARQL-concat Handlebars Rendering HTML Visualizations Table Features Table Example Response Gallery (TriplyDB Plugin) Variables Format Styling Gallery Example Chart (TriplyDB Plugin) Geo (TriplyDB Plugin) Variables Color values WMS tile-servers Geo-3D (TriplyDB-only) Variables Geo Events (TriplyDB Plugin) Pivot Table (TriplyDB Plugin) Timeline (TriplyDB Plugin) Network (TriplyDB Plugin) Markup (TriplyDB Plugin) Yasgui \u00b6 This section explains the use of SPARQL via Yasgui. Yasgui provides various advanced features for creating, sharing, and visualizing SPARQL queries and their results. SPARQL Editor \u00b6 The Yasgui SPARQL editor is a query editor that offers syntax highlighting, syntax validation, autocompletion, a variety of different SPARQL result visualizations, with a plugin architecture that enables customization . By default, the query editor provides autocomplete suggestions via the LOV API. Website maintainers can add their own autocompletion logic as well. For example, the Yasgui integration in TriplyDB uses the TriplyDB API to more accurately provide suggestions based on the underlying data. Sharing queries now involves less than having to copy/past complete SPARQL queries. Instead, you can share your query (and the corresponding visualization settings) using a simple URL. Supported key combinations \u00b6 The following table enumerates the key combinations that are supported by the SPARQL Editor. Key combination Behavior Alt + Left Move the cursor to the beginning of the current line. Alt + Right Move the cursor to the end of the current line. Alt + U Redo the last change within the current selection. Ctrl + Backspace Delete to the beginning of the group before the cursor. Ctrl + Delete Delete to the beginning of the group after the cursor. Ctrl + End Move the cursor to the end of the query. Ctrl + Home Move the cursor to the start of the query. Ctrl + Left Move the cursor to the left of the group before the cursor. Ctrl + Right Move the cursor to the right of the group the cursor. Ctrl + [ Decrements the indentation for the current line or the lines involved in the current selection. Ctrl + ] Increments the indentation for the current line or the lines involved in the current selection. Ctrl + / Toggles on/off the commenting of the current line or the lines involved in the current selection. Ctrl + A Select the whole query. Ctrl + D Deletes the current line or all lines involved in the current selection. Ctrl + U Undo the last change within the current selection. Ctrl + Y Redo the last undone edit action. Ctrl + Z Undo the last edit action. Ctrl + Shift + F Auto-formats the whole query or the lines involved in the current selection. Shift + Tab Auto-indents the current line or the lines involved in the current selection. Tab Indents the current line or the lines involved in the current selection. Templates \u00b6 SPARQL has standardized capabilities for constructing complex strings and literals. This allows human-readable label and HTML widgets to be generated from within SPARQL. Unfortunately, the syntax for constructing such labels and widgets is a bit cumbersome. SPARQL-concat \u00b6 For example, the following SPARQL query returns HTML widgets that can be displayed in a web browser (see SPARQL Gallery ). It uses the concat function which allows an arbitrary number of string arguments to be concatenated into one string. Notice that this requires extensive quoting for each argument (e.g., '<h3>' ), as well as conversions from literals to strings (e.g., str(?typeName) ). Finally, in order to return an HTML literal we need to first bind the concatenated string to some variable ?lex , and then apply the strdt function in order to construct a literal with datatype IRI rdf:HTML . You can try this query online . prefix def: <https://triplydb.com/academy/pokemon/def/> prefix foaf: <http://xmlns.com/foaf/0.1/> prefix rdf: <http://www.w3.org/1999/02/22-rdf-syntax-ns#> prefix rdfs: <http://www.w3.org/2000/01/rdf-schema#> select * { ?pokemon def:cry ?cry; def:type/rdfs:label ?typeName; foaf:depiction ?image; rdfs:label ?name. bind(concat('<h3>',str(?typeName),' \u300b ',str(?name),'</h3>', '<img src=\"',str(?image),'\">', '<audio controls src=\"',str(?cry),'\"></audio>') as ?lex) bind(strdt(?lex,rdf:HTML) as ?widget) } limit 25 Handlebars \u00b6 The SPARQL Editor in TriplyDB supports SPARQL Templates, which makes it easier to write human-readable labels and HTML widgets. SPARQL Templates are strings in which occurrences of {{x}} will be replaced with the to-string converted results of bindings to SPARQL variable ?x . The following example query produces the same result set as the above one, but allows the entire HTML string to be written at once as a SPARQL Template. Notice that this removes the need for concatenating ( concat/n ), explicit to-string conversion ( str/1 ), and also allows the HTML literal to be constructed more easily (no strdt/2 needed). You can try this query online . prefix def: <https://triplydb.com/academy/pokemon/def/> prefix foaf: <http://xmlns.com/foaf/0.1/> prefix rdf: <http://www.w3.org/1999/02/22-rdf-syntax-ns#> prefix rdfs: <http://www.w3.org/2000/01/rdf-schema#> select * { ?pokemon def:cry ?cry; def:type/rdfs:label ?typeName; foaf:depiction ?image; rdfs:label ?name. bind(''' <h3>{{typeName}} \u300b {{name}}</h3> <img src=\"{{image}}\"> <audio controls src=\"{{cry}}\"></audio>'''^^rdf:HTML as ?widget) } limit 25 SPARQL Templates can be combined with the SPARQL Gallery feature in order to generate galleries of HTML widgets. Rendering HTML \u00b6 To distinguish between text and HTML result values the visualization library checks for the rdf:HTML datatype. The following query will return as plain text select * { bind('<p>Test</p>' as ?widget) } This query will render the result as HTML PREFIX rdf: <http://www.w3.org/1999/02/22-rdf-syntax-ns#> select * { bind('<p>Test</p>'^^rdf:HTML as ?widget) } In order to guarantee safety, TriplyDB sanitizes HTML literals before rendering them. This means that tags like <embed> , <iframe> and <script> are sanitized away, as are attributes such as onerror and onload . Visualizations \u00b6 Table \u00b6 This view allows SPARQL results to be displayed in a table. Each column in the table corresponds to a variable that belongs to the outer projection. Each row in the table corresponds to one query result. Each cell contains an RDF term or NULL . Features \u00b6 In addition to displaying the SPARQL result set, the SPARQL Table has the following features: Abbreviations The SPARQL Table uses the prefix declarations in the SPARQL query in order to abbreviate IRIs that appear in table cells. Filter By entering a search string into the \u201cFilter query results\u201d field, the table will only display result rows in which the entered search string appears. Indices The first column in the table indicates the index of each row in the SPARQL result set. Pagination By default the Table displays at most 50 rows. This maximum value can be changed to 10, 100, 1.000, or \u201cAll\u201d. Sorting A sort widget appears to the right of each header label. By pressing on the upward pointing arrow of the sort widget, rows will be sorted based on the lexicographic order of the values within the corresponding column. By pressing the downward pointing arrow of the sort widget, rows will be inversely sorted according to the same lexicographic order. Table Example \u00b6 The following SPARQL query (or see here ) returns a table of Pok\u00e9mon dragons (column pokemon ) and their happiness (column happiness ). Notice that the prefix for pokemon is not used in the query, but is used in order to abbreviate the IRI syntax in the pokemon column. By clicking on the sort widget next to the happiness header, the results can be (inversely) sorted based on the happiness values. PREFIX pokemon: <https://triplydb.com/academy/pokemon/id/pokemon/> PREFIX type: <https://triplydb.com/academy/pokemon/id/type/>a PREFIX vocab: <https://triplydb.com/academy/pokemon/vocab/> select ?pokemon ?happiness { ?pokemon vocab:type type:dragon; vocab:happiness ?happiness. } Response \u00b6 This view shows the body of the response and offers an easy way to download the result as a file. Gallery ( TriplyDB Plugin ) \u00b6 This view allows SPARQL results to be displayed in an HTML gallery. Each individual result corresponds to one HTML widget. Widgets are displayed in rows and columns to make up a widget gallery. Variables \u00b6 The gallery will render an item based on variables in the following table: Variable name Purpose ?widget The text or HTML content. meant for creating widget from scrap ?widgetLabel Title of the widget. Also used as the alternative text for the image ?widgetLabelLink A url which converts the title into a link, depends on ?widgetLabel ?widgetImage A url of an image to display ?widgetImageLink A url which adds a link to the image, depends on ?widgetImage ?widgetImageCaption A text or HTML description of the image, depends on ?widgetImage ?widgetDescription A text or HTML description, meant for adding links and Format \u00b6 The widget will display the variables in the following order: - ?widgetLabel and ?widgetLabelLink - ?widgetImage and ?widgetImageLink - ?widgetImageCaption - ?widgetDescription - ?widget Styling \u00b6 The ?widget display is restricted in height. This might not always be desired. In such cases the following style tweaks can help to make them the right size: bind('''<div style=\"max-height:unset; width:275px;\"> # The HTML that composes the widget goes here. </div>'''^^rdf:HTML as ?widget) Gallery Example \u00b6 The following SPARQL query binds an HTML string consisting of a header ( h3 ), an image ( img ), and an audio element ( audio ) to the ?widget variable. This results in a gallery with 25 widgets, each displaying a Pok\u00e9mon. (This SPARQL query also uses [[SPARQL Templates]] in order to simplify its syntax.) This query can be run online . prefix def: <https://triplydb.com/academy/pokemon/def/> prefix foaf: <http://xmlns.com/foaf/0.1/> prefix rdf: <http://www.w3.org/1999/02/22-rdf-syntax-ns#> prefix rdfs: <http://www.w3.org/2000/01/rdf-schema#> select * { ?pokemon a def:Pokemon; def:baseAttack ?attack; def:baseDefense ?defense; def:baseExp ?experience; def:baseHP ?health; def:baseSpeed ?speed; def:cry ?cry; def:femaleRatio ?female; def:happiness ?happiness; def:maleRatio ?male; def:name ?name; foaf:depiction ?image; rdfs:label ?label. filter(langmatches(lang(?name),'ja')) bind(''' <h2>{{name}} ({{label}})</h2> <img src=\"{{image}}\"> <audio controls src=\"{{cry}}\"></audio> <ul> <li>Experience: {{experience}}</li> <li>Attack: {{attack}}</li> <li>Defense: {{defense}}</li> <li>Experience: {{experience}}</li> <li>Health: {{health}}</li> <li>Female ratio: {{female}}</li> <li>Happiness: {{happiness}}</li> <li>Male ratio: {{male}}</li> <li>Speed: {{speed}}</li> </ul>'''^^rdf:HTML as ?widget) } order by desc(?experience) limit 20 Chart ( TriplyDB Plugin ) \u00b6 The chart plugin renders geographical, temporal and numerical data in interactive charts such as bar-, line- and pie charts. The chart plugin also includes a treemap representation, that is suitable for displaying hierarchies. To use the treemap plugin, you must use the following projection variables in your SPARQL query (in this order): ?node ?parent ?size ?color The label of a tree node. Either the label of the node that is the parent of ?node , or the value UNDEF in case ?node is the root node. (optional) :: For leaf nodes, a positive integer indicating the relative size of ?node . (optional) :: For leaf nodes, a double indicating the relative color of ?node . Once the TreeMap is drawn it is possible to navigate the tree with the mouse: left clicking on a node will drill down into the corresponding subtree; right clicking on a node will move up to the subtree of its parent node. The chart configuration enables tweaking the treemap properties such as the number of displayed hierarchy levels. Geo ( TriplyDB Plugin ) \u00b6 This view allows SPARQL results that contain GeoSPARQL semantics to be automatically interpreted and displayed on a 2D map. Variables \u00b6 This view recognizes the following SPARQL variable names: Variable name Purpose ?x An arbitrary variable name that is bound to literals with datatype IRI geo:wktLiteral , and whose name is the prefix of the other variable names in this table. ?xColor The color of the shape bound to ?x . ?xLabel The text or HTML content of popups that appear when clicking the shape bound to ?x . ?xTooltip Text or HTML that will appear when the shape of bound to ?x is hovered ?mapEndpoint A URL pointing to a WMS tile-server Color values \u00b6 Variable ?xColor must include a value of the following types: CSS color names . RGB color codes . HSL color codes . Gradients : Strings of the form {{PALETTE}},{{VALUE}} , where {{VALUE}} is a floating-point number between 0.0 and 1.0 and {{PALETTE}} is the name of a color palette. We support color schemes from the Colormap and Color Brewer libraries WMS tile-servers \u00b6 To include layers from a WMS tile-server, use the mapEndpoint variable to refer to a server. The plugin will then retrieve the layer information from the server. Usage of the layers can be toggled using the layer selector. Try this one: https://maps.heigit.org/histosm/wms Geo-3D (TriplyDB-only) \u00b6 This view allows SPARQL results that contain GeoSPARQL semantics to be automatically interpreted and displayed on a 3D globe. It supports both 3D and 2.5D visualizations, depending on whether the GeoSPARQL data is stored in native 3D or in 2D Variables \u00b6 This view recognizes the following SPARQL variable names: Variable name Purpose ?x An arbitrary variable name that is bound to 2D or 3D literals with datatype IRI geo:wktLiteral , and whose name is the prefix of the other variable names in this table. ?xColor The color of the shape bound to ?x . ?xHeight The height in meters of the 2.5D shape that is based on the 2D shape that is bound to ?x . This variable is not needed if data is stored in native 3D. ?xLabel The text or HTML content of the popups that appears when the shape that is bound to ?x is clicked. ?xZ The height in meters at which the 2.5D shape that is based on the 2D shape that is bound to ?x starts. This variable is not needed if data is stored in native 3D. Geo Events ( TriplyDB Plugin ) \u00b6 The SPARQL Geo Events plugin renders geographical events as a story map ( example ). This view recognizes the following SPARQL variable names: Variable name Purpose ?eventLocation (required) A geo:wktLiteral . ?eventLabel Text or HTML event label. ?eventDescription Text or HTML event description. ?eventMedia A URL pointing to a media source. Supported media types are described here . ?eventMediaCaption Text or HTML media caption. ?eventMediaCredit Text or HTML media credit. Pivot Table ( TriplyDB Plugin ) \u00b6 This view renders SPARQL results in an interactive pivot table where you are able to aggregate the results by dragging your binding variables to columns or rows. Timeline ( TriplyDB Plugin ) \u00b6 The SPARQL timeline renders the SPARQL results on a Timeline ( example ) To get started with this visualization you need at least a result containing a ?eventStart or ?eventDate with either a ?eventDescription , ?eventLabel or a ?eventMedia . (Combinations are also possible) The following parameters can be used, Parameters in Italic are experimental: Variable name Purpose ?eventStart A date when an event started ?eventEnd A date when an event Stopped ?eventDate A date when an event happened ?eventDescription Text/ HTML about the event ?eventLabel Text/ HTML title ?eventMedia Link to most forms of media see documentation for which type of links are supported ?eventType Groups events ?eventColor Colors event ?eventBackground Background of the event when selected ?eventMediaCaption Text/ HTML caption of the Media ?eventMediaCredit Text/ HTML credit of the Media ?eventMediaThumbnail The thumbnail of Media ?eventMediaAlt The Alt text of the Media ?eventMediaTitle The Title of the Media ?eventMediaLink The URL the image should link to Network ( TriplyDB Plugin ) \u00b6 This view renders SPARQL Construct results in a graph representation. It works for Turtle , Trig , N-Triples and N-Quads responses. The maximum amount of results that can be visualized is 1.000 due to performance. Markup ( TriplyDB Plugin ) \u00b6 The markup view can be used to render a variety of markup languages. This requires the use of the ?markup variable to identify which variable to render. Based on the datatype of the variable the plugin will identify which markup language to use: Markup language Datatype HTML http://www.w3.org/1999/02/22-rdf-syntax-ns#HTML Mermaid https://triplydb.com/Triply/vocab/def/mermaid * Plain text Other * This is currently a placeholder IRI, If you find a (dereferenceable) IRI for one of these datatypes please contact us .","title":"Introduction"},{"location":"yasgui/#yasgui","text":"This section explains the use of SPARQL via Yasgui. Yasgui provides various advanced features for creating, sharing, and visualizing SPARQL queries and their results.","title":"Yasgui"},{"location":"yasgui/#sparql-editor","text":"The Yasgui SPARQL editor is a query editor that offers syntax highlighting, syntax validation, autocompletion, a variety of different SPARQL result visualizations, with a plugin architecture that enables customization . By default, the query editor provides autocomplete suggestions via the LOV API. Website maintainers can add their own autocompletion logic as well. For example, the Yasgui integration in TriplyDB uses the TriplyDB API to more accurately provide suggestions based on the underlying data. Sharing queries now involves less than having to copy/past complete SPARQL queries. Instead, you can share your query (and the corresponding visualization settings) using a simple URL.","title":"SPARQL Editor"},{"location":"yasgui/#supported-key-combinations","text":"The following table enumerates the key combinations that are supported by the SPARQL Editor. Key combination Behavior Alt + Left Move the cursor to the beginning of the current line. Alt + Right Move the cursor to the end of the current line. Alt + U Redo the last change within the current selection. Ctrl + Backspace Delete to the beginning of the group before the cursor. Ctrl + Delete Delete to the beginning of the group after the cursor. Ctrl + End Move the cursor to the end of the query. Ctrl + Home Move the cursor to the start of the query. Ctrl + Left Move the cursor to the left of the group before the cursor. Ctrl + Right Move the cursor to the right of the group the cursor. Ctrl + [ Decrements the indentation for the current line or the lines involved in the current selection. Ctrl + ] Increments the indentation for the current line or the lines involved in the current selection. Ctrl + / Toggles on/off the commenting of the current line or the lines involved in the current selection. Ctrl + A Select the whole query. Ctrl + D Deletes the current line or all lines involved in the current selection. Ctrl + U Undo the last change within the current selection. Ctrl + Y Redo the last undone edit action. Ctrl + Z Undo the last edit action. Ctrl + Shift + F Auto-formats the whole query or the lines involved in the current selection. Shift + Tab Auto-indents the current line or the lines involved in the current selection. Tab Indents the current line or the lines involved in the current selection.","title":"Supported key combinations"},{"location":"yasgui/#templates","text":"SPARQL has standardized capabilities for constructing complex strings and literals. This allows human-readable label and HTML widgets to be generated from within SPARQL. Unfortunately, the syntax for constructing such labels and widgets is a bit cumbersome.","title":"Templates"},{"location":"yasgui/#sparql-concat","text":"For example, the following SPARQL query returns HTML widgets that can be displayed in a web browser (see SPARQL Gallery ). It uses the concat function which allows an arbitrary number of string arguments to be concatenated into one string. Notice that this requires extensive quoting for each argument (e.g., '<h3>' ), as well as conversions from literals to strings (e.g., str(?typeName) ). Finally, in order to return an HTML literal we need to first bind the concatenated string to some variable ?lex , and then apply the strdt function in order to construct a literal with datatype IRI rdf:HTML . You can try this query online . prefix def: <https://triplydb.com/academy/pokemon/def/> prefix foaf: <http://xmlns.com/foaf/0.1/> prefix rdf: <http://www.w3.org/1999/02/22-rdf-syntax-ns#> prefix rdfs: <http://www.w3.org/2000/01/rdf-schema#> select * { ?pokemon def:cry ?cry; def:type/rdfs:label ?typeName; foaf:depiction ?image; rdfs:label ?name. bind(concat('<h3>',str(?typeName),' \u300b ',str(?name),'</h3>', '<img src=\"',str(?image),'\">', '<audio controls src=\"',str(?cry),'\"></audio>') as ?lex) bind(strdt(?lex,rdf:HTML) as ?widget) } limit 25","title":"SPARQL-concat"},{"location":"yasgui/#handlebars","text":"The SPARQL Editor in TriplyDB supports SPARQL Templates, which makes it easier to write human-readable labels and HTML widgets. SPARQL Templates are strings in which occurrences of {{x}} will be replaced with the to-string converted results of bindings to SPARQL variable ?x . The following example query produces the same result set as the above one, but allows the entire HTML string to be written at once as a SPARQL Template. Notice that this removes the need for concatenating ( concat/n ), explicit to-string conversion ( str/1 ), and also allows the HTML literal to be constructed more easily (no strdt/2 needed). You can try this query online . prefix def: <https://triplydb.com/academy/pokemon/def/> prefix foaf: <http://xmlns.com/foaf/0.1/> prefix rdf: <http://www.w3.org/1999/02/22-rdf-syntax-ns#> prefix rdfs: <http://www.w3.org/2000/01/rdf-schema#> select * { ?pokemon def:cry ?cry; def:type/rdfs:label ?typeName; foaf:depiction ?image; rdfs:label ?name. bind(''' <h3>{{typeName}} \u300b {{name}}</h3> <img src=\"{{image}}\"> <audio controls src=\"{{cry}}\"></audio>'''^^rdf:HTML as ?widget) } limit 25 SPARQL Templates can be combined with the SPARQL Gallery feature in order to generate galleries of HTML widgets.","title":"Handlebars"},{"location":"yasgui/#rendering-html","text":"To distinguish between text and HTML result values the visualization library checks for the rdf:HTML datatype. The following query will return as plain text select * { bind('<p>Test</p>' as ?widget) } This query will render the result as HTML PREFIX rdf: <http://www.w3.org/1999/02/22-rdf-syntax-ns#> select * { bind('<p>Test</p>'^^rdf:HTML as ?widget) } In order to guarantee safety, TriplyDB sanitizes HTML literals before rendering them. This means that tags like <embed> , <iframe> and <script> are sanitized away, as are attributes such as onerror and onload .","title":"Rendering HTML"},{"location":"yasgui/#visualizations","text":"","title":"Visualizations"},{"location":"yasgui/#table","text":"This view allows SPARQL results to be displayed in a table. Each column in the table corresponds to a variable that belongs to the outer projection. Each row in the table corresponds to one query result. Each cell contains an RDF term or NULL .","title":"Table"},{"location":"yasgui/#features","text":"In addition to displaying the SPARQL result set, the SPARQL Table has the following features: Abbreviations The SPARQL Table uses the prefix declarations in the SPARQL query in order to abbreviate IRIs that appear in table cells. Filter By entering a search string into the \u201cFilter query results\u201d field, the table will only display result rows in which the entered search string appears. Indices The first column in the table indicates the index of each row in the SPARQL result set. Pagination By default the Table displays at most 50 rows. This maximum value can be changed to 10, 100, 1.000, or \u201cAll\u201d. Sorting A sort widget appears to the right of each header label. By pressing on the upward pointing arrow of the sort widget, rows will be sorted based on the lexicographic order of the values within the corresponding column. By pressing the downward pointing arrow of the sort widget, rows will be inversely sorted according to the same lexicographic order.","title":"Features"},{"location":"yasgui/#table-example","text":"The following SPARQL query (or see here ) returns a table of Pok\u00e9mon dragons (column pokemon ) and their happiness (column happiness ). Notice that the prefix for pokemon is not used in the query, but is used in order to abbreviate the IRI syntax in the pokemon column. By clicking on the sort widget next to the happiness header, the results can be (inversely) sorted based on the happiness values. PREFIX pokemon: <https://triplydb.com/academy/pokemon/id/pokemon/> PREFIX type: <https://triplydb.com/academy/pokemon/id/type/>a PREFIX vocab: <https://triplydb.com/academy/pokemon/vocab/> select ?pokemon ?happiness { ?pokemon vocab:type type:dragon; vocab:happiness ?happiness. }","title":"Table Example"},{"location":"yasgui/#response","text":"This view shows the body of the response and offers an easy way to download the result as a file.","title":"Response"},{"location":"yasgui/#gallery-triplydb-plugin","text":"This view allows SPARQL results to be displayed in an HTML gallery. Each individual result corresponds to one HTML widget. Widgets are displayed in rows and columns to make up a widget gallery.","title":"Gallery (TriplyDB Plugin)"},{"location":"yasgui/#variables","text":"The gallery will render an item based on variables in the following table: Variable name Purpose ?widget The text or HTML content. meant for creating widget from scrap ?widgetLabel Title of the widget. Also used as the alternative text for the image ?widgetLabelLink A url which converts the title into a link, depends on ?widgetLabel ?widgetImage A url of an image to display ?widgetImageLink A url which adds a link to the image, depends on ?widgetImage ?widgetImageCaption A text or HTML description of the image, depends on ?widgetImage ?widgetDescription A text or HTML description, meant for adding links and","title":"Variables"},{"location":"yasgui/#format","text":"The widget will display the variables in the following order: - ?widgetLabel and ?widgetLabelLink - ?widgetImage and ?widgetImageLink - ?widgetImageCaption - ?widgetDescription - ?widget","title":"Format"},{"location":"yasgui/#styling","text":"The ?widget display is restricted in height. This might not always be desired. In such cases the following style tweaks can help to make them the right size: bind('''<div style=\"max-height:unset; width:275px;\"> # The HTML that composes the widget goes here. </div>'''^^rdf:HTML as ?widget)","title":"Styling"},{"location":"yasgui/#gallery-example","text":"The following SPARQL query binds an HTML string consisting of a header ( h3 ), an image ( img ), and an audio element ( audio ) to the ?widget variable. This results in a gallery with 25 widgets, each displaying a Pok\u00e9mon. (This SPARQL query also uses [[SPARQL Templates]] in order to simplify its syntax.) This query can be run online . prefix def: <https://triplydb.com/academy/pokemon/def/> prefix foaf: <http://xmlns.com/foaf/0.1/> prefix rdf: <http://www.w3.org/1999/02/22-rdf-syntax-ns#> prefix rdfs: <http://www.w3.org/2000/01/rdf-schema#> select * { ?pokemon a def:Pokemon; def:baseAttack ?attack; def:baseDefense ?defense; def:baseExp ?experience; def:baseHP ?health; def:baseSpeed ?speed; def:cry ?cry; def:femaleRatio ?female; def:happiness ?happiness; def:maleRatio ?male; def:name ?name; foaf:depiction ?image; rdfs:label ?label. filter(langmatches(lang(?name),'ja')) bind(''' <h2>{{name}} ({{label}})</h2> <img src=\"{{image}}\"> <audio controls src=\"{{cry}}\"></audio> <ul> <li>Experience: {{experience}}</li> <li>Attack: {{attack}}</li> <li>Defense: {{defense}}</li> <li>Experience: {{experience}}</li> <li>Health: {{health}}</li> <li>Female ratio: {{female}}</li> <li>Happiness: {{happiness}}</li> <li>Male ratio: {{male}}</li> <li>Speed: {{speed}}</li> </ul>'''^^rdf:HTML as ?widget) } order by desc(?experience) limit 20","title":"Gallery Example"},{"location":"yasgui/#chart-triplydb-plugin","text":"The chart plugin renders geographical, temporal and numerical data in interactive charts such as bar-, line- and pie charts. The chart plugin also includes a treemap representation, that is suitable for displaying hierarchies. To use the treemap plugin, you must use the following projection variables in your SPARQL query (in this order): ?node ?parent ?size ?color The label of a tree node. Either the label of the node that is the parent of ?node , or the value UNDEF in case ?node is the root node. (optional) :: For leaf nodes, a positive integer indicating the relative size of ?node . (optional) :: For leaf nodes, a double indicating the relative color of ?node . Once the TreeMap is drawn it is possible to navigate the tree with the mouse: left clicking on a node will drill down into the corresponding subtree; right clicking on a node will move up to the subtree of its parent node. The chart configuration enables tweaking the treemap properties such as the number of displayed hierarchy levels.","title":"Chart (TriplyDB Plugin)"},{"location":"yasgui/#geo-triplydb-plugin","text":"This view allows SPARQL results that contain GeoSPARQL semantics to be automatically interpreted and displayed on a 2D map.","title":"Geo (TriplyDB Plugin)"},{"location":"yasgui/#variables_1","text":"This view recognizes the following SPARQL variable names: Variable name Purpose ?x An arbitrary variable name that is bound to literals with datatype IRI geo:wktLiteral , and whose name is the prefix of the other variable names in this table. ?xColor The color of the shape bound to ?x . ?xLabel The text or HTML content of popups that appear when clicking the shape bound to ?x . ?xTooltip Text or HTML that will appear when the shape of bound to ?x is hovered ?mapEndpoint A URL pointing to a WMS tile-server","title":"Variables"},{"location":"yasgui/#color-values","text":"Variable ?xColor must include a value of the following types: CSS color names . RGB color codes . HSL color codes . Gradients : Strings of the form {{PALETTE}},{{VALUE}} , where {{VALUE}} is a floating-point number between 0.0 and 1.0 and {{PALETTE}} is the name of a color palette. We support color schemes from the Colormap and Color Brewer libraries","title":"Color values"},{"location":"yasgui/#wms-tile-servers","text":"To include layers from a WMS tile-server, use the mapEndpoint variable to refer to a server. The plugin will then retrieve the layer information from the server. Usage of the layers can be toggled using the layer selector. Try this one: https://maps.heigit.org/histosm/wms","title":"WMS tile-servers"},{"location":"yasgui/#geo-3d-triplydb-only","text":"This view allows SPARQL results that contain GeoSPARQL semantics to be automatically interpreted and displayed on a 3D globe. It supports both 3D and 2.5D visualizations, depending on whether the GeoSPARQL data is stored in native 3D or in 2D","title":"Geo-3D (TriplyDB-only)"},{"location":"yasgui/#variables_2","text":"This view recognizes the following SPARQL variable names: Variable name Purpose ?x An arbitrary variable name that is bound to 2D or 3D literals with datatype IRI geo:wktLiteral , and whose name is the prefix of the other variable names in this table. ?xColor The color of the shape bound to ?x . ?xHeight The height in meters of the 2.5D shape that is based on the 2D shape that is bound to ?x . This variable is not needed if data is stored in native 3D. ?xLabel The text or HTML content of the popups that appears when the shape that is bound to ?x is clicked. ?xZ The height in meters at which the 2.5D shape that is based on the 2D shape that is bound to ?x starts. This variable is not needed if data is stored in native 3D.","title":"Variables"},{"location":"yasgui/#geo-events-triplydb-plugin","text":"The SPARQL Geo Events plugin renders geographical events as a story map ( example ). This view recognizes the following SPARQL variable names: Variable name Purpose ?eventLocation (required) A geo:wktLiteral . ?eventLabel Text or HTML event label. ?eventDescription Text or HTML event description. ?eventMedia A URL pointing to a media source. Supported media types are described here . ?eventMediaCaption Text or HTML media caption. ?eventMediaCredit Text or HTML media credit.","title":"Geo Events (TriplyDB Plugin)"},{"location":"yasgui/#pivot-table-triplydb-plugin","text":"This view renders SPARQL results in an interactive pivot table where you are able to aggregate the results by dragging your binding variables to columns or rows.","title":"Pivot Table (TriplyDB Plugin)"},{"location":"yasgui/#timeline-triplydb-plugin","text":"The SPARQL timeline renders the SPARQL results on a Timeline ( example ) To get started with this visualization you need at least a result containing a ?eventStart or ?eventDate with either a ?eventDescription , ?eventLabel or a ?eventMedia . (Combinations are also possible) The following parameters can be used, Parameters in Italic are experimental: Variable name Purpose ?eventStart A date when an event started ?eventEnd A date when an event Stopped ?eventDate A date when an event happened ?eventDescription Text/ HTML about the event ?eventLabel Text/ HTML title ?eventMedia Link to most forms of media see documentation for which type of links are supported ?eventType Groups events ?eventColor Colors event ?eventBackground Background of the event when selected ?eventMediaCaption Text/ HTML caption of the Media ?eventMediaCredit Text/ HTML credit of the Media ?eventMediaThumbnail The thumbnail of Media ?eventMediaAlt The Alt text of the Media ?eventMediaTitle The Title of the Media ?eventMediaLink The URL the image should link to","title":"Timeline (TriplyDB Plugin)"},{"location":"yasgui/#network-triplydb-plugin","text":"This view renders SPARQL Construct results in a graph representation. It works for Turtle , Trig , N-Triples and N-Quads responses. The maximum amount of results that can be visualized is 1.000 due to performance.","title":"Network (TriplyDB Plugin)"},{"location":"yasgui/#markup-triplydb-plugin","text":"The markup view can be used to render a variety of markup languages. This requires the use of the ?markup variable to identify which variable to render. Based on the datatype of the variable the plugin will identify which markup language to use: Markup language Datatype HTML http://www.w3.org/1999/02/22-rdf-syntax-ns#HTML Mermaid https://triplydb.com/Triply/vocab/def/mermaid * Plain text Other * This is currently a placeholder IRI, If you find a (dereferenceable) IRI for one of these datatypes please contact us .","title":"Markup (TriplyDB Plugin)"},{"location":"yasgui-api/","text":"On this page: Yasgui API Reference About additional plugins Installation Via package managers npm yarn Via cdn API Reference Yasgui API Tab API Events Configuration Yasqe Yasqe API Events Configuration Yasr Yasr API Events Configuration Yasr plugins Table Raw Response Writing a Yasr plugin FAQ Using Yasgui in react Yasgui API Reference \u00b6 Yasgui consists of three components: Yasqe (a SPARQL Query Editor), Yasr (a SPARQL result visualizer), and Yasgui which binds the former together. Here you can find documentation on ways to include, configure and extend these components as suitable to your use-case. . About additional plugins \u00b6 Yasgui, Yasqe and Yasr are all open source and MIT licensed. Triply provides additional plugins that are only free to use via https://yasgui.triply.cc or TriplyDB . These additional plugins are not MIT licensed and cannot be used or included programmatically. Installation \u00b6 Via package managers \u00b6 To include Yasgui in a project include the package run the commands below. npm \u00b6 npm i @triply/yasgui yarn \u00b6 yarn add @triply/yasgui Via cdn \u00b6 To include Yasgui in your webpage, all that's needed is importing the Yasgui JavaScript and CSS files, and initializing a Yasgui object: <head> <link href=\"https://unpkg.com/@triply/yasgui/build/yasgui.min.css\" rel=\"stylesheet\" type=\"text/css\" /> <script src=\"https://unpkg.com/@triply/yasgui/build/yasgui.min.js\"></script> </head> <body> <div id=\"yasgui\"></div> <script> const yasgui = new Yasgui(document.getElementById(\"yasgui\")); </script> </body> If you only want to use Yasgui for querying a specific endpoint, you can add the following styling to disable the endpoint selector: <style> .yasgui .autocompleteWrapper { display: none !important; } </style> And pass a second argument to the Yasgui initializer to specify the default endpoint: const yasgui = new Yasgui(document.getElementById(\"yasgui\"), { requestConfig: { endpoint: \"http://example.com/sparql\" }, copyEndpointOnNewTab: false, }); Note: If you've already opened the Yasgui page before, you must first clear your local-storage cache before you will see the changes taking effect. API Reference \u00b6 Yasgui API \u00b6 Yasgui features tabs. Each tab has its own isolated query and results. These are persistent as the user switches between tabs. // Add a new Tab. Returns the new Tab object. yasgui.addTab( true, // set as active tab { ...Yasgui.Tab.getDefaults(), name: \"my new tab\" } ); // Get a Tab. Returns the current Tab if no tab id is given. yasgui.getTab(\"tab_id_x\"); Tab API \u00b6 // set the query of the tab tab.setQuery(\"select * where {...}\"); // close the tab tab.close(); // access the Yasqe API for the tab tab.yasqe; // access the Yasr API for the tab tab.yasr; Events \u00b6 Yasgui emits several Events. For information on how to use Events, see NodeJS's Event documentation . // Fires when a query is executed yasgui.on(\"query\", (instance: Yasgui, tab: Tab) => {}); // Fires when a query is finished yasgui.on(\"queryResponse\", (instance: Yasgui, tab: tab) => {}); Configuration \u00b6 This configuration object is accessible/changeable via Yasgui.defaults or yasgui.config . You can pass these along when initializing Yasgui as well. To change settings to the Yasqe and Yasr components used by Yasgui, you are best off changing the Yasgui.Yasqe.defaults and Yasgui.Yasr.defaults objects before initializing Yasgui. { /** * Change the default request configuration, such as the headers * and default yasgui endpoint. * Define these fields as plain values, or as a getter function */ requestConfig: { endpoint: 'https://example.org/sparql', //Example of using a getter function to define the headers field: headers: () => ({ 'key': 'value' }), method: 'POST', }, // Allow resizing of the Yasqe editor resizeable: true, // Whether to autofocus on Yasqe on page load autofocus: true, // Use the default endpoint when a new tab is opened copyEndpointOnNewTab: false, // Configuring which endpoints appear in the endpoint catalogue list endpointCatalogueOptions { getData: () => { return [ //List of objects should contain the endpoint field //Feel free to include any other fields (e.g. a description or icon //that you'd like to use when rendering) { endpoint: \"https://dbpedia.org/sparql\" }, { endpoint: \"https://query.wikidata.org\" } // ... ]; }, //Data object keys that are used for filtering. The 'endpoint' key already used by default keys: [], //Data argument contains a `value` property for the matched data object //Source argument is the suggestion DOM element to append your rendered item to renderItem: (data, source) => { const contentDiv = document.createElement(\"div\"); contentDiv.innerText = data.value.endpoint; source.appendChild(contentDiv); } } } Yasqe \u00b6 Yasqe extends the CodeMirror Library. For an overview of CodeMirror functionality, see the CodeMirror documentation . Note: Where CodeMirror provides CodeMirror in the global namespace, we provide Yasqe . Yasqe API \u00b6 The Yasqe API can be accessed via yasqe (if Yasqe is run standalone) or via a tab yasgui.getTab().yasqe when run in Yasgui // Set query value in editor yasqe.setValue(\"select * where {...}\"); // Get query value from editor yasqe.getValue(); // execute a query yasqe.query({ url: \"https://dbpedia.org/sparql\", reqMethod: \"POST\", // or \"GET\" headers: { Accept: \"...\" /*...*/ }, args: { arg1: \"val1\" /*...*/ }, withCredentials: false, }); // get whether we're in query or update mode yasqe.getQueryMode(); // get the query type (select, ask, construct, ...) yasqe.getQueryType(); // get prefixes map from the query string yasqe.getPrefixesFromQuery(); // Add prefixes to the query. yasqe.addPrefixes({ dbo: \"http://dbpedia.org/ontology/\" }); // Remove prefixes to the query. yasqe.removePrefixes({ dbo: \"http://dbpedia.org/ontology/\" }); // set size of input area yasqe.setSize(500, 300); // Collapsing prefixes if there are any. Use false to expand them. yasqe.collapsePrefixes(true); Events \u00b6 Yasqe emits several Events. For information on how to use Events, see NodeJS's Event documentation . // Fires when a query is executed yasqe.on(\"query\", (instance: Yasqe, req: superagent.SuperAgentRequest) => {}); // Fires when a query is finished yasqe.on(\"queryResponse\", (instance: Yasqe, req: superagent.SuperAgentRequest, duration: number) => {}); Configuration \u00b6 The configuration options, for Yasqe can be accessed through Yasgui.Yasqe or yasqe.options . Here are some configurable fields for Yasqe. They can be accessed and set through Yasqe.defaults and yasqe.options . The configuration object extends the CodeMirror config , meaning fields like for example tabSize may also be set. // number of seconds to persist query input, stored in the browser // set to 0 to always load the default query on page load persistencyExpire // default: 30 days // default settings for how to query the endpoint requestOpts: { endpoint: \"http://dbpedia.org/sparql\", method: \"POST\", headers: {} }, Yasr \u00b6 Yasr is an extendable library that renders SPARQL results. Yasr is responsible for gluing the different visualization plugins together, and providing utilities such as SPARQL result parsers. Yasr API \u00b6 // Set and draw a SPARQL response. The parameter is either // - a plain response string // - a SuperAgent response // - or an object with the specified keys yasr.setResponse({ data: \"...\"; contentType: \"application/sparql-results+json\"; status: 200; executionTime: 1000; // ms // error to show }) // Draw results with current plugin yasr.draw() // Check whether a result has been drawn yasr.somethingDrawn() // Select a plugin yasr.selectPlugin(\"table\") // Download a result set (if possible) yasr.download() Events \u00b6 // Fires just before a plugins draws the results yasr.on(\"draw\",(instance: Yasr, plugin: Plugin) => void); // Fires when a plugin finished drawing the results yasr.on(\"drawn\",(instance: Yasr, plugin: Plugin) => void); Configuration \u00b6 This configuration object is accessible/changeable via Yasr.defaults and yasr.options , and you can pass these along when initializing Yasr as well. Output visualizations are defined separately. // Ordered list of enabled output plugins pluginOrder = [\"table\", \"response\"] // The default plugin defaultPlugin = \"table\" // seconds before results expire in the browser // Set to 0 to disable results persistency persistencyExpire // default: 30 days // Map of prefixes to use in results views prefixes: {\"dbo\":\"http://dbpedia.org/ontology/\",/*...*/} Yasr plugins \u00b6 Each plugin has its own configuration options. These options can be accessed through Yasr.plugins . Table \u00b6 This plugin shows SPARQL results as a table, using the DataTables.net plugin. This plugin is defined in Yasr.plugins.table and can be configured using Yasr.plugins.table.defaults . // Open URIs in results in a new window rather than the current. openIriInNewWindow = true; Raw Response \u00b6 A plugin which uses CodeMirror to present the SPARQL results as-is. This plugin is defined at Yasr.plugins.response and can be configured using Yasr.plugins.response.defaults . // Number of lines to show before hiding rest of response // (too large value may cause browser performance issues) maxLines = 30; Writing a Yasr plugin \u00b6 To register a Yasr plugin, add it to Yasr by running Yasr.registerPlugin(pluginName: string, plugin: Plugin) . Below is an example implementation for rendering the result of an Ask query, which returns either true or false . See also the implementations of the Table and Raw Response plugins. class Boolean { // A priority value. If multiple plugin support rendering of a result, this value is used // to select the correct plugin priority = 10; // Whether to show a select-button for this plugin hideFromSelection = true; constructor(yasr) { this.yasr = yasr; } // Draw the result set. This plugin simply draws the string 'True' or 'False' draw() { const el = document.createElement(\"div\"); el.textContent = this.yasr.results.getBoolean() ? \"True\" : \"False\"; this.yasr.resultsEl.appendChild(el); } // A required function, used to indicate whether this plugin can draw the current // resultset from yasr canHandleResults() { return ( this.yasr.results.getBoolean && (this.yasr.results.getBoolean() === true || this.yasr.results.getBoolean() == false) ); } // A required function, used to identify the plugin, works best with an svg getIcon() { const textIcon = document.createElement(\"p\"); textIcon.innerText = \"\u2713/\u2717\"; return textIcon; } } //Register the plugin to Yasr Yasr.registerPlugin(\"MyBooleanPlugin\", Boolean); FAQ \u00b6 Using Yasgui in react \u00b6 To include Yasgui in React, use the following snippet. This snippet assumes a React repository configured via create-react-app , and a minimum React version of 16.8. import Yasgui from \"@triply/yasgui\"; import \"@triply/yasgui/build/yasgui.min.css\"; export default function App() { useEffect(() => { const yasgui = new Yasgui(document.getElementById(\"yasgui\")); return () => {}; }, []); return <div id=\"yasgui\" />; }","title":"For Developers"},{"location":"yasgui-api/#yasgui-api-reference","text":"Yasgui consists of three components: Yasqe (a SPARQL Query Editor), Yasr (a SPARQL result visualizer), and Yasgui which binds the former together. Here you can find documentation on ways to include, configure and extend these components as suitable to your use-case. .","title":"Yasgui API Reference"},{"location":"yasgui-api/#about-additional-plugins","text":"Yasgui, Yasqe and Yasr are all open source and MIT licensed. Triply provides additional plugins that are only free to use via https://yasgui.triply.cc or TriplyDB . These additional plugins are not MIT licensed and cannot be used or included programmatically.","title":"About additional plugins"},{"location":"yasgui-api/#installation","text":"","title":"Installation"},{"location":"yasgui-api/#via-package-managers","text":"To include Yasgui in a project include the package run the commands below.","title":"Via package managers"},{"location":"yasgui-api/#npm","text":"npm i @triply/yasgui","title":"npm"},{"location":"yasgui-api/#yarn","text":"yarn add @triply/yasgui","title":"yarn"},{"location":"yasgui-api/#via-cdn","text":"To include Yasgui in your webpage, all that's needed is importing the Yasgui JavaScript and CSS files, and initializing a Yasgui object: <head> <link href=\"https://unpkg.com/@triply/yasgui/build/yasgui.min.css\" rel=\"stylesheet\" type=\"text/css\" /> <script src=\"https://unpkg.com/@triply/yasgui/build/yasgui.min.js\"></script> </head> <body> <div id=\"yasgui\"></div> <script> const yasgui = new Yasgui(document.getElementById(\"yasgui\")); </script> </body> If you only want to use Yasgui for querying a specific endpoint, you can add the following styling to disable the endpoint selector: <style> .yasgui .autocompleteWrapper { display: none !important; } </style> And pass a second argument to the Yasgui initializer to specify the default endpoint: const yasgui = new Yasgui(document.getElementById(\"yasgui\"), { requestConfig: { endpoint: \"http://example.com/sparql\" }, copyEndpointOnNewTab: false, }); Note: If you've already opened the Yasgui page before, you must first clear your local-storage cache before you will see the changes taking effect.","title":"Via cdn"},{"location":"yasgui-api/#api-reference","text":"","title":"API Reference"},{"location":"yasgui-api/#yasgui-api","text":"Yasgui features tabs. Each tab has its own isolated query and results. These are persistent as the user switches between tabs. // Add a new Tab. Returns the new Tab object. yasgui.addTab( true, // set as active tab { ...Yasgui.Tab.getDefaults(), name: \"my new tab\" } ); // Get a Tab. Returns the current Tab if no tab id is given. yasgui.getTab(\"tab_id_x\");","title":"Yasgui API"},{"location":"yasgui-api/#tab-api","text":"// set the query of the tab tab.setQuery(\"select * where {...}\"); // close the tab tab.close(); // access the Yasqe API for the tab tab.yasqe; // access the Yasr API for the tab tab.yasr;","title":"Tab API"},{"location":"yasgui-api/#events","text":"Yasgui emits several Events. For information on how to use Events, see NodeJS's Event documentation . // Fires when a query is executed yasgui.on(\"query\", (instance: Yasgui, tab: Tab) => {}); // Fires when a query is finished yasgui.on(\"queryResponse\", (instance: Yasgui, tab: tab) => {});","title":"Events"},{"location":"yasgui-api/#configuration","text":"This configuration object is accessible/changeable via Yasgui.defaults or yasgui.config . You can pass these along when initializing Yasgui as well. To change settings to the Yasqe and Yasr components used by Yasgui, you are best off changing the Yasgui.Yasqe.defaults and Yasgui.Yasr.defaults objects before initializing Yasgui. { /** * Change the default request configuration, such as the headers * and default yasgui endpoint. * Define these fields as plain values, or as a getter function */ requestConfig: { endpoint: 'https://example.org/sparql', //Example of using a getter function to define the headers field: headers: () => ({ 'key': 'value' }), method: 'POST', }, // Allow resizing of the Yasqe editor resizeable: true, // Whether to autofocus on Yasqe on page load autofocus: true, // Use the default endpoint when a new tab is opened copyEndpointOnNewTab: false, // Configuring which endpoints appear in the endpoint catalogue list endpointCatalogueOptions { getData: () => { return [ //List of objects should contain the endpoint field //Feel free to include any other fields (e.g. a description or icon //that you'd like to use when rendering) { endpoint: \"https://dbpedia.org/sparql\" }, { endpoint: \"https://query.wikidata.org\" } // ... ]; }, //Data object keys that are used for filtering. The 'endpoint' key already used by default keys: [], //Data argument contains a `value` property for the matched data object //Source argument is the suggestion DOM element to append your rendered item to renderItem: (data, source) => { const contentDiv = document.createElement(\"div\"); contentDiv.innerText = data.value.endpoint; source.appendChild(contentDiv); } } }","title":"Configuration"},{"location":"yasgui-api/#yasqe","text":"Yasqe extends the CodeMirror Library. For an overview of CodeMirror functionality, see the CodeMirror documentation . Note: Where CodeMirror provides CodeMirror in the global namespace, we provide Yasqe .","title":"Yasqe"},{"location":"yasgui-api/#yasqe-api","text":"The Yasqe API can be accessed via yasqe (if Yasqe is run standalone) or via a tab yasgui.getTab().yasqe when run in Yasgui // Set query value in editor yasqe.setValue(\"select * where {...}\"); // Get query value from editor yasqe.getValue(); // execute a query yasqe.query({ url: \"https://dbpedia.org/sparql\", reqMethod: \"POST\", // or \"GET\" headers: { Accept: \"...\" /*...*/ }, args: { arg1: \"val1\" /*...*/ }, withCredentials: false, }); // get whether we're in query or update mode yasqe.getQueryMode(); // get the query type (select, ask, construct, ...) yasqe.getQueryType(); // get prefixes map from the query string yasqe.getPrefixesFromQuery(); // Add prefixes to the query. yasqe.addPrefixes({ dbo: \"http://dbpedia.org/ontology/\" }); // Remove prefixes to the query. yasqe.removePrefixes({ dbo: \"http://dbpedia.org/ontology/\" }); // set size of input area yasqe.setSize(500, 300); // Collapsing prefixes if there are any. Use false to expand them. yasqe.collapsePrefixes(true);","title":"Yasqe API"},{"location":"yasgui-api/#events_1","text":"Yasqe emits several Events. For information on how to use Events, see NodeJS's Event documentation . // Fires when a query is executed yasqe.on(\"query\", (instance: Yasqe, req: superagent.SuperAgentRequest) => {}); // Fires when a query is finished yasqe.on(\"queryResponse\", (instance: Yasqe, req: superagent.SuperAgentRequest, duration: number) => {});","title":"Events"},{"location":"yasgui-api/#configuration_1","text":"The configuration options, for Yasqe can be accessed through Yasgui.Yasqe or yasqe.options . Here are some configurable fields for Yasqe. They can be accessed and set through Yasqe.defaults and yasqe.options . The configuration object extends the CodeMirror config , meaning fields like for example tabSize may also be set. // number of seconds to persist query input, stored in the browser // set to 0 to always load the default query on page load persistencyExpire // default: 30 days // default settings for how to query the endpoint requestOpts: { endpoint: \"http://dbpedia.org/sparql\", method: \"POST\", headers: {} },","title":"Configuration"},{"location":"yasgui-api/#yasr","text":"Yasr is an extendable library that renders SPARQL results. Yasr is responsible for gluing the different visualization plugins together, and providing utilities such as SPARQL result parsers.","title":"Yasr"},{"location":"yasgui-api/#yasr-api","text":"// Set and draw a SPARQL response. The parameter is either // - a plain response string // - a SuperAgent response // - or an object with the specified keys yasr.setResponse({ data: \"...\"; contentType: \"application/sparql-results+json\"; status: 200; executionTime: 1000; // ms // error to show }) // Draw results with current plugin yasr.draw() // Check whether a result has been drawn yasr.somethingDrawn() // Select a plugin yasr.selectPlugin(\"table\") // Download a result set (if possible) yasr.download()","title":"Yasr API"},{"location":"yasgui-api/#events_2","text":"// Fires just before a plugins draws the results yasr.on(\"draw\",(instance: Yasr, plugin: Plugin) => void); // Fires when a plugin finished drawing the results yasr.on(\"drawn\",(instance: Yasr, plugin: Plugin) => void);","title":"Events"},{"location":"yasgui-api/#configuration_2","text":"This configuration object is accessible/changeable via Yasr.defaults and yasr.options , and you can pass these along when initializing Yasr as well. Output visualizations are defined separately. // Ordered list of enabled output plugins pluginOrder = [\"table\", \"response\"] // The default plugin defaultPlugin = \"table\" // seconds before results expire in the browser // Set to 0 to disable results persistency persistencyExpire // default: 30 days // Map of prefixes to use in results views prefixes: {\"dbo\":\"http://dbpedia.org/ontology/\",/*...*/}","title":"Configuration"},{"location":"yasgui-api/#yasr-plugins","text":"Each plugin has its own configuration options. These options can be accessed through Yasr.plugins .","title":"Yasr plugins"},{"location":"yasgui-api/#table","text":"This plugin shows SPARQL results as a table, using the DataTables.net plugin. This plugin is defined in Yasr.plugins.table and can be configured using Yasr.plugins.table.defaults . // Open URIs in results in a new window rather than the current. openIriInNewWindow = true;","title":"Table"},{"location":"yasgui-api/#raw-response","text":"A plugin which uses CodeMirror to present the SPARQL results as-is. This plugin is defined at Yasr.plugins.response and can be configured using Yasr.plugins.response.defaults . // Number of lines to show before hiding rest of response // (too large value may cause browser performance issues) maxLines = 30;","title":"Raw Response"},{"location":"yasgui-api/#writing-a-yasr-plugin","text":"To register a Yasr plugin, add it to Yasr by running Yasr.registerPlugin(pluginName: string, plugin: Plugin) . Below is an example implementation for rendering the result of an Ask query, which returns either true or false . See also the implementations of the Table and Raw Response plugins. class Boolean { // A priority value. If multiple plugin support rendering of a result, this value is used // to select the correct plugin priority = 10; // Whether to show a select-button for this plugin hideFromSelection = true; constructor(yasr) { this.yasr = yasr; } // Draw the result set. This plugin simply draws the string 'True' or 'False' draw() { const el = document.createElement(\"div\"); el.textContent = this.yasr.results.getBoolean() ? \"True\" : \"False\"; this.yasr.resultsEl.appendChild(el); } // A required function, used to indicate whether this plugin can draw the current // resultset from yasr canHandleResults() { return ( this.yasr.results.getBoolean && (this.yasr.results.getBoolean() === true || this.yasr.results.getBoolean() == false) ); } // A required function, used to identify the plugin, works best with an svg getIcon() { const textIcon = document.createElement(\"p\"); textIcon.innerText = \"\u2713/\u2717\"; return textIcon; } } //Register the plugin to Yasr Yasr.registerPlugin(\"MyBooleanPlugin\", Boolean);","title":"Writing a Yasr plugin"},{"location":"yasgui-api/#faq","text":"","title":"FAQ"},{"location":"yasgui-api/#using-yasgui-in-react","text":"To include Yasgui in React, use the following snippet. This snippet assumes a React repository configured via create-react-app , and a minimum React version of 16.8. import Yasgui from \"@triply/yasgui\"; import \"@triply/yasgui/build/yasgui.min.css\"; export default function App() { useEffect(() => { const yasgui = new Yasgui(document.getElementById(\"yasgui\")); return () => {}; }, []); return <div id=\"yasgui\" />; }","title":"Using Yasgui in react"}]}
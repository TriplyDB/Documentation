---
title: "Triply ETL: Overview"
path: "/docs/triply-etl/overview"
---

TriplyETL is a software solution and online service that allows you to build ETL pipelines that create linked data knowledge graphs.

## Tutorials

The TriplyETL documentation contains the following tutorials that get you up-to-speed with the core TriplyETL features:

- [**Getting Started**](/docs/triply-etl/getting-started)

## Overview

The TriplyETL documentation contains the following in-depth pages that cover all TriplyETL features and settings:

- [**Source Connectors**](/docs/triply-etl/source-connectors) allows your TriplyETL pipeline to connect to a large number of source systems: relational databases, APIs, spreadsheets, etc.
- [**Declarations**](/docs/triply-etl/declarations) allow you to declare and later reuse commonly uses IRI prefixes, graph names, etc.
- [**Assertions**](/docs/triply-etl/assertions) allow you to add linked data to your knowledge graph.
- [**Transformations**](/docs/triply-etl/transformations) allow you to modify, combine, and otherwise manipulate your source data.
- [**Publication Connectors**](/docs/triply-etl/publication) allow you to store the output of your TriplyETL pipeline in an online triple store or in a local file.
- [**Validation**](/docs/triply-etl/validation) ensures that data generated by your TriplyETL pipeline conforms to your data model.
- [**DTAP**](/docs/triply-etl/dtap) allows your TriplyETL pipelines to run in the four DTAP environments that are commonly used in production systems: Development, Testing, Acceptance, and Production.

## Why TriplyETL?

TriplyETL has the following core features, that set it apart from other data pipeline products:

- **Backend-agnostic**: TriplyETL supports any data source through it large and growing set of source connectors.
- **Multi-paradigm**: TriplyETL supports all major paradigms for transforming and asserting linked data: SPARQL Update, JSON-LD Algorithms (TBA), SHACL Rules, RML (TBA), and RATT.  You can also write your own transformations in TypeScript.
- **Scalable**: TriplyETL processes data in a stream of self-contained records.  This allows TriplyETL pipelines to run in parallel, ensuring a high pipeline throughput.
- **Standards-compliant**: TriplyETL implements the latest versions of the linked data standards and best practices: RDF 1.1, SHACL, XML Schema Datatypes 1.1, IETF RFC3987 (IRIs), IETF RFC5646 (Language Tags).
- **High Quality**: The output of TriplyETL pipelines is automatically validated against the specified data model.
- **Production-grade**: TriplyETL pipelines can run in the four DTAP environments that are common in production systems (Development, Testing, Acceptance, Production).
